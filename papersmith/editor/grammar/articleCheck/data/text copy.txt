Solving QBF Instances With Nested SAT Solvers

Bart Bogaerts and Tomi Janhunen and Shahab Tasharrofi

Helsinki Institute for Information Technology HIIT Department of Computer Science Aalto University, FI-00076 AALTO, Finland



Abstract

We present a new approach towards solving quantified Boolean formulas (QBFs) using nested SAT solvers with lazy clause generation. The approach has been implemented on top of the Glucose solver by adding mechanisms for nesting solvers as well as clause learning. Our preliminary experiments show that nested SAT solving performs (out of the box) relatively well on QBF, when taking into account that no particular QBF-oriented solving techniques were incorporated. The most important contribution of this work is that it provides a systematic way of lifting advances in SAT solvers to QBFs with low implementation effort.



ing's readability and succinctness, or enhance the strength of propagation via specialized propagators. Recent work by Janhunen, Tasharrofi, and Ternovska (2016) started from the following observation: "if SAT solvers are this efficient, then why not to use a SAT solver as a smart (in the sense that it learns good clauses) oracle for a SAT solver itself?". The idea here is to solve satisfiability problems for theories of the form T =   x : , where x is a sequence of propositional variables and  and  are CNF-theories. Propagation for T combines unit propagation on  with an oracle call to a SAT solver for  . From the result of this oracle call, a learned clause is generated and added to .1 Advantages of this approach are manifold: (1) It is a modular approach that allows plugging in any SAT solver as innermost solver and only requires minor modifications to the outermost solver, hence progress in SAT will automatically translate to solvers of this richer formalism; (2) It can be immediately combined with other SAT extensions (such as integer variables, acyclicity, or any other theory propagator); (3) No dedicated propagators need to be developed for the new extension because the nested solver is (automatically) used as a propagator for its internal theory; for example, it was shown by Janhunen, Tasharrofi, and Ternovska (2016) how using an internal SAT solver to propagate reachability constraints leads to a simple encoding of Hamiltonian paths that performs much better when compared to a direct encoding (i.e., a SAT encoding without second-order structure). A solver, called SAT- TO -SAT, that implements this idea was presented (Janhunen, Tasharrofi, and Ternovska 2016). Since in principle any solver2 can be nested, it is also possible to nest SAT- TO -SAT in itself, as an oracle. By allowing such arbitrarily deep nesting, we essentially obtain a QBF solver. This paper presents how SAT- TO -SAT can be used for QBF solving. We evaluate how this technique performs with respect to state-of-the art QBF solvers and conclude that SAT- TO -SAT is still slower than the best QBF solver around. However, it deserves to be mentioned that our implementation is generic and performs no optimizations

1 2



1



Introduction



Since the addition of conflict-driven clause learning (Marques-Silva and Sakallah 1999), SAT solvers have made huge leaps forward in two respects: their popularity for tackling real-life problems and their efficiency. Now that these highly-performant SAT-solvers exist, research often stretches beyond SAT, either because of trying to tackle problems of a complexity higher than NP or because the input format of SAT solvers (propositional logic) is too limited to concisely and naturally express certain domain specific constraints, such as graph properties. For this reason, researchers have extended the SAT language with new language constructs, sometimes also called constraints, and have extended SAT solvers with dedicated propagators for those constraints that communicate with the underlying SAT solver through clauses. This has happened for example in the field of constraint programming (Apt 2003) in the form of solvers with lazy clause generation (Ohrimenko, Stuckey, and Codish 2009), in SAT modulo theories (Barrett et al. 2009) in the form of DPLL(T) solvers (Ganzinger et al. 2004), and in answer set programming (Marek and Truszczy nski 1999) where all modern solvers use this architecture (Gebser, Kaufmann, and Schaub 2012; 2013; De Cat et al. 2013; Alviano et al. 2015). Several further extensions to SAT use the same approach (Gebser, Janhunen, and Rintanen 2014; Bayless et al. 2015). Such extensions may (1) increase complexity (for applications in which tasks of a higher complexity than NP are required to be tackled), or (2) remain in NP but either improve an encodCopyright c 2015, The authors. All rights reserved.



See Section 3 for a detailed explanation. Any solver that respects the interface given in Definition 3.3.



designed for QBF specifically. Furthermore, our current implementation is built on the popular SAT solver G LUCOSE (Audemard and Simon 2009). In principle, any SAT solver can be plugged in, resulting in a strongly improved performance. The ideas presented here also shed new light on techniques used in QBF. For example, conflict-driven clauselearning and solution-driven cube-learning (Giunchiglia, Narizzano, and Tacchella 2002; Letz 2002; Zhang and Malik 2002; Chu and Stuckey 2014) are completely unified in our framework as clause-learning occurring in even (respectively odd) levels of nesting depth. The main contributions of this paper are as follows: (1) We show how SAT- TO -SAT can be extended to a QBFsolver. This results in a principled, low-cost way to transfer improvements from SAT to QBF; (2) Furthermore, since the nesting idea is completely orthogonal to other language extensions, we can also lift extensions of SAT to QBF, for example resulting in QBF modulo theories (QBF(T)) or solvers for QBF modulo acyclicity (Acyc-QBF).



is called a QBF sentence. We use  :  to abbreviate p1 . . . pn :  if  = {p1 , . . . , pn }. If  is a propositional formula, we use ( ) to denote that the free symbols of  are all in  , i.e., that  is a  -QBF. A prenex QBF is a QBF in which all quantifiers are in the front, i.e., a set of quantifiers followed by a propositional formula. The QDIMACS format is a numerical format to describe a prenex QBF in which the propositional formula is a CNF formula. The format is the de-facto standard for representing QBF instances. Satisfiability Relation for QBFs. The satisfiability relation between  -interpretations I and a  -QBFs , denoted by I |= , is defined recursively in the standard way: * I |= p if I (p) = t. * I |=  if I |= ; * I |=    (resp. I |=    ) if I |=  and (resp. or) I |=  ; * I |= x :  (resp. I |= x : ) if (I  {xt }) |=  and (resp. or) (I  {xf }) |= . Let I be a (partial)  -interpretation. We call a  -QBF  I -satisfiable if there exists a model of  more precise than I and I -unsatisfiable otherwise.



2



Background



Vocabularies and Interpretations. A vocabulary is a set of symbols, also called atoms; we use , ,  to refer to vocabularies. If  is a vocabulary, a (two-valued)  interpretation is a mapping   {t, f } where t denotes true and f false. A partial  -interpretation is a mapping   {t, f , u}, where u denotes unknown. We often identify a partial  -interpretation J with a set of tuples pv with p  , v  {t, f } (with each atom occurring at most once), meaning that J sends all atoms occurring in this set to their corresponding values, and all others to unknown. This allows us to define the "union" of two interpretation. E.g., if  and  are disjoint, I is a (partial)  -interpretation and J a (partial)  -interpretation, we use I  J to interpret symbols in  in the same way as I and symbols in  in the same way as J . If I and J are two  -interpretations, we will use the expression I J only if I J indeed defines a partial interpretation (i.e., contains not both pt and pf ). The truth order <t on truth values is induced by f <t u <t t. The precision order <p on truth values is induced by u <p t, u <p f . This order is extended pointwise to partial interpretations: I <p J if I (q ) <p J (q ) for all q in  . If I is a  -interpretation and    , any (partial)  -interpretation is identified with the partial  -interpretation equal to I on  and mapping all symbols in  \  to u. Formulas. A propositional formula is recursively built from propositional atoms p, q, r, . . . using connectives ,  and . A propositional formula is a  -formula if its atoms are in  . A literal is an atom or its negation. A clause is a disjunction of literals. A CNF is a conjunction of clauses. A sub-formula occurs positively (resp. negatively) if it is within the scope of an even (resp. odd) number of negations. A quantified Boolean formula (QBF) is built using the same recursive rules, but with added quantifiers  and  to quantify over propositional atoms. A  -QBF is a QBF with free symbols belonging to  . A  -QBF with  = {}



3 SAT- TO -SAT In order to discuss the working of SAT- TO -SAT, we first present a formalisation of SAT-solvers for our purposes.

SAT solving. The principal goal of a SAT solver is to find a model for a CNF, i.e., to check the validity of a formula  : , where  is a CNF. Many modern SAT solvers do more than that: they explain their answer in terms of a set of so-called assumptions (Nadel and Ryvchin 2012). In this text, we assume3 that  is the disjoint union of two vocabularies  and  , an assumption vocabulary  and an internal vocabulary  . A solver not only returns a model or UNSAT but also explains this in terms of the assumptions. Definition 3.1 (Explaining Satisfiability). Let  be any  formula and  =    where  and  are disjoint. Let J be a partial  -interpretation and M be a partial  -interpretation. We say that (J, M ) explains the satisfiability of  if each  interpretation more precise than J  M is a model of  Example 3.2. Let  = {o, p, q },  = {r} and 1 = (p  q  r)  (o  r). Furthermore, let J be the partial  -interpretation {pt } and M the  -interpretation {rf }. In this case (J, M ) explains the satisfiability of 1 . Indeed, J guarantees that the first clause of 1 is satisfied, while M guarantees that the second is. Definition 3.3 (SAT-solver). Suppose that  =    . A SAT-solver is a procedure that takes as input a  -CNF T and a two-valued  -interpretation I . * If T is I -satisfiable, it returns (SAT, J, M ) such that J p I and (J, M ) explains the satisfiability of .

3



This assumption is not vital but simplifies the presentation



* Otherwise, it returns (UNSAT, J ) where J p I is such that T is J -unsatisfiable. Hence a SAT solver solves the problem  : T under assumptions I . Note that in formalisations of SAT solvers, often a J that explains the answer of the solver is not present. In this case, J can always be equal to I . Several SAT solvers, such as MiniSAT (E en and S orensson 2003), support smart reasoning methods to generate better (less precise) J . In order to solve problems of form  : T , state-of-the-art SAT solvers use the conflict-driven clause learning (CDCL) algorithm (Silva, Lynce, and Malik 2009). The CDCL algorithm works by maintaining a state that represents a partial  -interpretation. We use S(S ) to denote the state of a solver S . The algorithm uses operations of propagation, decision, backjumping and restart to manipulate its state. Propagation takes a state S(S ) and either returns a (possibly) more precise state that is the consequence of its previous state or returns a conflict clause showing no model can extend the current state. The decision operation takes a non-conflicting state S(S ) and branches the search on a variable v (decision variable) that is currently unassigned in S(S ). Backjumping takes a conflicting state S(S ), learns a clause from it and returns to a less precise non-conflicting state. The restart operation restarts the search while remembering learnt clauses. SAT- TO -SAT. Recently, Janhunen, Tasharrofi, and Ternovska (2016) introduced SAT- TO -SAT, a framework for combining SAT solvers so that, together, they solve QBF problems. Essentially, this framework performs lazy clause generation (Ohrimenko, Stuckey, and Codish 2009) where clauses are obtained from calls to another SAT solver. In this section, we recall how SAT- TO -SAT works in a slightly generalised setting. The input for SAT- TO -SAT is an QBF of the form T =  :   (1 : 1 )  * * *  (n : n ), where  is a  -CNF and the i 's are i -CNFs with i =   i . Without loss of generality, from now on, we assume that n = 1 and use  for 1 ,  for 1 and  for    . SAT- TO -SAT checks validity of T , i.e., it returns SAT iff there exists a  -interpretation I that satisfies  such that  is I -unsatisfiable and returns UNSAT otherwise. To explain how SAT- TO -SAT works, we need some terminology: Definition 3.4 (Lowerbound/Upperbound Mapping). The LU-mapping of vocabulary  is lu = {pu | p   }  {pl | p   } with pu (resp. pl ) representing upper- (resp. lower) bound of p. The LU-mapping of a partial interpretation I , denoted as Ilu , is a 2-valued lu -interpretation so that Ilu (pu ) = t if and only if I (p) = f and Ilu (pl ) = t if and only if I (p) = t. Note that for each atom p in the vocabulary  , Ilu satisfies Ilu (pl ) t I (p) t Ilu (pu ), i.e., pl (respectively pu ) is a lower (respectively upper) bound on the truth of p. Definition 3.5 ( -under-approximation). Let  be a    formula. A  -under-approximation of  is any lu   formula  such that for all interpretations I : (1) if I is a two-valued  -interpretation, then  :  is satisfied in Ilu iff  :  is satisfied in I , and



(2) if I is a partial  -interpretation, then Ilu |=  :  implies that every two-valued  -interpretation more precise than I satisfies  :  . The first condition guarantees that in two-valued interpretations the approximation coincides with the original formula. The second states that if Ilu can be expanded to a model of  , then I can be expanded to a model of  . Example 3.6 (Example 3.2 continued). Let 1 = (pl  ql  r)  (ou  r). In this case 1 is a  -under-approximation of 1 . We show this for some partial  -interpretations. * When I is 2-valued, Ilu (xl ) = I (x) = Ilu (xu ) for all x   . Hence Condition (1) in Definition 3.5 is satisfied. * Let I0 be the partial  -interpretation that maps all atoms to u. In this case, 1 is (I0 )lu -unsatisfiable, hence Condition (2) in Definition 3.5 is clearly satisfied as well. * Now, let I1 be the partial  -interpretation {pt }. Since (I1 )lu (pl ) = t, M = {rt } is a model of  : 1 . For each two-valued interpretation I p I1 , it holds that I  M is a model of the original formula 1 . Assuming a  -under-approximation  for  , the SAT- TO -SAT algorithm instantiates two CDCL-solvers S (tasked with solving ) and S (tasked with solving  ). After each unit propagation phase of S , solver S is called with assumptions S(S )lu . * If S returns (SAT, J, M ), it then follows from the fact that  is a  -under-approximation of  that  :  (and hence also T ) is I -unsatisfiable. In this case, J is used to create a clause that falsifies S 's current assignment; this clause is added to . * If S returns (UNSAT, J ), nothing can be concluded. Literals in J are used as watched literals to avoid calling S again as long as S(S )lu is more precise than J . The use of the under-approximation has the effect that if S(S ) is not exact, the call to the nested solver S remains sound in the sense that whenever S finds a model, a conflict clause can be added to . In case S is unsatisfiable (with the given assumption), nothing final can be concluded yet. In this case, the explanation of unsatisfiability can be used to avoid calling the nested solver too often. The use of the under-approximation roughly has the same effect as calling a SAT-solver for  , with assumptions S(S ), but obliging the nested solver to keep all variables in  that are unassigned in S(S ) unassigned. This would require us to modify the internals of the solver S to find models in a partial context, which is something we try to avoid. As such, the under-approximation serves two purposes (1) it allows us to call the nested solver after each unit propagation, (2) it ensures we can use the nested SAT solver as a blackbox. Example 3.7 (Example 3.6 continued). Let T be the QBF 1   : 1 , where 1 = p  q. SAT- TO -SAT solves the satisfiability task for T as follows: * SAT- TO -SAT starts from the partial  -interpretation I0 in which all atoms in  are unknown. In this case, as



shown in Example 3.6, 1 is unsatisfiable. A SAT-solver f t for 1 can return (UNSAT, {pf l , ql , ou }). SAT- TO -SAT interprets this result by watching literals p, q , and o. As soon as one of these literals becomes false, the subsolver will be called again. * SAT- TO -SAT chooses I1 = {pf }. None of the watches fire hence the internal solver is not called. * SAT- TO -SAT chooses I2 = {pf , q t }. In this case 1 has a model (M = {rf }) more precise than (I2 )lu . The internal t solver returns (SAT, {ql }, M ). SAT- TO -SAT interprets this result by adding the clause q to 1 . * The solver for 1 now finds a conflict, backjumps and continues search. The only thing that remains to be explained is how SAT- TO -SAT obtains a  -approximation of  . This is done by the following syntactical transformation. Lemma 3.8. Let  be a    -CNF. Let lu be the lu   CNF obtained from  by 1. replacing each literal p in  with p   by pu , and 2. replacing each literal p in  with p   by pl . Then, lu is a  -under-approximation of  . This lemma follows the fact that, for each partial  interpretation I , we have Ilu (pl ) t I (p) t Ilu (pu ). Example 3.9 (Example 3.6 continued). The formula 1 equals (1 )lu as defined in Lemma 3.8.



Example 4.2 (Example 3.7 continued). Let 1 {p, o}, 2 = {q } and T (1 ) = 2 : (1   : 1 ).



=



Furthermore, let J = {pf } and J = {pf , q f , ot }. In this case, J explains satisfiability of 1 and J explains unsatisfiability of 1 . Theorem 4.1 shows that J = {pf , ot } explains satisfiability of T (1 ). The case of unsatisfiability is easier: every time SAT- TO -SAT finds a model for its nested expression, a clause is added to  that invalidates the current partial interpretation. Hence, in the end, the only way for T to become unsatisfiable is that  becomes unsatisfiable. Theorem 4.3. Let T be a QBF of the form T ( ) =  :   ( :  ), where  is a CNF and  is an arbitrary (QBF) formula. Let I be a  -interpretation. Suppose  is J -unsatisfiable and J p I , then T is J -unsatisfiable as well. We now show how Lemma 3.8 extends to general QBFs. Lemma 4.4. Let  be a    -QBF. Let lu be the lu   QBF obtained from  by 1. replacing each negative occurrences of p   by pu , and 2. replacing each positive occurrences of p   by pl . Then, lu is a  -under-approximation of  . Again, this lemma follows from the fact that, for all partial  -interpretations I , we have Ilu (pl ) t I (p) t Ilu (pu ). Lemma 4.4 shows how to obtain under-approximations for QBFs in general. Together with Theorems 4.1 and 4.3, we obtain all ingredients to extend SAT- TO -SAT for general QBFs. The resulting solver is a SAT solver S1 extended with an oracle S2 , also an instantiation of SAT- TO -SAT. The results from calls to S2 are used either as learnt clauses in the theory of S1 or watches that avoid unnecessary calls to S2 . The solver works exactly as described in Section 3 except that the "black box" nested solver is now another instance of SAT- TO -SAT rather than an instance of a SAT solver.



4



Solving QBF With SAT- TO -SAT



The previous section discussed how SAT- TO -SAT solves QBF validity problems. These ideas easily generalise to QBF validity problems: instead of nesting a SAT-solver in another SAT-solver, we can nest SAT- TO -SAT inside a SAT-solver (recursively). In order to do this, two obstacles are to be overcome. First, we must extend SAT- TO -SAT so that it not only outputs SAT or UNSAT, but also explains this result in terms of assumptions given to it, i.e., it should respect the interface we specified in Definition 3.3. Second, it is necessary to define how an approximation of a QBF theory can be obtained, i.e., extend Lemma 3.8 to general QBFs. For the first, we use the following theorem. Theorem 4.1. Let T be a QBF of the form T ( ) =  :   ( :  ), where  is a CNF and  is an arbitrary QBF. Let I be a  -interpretation. Suppose the following hold: * J1 p I is a partial  -interpretation, and M a 2-valued  interpretation s.t. (J1 , M ) explains 's satisfiability, and * J2 is a partial (   )-interpretation, J2 p I  Mlu and  is J2 -unsatisfiable. Then, with J = J1  J2 | , it holds that J p I and (J, M ) explains the satisfiability of   ( :  ). Theorem 4.1 shows how an explanation of satisfiability of  and one of unsatisfiability of  can be combined to provide an explanation of satisfiability of the combined expression. This can be directly used to extend SAT- TO -SAT's output in case of satisfiability.



Translating QDIMACS into SAT- TO -SAT Input

Minor syntactical differences aside, a QDIMACS specification could be fed directly to SAT- TO -SAT. The only difference is that QDIMACS supports universal quantifications, while SAT- TO -SAT supports existential quantifications under negation, a difference that can be trivially eliminated. However, contrary to QDIMACS, SAT- TO -SAT does not require theories to be in prenex normal form. When feeding QDIMACS input to SAT- TO -SAT, all clauses are in the innermost solver. During search, other solver instances will gradually learn clauses as well. However, some clauses can be pulled out to prior to any search. This will speed up search, since SAT- TO -SAT will not waste time rediscovering information that was present in the first place. Starting from a QDIMACS specification, we perform two preprocessing steps before feeding it to SAT- TO -SAT. 1. Remove tautological clauses. We remove all clauses containing both a literal and its negation from the theory.



2. Pull clauses outwards. When certain clauses only use certain variables, they can be pulled out. We iteratively apply the following lemma to obtain an equivalent theory that no longer is in prenex normal form and in which each clause is located at the "right" level. Lemma 4.5. Let  denote the QBF (0 ) = 1 : 1 2 : 3 : 3  (1 (0 , 1 )  2 (2 )), where the i 's are arbitrary formulas and the i 's are clauses. If 2 is no tautology, then (0 ) is equivalent to 1 : 1 (0 , 1 )  1  2 : 3 : 3 .



5



Evaluation & Future Work



We implemented the aforementioned techniques on top of the Glucose solver (Audemard and Simon 2009). We evaluated the resulting solver on 276 instances from the QBFLIB problems suite (Giunchiglia, Narizzano, and Tacchella 2001) , namely those that were used in the latest QBF competition (QBFEVAL 2014). We compared running times of SAT- TO -SAT with GhostQ (Klieber et al. 2010), the winner of the competition on the QBFLIB track. We only compared a plain version of our solver with the plain version of GhostQ. All tests were ran with a time limit of 900 seconds on an Intel c Xeon c E5-4652 CPU clocked at 2.70GHz with 260Gb of RAM running Ubuntu 14.04LTS. Table 1: The numbers of satisfiable and unsatisfiable instances solved by SAT- TO -SAT and GhostQ. GhostQ SAT- TO -SAT SAT 66 28 UNSAT 57 43 Total 123 71



These kind of observations allow us to reduce the nesting depth or to pull variables and clauses higher in the nesting hierarchy. Since SAT- TO -SAT is designed to gradually pass more and more information upwards in the hierarchy, doing this in advance could save precious time; in this case, without any memory overhead. Reverse Tseitin engineering is used for example by Goultiaeva and Bacchus (2013). Implementing such techniques is a topic for future work. (ii) Sometimes when the internal solver in SAT- TO -SAT finds a model, there are several choices on how to construct a conflict clause. More formally, for a given model M of the internal theory, there might be multiple J 's such that (J, M ) explains satisfiability of the internal theory, as defined in Definition 3.1. Each of these J 's gives rise to a different learned clause in the outermost solver. Example 5.1. Let T be the following theory:   o, p, q :     (p  q ) T =    r :  (p  q  r)  (r  q )  (o  q  r) If a SAT-solver for the internal theory is called with assumptions I = {ot , pt , q t }, then M = {rf } is a model. If J1 = {ot , pt } and J2 = {ot , q t }, then both (J1 , M ) and (J2 , M ) explain satisfiability of (p  q  r)  (r  q ). Returning the first of these would result in the addition of a clause o  p to the outermost theory, while the second would result in the addition of a clause o  q . In principle, both clauses found in Example 5.1 are valid consequences and could be added to the top theory. However, there could be exponentially many such clauses. Using ideas from extended resolution (Tseitin 1968), we can summarise all of these clauses in linear size in terms of the original theory by introducing new variables. In the example, this would boil down to introducing a variable t and adding an encoding of the following definition t  p  q and the clause to to invalidate the current assignment. This generalises both of the above clauses with the cost of introducing an extra variable. We need to research the impact of such learned clauses both on time and memory consumption. (iii) Several of the preprocessing techniques discussed here involved transforming a prenex normal form QBF into a non-prenex normal form sentence. Sometimes this even involves "rediscovering" problem structure that was probably present at the time of the encoding, but that was lost because of the low-level format used. An example of this is the case (i) above, where we need to do clever reasoning to rediscover that one variable is defined functionally in terms of other variables, since CNF's have no native language construct to express this definition. A richer language could directly present this information in the encoding. Hence, we intend to generalise SAT- TO -SAT to accept non-prenex and non-CNF input format such as QCIR (QBF Gallery 2014).



Table 1 depicts the number of instances solved by SAT- TO -SAT and GhostQ. As can be seen, SAT- TO -SAT's performance still lags behind the best available QBF solver. The difference seems big. However, some remarks, that also define our future work directions, need to be made. Firstly, combining these results with the results from the latest competition (QBFEVAL 2014) puts SAT- TO -SAT approximately on-par with the plain version of RAReQS (Janota et al. 2012), while a version of RAReQS with QBF preprocessors ended up in the third place of this competition. Finding out which preprocessors boost SAT- TO -SAT's performance is a topic for future work. Secondly, our implementation was built on top of Glucose. It is not hard to replace Glucose with any SAT solver that implements the interface given in Definition 3.3. Related, SAT- TO -SAT allows us to lift all future improvements in SAT solving to QBF. Thirdly, our solver was not optimised for QBF solving. Several optimisations are possible. We discuss three of them below. (i) One particularly useful optimisation would be to detect Tseitin variables. QBF specifications in the QBFLIB often contain patterns of the form  :  :  : (p  (,  ))  , with p   , which is equivalent to  : , p : (p  (,  ))  ( \ {p}) : .



6



Related Work



There exist many CDCL-based algorithms to solve QBF instances. Our approach differs from most of them in three aspects. (1) Using under-approximations, we circumvent a



common limitation in QBF solving that variables must be chosen in accordance to the quantifier prefix. Thus, nested solvers can be called earlier, during the search process of a solver, before a solver has found a complete assignment. This leads to faster propagation. (2) We treat existential and universal quantifiers symmetrically. That is, all our algorithms are defined for theories of the form  :    :  , where neither the structure of  , nor the context in which this formula occurs matters. (3) Given that our approach is applicable to any SAT solver, we gain an engineering advantage over many existing QBF solving techniques. The idea that using an NP oracle inside an NP-solver results in a solver for P 2 (and, similarly, for the rest of polynomial hierarchy) is not new and directly follows the definition of these complexity classes. This idea has also been applied before to obtain a QBF solver (Ranjan, Tang, and Malik 2004). Our approach is different in two main ways from Ranjan, Tang, and Malik (2004): (1) we use of underapproximations, and (2) we are not limited to 2QBF . These ideas have been integrated with a new learning technique known as Counterexample Guided Abstraction Refinement (CEGAR) (Janota et al. 2012). CEGAR enables gradual expansion of a QBF instance and, in that sense, our approach is similar to CEGAR. However, unlike CEGAR, our approach generates only one solver per quantification level and maintains the states of those solvers for faster search. Recently, Rabe and Tentrup (2015) introduced a new extension of the CEGAR approach that also uses one solver per quantifier level as well as a form of clause selection. Our work is different from theirs because of our underapproximation technique and our uniform treatment of existential and universal quantifiers. Also, recently, Janota and Marques-Silva (2015) presented a QBF solver based on clause selection which, again, is different from our algorithm due to our underapproximation technique and our uniform treatment of quantifiers. Another point where our approach differs from theirs relates to the learnt clauses. Their approach uses an extended language of learning where each clause is associated with variables that express whether that clause is selected (i.e., should be satisfied in lower levels) or deselected (i.e., is already satisfied) at a certain level. Learnt clauses are always over these new, so-called clause selection variables. Their method allows to succinctly represent many clauses invalidating different J 's that explain satisfiability at once (similar to the technique presented in Example 5.1). We are convinced that the extended language of learning is beneficial to QBF solving. Investigating the exact effect of this aspect on solver performance is a topic for future work. Several QBF solvers combine CDCL with solution-driven cube learning (Zhang and Malik 2002; Goultiaeva, Seidl, and Biere 2013). A cube is a conjunction of literals; a formula is in Disjunctive Normal Form (DNF) if it is a disjunction of cubes. Zhang and Malik (2002) introduced Augmented CNF (ACNF): a formula is an ACNF if it is of the form    where  is a CNF and  a DNF. They also introduced solution-driven cube learning: a technique where a QBF solver reasons on an ACNF and gradually adds cubes to 2 . Currently, many QBF solvers implement solution-



driven cube learning in addition to conflict-driven clause learning. Goultiaeva, Seidl, and Biere (2013) show that dual propagation for QBF can be represented using the existing clause-learning and cube-learning methods in QBF solvers. Let us give the outermost solver in SAT- TO -SAT level one and each nested solver level one plus its parent's level. We show that cube learning and clause learning in QBF both boil down to clause learning in SAT- TO -SAT. That is, clauses learnt in a QDPLL algorithm correspond to learnt clauses in odd levels of SAT- TO -SAT and cubes learnt in a QDPLL algorithm correspond to SAT- TO -SAT clauses in even levels. Informally, this follows easily from the fact that a cube is the negation of a clause and vice versa. Since the clauses in even levels of SAT- TO -SAT have an odd number of negations preceding them, they indeed represent a negated clause, i.e., a cube. Similarly, the even number of negations that precede a clause in an odd level of SAT- TO -SAT cancel each other out to represent a normal QBF clause. Formally, If T = 1 : 2 : 3 : . . . n-1 : n :  is a QBF with  in ACNF, then all learned cubes C have the property that T is equivalent to 1 : 2 : 3 : . . . n-1 : n : C  . (1) Lemma 6.1. Suppose T = 1 : 2 : 3 : . . . n-1 : n :  is a QBF with  in ACNF and C = C  Cn is a cube with C a i<n i -cube and Cn a n -cube. Suppose also that T is equivalent to (1). Then T is also equivalent to 1 : 2 : 3 : . . . n-1 : C  n :  and to 1 : 2 : 3 : . . . n-1 : C  n : . From Lemma 6.1, we indeed see that (since C is a clause) learned cubes in QBF correspond to learned clauses in the corresponding SAT- TO -SAT specification.



7



Conclusion



This paper introduced an extension of SAT- TO -SAT with an arbitrary nesting depth and showed how it can be used to solve the validity problem of Quantified Boolean Formulas. Our experiments show that, even without any QBF-specific optimizations, SAT- TO -SAT performs relatively well on QBF instances from the latest QBF competition. Moreover, the generic architecture of SAT- TO -SAT with respect to its underlying SAT solvers allows us to uniformly lift SAT-related optimizations to QBF. In particular, G LUCOSE, the SAT-solver SAT- TO -SAT is currently based on, can be easily replaced with other SAT solvers. In addition, in Section 5, we have identified several topics for future work: (i) detecting when variables are completely defined in terms of variables form higher levels in order to pull larger parts of the theory to higher levels (ii) learning stronger clauses based on extended resolution when conflicts arise because the internal solver finds a model, and (iii) moving towards a richer input language.



Acknowledgments

This work is supported by the Finnish Center of Excellence in Computational Inference Research (COIN) funded by the Academy of Finland (under grant #251170).



References

Alviano, M.; Dodaro, C.; Leone, N.; and Ricca, F. 2015. Advances in WASP. In Logic Programming and Nonmonotonic Reasoning 13th International Conference, LPNMR 2015. Proceedings, 40-54. Apt, K. R. 2003. Principles of Constraint Programming. Cambridge University Press. Audemard, G., and Simon, L. 2009. Predicting learnt clauses quality in modern SAT solvers. In Boutilier, C., ed., IJCAI, 399-404. Barrett, C. W.; Sebastiani, R.; Seshia, S. A.; and Tinelli, C. 2009. Satisfiability modulo theories. In Biere, A.; Heule, M.; van Maaren, H.; and Walsh, T., eds., Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications. IOS Press. 825-885. Bayless, S.; Bayless, N.; Hoos, H. H.; and Hu, A. J. 2015. SAT modulo monotonic theories. In Bonet, B., and Koenig, S., eds., Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015., 3702-3709. AAAI Press. Chu, G., and Stuckey, P. J. 2014. Nested constraint programs. In O'Sullivan, B., ed., Principles and Practice of Constraint Programming - CP 2014. Proceedings, volume 8656 of Lecture Notes in Computer Science, 240-255. Springer. De Cat, B.; Bogaerts, B.; Devriendt, J.; and Denecker, M. 2013. Model expansion in the presence of function symbols using constraint programming. In 2013 IEEE 25th International Conference on Tools with Artificial Intelligence, 2013, 1068-1075. IEEE Computer Society. E en, N., and S orensson, N. 2003. An extensible SAT-solver. In Giunchiglia, E., and Tacchella, A., eds., SAT, volume 2919 of LNCS, 502-518. Springer. Ganzinger, H.; Hagen, G.; Nieuwenhuis, R.; Oliveras, A.; and Tinelli, C. 2004. DPLL(T): Fast decision procedures. In Alur, R., and Peled, D., eds., CAV, volume 3114 of LNCS, 175-188. Springer. Gebser, M.; Janhunen, T.; and Rintanen, J. 2014. SAT modulo graphs: Acyclicity. In Logics in Artificial Intelligence - 14th European Conference, JELIA 2014. Proceedings, 137-151. Gebser, M.; Kaufmann, B.; and Schaub, T. 2012. Conflict-driven answer set solving: From theory to practice. Artif. Intell. 187:52- 89. Gebser, M.; Kaufmann, B.; and Schaub, T. 2013. Advanced conflict-driven disjunctive answer set solving. In IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence. Giunchiglia, E.; Narizzano, M.; and Tacchella, A. 2001. Quantified Boolean formulas satisfiability library (QBFLIB). http://www. qbflib.org. Giunchiglia, E.; Narizzano, M.; and Tacchella, A. 2002. Learning for quantified boolean logic satisfiability. In Proceedings of the Eighteenth National Conference on Artificial Intelligence and Fourteenth Conference on Innovative Applications of Artificial Intelligence., 649-654. Goultiaeva, A., and Bacchus, F. 2013. Recovering and utilizing partial duality in QBF. In Theory and Applications of Satisfiability Testing - SAT 2013 - 16th International Conference, Helsinki, Finland, July 8-12, 2013. Proceedings, 83-99. Goultiaeva, A.; Seidl, M.; and Biere, A. 2013. Bridging the gap between dual propagation and cnf-based qbf solving. In Design, Automation Test in Europe Conference Exhibition (DATE), 2013, 811-814.



Janhunen, T.; Tasharrofi, S.; and Ternovska, E. 2016. SAT- TO -SAT: Declarative extension of SAT solvers with new propagators. In To Appear in the Proceedings of 30th AAAI Conference on Artificial Intelligence (AAAI-16). Preprint available on https://www.cs.sfu.ca/sta44/personal/ files/jtt-aaai-2016.pdf. Janota, M., and Marques-Silva, J. 2015. Solving QBF by clause selection. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015., 325-331. Janota, M.; Klieber, W.; Marques-Silva, J.; and Clarke, E. M. 2012. Solving QBF with counterexample guided refinement. In Theory and Applications of Satisfiability Testing - SAT 2012 - 15th International Conference, 2012. Proceedings, 114-128. Klieber, W.; Sapra, S.; Gao, S.; and Clarke, E. M. 2010. A nonprenex, non-clausal QBF solver with game-state learning. In Theory and Applications of Satisfiability Testing - SAT 2010, 13th International Conference, Proceedings, 128-142. Letz, R. 2002. Lemma and model caching in decision procedures for quantified boolean formulas. In Automated Reasoning with Analytic Tableaux and Related Methods, International Conference, TABLEAUX 2002, Proceedings, 160-175. Marek, V., and Truszczy nski, M. 1999. Stable models and an alternative logic programming paradigm. In Apt, K. R.; Marek, V.; Truszczy nski, M.; and Warren, D. S., eds., The Logic Programming Paradigm: A 25-Year Perspective. Springer-Verlag. 375-398. Marques-Silva, J. P., and Sakallah, K. A. 1999. GRASP: A search algorithm for propositional satisfiability. IEEE Transactions on Computers 48(5):506-521. Nadel, A., and Ryvchin, V. 2012. Efficient SAT solving under assumptions. In Theory and Applications of Satisfiability Testing - SAT 2012 - 15th International Conference, 2012. Proceedings, 242-255. Ohrimenko, O.; Stuckey, P. J.; and Codish, M. 2009. Propagation via lazy clause generation. Constraints 14(3):357-391. QBF Gallery. 2014. QCIR-G14: A non-prenex non-CNF format for quantified boolean formulas. Technical report. 2014. QBF gallery 2014 (competition). http://qbf. satisfiability.org/gallery/index.html. Rabe, M. N., and Tentrup, L. 2015. Caqe: A certifying qbf solver. In Proceedings of the 15th Conference on Formal Methods in Computer-aided Design (FMCAD'15), 136-143. Ranjan, D. P.; Tang, D.; and Malik, S. 2004. A comparative study of 2qbf algorithms. In SAT 2004 - The Seventh International Conference on Theory and Applications of Satisfiability Testing, Online Proceedings. Silva, J. P. M.; Lynce, I.; and Malik, S. 2009. Conflict-driven clause learning SAT solvers. In Biere, A.; Heule, M.; van Maaren, H.; and Walsh, T., eds., Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications. IOS Press. 131-153. Tseitin, G. S. 1968. On the complexity of derivation in the propositional calculus, Zapiski nauchnykh seminarov. LOMI 8:234-259. English translation of this volume: Studies in Constructive Mathematics and Mathematical Logic, Part 2, A. O. Slisenko, eds. Consultants Bureau, N.Y., 1970, pp. 115-125. Zhang, L., and Malik, S. 2002. Towards a symmetric treatment of satisfaction and conflicts in quantified boolean formula evaluation. In Principles and Practice of Constraint Programming - CP 2002, 8th International Conference, CP 2002, Proceedings, 200-215.



Multi-Agent Path Finding with Delay Probabilities

Hang Ma

Department of Computer Science University of Southern California hangma@usc.edu



T. K. Satish Kumar

Department of Computer Science University of Southern California tkskwork@gmail.com



Sven Koenig

Department of Computer Science University of Southern California skoenig@usc.edu



Abstract

Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.



Start State Goal State



v1 v2 s2 s1 v3 g1 v4 g2 v5



Figure 1: A MAPF-DP instance. an undirected graph (that models the environment) to move from its start vertex to its goal vertex. At any discrete time step, the agent can either execute 1) a wait action, resulting in it staying in its current vertex, or 2) a move action with the intent of traversing an outgoing edge of its current vertex, resulting in it staying in its current vertex with the delay probability and traversing the edge otherwise. The MAPFDP problem is the problem of finding 1) a MAPF-DP plan that consists of a path for each agent from its start vertex to its goal vertex (given by a sequence of wait and move actions) and 2) a plan-execution policy that controls with GO or STOP commands how each agent proceeds along its path such that no collisions occur during plan execution. There are 2 kinds of collisions, namely vertex collisions (where 2 agents occupy the same vertex at the same time step) and edge collisions (where 2 agents traverse the same edge in opposite directions at the same time step). We make the following contributions to solve the MAPFDP problem with small average makespans: First, we formalize the MAPF-DP problem, define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.



Introduction

Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths for a given number of agents from their given start locations to their given goal locations in a given environment. MAPF problems arise for aircraft towing vehicles (Morris et al. 2016), office robots (Veloso et al. 2015), video game characters (Silver 2005) and warehouse robots (Wurman, D'Andrea, and Mountz 2008), among others. Several recently developed MAPF solvers scale to large MAPF instances. However, agents typically cannot execute their MAPF plans perfectly since they often traverse their paths more slowly than intended. Their delay probabilities can be estimated but current MAPF solvers do not use this information, which often leads to frequent and runtimeintensive replanning or plan-execution failures. We thus formalize the MAPF Problem with Delay Probabilities (MAPF-DP), where each agent traverses edges on

Our research was supported by NSF under grant numbers 1409987 and 1319966. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the sponsoring organizations, agencies or the U.S. government. Copyright c 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.





Background and Related Work

The MAPF problem is NP-hard to solve optimally for flowtime minimization and to approximate within any constant factor less than 4/3 for makespan minimization (Ma et al. 2016). Search-based MAPF solvers can be optimal, bounded suboptimal or suboptimal (Standley 2010; Luna



and Bekris 2011; Wang and Botea 2011; Goldenberg et al. 2014; Sharon et al. 2013; 2015; Boyarski et al. 2015; Wagner and Choset 2015; Ma and Koenig 2016; Cohen et al. 2016). Current MAPF solvers typically assume perfect plan execution. However, utilizing probabilistic information about imperfect plan execution can reduce frequent and time-intensive replanning and plan-execution failures. Partially Observable Markov Decision Processes (POMDPs) are a general probabilistic planning framework. The MAPF-DP problem can be solved with POMDPs but this is tractable only for very few agents in very small environments since the size of the state space is proportional to the size of the environment to the power of the number of agents and the size of the belief space is proportional to the size of the state space to the power of the length of the planning horizon (Kurniawati, Hsu, and Lee 2008; Ma and Pineau 2015). Several specialized probabilistic planning frameworks, such as transition-independent decentralized Markov Decision Processes (DecMDPs) (Becker et al. 2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs. In transition-independent Dec-MDPs, the local state of each agent depends only on its previous local state and the action taken by it (Goldman and Zilberstein 2004). MAPF-DP is indeed transition independent. However, there are interactions among agents since the reward of each agent depends on whether it is involved in a collision and thus on the local states of other agents and the actions taken by them. Fully decentralized probabilistic planning frameworks thus cannot prevent collisions. Fully centralized probabilistic planning frameworks can prevent collisions but are more runtime-intensive and can thus scale poorly. For example, the MAPF-DP problem can be solved with transition-independent MMDPs (Scharpff et al. 2016). In fact, the most closely related research to ours is that on approximating MMDPs (Liu and Michael 2016) although it handles different types of dynamics than we do. The runtime of probabilistic planning frameworks can be reduced by exploiting the problem structure, including when interactions among agents are sparse. For example, decentralized sparse-interaction Markov Decision Processes (Dec-SIMDPs) (Melo and Veloso 2011) assume that interactions among agents occur only in well-defined interaction areas in the environment (which is not the case for MAPF-DP in general), but typically still do not scale to more than 10 agents. The model shaping technique for decentralized POMDPs (Velagapudi et al. 2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions.



ai has a unique start vertex si  V , a unique goal vertex gi  V and a delay probability pi  (0, 1). A path for agent ai is expressed by a function li that maps each time index x = 0, 1 . . . Xi to a vertex li (x)  V such that li (0) = si , consecutive vertices li (x) and li (x + 1) are either identical (when agent ai is scheduled to execute a wait action) or connected by an edge (when agent ai is scheduled to execute a move action from vertex li (x) to vertex li (x + 1)) and li (Xi ) = gi . A MAPF plan consists of a path for each agent.



Problem Definition: Plan Execution

The local state xt i of agent i at time step t = 0, 1 . . .  during plan execution is a time index. We set x0 i := 0 and always update its local state such that it is in vertex li (xt i ) at time step t. The agent knows its current local state and receives messages from some of the other agents about their local states. At each time step, its plan-execution policy maps this knowledge to one of the commands GO or ST OP that control how it proceeds along its path. 1. If the command is GO at time step t: (a) If xt i = Xi , then agent ai executes no action and remains in its current vertex li (xt i ) since it has entered its last local state (and thus the end of its path). We thus +1 update its local state to xt := xt i. i t t (b) If xt = X and l ( x ) = l i i i i (xi + 1), then agent ai i executes a wait action to remain in its current vertex li (xt i ). The execution of wait actions never fails. We +1 thus update its local state to xt := xt i + 1 (success). i t t t (c) If xi = Xi and li (xi ) = li (xi + 1), then agent ai executes a move action from its current vertex li (xt i ) to vertex li (xt + 1) . The execution of move actions fails i with delay probability pi with the effect that the agent executes no action and remains delayed in its current t+1 vertex li (xt := i ). We thus update its local state to xi t +1 t t xi with probability pi (failure) and xi := xi + 1 with probability 1 - pi (success). 2. If the command is ST OP at time step t, then agent ai executes no action and remains in its current vertex li (xt i ). +1 t We thus update its local state to xt := x . i i Our objective is to find a combination of a MAPF plan and a plan-execution policy with small average makespan, which is the average earliest time step during plan execution when all agents have entered their last local states. The MAPF problem is a special case where the delay probabilities of all agents are zero and the plan-execution policies always provide GO commands.



Valid MAPF-DP Plans

Definition 1. A valid MAPF-DP plan is a plan with 2 properties: 1. i, j, x with i = j : li (x) = lj (x) [two agents are never scheduled to be in the same vertex at the same time index, that is, the vertices of two agents in the same local state are different]. 2. i, j, x with i = j : li (x + 1) = lj (x) [an agent is never scheduled to be in a vertex at a time index x + 1 when any other agent is scheduled to be in the same vertex at time index x, that is, the vertex of an agent in a local state x +1



Problem Definition: Planning

A MAPF-DP instance is characterized by an undirected graph G = (V, E ) whose vertices V correspond to locations and whose edges E correspond to transitions between locations. We are given m agents a1 , a2 . . . am . Each agent



has to be different from the vertex of any other agent in local state x]. Figure 1 shows a sample MAPF-DP instance where the blue agent a1 has to move from its start vertex v3 to its goal vertex v4 and the red agent a2 has to move from its start vertex v2 to its goal vertex v5 . Agent a1 has to move north to let agent a2 pass. The paths l1 = v3 , v1 , v1 , v1 , v3 , v4 and l2 = v2 , v2 , v3 , v4 , v5 form a valid MAPF-DP plan. However, the paths l1 = v3 , v1 , v1 , v3 , v4 and l2 = v2 , v3 , v4 , v5 a valid MAPF plan but not a valid MAPF-DP plan since l2 (1) = l1 (0) = v3 violates Property 2. Property 1 of Definition 1 is necessary to be able to execute valid MAPF-DP plans without vertex collisions because two agents could otherwise be in the same vertex at the same time step (under perfect or imperfect plan execution). Property 2 is also necessary because an agent could otherwise enter the vertex of some other agent that unsuccessfully tries to leave the same vertex at the same time step (under imperfect plan execution). Property 2 is also necessary to be able to execute valid MAPF-DP plans without edge collisions (under perfect or imperfect plan execution).



Robust Plan-Execution Policies

We study 2 kinds of decentralized robust plan-execution policies for valid MAPF-DP plans, which are plan-execution policies that prevent all collisions during the imperfect plan execution of valid MAPF-DP plans.



Fully Synchronized Policies (FSPs)

Fully Synchronized Policies (FSPs) attempt to keep all agents in lockstep as much as possible by providing a GO command to an agent if and only if the agent has not yet entered its last local state and all other agents have either entered their last local states or have left all local states that precede the local state of the agent itself. FSPs can be implemented easily if each agent sends a message to all other agents when it enters a new local state. An agent can implement its FSP simply by counting how many messages it has received from each other agent and providing a GO command to itself in local state x if and only if it has not yet entered its last local state and has received x messages over the course of plan execution from each other agent.



Minimal Communication Policies (MCPs)

FSPs have 2 drawbacks. First, agents wait unnecessarily, which results in large average makespans. Second, each agent always needs to know the local states of all other agents, which results in many sent messages. Property 2 of Definition 1 suggests that robust plan-execution policies for valid MAPF-DP plans could provide a GO command to an agent if and only if the agent has not yet entered its last local state and all other agents have left all local states that precede the local state of the agent itself and whose vertices are the same as the vertex of the next local state of the agent itself. This way, it is guaranteed that the vertex of the next local state of the agent is different from the vertices of all other agents in their current local states. Minimal Communication



Policies (MCPs) address these drawbacks by identifying such critical dependencies between agents and obeying them during plan execution, an idea that originated in the context of centralized non-robust plan-execution policies (H onig et al. 2016). The local state of an agent ai at any time step during plan execution is a time index x. Since we need to relate the local states of different agents, we use li (x) in the following not only to refer to the vertex assigned to local state x of agent ai but also to the local state x of agent ai itself (instead of x), depending on the context. Every valid MAPF-DP plan defines a total order on the local states of all agents, which we relax to a partial order  as follows: 1. i, x : li (x)  li (x + 1) [agent ai enters a local state x + 1 during plan execution only after it enters local state x]. 2. i, j, x, x with i = j , x < x and l = lj (x ) = li (x + 1) : lj (x + 1)  li (x + 1) [agent ai enters a local state x + 1 with a vertex l during plan execution only after agent aj has left a local state x with vertex l (and thus entered local state x + 1) that precedes local state x]. Property 1 of the partial order enforces that each agent visits its locations in the same order as in the MAPFDP plan. Property 2 enforces that any two agents visit the same location in the same order as in the MAPFDP plan. We can express the partial order with a directed graph G = (V , E ) whose vertices correspond to local states and whose edges correspond to the partial order given by the two properties above. Property 2 specifies the critical dependencies between agents. Edges are redundant and can then be removed from the directed graph when they are implied by the other edges due to transitivity. A transitive reduction of the directed graph minimizes the number of remaining edges. It can be computed in time O(|V||E|) (Aho, Garey, and Ullman 1972), is unique, contains all edges between local states of the same agent (since they are never redundant) and thus minimizes the number of edges between the local states of different agents. MCPs can be implemented easily if each agent aj sends a message to each other agent ai when agent aj enters a new local state x  (= x + 1 in Property 2) if and only if the transitive reduction contains an edge lj ( x )  li ( x) for some local state x  (= x + 1 in Property 2) of agent ai . Since the transitive reduction minimizes the number of edges between the local states of different agents, it also minimizes the number of sent messages. An agent ai can implement its MCP simply by counting how many messages it has received from each other agent and providing a GO command to itself in local state x if and only if it has not yet entered its last local state and has received a number of messages over the course of plan execution from each other agent aj that corresponds to the number of incoming edges from local states of agent aj to its local states 0, 1 . . . x + 1. Figure 2 shows a sample partial order on the local states for the MAPF-DP instance from Figure 1 and its valid MAPF-DP plan l1 = v3 , v1 , v3 , v1 , v1 , v1 , v3 , v4 and l2 = v2 , v2 , v2 , v2 , v3 , v4 , v5 . l1 (1)  l2 (4), for example, is implied by l1 (1)  l1 (2)  l1 (3)  l2 (4) and can thus



v3

l1 (0)



v1

l1 (1)



v3

l1 (2)



v1

l1 (3)



v1

l1 (4)



v1

l1 (5)



v3

l1 (6)



v4

l1 (7)



v2

l2 (0)



v2

l2 (1)



v2

l2 (2)



v2

l2 (3)



v3

l2 (4)



v4

l2 (5)



v5

l2 (6)



Figure 2: A directed graph that specifies a partial order on the local states for the MAPF-DP instance from Figure 1 and its valid MAPF-DP plan l1 = v3 , v1 , v3 , v1 , v1 , v1 , v3 , v4 and l2 = v2 , v2 , v2 , v2 , v3 , v4 , v5 .

v3

l1 (0)



li (x) = lj (y + 1) = lj (x + 1), which is a contradiction with the State Property. Case 2) If x < y , then li (x + 1)  lj (y +1) according to Property 2 of the partial order  since li (x) = lj (y +1) according to our edge collision assumption and x < y according to the case assumption. Thus, agent aj can leave local state y only when agent ai reaches local state x + 1, which is a contradiction with the edge collision assumption.



v1

l1 (1)



v3

l1 (2)



v1

l1 (3)



v1

l1 (4)



v1

l1 (5)



v3

l1 (6)



v4

l1 (7)



Approximate Minimization in Expectation

MCPs are robust plan-execution policies for valid MAPFDP plans that do not stop agents unnecessarily and result in few sent messages. We present a MAPF-DP solver, called Approximate Minimization in Expectation (AME), that determines valid MAPF-DP plans so that their combination with MCPs results in small average makespans. AME is a 2-level MAPF-DP solver that is based on Conflict-Based Search (CBS) (Sharon et al. 2015). Its highlevel search imposes constraints on the low-level search that resolve violations of Properties 1 and 2 of Definition 1 (called conflicts). Its low-level search plans paths for single agents that obey these constraints and result in small average makespans. The average makespan of a MAPF-DP plan is the expectation of the maximum of (one or more) random variables that represent the time steps when all agents enter their last local states. Moreover, the average time step when an agent enters a local state is the expectation of the maximum of random variables as well. It is often difficult to obtain good closed-form approximations of the expectation of the maximum of random variables. AME thus approximates it with the maximum over the expectations of the random variables, which typically results in an underestimate but, according to our experimental results, a close approximation. The approximate average time step  li ( x ) when agent ai enters a local state x for a given MAPF-DP plan is 0 for x = 0 and

max( li (x - 1), maxj,x = maxj,x

:i=j,x <x,lj (x )li (x) (lj (x



v2

l2 (0)



v2

l2 (1)



v2

l2 (2)



v2

l2 (3)



v3

l2 (4)



v4

l2 (5)



v5

l2 (6)



Figure 3: The transitive reduction for Figure 2. be removed from the directed graph. Figure 3 shows the resulting transitive reduction, which implies that agent a2 has to wait in local state 3 until it has received one message from agent a1 during the course of plan execution but can then proceed through all future local states without waiting.



Properties of FSPs and MCPs

Both FSPs and MCPs do not result in deadlocks during the plan execution of valid MAPF-DP plans because there always exists at least one agent that is provided a GO command before all agents have entered their last local states (namely an agent with the smallest local state among all agents that have not yet entered their last local states since an agent can wait only for other agents with smaller local states). Both FSPs and MCPs are robust plan-execution policies due to Properties 1 and 2 of valid MAPF-DP plans. We now provide a proof sketch for the robustness of MCPs. First, consider a valid MAPF-DF plan and assume that li (x) = lj (y ) for two agents ai and aj with i = j . Then, 1) y = x since li (x) = lj (x) according to Property 1 of Definition 1 and 2) y = x + 1 since lj (x + 1) = li (x) according to Property 2 of Definition 1 (State Property). Second, we show by contradiction that no vertex collisions can occur during plan execution. Assume that a vertex collision occurs between agents ai and aj with i = j when agent ai is in local state x and agent aj is in local state y . Assume without loss of generality that x  y . Then, li (x +1)  lj (y ) according to Property 2 of the partial order  since li (x) = lj (y ) according to our vertex collision assumption and x < y - 1 according to the State Property. Thus, agent aj can leave local state y - 1 only when agent ai reaches local state x + 1, which is a contradiction with the vertex collision assumption. Third, we show by contradiction that no edge collisions can occur during plan execution. Assume that an edge collision between agents ai and aj with i = j occurs when agent ai changes its local state from x to x + 1 and agent aj changes its local state from y to y + 1. Assume without loss of generality that x  y . Case 1) If x = y , then







i ))) + t (1)



:x <x,lj (x



  )li (x) (lj (x )) + ti



otherwise since agent ai first enters local state x - 1 at approximate average time step  li (x - 1), then might have to wait for messages from other agents aj that they send when they enter their local states x at approximate average time steps  lj (x ) and finally has to successfully execute one action (perhaps repeatedly) to enter local state i of time steps that it needs for x. The average number t the successful execution of the action is 1 (for a wait action) if li (x) = li (x - 1) and 1/(1 - pi ) (for a move action) otherwise. The approximate average makespan of the given MAPF-DP plan is then maxi  li (Xi ) since all agents need to enter their last local states. One might be able to obtain better approximations with more runtime-intensive importance sampling or dynamic programming methods but the runtime of the resulting AME variant would be large since it needs to compute many such approximations.



Algorithm 1: High-Level Search of AME.

1 Root.constraints := ; 2 Root.plan := ; 3 for each agent ai do 4 if LowLevelSearch(ai , Root, 0) returns no path (nor its labels) then 5 return "No solution exists"; 6 Add the returned path (and its labels) to Root.plan;



the second child node [Line 18], thus preventing the conflict in both cases.



Low-Level Search

LowLevelSearch(ai , N , key) finds a new path for agent ai and the labels  li (x) of this path. It uses the paths of the other agents and their labels in N.plan but does not update them. (The paths are empty directly after the execution of Line 2.) It performs a focal search with re-expansions in a state space whose states correspond to pairs of vertices and local states (except for those pairs ruled out by constraints in N.constraints that pertain to agent ai ) and whose edges connect state (l, x) to state (l , x +1) if and only if l = l (for a wait action) or (l, l )  E (for a move action). The g-value of a state (l, x) approximates (sic!) the approximate average time step  li (x). The start state is (si , 0) and its g-value is 0. When the low-level search expands state (l, x - 1), it sets the g-value of its successor (l , x) according to Equation (1) to the minimum of its current g-value g ((l , x)) and

max(g ((l, x - 1)), maxj,x

:i=j,x <x,lj (x )li (x) (lj (x



7 Root.key := ApproximateAverageMakespan(Root.plan); 8 Priorityqueue := {Root}; 9 while Priorityqueue =  do 10 N := Priorityqueue.pop(); 11 if FindConflicts(N.plan) returns no conflicts then 12 return "Solution is" N.plan; 13 14 15 16 17 18 19 20 21 22 Conflict := earliest returned conflict; for each agent ai involved in Conflict do N := new node with parent node N ; N .constraints := N.constraints; N .plan := N.plan; Add one new constraint for agent ai to N .constraints (see main text); if LowLevelSearch(ai , N , N.key) returns a path (and its labels) then Replace the path (and its labels) of agent ai in N .plan with the returned path (and its labels); N .key := ApproximateAverageMakespan(N .plan); Priorityqueue.insert(N );



23 return "No solution exists";







i , ))) + t



High-Level Search

Algorithm 1 shows the high-level search of AME, which is similar to the high-level search of CBS. In the following, we point out the differences. Each high-level node N contains the following items: 1. A set N.constraints of constraints of the form (ai , l, x) that states that the vertex of agent ai in local state x has to be different from vertex l. 2. A (labeled) MAPF-DP plan N.plan that contains a path li for each agent ai (that obeys the constraints N.constraints) and an approximation  li (x) (called label) of each average time step when agent ai enters local state x during plan execution with MCPs. 3. The key N.key of high-level node N that encodes its priority (smaller keys have higher priority) and is equal to the approximate average makespan of MAPF-DP plan N.plan given by ApproximateAverageMakespan(N.plan) = maxi  li (Xi ). When a conflict exists in MAPF-DP plan N.plan, then the high-level search creates 2 child nodes of node N [Line 15] whose constraints are initially set to the constraints N.constraints [Line 16] and whose MAPF-DP plan is initially set to MAPF-DP plan N.plan [Line 17]. Assume that the earliest conflict is a violation of Property 1 in Definition 1, in which case the vertices of two agents ai and aj in a local state x are both identical to a vertex l. In this case, AME adds the constraint (ai , l, x) to the constraints of the first child node and the constraint (aj , l, x) to the constraints of the second child node [Line 18], thus preventing the conflict in both cases. Assume that the earliest conflict is a violation of Property 2 in Definition 1, in which case the vertex of an agent ai in a local state x + 1 and the vertex of some other agent aj in the immediately preceding local state x are both identical to a vertex l. In this case, AME adds the constraint (ai , l, x + 1) to the constraints of the first child node and the constraint (aj , l, x) to the constraints of



i is 1 if l = l and 1/(1 - pi ) otherwise. The lowwhere t level search decides which state (l, x) to expand next based on 1) the f-value of the state, which is the sum of its g-value and its h-value, where the h-value is 1/(1 - pi ) times the distance from location l to location gi in graph G (which is an optimistic estimate of the average number of time steps required to move from location l to location gi ) and 2) the number of conflicts of the path for agent ai that corresponds to the locations in the states on the found path from the start state to (l, x) with the paths of other agents. The low-level search starts in Phase 1. The objective in this phase is to find a path for agent ai so that it enters its last local state with a reasonably small approximate average number of time steps, namely one that is no larger than the approximate average makespan key of the MAPF-DP plan in the parent node of node N in the high-level search, and has a small number of conflicts. The first part of the objective tries to ensure that the approximate average makespan of the resulting MAPF-DP plan in node N is no larger than the one of the MAPF-DP plan in the parent node of node N , and the second part tries to ensure that the resulting MAPF-DP plan has a small number of conflicts so that the high-level search has a small runtime since it needs to resolve only a small number of conflicts. The low-level search thus repeatedly expands a state with the smallest number of conflicts among all states in the priority queue whose f-values are no larger than key. If no such state exists, then the low-level search switches to Phase 2. The objective in this phase is to find a path for agent ai so that it enters its last local state with a small approximate average number of time steps. This objective tries to ensure that the approximate average makespan of the resulting MAPF-DP plan in node N is not much larger than the one of the MAPF-DP plan in the parent node of node N . The low-level search thus repeatedly expands a state with the smallest f-value among all states in the priority queue.



The low-level search terminates successfully when it is about to expand a state (l, x) with l = gi and N.constraints contains no constraints of the form (ai , gi , x ) with x > x. It then sets Xi := x, the locations li (x) that form the path of agent ai to the corresponding locations in the states on the found path from the start state to (l, x) and the approximate average time steps  li (x) to the corresponding g-values of these states. The low-level search terminates unsuccessfully when the priority queue becomes empty. The low-level search currently does not terminate otherwise but we might be able to make it complete by using an upper bound on the smallest average makespan of any valid MAPF-DP plan, similar to upper bounds in the context of valid MAPF plans (Kornhauser, Miller, and Spirakis 1984).



Future Work

The low-level search is currently the weakest part of AME due to the many approximations to keep its runtime small which is important since the high-level search runs many low-level searches. We expect that future work will be able to improve the low-level search substantially. For example, the approximate average time steps  lj (x) for agents aj different from agent ai could be updated before, during or after the local search, which would provide more accurate values for the current and future low-level searches as well as the current high-level search. Once the low-level search finds a path for agent ai and the high-level search replaces the path of agent ai in the MAPF-DP plan in the current high-level node with this path, it could update the approximate average time steps of all agents to the ideal approximate average time steps given by Equations (1), for example as part of the execution of ApproximateAverageMakespan on Lines 7 and 21. Many other improvements are possible as well.



Figure 4: Two MAPF-DP instances: random 1 (top) and warehouse 1 (bottom). Blocked cells are shown in black. The start and goal cells for each agent are represented by a solid circle and a hollow circle of the same color, respectively. Table 1 reports for each MAPF-DP instance the runtime, the approximate average makespan calculated by AME, the average makespan over 1,000 plan-execution runs with MCPs together with 95%-confidence intervals and the number of sent messages. Dashes indicate that the MAPFDP instance was not solved within a runtime limit of 5 minutes. There is no obvious difference in the numbers of sent messages of the 3 MAPF(-DP) solvers. However, AME seems to find MAPF-DP plans with smaller average makespans than Adapted CBS, which seems to find MAPFDP plans with smaller average makespans than Push and Swap. The approximate average makespans calculated by AME are underestimates but reasonably close to the average makespans. AME and Push and Swap seem to run faster than Adapted CBS. In fact, Adapted CBS did not solve MAPFDP instances with more than 35 agents within the runtime limit while AME and Push and Swap seem to scale to larger numbers of agents than reported here (see also Experiment 3).



Experiments

We evaluate AME with MCPs on a 2.50 GHz Intel Core i52450M PC with 6 GB RAM.



Experiment 1: MAPF Solvers

We compare AME to 2 MAPF solvers, namely 1) Adapted CBS, a CBS variant that assumes perfect plan execution and computes valid MAPF-DP plans, minimizes maxi Xi and breaks ties toward paths with smaller Xi and thus fewer actions and 2) Push and Swap (Luna and Bekris 2011), a MAPF solver that assumes perfect plan execution and computes valid MAPF-DP plans where exactly one agent executes a move action at each time step and all other agents execute wait actions. We generate 10 MAPFDP instances (labeled random 1-10) in 30x30 4-neighbor grids with 10% randomly blocked cells and random but unique start and unique goal cells for 35 agents whose delay probabilities for AME are sampled uniformly at random from the delay probability range (0, 1/2). In the same way, we generate 10 MAPF-DP instances (labeled warehouse 110) in a simulated warehouse environment with random but unique start and unique goal cells on the left and right sides. Figure 4 shows two MAPF-DP instances: random 1 (top) and warehouse 1 (bottom).



Experiment 2: Delay Probability Ranges

We use AME with different delay probability ranges. We repeat Experiment 1 with 19 MAPF-DP instances generated from the MAPF-DP instance labeled "random 1" in Experimax = 2, 3 . . . 20. For each MAPFment 1, one for each t DP instance, the delay probabilities pi of all agents are max ) by sampled from the delay probability range (0, 1 - 1/t  sampling the average number of time steps ti = 1/(1 - pi ) needed for the successful execution of single move actions max ) and then calculating uniformly at random from (1, t i . pi = 1 - 1/t Table 2 reports the same measures as used in Experiment



800



Table 1: Results of different MAPF(-DP) solvers for MAPFDP instances with 35 agents and delay probability range (0, 1/2).

AME Push and Swap Adapted CBS approxruntime imate average mess- runtime average messaverage mess- runtime id (s) makespan ages (s) makespan ages average makespan ages (s) makespan random 1 0.058 63.15 71.28  0.34 267 0.031 812.41  0.40 287 random 2 0.052 66.22 73.02  0.29 257 0.025 768.30  0.43 257 random 3 0.080 78.44 84.90  0.40 373 0.052 934.59  0.33 387 random 4 0.063 67.00 72.89  0.37 251 0.028 755.95  0.33 255 random 5 0.050 65.13 73.98  0.31 255 0.029 875.48  0.47 318 282.079 84.11  0.40 282 random 6 0.052 62.89 66.98  0.36 257 0.031 830.77  0.32 290 random 7 0.495 67.22 71.34  0.36 269 0.038 785.55  0.46 274 random 8 0.042 49.33 51.72  0.35 164 0.024 648.80  0.35 199 197.911 52.35  0.37 163 random 9 0.051 56.27 61.30  0.27 247 0.052 780.60  0.30 294 random 10 0.487 60.06 64.77  0.38 234 0.032 750.12  0.35 284 warehouse 1 0.124 114.32 124.18  0.44 705 0.055 1,399.14  0.43 703 warehouse 2 0.106 119.74 124.63  0.51 762 0.055 1,620.03  0.60 810 warehouse 3 0.107 112.96 117.00  0.53 609 0.032 1,295.75  0.53 616 warehouse 4 0.090 114.90 117.31  0.52 541 0.043 1,246.47  0.67 571 warehouse 5 0.060 1,453.36  0.54 783 warehouse 6 0.111 127.65 131.10  0.59 710 0.037 1,437.01  0.58 664 warehouse 7 0.142 87.45 96.54  0.34 488 0.028 1,154.21  0.60 403 warehouse 8 0.024 1,233.13  0.58 401 warehouse 9 0.087 103.51 107.33  0.42 462 0.024 1,088.53  0.44 422 warehouse 10 0.183 120.76 127.36  0.53 909 0.057 1,541.56  0.62 678 -



700



600

500 400 300



200

100 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20



Figure 5: Visualization of Table 2, where the x-axis shows max needed for the the average number of time steps t successful execution of single move actions. The average makespans are shown in red, and the approximate average makespans calculated by AME are shown in blue. The grey max . line corresponds to 30t



Table 2: Results of AME for MAPF-DP instances with 35 agents on a 30x30 4-neighbor grid with 10% randomly blocked cells and different delay probability ranges (0, 1 - 1 ).  t

max



Table 3: Results of AME for MAPF-DP instances with different numbers of agents on 30x30 4-neighbor grids with 10% randomly blocked cells and delay probability range (0, 1/2).

agents 50 100 150 200 solved (%) 0.94 0.68 0.10 0 runtime (s) 0.166 4.668 134.155 approximate average makespan 69.32 78.48 81.77 average makespan 75.19 87.29 96.43 messages 474.62 1,554.71 2,940.40



max t 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20



runtime (s) 0.073 0.525 0.356 0.311 0.623 0.346 0.236 0.779 1.751 2.528 1.374 0.683 2.583 1.414 7.072 2.116 3.410 5.708 7.812



approximate average makespan 77.92 123.92 144.61 133.55 168.51 264.78 333.09 260.58 307.63 337.15 323.87 381.63 440.94 470.06 554.32 451.32 763.44 462.71 490.26



average makespan 84.30  0.42 131.12  0.79 157.88  0.96 157.00  0.98 192.76  1.46 279.51  2.05 349.72  2.69 271.71  2.28 336.95  2.26 375.46  2.74 383.25  2.53 413.18  3.19 498.30  3.32 524.94  3.95 607.20  4.26 570.15  3.90 782.40  6.08 666.42  5.29 591.35  3.73



messages 251 301 287 278 299 289 293 294 305 312 300 282 278 295 316 275 306 309 323



Experiment 4: Plan-Execution Policies

We use AME with 3 plan-execution policies, namely 1) MCPs, 2) FSPs and 3) dummy (non-robust) plan-execution policies that always provide GO commands. We repeat Experiment 1 for each plan-execution policy. Table 4 reports for each solved MAPF-DP instance and plan-execution policy the average makespan over 1,000 plan-execution runs together with 95%-confidence intervals, the number of sent messages for MCPs and FSPs and the average number of collisions for dummy plan-execution policies. The number of sent messages is zero (and thus not shown) for dummy plan-execution policies since, different from MCPs and FSPs, they do not prevent collisions. The average makespan for MCPs seems to be only slightly larger than that for dummy plan-execution policies, and the average makespan and number of sent messages for MCPs seem to be smaller than those for FSPs.



1, and Figure 5 visualizes the results. Larger delay probability ranges seem to result in larger runtimes, approximate average makespans calculated by AME and average makespans (although there is lots of noise). The differences between the approximate average makespans calculated by AME and average makespans are larger as well but remain reasonable.



Experiment 3: Numbers of Agents

We use AME with different numbers of agents. We repeat Experiment 1 with 50 MAPF-DP instances in 30x30 4neighbor grids generated as in Experiment 1 for each number of agents. Table 3 reports the same measures as used in Experiment 1, averaged over all MAPF-DP instances that were solved within a runtime limit of 5 minutes. AME solves most MAPF-DP instances with 50 agents and then degrades gracefully with the number of agents.



Conclusions

In this paper, we formalized the Multi-Agent Path-Finding Problem with Delay Probabilities (MAPF-DP) to account for imperfect plan execution and then developed an efficient way of solving it with small average makespans, namely with Approximate Minimization in Expectation (a 2-level MAPF-DP solver for generating valid MAPF-DP plans) and Minimal Communication Policies (decentralized robust plan-execution policies for executing valid MAPF-DP plans without collisions).



Table 4: Results of AME for the 18 solved MAPF-DP instances from Experiment 1 and different plan-execution policies.

Dummy Plan-Execution Policies average average average average id messages messages makespan makespan makespan collisions random 1 71.28  0.34 267 140.29  0.50 23,109 67.82  0.35 16.68 random 2 73.02  0.29 257 143.55  0.55 19,316 71.96  0.31 14.27 84.90  0.40 373 160.43  0.59 24,098 81.20  0.37 27.71 random 3 random 4 72.89  0.37 251 141.71  0.52 19,587 69.16  0.36 25.38 random 5 73.98  0.31 255 141.49  0.54 20,794 69.59  0.32 14.98 random 6 66.98  0.36 257 115.98  0.51 20,597 66.76  0.37 15.19 random 7 71.34  0.36 269 124.03  0.54 20,481 70.79  0.38 16.53 random 8 51.72  0.35 164 96.04  0.46 16,665 51.65  0.38 8.81 61.30  0.27 247 113.76  0.46 20,976 58.52  0.23 10.33 random 9 random 10 64.77  0.38 234 114.04  0.50 19,834 64.00  0.38 17.51 warehouse 1 124.18  0.44 705 219.63  0.65 28,794 122.42  0.42 34.59 762 235.35  0.72 34,154 124.40  0.60 68.68 warehouse 2 124.63  0.51 warehouse 3 117.00  0.53 609 206.29  0.65 26,647 117.89  0.54 29.61 warehouse 4 117.31  0.52 541 194.07  0.59 24,889 116.02  0.53 28.09 warehouse 6 131.10  0.59 710 205.54  0.71 29,462 131.54  0.60 37.41 warehouse 7 96.54  0.34 488 187.90  0.59 22,401 95.80  0.35 24.91 warehouse 9 107.33  0.42 462 187.80  0.56 18,950 105.63  0.45 22.21 warehouse 10 127.36  0.53 909 226.95  0.73 32,903 127.59  0.55 43.78 MCPs FSPs



References

Aho, A. V.; Garey, M. R.; and Ullman, J. D. 1972. The transitive reduction of a directed graph. SIAM Journal on Computing 1(2):131-137. Becker, R.; Zilberstein, S.; Lesser, V.; and Goldman, C. V. 2004. Solving transition independent decentralized Markov decision processes. Journal of Artificial Intelligence Research 22(1):423-455. Boutilier, C. 1996. Planning, learning and coordination in multiagent decision processes. In Conference on Theoretical Aspects of Rationality and Knowledge, 195-210. Boyarski, E.; Felner, A.; Stern, R.; Sharon, G.; Tolpin, D.; Betzalel, O.; and Shimony, S. E. 2015. ICBS: Improved conflict-based search algorithm for multi-agent pathfinding. In International Joint Conference on Artificial Intelligence, 740-746. Cohen, L.; Uras, T.; Kumar, T. K. S.; Xu, H.; Ayanian, N.; and Koenig, S. 2016. Improved solvers for bounded-suboptimal multiagent path finding. In International Joint Conference on Artificial Intelligence, 3067-3074. Goldenberg, M.; Felner, A.; Stern, R.; Sharon, G.; Sturtevant, N. R.; Holte, R. C.; and Schaeffer, J. 2014. Enhanced Partial Expansion A*. Journal of Artificial Intelligence Research 50:141- 187. Goldman, C. V., and Zilberstein, S. 2004. Decentralized control of cooperative systems: Categorization and complexity analysis. Journal of Artificial Intelligence Research 22:143-174. H onig, W.; Kumar, T. K. S.; Cohen, L.; Ma, H.; Xu, H.; Ayanian, N.; and Koenig, S. 2016. Multi-agent path finding with kinematic constraints. In International Conference on Automated Planning and Scheduling, 477-485. Kornhauser, D.; Miller, G.; and Spirakis, P. 1984. Coordinating pebble motion on graphs, the diameter of permutation groups, and applications. In Annual Symposium on Foundations of Computer Science, 241-250. Kurniawati, H.; Hsu, D.; and Lee, W. S. 2008. SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Robotics: Science and Systems, 65-72. Liu, L., and Michael, N. 2016. An MDP-based approximation method for goal constrained multi-MAV planning under action uncertainty. In IEEE International Conference on Robotics and Automation, 56-62.



Luna, R., and Bekris, K. E. 2011. Push and Swap: Fast cooperative path-finding with completeness guarantees. In International Joint Conference on Artificial Intelligence, 294-300. Ma, H., and Koenig, S. 2016. Optimal target assignment and path finding for teams of agents. In International Conference on Autonomous Agents and Multiagent Systems, 1144-1152. Ma, H., and Pineau, J. 2015. Information gathering and reward exploitation of subgoals for POMDPs. In AAAI Conference on Artificial Intelligence, 3320-3326. Ma, H.; Tovey, C.; Sharon, G.; Kumar, T. K. S.; and Koenig, S. 2016. Multi-agent path finding with payload transfers and the package-exchange robot-routing problem. In AAAI Conference on Artificial Intelligence, 3166-3173. Melo, F. S., and Veloso, M. 2011. Decentralized MDPs with sparse interactions. Artificial Intelligence 175(11):1757-1789. Morris, R.; Pasareanu, C.; Luckow, K.; Malik, W.; Ma, H.; Kumar, S.; and Koenig, S. 2016. Planning, scheduling and monitoring for airport surface operations. In AAAI-16 Workshop on Planning for Hybrid Systems, 608-614. Scharpff, J.; Roijers, D. M.; Oliehoek, F. A.; Spaan, M. T. J.; and de Weerdt, M. M. 2016. Solving transition-independent multiagent MDPs with sparse interactions. In AAAI Conference on Artificial Intelligence, 3174-3180. Sharon, G.; Stern, R.; Goldenberg, M.; and Felner, A. 2013. The increasing cost tree search for optimal multi-agent pathfinding. Artificial Intelligence 195:470-495. Sharon, G.; Stern, R.; Felner, A.; and Sturtevant, N. R. 2015. Conflict-based search for optimal multi-agent pathfinding. Artificial Intelligence 219:40-66. Silver, D. 2005. Cooperative pathfinding. In Artificial Intelligence and Interactive Digital Entertainment, 117-122. Standley, T. S. 2010. Finding optimal solutions to cooperative pathfinding problems. In AAAI Conference on Artificial Intelligence, 173-178. Velagapudi, P.; Varakantham, P.; Sycara, K. P.; and Scerri, P. 2011. Distributed model shaping for scaling to decentralized POMDPs with hundreds of agents. In International Conference on Autonomous Agents and Multi-agent Systems, 955-962. Veloso, M.; Biswas, J.; Coltin, B.; and Rosenthal, S. 2015. CoBots: Robust symbiotic autonomous mobile service robots. In International Joint Conference on Artificial Intelligence, 4423- 4429. Wagner, G., and Choset, H. 2015. Subdimensional expansion for multirobot path planning. Artificial Intelligence 219:1-24. Wagner, G. 2015. Subdimensional Expansion: A Framework for Computationally Tractable Multirobot Path Planning. Ph.D. Dissertation, Carnegie Mellon University. Wang, K., and Botea, A. 2011. MAPP: a scalable multi-agent path planning algorithm with tractability and completeness guarantees. Journal of Artificial Intelligence Research 42:55-90. Wurman, P. R.; D'Andrea, R.; and Mountz, M. 2008. Coordinating hundreds of cooperative, autonomous vehicles in warehouses. AI Magazine 29(1):9-20.



Multiscale Manifold Learning

Chang Wang

IBM T. J. Watson Research Lab 1101 Kitchawan Rd Yorktown Heights, New York 10598 wangchan@us.ibm.com



Sridhar Mahadevan

Computer Science Department University of Massachusetts Amherst, Massachusetts 01003 mahadeva@cs.umass.edu



Abstract

Many high-dimensional data sets that lie on a lowdimensional manifold exhibit nontrivial regularities at multiple scales. Most work in manifold learning ignores this multiscale structure. In this paper, we propose approaches to explore the deep structure of manifolds. The proposed approaches are based on the diffusion wavelets framework, data driven, and able to directly process directional neighborhood relationships without ad-hoc symmetrization. The proposed multiscale algorithms are evaluated using both synthetic and real-world data sets, and shown to outperform previous manifold learning methods.



Introduction

In many application domains of interest, from information retrieval and natural language processing to perception and robotics, data appears high dimensional, but often lies near or on low-dimensional structures, such as a manifold or a graph. By explicitly modeling and recovering the underlying structure, manifold learning methods (Belkin and Niyogi 2003; Roweis and Saul 2000; He and Niyogi 2003) have been shown to be significantly more effective than previous dimensionality reduction methods. Many existing manifold learning approaches are largely based on extending classical Fourier analysis to graphs and manifolds. In particular, spectral graph theory (Chung 1997) combined with classical differential geometry and global analysis on manifolds forms the theoretical basis for "Laplacian" techniques for function approximation and learning on graphs and manifolds, using the eigenfunctions of a Laplace operator naturally defined on the data manifold to reveal hidden structure. While Fourier analysis is a powerful tool for global analysis of functions, it is known to be poor at recovering multiscale regularities across data and for modeling local or transient properties (Mallat 1998). Consequently, one limitation of these techniques is that they only yield a "flat" embedding but not a multiscale embedding. However, when humans try to solve a particular problem (such as natural language processing), they often exploit their intuition about how to decompose the problem into sub-problems and construct multiple levels of representation. As a consequence, there has been rapidly

Copyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



growing interest in the problem of "deep learning", wherein learning methods are designed that construct multiple layers of latent representations from data (Hinton and Salakhutdinov 2006; Hinton, Osindero, and Teh 2006; Lee et al. 2007; Bengio 2009). Another problem with such Fourier analysis based methods is that they cannot handle the relationships characterized by directed graphs without some ad-hoc symmetrization. Some typical examples where non-symmetric matrices arise are when k -nearest neighbor relationships are used, in information retrieval/data mining applications based on network topology (Shin, Hill, and Raetsch 2006), and state space transitions in a Markov decision process. For a general weight matrix W representing the edge weights on a directed graph, its eigenvalues and eigenvectors are not guaranteed to be real. Many current approaches to this problem convert the directed graphs to undirected graphs. A simple solution is setting W to be W + W T or W W T . It is more desirable to find an approach that handles directed graphs without the need for symmetrization. To address the need for multiscale analysis and directional neighborhood relationships, we explore multiscale extensions of Fourier analysis based approaches using wavelet analysis (Mallat 1998). Classical wavelets in Euclidean spaces allow a very efficient multiscale analysis much like a highly flexible tunable microscope probing the properties of a function at different locations and scales. Diffusion wavelets (DWT) (Coifman and Maggioni 2006) extends the strengths of classical wavelets to data that lie on graphs and manifolds. The term diffusion wavelets is used because it is associated with a diffusion process that defines the different scales, allows a multiscale analysis of functions on manifolds and graphs. We focus on multiscale extensions of Laplacian eigenmaps (Belkin and Niyogi 2003) and LPP (He and Niyogi 2003). Laplacian eigenmaps constructs embeddings of data using the low-order eigenvectors of the graph Laplacian as a new coordinate basis (Chung 1997), which extends Fourier analysis to graphs and manifolds. Locality Preserving Projections (LPP) is a linear approximation of Laplacian eigenmaps. Our paper makes the following specific contributions: (1) We investigate the relationships between DWT and (multiscale) Laplacian eigenmaps and LPP. To extend LPP to a multiscale variant requires solving a generalized eigenvalue



problem using diffusion wavelets. This extension requires processing two matrices, and was not addressed in previous work on diffusion wavelets. (2) We also show how to apply the method to directed (non-symmetric) graphs. Previous applications of diffusion wavelets did not focus on nonsymmetric weight matrices. Similar to Laplacian eigenmaps and LPP, our approach represents the set of instances by vertices of a graph, where an edge is used to connect instances x and y using a distance measure, such as if y is among the k -nearest neighbors of x. The weight of the edge is specified typically using either a symmetric measure, such as the heat kernel or a non-symmetric measure, such as a directional relationship induced by non-symmetric actions in a Markov decision process. Such pairwise similarities generate a transition probability matrix for a random walk P = D-1 W , where W is the weight matrix, and D is a diagonal "valency" matrix of the row-sums of W . In contrast to almost all previous graph-based eigenvector methods, we do not require W to be symmetric. In Laplacian eigenmaps and LPP, dimensionality reduction is achieved using eigenvectors of the graph Laplacian. In the new approach, we use diffusion scaling functions, which are defined at multiple scales. In the special case of symmetric matrices, these span the same space as selected spectral bands of eigenvectors. The remainder of this paper is organized as follows. The next section discusses the diffusion wavelets model. Then, we explain the main multiscale manifold learning algorithms and the rationale underlying our approaches. We finish with a presentation of the experimental results and conclusions.



{j , Tj } = DW T (T, 0 , QR, J, ) //INPUT: //T : Diffusion operator. 0 : Initial (unit vector) basis matrix. QR: A modified QR decomposition. //J : Max step number. This is optional, since the algorithm automatically terminates. //: Desired precision, which can be set to a small number or simply machine precision. //OUTPUT : j : Diffusion scaling functions at scale j . Tj = j  [T 2 ]j . j F or j = 0 to J - 1{ j j +1 j  ([j +1 ]j , [T 2 ]j )  QR([T 2 ]j ,  ); j [T 2 }

j +1 j +1 j +1 = ([T 2 ]j [j +1 ]j )2 ; ] j +1







j







Figure 1: Diffusion Wavelets construct multiscale representations

denotes matrix T whose colat different scales. The notation [T ]b a umn space is represented using basis b at scale b, and row space is represented using basis a at scale a. The notation [b ]a denotes basis b represented on the basis a . At an arbitrary scale j , we  have pj basis functions, and length of each function is lj . [T ]b is a a pb x la matrix, [b ]a is an la x pb matrix. Typically the initial basis for the algorithm 0 is assumed to be the delta functions (represented by an identity matrix), but this is not strictly necessary.





j 0



T



2



j



QR Decomposition [j+1] [T ] 

2

j



Extended Bases

[j+1]

0



j+1 j



j



.



Diffusion Wavelets Model

The procedure for performing multiscale decompositions using diffusion wavelets and the relevant notation are explained in Figure 1. The main procedure can be explained as follows: an input matrix T is orthogonalized using an approximate QR decomposition in the first step. T 's QR decomposition is written as T = QR, where Q is an orthogonal matrix and R is an upper triangular matrix. The orthogonal columns of Q are the scaling functions. They span the column space of matrix T . The upper triangular matrix R is the representation of T on the basis Q. In the second step, we compute T 2 . Note this is not done simply by multiplying T by itself. Rather, T 2 is represented on the new basis Q: T 2 = (RQ)2 . This result is based on matrix invariant subspace theory (Stewart and Sun 1990). Since Q may have fewer columns than T , T 2 may be a smaller square matrix. The above process is repeated at the next level, generating j compressed dyadic powers T 2 , until the maximum level is reached or its effective size is a 1 x 1 matrix. Small powers of T correspond to short-term behavior in the diffusion process and large powers correspond to long-term behavior. Scaling functions are naturally multiscale basis functions because they account for increasing powers of T (in particular, j the dyadic powers 2j ). At scale j , the representation of T 2 is compressed based on the amount of remaining information and the precision we want to keep. Figure 2 illustrates this procedure.

1



.



.

J-1



Figure 2: Multiscale diffusion analysis.



We use the "Olivetti Faces" data to illustrate the difference between eigenvector basis and diffusion wavelets basis (scaling functions). The dataset contains 200 face images represented over pixels. The well-known eigenface approach (Turk and Pentland 1991) first computes the pixelpixel covariance matrix, and then computes the corresponding eigenvectors. Each eigenvector is an "eigenface". Using this approach, each image can be written as a linear combination of eigenfaces. In our approach, we start with the same covariance matrix, but we use diffusion wavelets instead of eigenvectors. Each column of [j ]0 is used as a "diffusion face". Diffusion wavelets model identifies a 4 level hierar-



Figure 3: All 9 Diffusion Wavelets Basis Functions at Level 3.



1. Construct diffusion matrix T characterizing the given data set: * T = I - L is an n x n diffusion matrix. 2. Construct multiscale basis functions using diffusion wavelets: * {j , Tj } = DW T (T, I, QR, J, ). * The resulting [j ]0 is an n x pj matrix (Equation (1)). 3. Compute lower dimensional embedding (at level j ): * The embedding xi  yi = row i of [j ]0 .



Figure 4: Selected Diffusion Wavelets Basis Functions at Level 1.



1. Construct relationship matrix T characterizing the given data set: * T = (F + X LX T (F T )+ )+ is an r x r matrix.. 2. Apply diffusion wavelets to explore the intrinsic structure of the data: * {j , Tj } = DW T (T, I, QR, J, ). * The resulting [j ]0 is an r x pj matrix (Equation (1)). 3. Compute lower dimensional embedding (at level j ): * The embedding xi  yi = ((F T )+ [j ]0 ) xi .

T



Figure 5: Eigenfaces.

199 Bases 52 Bases 9 Bases 2 Bases



Figure 7: Top: Multiscale Laplacian Eigenmaps; Bottom: Multiscale LPP.



Figure 6: Image Reconstruction at Different Scales chy of diffusionfaces, and dimensionality of each level is: 199, 52, 9, 2. We plot all 9 diffusionfaces at level 3 in Figure 3, and the top 24 diffusionfaces at level 1 in Figure 4. We also plot the top 24 eigenfaces in Figure 5. It is clear that these two types of basis are quite different: eigenvectors are global, and almost all such bases model the whole face. Diffusion faces are defined at multiple scales, where the finer scale (e.g. Figure 4) characterizes the details about each image, while the coarser scales (e.g. Figure 3) skip some of the details and only keep the lower frequency information. Scaling functions (especially those at low levels) are usually sparse, and most of them focus on just one particular feature on the face, like eyes and noses. Given an image written as a summation of diffusionfaces, we can estimate what the image looks like based on the coefficients (contributions) of each type of eyes, noses, etc. Figure 6 shows the face reconstruction results using diffusion faces at different scales.



Multiscale Manifold Learning

In this section, we discuss how to extend Laplacian eigenmaps and LPP to multiple scales using diffusion wavelets. Notation: X = [x1 , * * * , xn ] be an p x n matrix representing n instances defined in a p dimensional space. W



is an n x n weight matrix, where Wi,j represents the sim2 ilarity of xi and xj (Wi,j can be defined by e- xi -xj ). D is a diagonal valency matrix, where Di,i = j Wi,j . -0.5 -0.5 W = D WD . L = I - W , where L is the normalized Laplacian matrix and I is an identity matrix. XX T = F F T , where F is a p x r matrix of rank r. One way to compute F from X is singular value decomposition. (*)+ represents the Moore-Penrose pseudo inverse. (1) Laplacian eigenmaps minimizes the cost function 2 i,j (yi - yj ) Wi,j , which encourages the neighbors in the original space to be neighbors in the new space. The c dimensional embedding is provided by eigenvectors of Lx = x corresponding to the c smallest non-zero eigenvalues. The cost function for multiscale Laplacian eigenmaps 1 n is defined as follows: given X , compute Yk = [yk , * * * , yk ] i at level k (Yk is a pk x n matrix) to minimize i,j (yk - j 2 yk ) Wi,j . Here k = 1, * * * , J represents each level of the underlying manifold hierarchy. (2) LPP is a linear approximation of Laplacian eigenT maps. LPP minimizes the cost function i,j (f xi - f T xj )2 Wi,j , where the mapping function f constructs a c dimensional embedding, and is defined by the eigenvectors of X LX T x = XX T x corresponding to the c smallest non-zero eigenvalues. Similar to multiscale Laplacian eigenmaps, multiscale LPP learns linear mapping functions defined at multiple scales to achieve multilevel decompositions.



The Multiscale Algorithms

Multiscale Laplacian eigenmaps and multiscale LPP algorithms are shown in Figure 7, where [j ]0 is used to compute a lower dimensional embedding. As shown in Figure 1, the scaling functions [j +1 ]j are the orthonormal bases to span the column space of T at different levels. They define a set of new coordinate systems revealing the information in the original system at different scales. The scaling functions also provide a mapping between the data at longer spatial/temporal scales and smaller scales. Using the scaling functions, the basis functions at level j can be represented in terms of the basis functions at the next lower level. In this manner, the extended basis functions can be expressed in terms of the basis functions at the finest scale using:

[j ]0 = [j ]j -1 [j -1 ]0 = [j ]j -1 * * * [1 ]0 [0 ]0 , (1)



Since the columns of both V1:pj and [j ]0 are orthonormal, it is easy to verify that

T T V1: pj V1:pj = [j ]0 [j ]0 = I,



where I is a pj x pj identity matrix. So

T T T V1:pj = V1:pj V1: pj V1:pj = [j ]0 [j ]0 V1:pj = [j ]0 ([j ]0 V1:pj ).



Next, we show Q = [j ]T 0 V1:pj is a rotation matrix.

T T T T QT Q = V1: pj [j ]0 [j ]0 V1:pj = V1:pj V1:pj V1:pj V1:pj = I.

T T T QQT = [j ]T 0 V1:pj V1:pj [j ]0 = [j ]0 [j ]0 [j ]0 [j ]0 = I.



det(QT Q) = (det(Q))2 = 1 = det(Q) = 1



So Q is a rotation matrix. The embeddings constructed by LPP reduces to solving the generalized eigenvalue decomposition X LX T x = XX T x, where we have two input matrices X LX T and XX T to process. However, using the DW T procedure requires converting the generalized eigenvalue decomposition to a regular eigenvalue decomposition problem (with one input matrix). Theorem 2: Solution to generalized eigenvalue decomposition X LX T v = XX T v is given by ((F T )+ x, ), where x and  are eigenvector and eigenvalue of F + X LX T (F T )+ x = x. Proof: We know XX T = F F T , where F is a p x r matrix of rank r. Case 1: When XX T is positive definite: It follows immediately that r = p. This implies that F is an p x p full rank matrix: F -1 = F + .

X LX T v = XX T v = X LX T v = F F T v = X LX T v = F F T (F T )-1 F T v = X LX T v = F (F T v ) = X LX T (F T )-1 (F T v ) = F (F T v ) = F -1 X LX T (F T )-1 (F T v ) = (F T v )



where each element on the right hand side of the equation is created by the procedure shown in Figure 1. In our approach, [j ]0 is used to compute lower dimensional embeddings at multiple scales. Given [j ]0 , any vector/function on the compressed large scale space can be extended naturally to the finest scale space or vice versa. The connection between vector v at the finest scale space and its compressed representation at scale j is computed using the equation [v ]0 = ([j ]0 )[v ]j . The elements in [j ]0 are usually much coarser and smoother than the initial elements in [0 ]0 , which is why they can be represented in a compressed form.



Theoretical Analysis

It is well-known that regular Laplacian eigenmaps and LPP both return the optimal dimensionality reduction results with respect to the cost functions described at the beginning of this section (Belkin and Niyogi 2003; He and Niyogi 2003). If the input matrix is symmetric, there is an interesting connection between our algorithms and regular approaches. Theorem 1 and 3 below prove that the dimensionality reduction results produced by the proposed approaches at level k and the results from Laplacian eigenmaps and LPP (with top pk eigenvectors) are the same up to a rotation and a precision. So the proposed approaches are also optimal with respect to the same cost functions up to a precision. Theorems 2 proves some intermediate results, which are subsequently used in Theorem 3. One significant advantage of our approach is that it also directly generalizes to non-symmetric input matrices. Theorem 1: Laplacian eigenmaps (with eigenvectors corresponding to pj smallest non-zero eigenvalues) and Multiscale Laplacian eigenmaps (at level j ) return the same pj dimensional embedding up to a rotation Q and a precision. Proof: In Laplacian eigenmaps, we use row i of V1:pj to represent pj dimensional embedding of xi , where V1:pj is an n x pj matrix representing the pj smallest eigenvectors of L. When T = I - L, the largest eigenvectors of T are the smallest eigenvectors of L. Let [j ]0 represent the scaling functions of T at level j , then V1:pj and [j ]0 span the same space up to a precision (Coifman and Maggioni 2006), i.e.

T T V1:pj V1: pj = [j ]0 [j ]0 .



So solution to X LX T v = XX T v is given by ((F T )+ x, ), where x and  are eigenvector and eigenvalue of

F + X LX T (F T )+ x = x.



Case 2: When XX T is positive semi-definite, but not positive definite: In this case, r < p and F is a p x r matrix of rank r. Since X is a p x n matrix, F is a p x r matrix, there exits a matrix G such that X = F G. This implies G = F +X .

X LX T v = XX T v = F GLGT F T v = F F T v = F GLGT (F T v ) = F (F T v ) = (F + F )GLGT (F T v ) = (F T v ) = GLGT (F T v ) = (F T v ) = F + X LX T (F T )+ (F T v ) = (F T v )



So one solution to X LX T v = XX T v is ((F T )+ x, ), where x and  are eigenvector and eigenvalue of

F + X LX T (F T )+ x = x.



Note that the eigenvector solution to Case 2 is not unique.



Theorem 3: For any instance u, its embedding under LPP (using the top pj eigenvectors) is the same as its embedding under multiscale LPP (at level j ) up to a rotation and a precision. Proof: It is well known that the normalized graph Laplacian L is positive semi-definite (PSD), so F + X LX T (F T )+ is also PSD, and all its eigenvalues are  0. This implies that eigenvectors corresponding to F + X LX T (F T )+ 's smallest non-zero eigenvalues are the same as eigenvectors corresponding to (F + X LX T (F T )+ )+ 's largest eigenvalues. Let T = (F + X LX T (F T )+ )+ , [j ]0 (a p x pj matrix) represent the diffusion scaling functions of T at level j . From Theorem 1, it follows that V1:pj = [j ]0 Q where V1:pj is a p x pj matrix, represents the pj smallest eigenvectors of F + X LX T (F T )+ and Q is a rotation. Given an instance u (p x 1 vector): its embedding result with LPP is

T + ((F T )+ V1:pj )T u = V1: pj F u;



its embedding result with multiscale LPP is

+ + T ((F T )+ [j ]0 )T u = [j ]T 0 F u = QV1:pj F u.



eigenmaps with the original weight matrix W reconstructs the original structure, while both approaches based on symmetrized W fail. The reason that symmetrization does not work is that for the points (red) on the rim of the sphere, their 20 neighbors are mostly red points. For the points (yellow) in the middle, some of their 20 neighbors are red, since the yellow points are sparse. Symmetrizing the relationship matrix will add links from the red to the yellow. This is equal to reinforcing the relationship between the red and yellow, which further forces the red to be close to the yellow in the low dimensional space. The above process weakens the relationship between the red points. So in the 3D embedding, we see some red points are far away from each other, while the red-yellow relationship is well preserved. Directed Laplacian also fails to generate good embeddings in this task. Finally, all three linear dimensionality reduction approaches (LPP, multiscale LPP with W and W ) can reconstruct the original structure. A possible reason for this is that the strong linear mapping constraint prevents overfitting from happening for this task.

1

2 2



So, the two embeddings are the same up to a rotation Q and a precision.



(T)

1.5



0.8



(T2) (T )

4



1



1



0.6



0



Experimental Results

To test the effectiveness of our multiscale manifold learning approaches, we compared "flat" and "deep" multiscale approaches using dimensionality reduction tasks on both synthetic and real-wold data sets. It is useful to emphasize that the intrinsic structure of the data set does not depend on the parameters. The structure only depends on the given data. The input parameters decide the way to explore the structure. The time complexity of the proposed approaches are similar to the corresponding eigenvector based approaches.



0.5



-1



0.4

0 1 0.5 0 -0.5 -1 -1 -0.5 0.5 0 1 -2 2



0.2



1 0 -1 1 0 -1 -2 -2



2



0 0



200



400



600



800



(A)

0.06 0.04 0.05 0.02 0 -0.02 -0.05 -0.04 -0.06 0.1 0.05 0 -0.05 -0.1 -0.1 -0.05 0.05 0 0.1 -0.1 0.1 0.05 0 -0.05 0 0.1



(B)

0.1 0.05 0



(C)



-0.05



-0.1 0.1 0.1 0.05 0 -0.05 -0.1 -0.1 0.05 0 -0.05 -0.1 -0.1 -0.06 -0.08 -0.02 -0.04 0



(D)

1.5 0.06 0.04 0.02 1 0.5 0 -0.5 0



(E)

0 -0.01 -0.02 -0.03 -0.02 -0.04 -0.06 0.06 -0.04 -0.05 0.1 0.04 0.02 -0.05 0 -0.1 0 0.1 0.05 0.05 0 -0.05



(F)



Punctured Sphere Example

Consider the punctured sphere in Figure 8(A) based on 800 samples. We use the heat kernel to generate its weight matrix, and for each point, we compute the weights for 20 nearest neighbors (in each row). This results in a non-symmetric matrix W . To apply Laplacian eigenmaps and LPP, we symmetrize W : W = (W + W T )/2. Figure 8(B) shows the spectrums of W and its higher powers. The high powers have a spectrum that decays much more rapidly than the low powers. This spectral decay property is characteristic of "diffusion-like" matrices, particularly those generated by the k nearest neighbor similarity metric. The embedding results are in Figure 8(C)-(I). The results verify Theorem 1 and 3, showing multiscale approaches (using diffusion scaling functions at level j ) and eigenmap approaches (using top pj eigenvectors) result in the same embeddings up to a rotation and a precision. Furthermore, multiscale Laplacian eigenmaps can successfully identify the intrinsic structures of the data set. Dimensionality of the finest scales of multiscale Laplacian eigenmaps is 3, which is the intrinsic dimensionality of the given data. Also, among all four nonlinear dimensionality reduction approaches (Direct Laplacian (Chung 2005), Laplacian eigenmaps, Multiscale Laplacian eigenmaps with W and W ), only Multiscale Laplacian



-1 -1.5 0 -0.5 -1 -1.5 -2 -2 -1 1 0 2



0.1 0.05 0 -0.05 -0.1 -0.1



(G)



(H)



(I)



Figure 8: Punctured Sphere Example: (A) Puncture Sphere; (B)

Spectrum of W ; (C) Directed Laplacian with W ; (D) Laplacian eigenmaps with W ; (E) Multiscale Laplacian eigenmaps with W (finest scale); (F) Multiscale Laplacian eigenmaps with W (finest scale); (G) LPP with W ; (H) Multiscale LPP with W ; (I) Multiscale LPP with W .



Citation Graph Mining

The citation data set in KDD Cup 2003 consists of scientific papers (about 29, 000 documents) from arXiv.org. These papers are from high-energy physics. They cover the period from 1992 through 2003. We sampled 3,369 documents from the data set and created a citation graph, i.e. a set of pairs of documents, showing that one paper references another. To evaluate the methods, we need to assign each document a class type. To compute this, we first represent each paper using a TF-IDF vector based on the text of its abstract and the title, then we use the dot product to



0.8



NSF Research Awards Abstracts Data

We also ran a test on a selected set of the NSF research awards abstracts (Frank and Asuncion 2010), which includes 5,000 abstracts describing NSF awards for basic research. The data set is represented by bag-of-words and has already been cleaned. Each abstract has a corresponding label: "division" (37 different values). Using Multiscale Laplacian eigenmaps, a 9 level manifold hierarchy was automatically constructed. Dimensionality discovered at each level was: 5000, 3069, 3052, 2524, 570, 54, 20, 13, 9. We applied the same quantitative comparison approach as that used in the previous section to compare Multiscale Laplacian eigenmaps (level 5) and regular Laplacian eigenmaps (with varying numbers of eigenvectors: 100, 1200, 1600, 2000). The results are summarized in Figure 10. The 570 dimensional embedding returned the best results. From the figures, we can see that choosing an appropriate scale for embedding can help improve learning performance. Using too many or too few bases may result in a redundant feature space or loss of valuable information. Finding an appropriate value for dimensionality is quite difficult. In previous approaches, the users need to specify this value. Generally speaking, even though a given problem may have tens of thousands of instances, the number of levels identified by the new approach is a much smaller number (often < 10). Also, some levels are defined by either too many or too few features. This eliminates from consideration additional levels, usually leaving a handful of levels as possible candidates. In this example, we chose the space defined by 570 features, since the levels below and above this have too few or too many features, respectively. Manifold hierarchy is task independent. For different tasks, users can select the most appropriate level by testing his/her data at different levels. For simplicity, the paper focuses on selecting scaling functions at a single level, but the methods can be extended to use multiple levels together.



0.7



0.6



Correctness



0.5



0.4



0.3



0.2



Multiscale Laplacian Projections (directed) Laplacian Eigenmaps



0.1 1



2



3



4



5 K



6



7



8



9



10



Figure 9: Comparison of citation graph embeddings.



0.75



0.7



Probability of Matching



0.65



0.6



0.55



0.5



0.45



0.4



0.35



d=570 (multiscale) Laplacian eigenmaps, d=100 Laplacian eigenmaps, d=1200 Laplacian eigenmaps, d=1600 Laplacian eigenmaps, d=2000

2 3 4 5 6 7 8 9 10



1



K



Figure 10: Comparison of NSF abstracts embeddings (using

`division' as label).



compute the similarity between any two papers. Hierarchical clustering is used to assign each document with a class. As a result, we get 7 classes. We apply both Multiscale Laplacian eigenmaps and regular Laplacian eigenmaps to the citation graph (without using document contents). Since the input is a graph, LPP and multiscale LPP can not be used. Multiscale approach results in a 8 level hierarchy. Dimensionality of each level is: 3369, 1442, 586, 251, 125, 105, 94, 7. From the result, we can see that multiscale approach successfully identifies the real intrinsic dimensionality at the highest level: 7 classes. Obviously, the citation graph is non-symmetric, and to apply Laplacian eigenmaps, we symmetrize the graph as before. A leave-one-out test is used to compare the low dimensional embeddings. We first map the data to a d dimensional space (we run 10 tests: d = 10, 20, 30 * * * 100) using both multiscale approach (using basis functions at level 6) and regular Laplacian eigenmaps. For each document in the new space, we check whether at least one document from the same class is among its K nearest neighbors. The multiscale approach using a non-symmetric graph performs much better than regular Laplacian eigenmaps with a symmetric graph in all 10 tests. We plot the average performance of these tests in Figure 9. Laplacian eigenmaps is less effective because the citation relationship is directed, and a paper that is cited by many other papers should be treated as completely different from a paper that cites many others but is not cited by others.



Conclusions

This paper presents manifold learning techniques that yield a multiscale decomposition of high-dimensional data. The proposed approaches are based on the diffusion wavelets framework, and are data driven. In contrast to "flat" eigenvector based approaches, which can only deal with symmetric relationships, our approach is able to analyze non-symmetric relationship matrices without ad-hoc symmetrization. The superior performance of the multiscale techniques and some of their advantages are illustrated using both synthetic and real-world data sets.



Acknowledgments

This research is supported in part by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-101-0383, and the National Science Foundation under Grant Nos. NSF CCF-1025120, IIS-0534999, IIS-0803288, and IIS-1216467. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the AFOSR or the NSF.



References

Belkin, M., and Niyogi, P. 2003. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation 15:1373-1396. Bengio, Y. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning 2(1):1-127. Chung, F. 1997. Spectral graph theory. Regional Conference Series in Mathematics 92. Chung, F. 2005. Laplacians and the Cheeger inequality for directed graphs. Annals of Combinatorics 9. Coifman, R., and Maggioni, M. 2006. Diffusion wavelets. Applied and Computational Harmonic Analysis 21:53-94. Frank, A., and Asuncion, A. 2010. UCI machine learning repository. [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. He, X., and Niyogi, P. 2003. Locality preserving projections. In Proceedings of the Advances in Neural Information Processing Systems (NIPS). Hinton, G. E., and Salakhutdinov, R. R. 2006. Reducing the dimensionality of data with neural networks. Science 313:504-507. Hinton, G. E.; Osindero, S.; and Teh, Y. W. 2006. A fast learning algorithm for deep belief nets. Neural Computation 18:1527-1554. Lee, H.; Battle, A.; Raina, R.; and Ng, A. 2007. Efficient sparse coding algorithms. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 801-8084. Mallat, S. 1998. A wavelet tour in signal processing. Academic Press. Roweis, S., and Saul, L. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science 290:2323- 2326. Shin, H.; Hill, N.; and Raetsch, G. 2006. Graph-based semisupervised learning with sharper edges. In Proceedings of the European Conference on Machine Learning, 401-412. Stewart, G. W., and Sun, J. 1990. Matrix perturbation theory. Academic Press. Turk, M., and Pentland, A. 1991. Eigenfaces for recognition. Journal of Cognitive Neuroscience 3:71-86.



Online Transfer Learning for Differential Diagnosis Determination

Jie Xu , Daby Sow , Deepak Turaga , Mihaela van der Schaar

 



University of California at Los Angeles, Los Angeles, California 90095 IBM T. J. Watson Research Center, Yorktown Heights, New York 10598



Abstract

In this paper we present a novel online transfer learning approach to determine the set of tests to perform, and the sequence in which they need to be performed, in order to develop an accurate diagnosis while minimizing the cost of performing the tests. Our learning approach can be incorporated as part of a clinical decision support system (CDSS) with which clinicians can interact. The approach builds on a contextual bandit framework and uses online transfer learning to overcome limitations with the availability of rich training data sets that capture different conditions, context, test results as well as outcomes. We provide confidence bounds for our recommended policies, which is essential in order to build the trust of clinicians. We evaluate the algorithm against different transfer learning approaches on real-world patient alarm datasets collected from Neurological Intensive Care Units (with reduced costs by 20%).



Introduction

Recent advances in sensing and measurement technologies are enabling us to monitor complex human, engineered, physical, biological and chemical systems and processes in many sophisticated ways. This enables improved ability to understand the state of health of these systems, diagnose problems, and use this to design interventions to maximize health at varying timescales. However, while several such measurements can be made (e.g. by performing different tests on a patient), the decision on which test to perform and when to perform it remains a very challenging problem. Challenges stem from multiple factors: i) There are complex relationships between different attributes that are being measured. ii) Tests have varying degrees of costs associated with them (e.g. some tests are very expensive). iii) Tests are significantly impacted by context, i.e. the best set of tests, measurements and interventions may be different depending on the context in which it takes place. iv) The determination of tests is often challenged by the limited access to relevant data. For instance, existing patients datasets often have distributions that do not necessarily capture the information needed for the accurate diagnosis in a

c 2014, Association for the Advancement of Artificial Copyright  Intelligence (www.aaai.org). All rights reserved.



novel problem domain. Thus, the resulting diagnosis policies may perform poorly. This prompts the need for a system that can effectively perform context-specific diagnosis that maximizes diagnosis accuracy and minimizes test costs even when highly relevant data pertaining to the diagnosis decision is missing. In this paper, we present a novel decision support system that addresses these challenges by transferring knowledge from multiple related problem domains and incrementally learning the best policies (i.e. sequences of test) to adopt depending on the context of the diagnosis problem. These contexts can be exogenous facts or meta-data about the problem. In the medical setting, they can be patient's age, gender and weight. Note that the contexts are different than the endogenous testing results. The use of multiple related problem domains enables transferring knowledge from the most relevant domains for different diagnosis contexts; it also creates a way to measure the semantic similarity between contexts: contexts are similar if their most related existing domains are the same. The learned semantic similarity is then used to develop context-specific solutions in the novel problem domain. The proposed approach is able to provide diagnosis confidence bounds which are important to ensure the trust of domain professionals.



Related Work

Support systems for decision making have been extensively studied. A first strand of related research focuses on costsensitive learning (Turney 2000)(Greiner, Grove, and Roth 2002)(Zubek, Dietterich, and others 2004). A disadvantage of these approaches is that they rely on training datasets to learn the appropriate model. Our focus in this paper is on how to overcome the lack of initial training data by using transfer of knowledge from relevant datasets. The majority of the transfer learning literature assumes a single source from which knowledge can be transferred (Marx et al. 2005). Transfer learning from multiple sources is much more challenging; most works aiming to address this problem focus on classification problems (Duan et al. 2009)(Yao and Doretto 2010). In our considered setting, the target data arrives sequentially and the features are not given but need to be discovered (by performing various tests). Imitationtype transfer learning techniques are often adopted where source policies are applied to the target task initially, while the target task solution is learned gradually (Fern andez and



Entity (e.g. patient)



Test Results t x te n o C n tio a m r fo n i Recommend Diagnostic Diagnostic Action Policy Selection Source Policy 1 Source Policy K

Domain Expert (e.g. doctor)



Which further test to execute



?

Diagnose Case closed (Subsequent actions follow) Update



...



Target Policy



Figure 1: Computer-aided diagnosis system Veloso 2006). However, such works only consider the availability of a single source, while our work focuses on multiple sources. Our solution builds on the contextual bandit framework (Slivkins 2009). While conventional works on multiarmed bandits focus on learning the best policy (or policies) among an fixed set, our algorithm uses the learned semantic similarity between contexts to produce new context-specific target policies (which may be distinct from existing policies) using the data accumulated so far for the target domain.



At any point in time, the execution of tests on entities provides additional information on the entities. Such state transitions are probabilistic and specific to the domain. Let p(s |s, q ) denote the transition probability from state s to s when test q is executed. Since taking an action d  D always leads to a diagnosis and closes the current case, we have p(|s, d) = 1, d  D. Let c(q |s) denote the expected cost of performing test q on entities in state s. Let c(d|s) denote the expected cost of making a diagnose d on entities in state s. We unify these two types of costs in a cost function c : AxS  R as a mapping from the action space and the state space to a real value. In sum, we call the set of transition probabilities p and the diagnosis cost function c the problem parameters. These parameters are Markovian; they depend only on the last state. This is a reasonable approximation since a state represents all the knowledge revealed about the entity so far. The optimal diagnostic policy that minimizes the expected diagnostic cost in each state is defined using the Bellman equation:J (a|s) = c(a|s) + p(s |s, a)V (s ) where V (s ) = mina J (s |a ). Thus  opt = {aopt (s)}sS such that s, aopt (s) = arg mina J (a|s). Since the entity comes with the initial state sinit , the expected diagnostic reward is V (sinit ). With abuse of notation, we let V ( ) = V (sinit | ) denote the diagnostic cost by using  . If the problem parameters were known, then the optimal diagnostic policy making problem can be solved by backward induction using the estimated problem parameters from an existing dataset. Since state space size is exponential in the test set size, the complexity grows as the number of tests increases. Reducing the solution complexity of this problem is not the main focus of the present paper; we refer readers interested in this topic to existing work that provides efficient heuristics algorithms such as (Zubek, Dietterich, and others 2004).

s



Computer-aided Diagnosis System

We use    to denote the initial exogenous context information about the entity being analyzed, such as the patient's basic symptoms and personal medical profile (e.g. gender, age, weight, medical history etc.). Let Q = {1, 2, ..., N } denote the set of possible tests, N < . We assume that each test q  Q has a finite set of possible results, denoted by Oq . We also define an "unknown" test result to be assigned to tests that have not been performed. At any point in time, an entity is assigned with a state s that represents the known test results that have been performed. This state does not reflect the patient's medical condition but rather the knowledge about the patient with respect to the medical tests. This state evolves as more medical tests are executed. Let S denote the state space. The initial state of an entity is sinit q = unknown, q  Q. Depending on the current entity state s, the computer-aided diagnostic system either recommends new tests to be performed to extract more knowledge or recommends a diagnosis decision if it has enough information about the entity. Let the action space be A = {Q, D} where D represents the diagnosis space; they are kept fixed. We assume that if the expert follows a  D, then the diagnosis for the current entity case is closed and subsequent intervention actions follow. Let  be a special terminal state which denotes that the case is k k , ..., qn closed. For an entity k , let {q1 k } be the sequence of k tests that are executed and d be the final diagnosis decision. The diagnosis cost ck for this entity is defined as ck =  k k ck (qi ) + ck (dk ) where ck (qi ), i = 1, ..., nk

i{1,...,nk }



Transfer Learning in Diagnosis

One of the key challenges for many diagnosis systems is that access to relevant data is limited. In a medical setting, existing patient datasets often have distributions that do not necessarily capture the information needed for the accurate diagnosis in a novel problem domain. The resulting diagnosis policies constructed may perform poorly. To address this issue, we propose to efficiently reuse and transfer knowledge from other older domains to minimize as much as possible the diagnosis cost in the new domain. In what follows, we call the diagnosis problem in the new domain the target problem and the diagnosis problem in the old domain the source problem.



Algorithm

We consider an online setting where data on entities in the target domain are received in sequence, indexed by {1, 2, ..., k, ...}. Due to the lack of a training dataset in the new domain, it is initially impossible to construct a good policy for the target problem. Instead, we have a set of K source policies  constructed for K related source problems (e.g. similar diseases or datasets of patients with a similar demography). However, the exact relationship and the effectiveness of these source policies on the target problem



are the costs incurred by executing the tests, c (d ) is the costs due to incorrect diagnosis and   [0, 1] is a tradeoff factor. A diagnostic policy is defined as a set of actions that are recommended to the domain expert in the various states. Specifically, a policy is denoted by  = {a(s)}sS . Hence, given a diagnostic policy, after observing the entity state, the diagnostic system can recommend an action to the domain expert. Our goal is to develop diagnosis policies that minimize the diagnosis cost.



k



k



are unknown a priori. Our algorithm begins by exploring the source policies for entities in the target domain. After accumulating sufficient data on entities for the target problem, it builds the target policy using the information extracted from applying the source policies. The algorithm is provided next in Algorithm 1. The parameter k  [0, 1] is used to control when to adopt source policies and when to use the newly built target policies; it is decreasing in k and lim k = 0.

k



Algorithm 2 Policy Selection and Adaptive Clustering 1: Initialize H = , r  ( ) = 0, M ( ) = 0,   . 2: for each entity kt do 3: Determine active cluster C  Ht such that t  C 4: Case 1:    such that MC ( ) <  (t) 5: Randomly select among such policies  t =  6: Case 2:   , MC ( )   (t) 7: Select  t = arg min r C ( ).

8: Set MC ( t )  MC ( t ) + 1 9: (The diagnosis reward rt is observed.) 10: Update r C ( t )  11: Update   MC ( ) using all past cases.  12: if  MC ( )   (l) then 13: Uniformly partition C into 2W level-(l + 1) 14: clusters. 15: Update the set of active clusters Ht . 16: Update the counters and cost estimates for all 17: new clusters using the entity cases received 18: so far. 19: end if 20: end for

 



Algorithm 1 Transfer Learning with Multiple Sources 1: for each entity k do 2: With probability k , select a source policy to apply 3: With probability 1 - k , apply the target policy 4: After the current case is closed 5: Build the target policy using received data 6: end for In Algorithm 1, there are two major questions that remain to be addressed: which source policy to apply (line 3) and how to build the target policy (line 7). We discuss them next. Let (k1 , k2 , ..., kt , ...) be the subsequence of received entity cases where a source policy is adopted according to Algorithm 1. Without loss of generality, we normalize the entity context space to be   [0, 1]W where W is the context space dimension. We introduce some concepts of the algorithm as follows: 1) Entity cluster. An entity cluster is represented by the range of context information that is associated with entities in the cluster. In this paper, we will consider clusters with the form [iw 2-(l-1)w , (iw + 1)2-(l-1)w ] where iw  {0, 1, ..., 2(l-1)w - 1} for each context dimension w = 1, ..., W for some positive integer l. Such a cluster is called a level-l cluster. At each time kt when source policies are applied, the algorithm keeps a set of mutually exclusive clusters that cover the entire context space. We call these clusters the active clusters, and denote this set by Ht . Clearly, we have C Ht = , t. 2) Counters. For each active cluster C , the algorithm maintains several counters: for each source policy   , MC ( ) records the number of entity cases so far in which  is applied. 3) Diagnosis cost estimates. For each active cluster C , the algorithm also maintains the sample mean diagnosis cost estimate r C ( ) for each source policy   , using the observed diagnosis costs of cases that belong to C so far. The algorithm is described in Algorithm 2. When an entity case kt is received, the algorithm first checks which active cluster C  Ht it belongs to. Then it investigates counter MC ( ) for all    to see if there exists any under-explored source policy  such that MC ( )   (t, l) where  (t, l) is a time- and level-dependent control function. If there exists such an under-explored policy  , then the algorithm selects this policy for the current entity case. This is called an exploration step. If there does not exist any under-explored policy, then the algorithm selects the policy with the lowest cost estimate arg min r C ( ). This is called

 



is a level-dependent control function, the current cluster C is partitioned in to 2W level-(l + 1) clusters. From the next entity case on, C is deactivated and the new level-(l + 1) clusters are activated. We will show how to select the control functions  (t, l) and  (l) in the next section. Entity clusters for which the estimated best source policies are the same are considered to be similar and hence, they are grouped together to form a dataset from which the problem parameters can be estimated. Using these K set of parameters, we can produce K context-specific target policies.



Confidence Bound

We make the following widely adopted technical assumption below; however, this is not needed for running the algorithm. Assumption. (Lipschitz) For each   , there exists L > 0,  > 0 such that for all ,   , we have |V ( ) - V (  )|  L,   . The above assumption states that if the entity context information is similar, then the expected diagnosis cost by selecting the same diagnostic policy is also similar. We can derive a confidence level of the learned effectiveness of diagnosis policies as follows: Proposition. For any active level-l context cluster C , at any time when policy  has been adopted for MC times on entities which belong to C , then for any individual context   C , the following confidence relation between the estimated diagnosis reward reward holds  and the true diagnosis 2 P (|r C - V | > L( W /2-l ) + ) < e-2 MC .



Experiments

Real-world patient dataset

We test our proposed algorithm using an alarm data set obtained from the Columbia Medical Center neurological



an exploitation step. After the diagnosis cost of the current entity case is observed, the cost  estimate of the selected policy is updated. Moreover, if MC ( )   (l), where  (l)

 



Intensive Care Unit (ICU). This dataset contains over a million alarm events produced by patient monitoring systems for 581 patients.



K = 2, c1 = 0.3 K = 2, c1 = 0.5 K = 3, c1 = 0.3



Methodology

We artificially treat the patient alarm dataset on each day as a separate dataset. K such datasets are picked as the source datasets and another one is picked as the target dataset. Moreover, the target data is made available to the system in sequence. We treat each alarm as a medical test. Hence, only when the test is performed, the corresponding alarm status is revealed. In the experiments, we focus on predicting whether the patient will have at least one of the two secondary complications: Pneumonia and Respiratory failure. The set of tests (alarms) that we consider includes Bradycardia, Tachycardia, High Blood Pressure, Low Blood Pressure, High Respiratory Rate. We assign different costs to differen types of prediction errors. Specifically, we normalized the cost of a miss detection to be 1 and the cost of a false alarm to be c1 . A uniform cost c2 is assigned for the execution of any test. In the experiments, we use a single patient context: the APACHE II ("Acute Physiology and Chronic Health Evaluation II") score that evaluates the severity of illness of our patients upon admission in the ICU.



c2 0.01 0.005 0.001 0.01 0.005 0.001 0.01 0.005 0.001



EM 0.299 0.268 0.245 0.357 0.332 0.302 0.274 0.258 0.250



AT 0.259 0.241 0.233 0.309 0.290 0.277 0.260 0.239 0.226



MSTAB 0.246 0.231 0.224 0.312 0.282 0.262 0.265 0.232 0.224



Proposed 0.212 0.193 0.187 0.292 0.267 0.260 0.218 0.191 0.192



Table 1: Diagnosis cost comparison

Context Diag. cost Age 0.222 GCS ApacheII 0.225 0.193 ApachePhys 0.205



Table 2: Impact of contexts



Conclusion

In this paper, we proposed an online transfer learning approach for differential diagnosis determination and showed how it can be incorporated as part of a clinical decision support system to improve the diagnosis performance. We envision that the proposed methodology can also be used in other complex diagnosis systems besides clinical diagnosis, such as cyber-security, biological and mechanical diagnosis.



Baseline approaches

1) Empirical Diagnosis (EM): All medical tests are executed and all alarms are revealed. It predicts the complication if any of the alarms are positive. 2) Average Transfer (AT): In this approach, we combine all source datasets to estimate the average problem parameters and construct a new diagnostic policy. This average source policy is applied to the target cases while the target policy is gradually learnt. 3) MultiSourceTriAdaBoost (MSTAB): This is a modified version of the state-of-the-art MultiSourceTriAdaBoost algorithm in (Yao and Doretto 2010) for transfer learning with multiple sources. We modified the weight update structure to incorporate the diagnosis cost instead of a plain diagnosis accuracy. Since the original algorithm is an offline algorithm that assumes a training set for the target problem, we also extended it to produce an online version using batch updates as more target cases are received.



References

Duan, L.; Tsang, I. W.; Xu, D.; and Chua, T.-S. 2009. Domain adaptation from multiple sources via auxiliary classifiers. In Proceedings of the 26th Annual International Conference on Machine Learning, 289-296. ACM. Fern andez, F., and Veloso, M. 2006. Probabilistic policy reuse in a reinforcement learning agent. In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, 720-727. ACM. Greiner, R.; Grove, A. J.; and Roth, D. 2002. Learning cost-sensitive active classifiers. Artificial Intelligence 139(2):137-174. Marx, Z.; Rosenstein, M. T.; Kaelbling, L. P.; and Dietterich, T. G. 2005. Transfer learning with an ensemble of background tasks. Inductive Transfer 10. Slivkins, A. 2009. Contextual bandits with similarity information. arXiv preprint arXiv:0907.3986. Turney, P. 2000. Types of cost in inductive concept learning. In Workshop on Cost-Sensitive Learning at 17th International Conference on Machine learning. Yao, Y., and Doretto, G. 2010. Boosting for transfer learning with multiple sources. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 1855- 1862. IEEE. Zubek, V. B.; Dietterich, T. G.; et al. 2004. Pruning improves heuristic search for cost-sensitive learning. Technical report, Corvallis, OR: Oregon State University, Dept. of Computer Science.



Results

We report the diagnosis cost results for three sets of experiments in Table 1 for various parameters. The diagnosis cost is computed by averaging the diagnosis cost of patient cases from the target patient case set. In all experiments, the proposed transfer learning algorithm significantly outperforms the baseline approaches by reducing the diagnosis cost up to 20% against the best baseline approach. We also investigate the impact of different choices of contexts on the diagnosis performance. Table 2 shows the achieved diagnosis cost by using different contexts for K = 2, c1 = 0.3 and c2 = 0.005. The experiment results indicate that the ApacheII score is the best context in our problem. Nevertheless, the diagnosis cost by using any context is lower than those achieved by the baseline approaches.



Role-aware Conformity Influence Modeling and Analysis in Social Networks

Jing Zhang , Jie Tang , Honglei Zhuang , Cane Wing-Ki Leung and Juanzi Li

Department of Computer Science and Technology, Tsinghua University Department of Computer Science, University of Illinois at Urbana-Champaign Huawei Noah's Ark Lab zhangjing12@mails.tsinghua.edu.cn, {jietang, lijuanzi}@tsinghua.edu.cn, hzhuang3@illinois.edu, cane.leung@huawei.com

 



Abstract

Conformity influence is the inclination of a person to be influenced by others. In this paper, we study how the conformity tendency of a person changes with her role, as defined by her structural properties in a social network. We first formalize conformity influence using a utility function based on the conformity theory from social psychology, and then propose a probabilistic graphical model, referred to as Role-Conformity Model (RCM), for modeling the role-aware conformity influence between users by incorporating the utility function. We apply the proposed RCM to several academic research networks, and discover that people with higher degree and lower clustering coefficient are more likely to conform to others. We also evaluate RCM through the task of word usage prediction in academic publications, and show significant improvements over baseline models.



1



Introduction



In social networks, conformity influence is the inclination of a person to be influenced by others by yielding to perceived group pressure and copying the behavior and beliefs of others. The earliest study on conformity influence dates back to the 1930's by social psychologists (Jenness 1932; Sherif 1935). Since then, precedent work extensively studied how conformity affects individuals' actions. The wellknown experiments in (Asch 1955) showed that over 75% of people tend to conform to others in varying degrees. Existing work (Bernheim 1994; Cialdini and Goldstein 2003; Kelman 1958; Aronson, Wilson, and Akert 2007) has repeatedly verified the significant effect of conformity influence in our social life. With the rapid proliferation of online social networks such as Facebook and Twitter, quantitatively estimating the conformity tendency of each individual becomes more and more critical for applications such as viral marketing, social influence maximization, etc. Yet, research on conformity influence in online social networks is just beginning. For instance, Li et al. (Li, Bhowmick, and Sun 2011; 2013) studied the interplay between the influence and conformity of an individual, while Tang et al. (Tang, Wu, and

Copyright c 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



Sun 2013) proposed a probabilistic factor graph model that takes the effect of conformity into account when predicting user behavior. Both focus on modeling the effect of conformity influence at the individual level. In this paper, we explore whether it is possible to summarize several "roles", i.e., prototypes, to concisely describe the correlation between the conformity tendency of an individual and her structural features in a network. Such rolebased modeling and analysis of conformity influence is beneficial for applications such as recommendations, where data sparsity or the cold-start problem cannot be overlooked. It is also the key difference from existing social contagion studies (Kempe, Kleinberg, and Tardos 2003; Gruhl et al. 2004), which ignore the effect of roles in information diffusion. There are several questions to address when modeling the conformity tendency of individuals based on their roles. First, how to formalize the conformity theory in social psychology? Second, how can roles be incorporated into such formalization to model user actions? Furthermore, how can role-based conformity be used to real applications? We summarize our answers to these questions and our main results in what follows. Results We formalize conformity influence in terms of a utility function based on the conformity theory (Bernheim 1994), and justify the proposed utility function by proving the existence of Nash equilibria when users conduct actions according to it. We further incorporate the utility function into an application-oriented probabilistic model, known as the Role-Conformity Model (RCM), to describe user behaviors. To the best of our knowledge, this is the first attempt to formally connect the conformity theory from social psychology to a computational model. We apply the proposed model on several academic networks to observe correlations between people's latent roles and their conformity tendency. Interestingly, people with higher degree and lower clustering coefficient are more likely to conform to others. The phenomenon may be explained as that when a person has more collaborators with the structure among them more diverse (i.e., the collaborations between the neighbors in the local network are infrequent), she may become more open-minded and tend to accept new ideas from others. However, when the social circle is restricted to a few collaborators, the person will limit her mind and tend not to accept other ideas.



We evaluate the proposed RCM through the task of word usage prediction, and results indicate that our model performs much better (+3.6-4.1% in terms of average MAP and +7.1-47.4% in terms of average AUC) than the basic TF-IDF and PLSA.



2



Formalizing Conformity



In this section, we formalize conformity influence based on the conformity theory from social psychology in terms of a utility function, and prove the existence of Nash equilibria if all users in a network behave according to it. Conformity utility function The conformity theory suggests that heterogeneous preferences do result in heterogeneous behaviors (Bernheim 1994). Everyone in a group expresses her own individuality. Yet, even individualists succumb somewhat to the desire for status (esteem or popularity) and shade their choices toward the social norm. This is because people seeking status care about how someone else feels about them through their actions. They are therefore willing to suppress their individuality and conform to the social norm, worrying that even small departures from the social norm may seriously impair their popularity. We formalize the conformity theory in terms of a utility function. We use a binary value to represent whether a user vi adopts an action (yi = 1) or not (yi = 0). Given the decision yi , we model the utility vi obtained from her decision from two aspects. One is the individual's intrinsic utility in the absence of all other neighbors, the other is the esteem acquired through conforming: f (yi ) = (1 - i ) d(yi , y i ) + i

j N (i)



when vk+1 joined a k -network that has already arrived at a Nash equilibrium. We assume that the preferred selection of vk+1 is 1, i.e., y k+1 = 1. The proof is the same when y k+1 = 0. Given an existing k -network, we denote the number of vk+1 's neighbors with y = 1 as N 1 , and the number of vk+1 's neighbors with y = 0 as N 0 . Thus, the utility of vk+1 is calculated as: (1 - ) + N 1 if yk+1 = 1 f (vk+1 ) = N 0 if yk+1 = 0 The utility of a neighbor vi of vk+1 is represented as: f (yi ) = (1 - ) d(yi , y i )+ 

j N (i)



d(yi , yj )+  d(yi , yk+1 )



d(yi , yj )



(1)



where y i represents the intrinsic preferred selection of user vi , i represents the conformity tendency of vi , and N (i) denotes the neighbors of vi at the time when vi makes the decision. d(., .) is a metric that gives a utility of 1 when two decisions are the same, and 0 otherwise. Nash equilibria We provide an induction method to prove that there exists Nash equilibria if all users in a network make the decisions for a given action according to the utility function defined by Eq. (1). For brevity, we assume that the parameter  in Eq. (1) is fixed for different users. The proof is the same for different . The proof is straightforward when there is only one user in a network. For a network with two users, when their intrinsic preferred selections are the same, a Nash equilibrium exists because they will make the same decision. When their preferred selections are different,  determines the final selection. If  < 0.5, a Nash equilibrium exists because they will select their own preferences respectively. If  > 0.5, two Nash equilibria exist because they will both select y 1 or y 2 . Finally, we prove that if a Nash equilibrium exists in a network with k users (k -network), a Nash equilibrium will definitely exist in any (k +1)-network obtained by adding an additional user, vk+1 , to it. The general idea is to investigate whether the neighbors of vk+1 will change their decisions



Suppose (1 - ) + N 1 > N 0 , vk+1 will decide to adopt the action (i.e., yk+1 = 1). The proof is the same when (1 - ) + N 1 < N 0 . We observe that the neighbors with yi = 1 will not change their decisions. Otherwise, the utility obtained from the k network will decrease because the Nash equilibrium is damaged, and the utility obtained from vk+1 will also decrease because yi is changed differently from yk+1 . For the neighbors with yi = 0, if they change their decisions, the marginal utility is  - ci , where -ci is the decreased utility triggered from the k -network because the Nash equilibrium is damaged.  is the increased utility caused by vk+1 because yi is changed to be the same as yk+1 . If   ci , none of the neighbors will change their decisions. If  > ci , the neighbors will change their decisions. However, in such situation, vk+1 will not change back to 0, because the utility will be reduced from (1 - ) + (N 1 + 1) to (N 0 - 1). To summarize, we can find a Nash equilibrium when an additional user vk+1 is added to any k -network with a Nash equilibrium already arrived.



3



Role-Conformity Model (RCM)



The aforementioned conformity utility function presents elegant theoretical properties, although it is too simple for real cases. In this section, we further extend it into an application-oriented probabilistic model, named RoleConformity Model (RCM), to describe user behaviors. We introduce in the model discrete time slices from t = 1 to T , and two hidden variables for characterizing the "role" of a user as well as the "topic" of a certain action. Definition 1 Individual attributes At time slice t, each user vi is associated with an attribute vector of length H , where the h-th attribute's value is denoted by xi,t,h . Different network properties of vi , such as clustering coefficient and degree, can be used as individual attributes, with the choice of which being application-dependent. Definition 2 Role distribution We adopt the concept of "role" to summarize user attributes into several clusters. A user can play different roles at different time slices. Formally, we associate each user vi at each time slice t with a vector i,t  RR , where R is the number of roles in the R r model ( r=1 r i,t = 1). Each element i,t is the probability that user vi belongs to role r at t.



Definition 3 Topic distribution In social networks, a user is usually interested in multiple topics. Formally, each user vi is associated with a vector i  RK , where K is the K z z number of topics ( z=1 i = 1). Each i is the probability (intrinsic preference) of user vi choosing topic z . Model description Based on the above definitions, we explain the proposed Role-Conformity Model. The basic idea is that users' role distribution is determined by not only attributes but also actions. We use users' attributes to determine her role distribution, which is then used as a prior to guide the sampling process for users' actions. Specifically, the model consists of two parts. The first part models the generation of individual attributes. For an individual attribute, we first draw a role r from a multinomial distribution, and then draw the value of the attribute from a normal distribution with respect to r. The second part models the total utility of generating all the actions. Specifically, we extend the utility function in Eq. (1) to further incorporate the role and topic distributions of a user. Instead of binary actions, each user is now allowed to take a set of actions, denoted by W = {w}. When in role r, the utility function of vi taking an action w is defined as:   K K 1 w z w z w i,r = (1 - r ) i z + r j z (2) | N | i z =1 z =1

j Ni



A



CAd



Ad





AXT



j

S=1



i

S=0



r



x

T A



, 

H 



z



s





R





K



w

Nd D



Figure 1: Role-conformity model. Application example: We are given a bibliographic dynamic network Gt = (V t , E t ), where V t is the set of authors up to time t and E t is the set of coauthor relationships among them up to time t. Each author vi  V t is associated with an attribute vector xi , containing her individual attributes in the network. There is also a set of documents D, where each d  D can be represented by (Ad , Wd , Cd , t). Ad  V t stands for the author set of d, Wd is the list of words w in d, Cd  D is the set of documents cited by d, and t is the time slice when d is published. By regarding each word as an action, we can naturally plug this data set into RCM. The only concern is that we usually do not know which author wrote down which word w in a document d with multiple authors, thus we assume that each word is generated from an author randomly chosen from Ad . We also need to prudently define the neighborhood Ni of each author. Since the author is more likely to be influenced by the documents she is citing, we model Ni for author vi in document d by CAd = d Cd Ad . In this application, the individual features are defined based on the co-author network. The neighbors an author conforms with can also be their coauthors. However, we discover that conformity influence caused by coauthor relationships is not as significant as that by citation relationships. Thus we investigate conformity influence caused by citation relationships in this paper. We omit the analysis result for space limitation. Without loss of generality, we continue our discussions based on the bibliographic data set to provide more technical details about how to apply our model in a real application. Figure 1 summarizes the RCM on a bibliographic data set. Model learning We adopt the probabilistic interpretation of Eq. (2) and use maximum-likelihood estimation (MLE) for model learning. The likelihood for individual attribute generation can be written as:

A T H R



is a non-negative score of taking action w under where topic z , satisfying w w z = 1, Ni is the (directed) neighborhood of vi in the social network, and r is a weighting factor similar to i in Eq. (1). Note that we utilize a unified r for all the users in role r, to reduce the number of parameters. Since the neighborhood of different users in r can be different, we normalize the gain a user obtained from her neighbors by her neighborhood size. This modification does not affect the conclusion of Section 2 since it is equivalent to directly assigning the individual conformity tendency of r each user vi as i = |Ni |-(| Ni |-1)r . The extended utility function is more general than the one described in Eq. (1). It also has another probabilistic interpretation and can be regarded as the likelihood of generating action w, by tossing a coin s with distribution Bern(r ). Then, if s = 1, w is determined by the individual's intrinsic z w topic distribution and is drawn from P (w|i) = z  i z . Otherwise, w is influenced by the neighbors' topic distribuz w tion and is drawn from P (w|Ni ) = j Ni z j z /|Ni |. The complete generative process is summarized as: * For the h-th attribute of user vi at time slice t: - Draw a role r from multinomial distribution i,t ; - Draw the value of the attribute xi,t,h  N (r,h , r,h ). * For an action w conducted by user vi at time t: - Draw vi 's role r from multinomial distribution i,t ; w - Obtain the utility of action w denoted by i,r (or apply the probabilistic interpretation here). For a given data set, we need to learn the parameters i,t , r,h , r,h , as well as i , z and r . We provide an application example of RCM in what follows.



w z



L1 =

i=1 t=1 h=1 r =1



r i,t

2 2r,h



exp -



(xi,t,h - r,h ) 2 2r,h



2



The likelihood of action generation can be written as: R r w r =1 i,t r,i L2 = |Ad |

d,w iAd



The unified likelihood function is L = L1 L2 . It is intractable to directly solve L. Thus we optimize L1 and L2



by EM algorithm respectively at each iteration. The product operation provides a theoretical guarantee that the product of lower bounds is also the lower bound of the product. We explain the EM steps for L1 and L2 respectively as below. To optimize L1 , we first estimate the posterior distribution over r for each individual attribute xi,t,h in the E-step: 

r qi,t,h r i,t

2 2r,h



derived from both L1 and L2 : r i,t =

H r h=1 qi,t,h + R H r r =1 ( h=1 qi,t,h d,w



ar d,w,i

d,w



+



ar d,w,i )



exp -



=



(xi,t,h -r,h ) 2 2r,h



2



Please refer to the supplementary materials for derivation details.



R r =1







r i,t

2 2r,h



exp -



(xi,t,h -r,h )2 2 2r,h



4



Experiments



Then in M-step, we update parameters r,h , r,h by: r,h =

A T r i=1 t=1 qi,t,h xi,t,h A T r i=1 t=1 qi,t,h A i=1 T r t=1 qi,t,h (xi,t,h - A T r i=1 t=1 qi,t,h



In this section, we apply our proposed RCM on a public available academic research data set1 to investigate the conformity tendency of authors when they write papers.



4.1

r,h )2



Experimental Setup



r,h =



where r,h and r,h are the mean and variance of the h-th attribute in role r. In order to optimize the likelihood of action generation L2 , we first apply the E-step as: ar d,w,i = bd,w,i,r = cz d,w,i,r =

w r i,t r,i R r =1 w r i,t r,i 1 |CAd | K z w z =1 i z z w i z K z w z =1 i z j CAd 1 |CAd | K z w z =1 j z K z w j CAd z =1 j z



+



where for each w in d conducted by user vi , ar d,w,i is the posterior distribution over r, bd,w,i,r is the posterior distribution of conforming, and cz d,w,i,r is the posterior distribution over z topic z . And then in M-step, we update i , w z , and r as:

D z i  d=1 wNd r =1 D R R z ar d,w,i bd,w,i,r cd,w,i,r



 ar d,w,i (1 - bd,w,i,r )

j CAd



  cz d,w,j,r



+

d=1 wNd j Ad r =1



D



R z ar d,w,i bd,w,i,r cd,w,i,r



w z 

d=1 iAd r =1 D R



 ar d,w,i (1 - bd,w,i,r )

j CAd R



  cz d,w,j,r



+

d=1 iAd r =1 D



r =

d=1 wNd iAd r =1



ar d,w,i bd,w,i,r

w r w z = 1. The parameter i,t is



Data sets We collect the data sets as follows. We first select eight domains from computer science, including database and data mining (DB&DM), human computer interaction (HCI), system and high performance computing (HP), software engineering (SE), computational theory (CT), artificial intelligence and machine learning (AI&ML), computer networks (CN), as well as computer vision and multimedia (CV&MM). For each domain, we then collect all the papers from the well-known journals and conferences in the domain and the citation relationships among them. There are in total 231,728 papers, 269,508 authors and 347,735 citation relationships, where each author has on average 3.44 papers and each paper has on average 1.68 citation relationships. We design a task of word usage prediction to evaluate our proposed model. The objective of the task is to predict whether a user will write a given word in her paper title in a given time period. Using word usage patterns to study social behaviors has been adopted in existing literature such as (Danescu-Niculescu-Mizil et al. 2013). Specifically, we split each data set into training and test set. The training set contains the papers published in or before 2009, and the test set contains the papers published after 2009. We construct a coauthor network at each time slice and use the degree and clustering coefficient as the individual attributes at each time. Each paper can be viewed as a document with a list of words as the actions performed by the authors. We run our model on the training set and then predict the candidate words (all the words appeared in both the training and test set with stop words removed) that will be used for each candidate user (the user appeared in both the training and test set). The probability of one user using a word, P (w|i), is w calculated as the expectation of i,r in Eq. (2) over role r at time t, where t is the ending time of the training set, i.e., 2009 in our setting, and Ni in Eq. (2) is the collection of authors whose papers are cited by user vi within [t - , t]. We empirically set  as 3 years. Since the word usage prediction is more like a ranking problem, precision at top ranked results is preferred in evaluating the results. Specifically, given a candidate user vi , we rank all the candidate words based on P (w|i). We view the co-occurrence of word and user pairs in the test set as the ground truth and use P@5 (Precision of top-5 predictions),

1



where



z z i



= 1 and



http://arnetminer.org/citation/



P@10, Mean Average Precision (MAP), and area under the ROC curve (AUC) to evaluate the ranking results for each user and then aggregate the results for all the users together. Baselines We compare our model with TF-IDF, the traditional probabilistic latent semantic analysis (PLSA) (Hofmann 1999) and the citation influence model (CIM) (Dietz, Bickel, and Scheffer 2007). TF-IDF: In TF-IDF, the probability of a user using a word in the test set is calculated as the TF-IDF value of the user writing the word in the training set. We view a document as the aggregation of all the paper titles of a user to calculate TF-IDF value. PLSA: In PLSA, the probability of a user using a word is K z w calculated as P (w|i) = z=1 i z . This method ignores conformity influence and assumes that users write words only based on their intrinsic preferences. CTM: In CTM, the probability of one user using a word w is calculated as i,r in Eq. (2), where i is directly learned for each user in CTM, instead of for each role in RCM.



4.2



Experimental Results



Table 1 shows the performance of word usage prediction in the collected data sets from eight different domains. Better performance From Table 1, we can see that RCM clearly outperforms TF-IDF and PLSA on all the eight data sets (+3.6-4.1% in terms of average MAP and +7.1-47.4% in terms of average AUC). TF-IDF and PLSA predict word usage only based on the intrinsic preference of a given user, and ignore the situation where a user's topic distribution may change and become closer to her neighbors' topic distribution over time. TF-IDF performs worse because it directly counts the frequency of words, which may be sparse in paper titles. RCM also outperforms CIM on almost all the data sets. Although CTM also considers both the intrinsic preference of a user and her conformity tendency, it suffers from the problem of data sparsity. Specifically, CTM directly learns the conformity tendency of each user, which is very difficult to be estimated accurately when very few historical actions of the user and/or her neighbors are available for model learning. In contrast, our model clusters similar users (with similar individual attributes) into roles, and then learns the conformity tendency of each role. The data sparsity problem can be well avoided by RCM. Parameter Analysis There are two tunable parameters, K and R, in RCM. K is the number of topics, which has been analyzed in many previous research (Blei, Ng, and Jordan 2003). We experiment with different values of K, and observe that perplexity first rises and then stabilizes as K gets large. We then fix K = 30 where the perplexity is stable and then analyze the number of roles R. Figure 2 plots the correlation between MAP/AUC and the number of roles on the eight data sets. We find that both MAP and AUC present increasing trend at the very beginning and soon become stable when R gets large. The results indicate that our RCM model is insensitive to the number of roles. Finally, we empirically set R = 13 in our experiments.



Table 1: Performance of word usage prediction (%). Query Method P@5 P@10 MAP AUC TF-IDF 15.84 12.67 6.68 36.20 PLSA 20.10 15.49 9.26 77.61 DB&DM CIM 22.26 17.98 11.59 85.50 RCM 30.40 24.94 14.16 86.90 TF-IDF 13.57 11.42 5.40 27.59 PLSA 14.25 11.65 5.71 67.37 HCI CIM 18.67 15.34 8.12 73.39 RCM 19.16 15.40 8.92 75.32 TF-IDF 15.71 12.95 7.08 38.70 HP PLSA 17.33 14.39 8.47 79.96 CIM 19.62 16.25 10.83 88.67 RCM 20.57 17.12 11.37 89.21 TF-IDF 16.81 13.21 7.82 38.07 SE PLSA 4.20 2.60 2.60 81.15 CIM 21.43 16.42 12.16 85.55 RCM 25.31 19.98 12.54 85.27 TF-IDF 19.18 15.10 11.56 46.80 CT PLSA 17.52 13.37 9.88 81.09 CIM 19.36 14.50 11.04 85.31 RCM 20.13 15.20 11.46 85.93 TF-IDF 19.14 15.39 8.25 42.02 PLSA 19.92 15.50 9.40 84.10 AI&ML CIM 21.24 16.41 10.85 90.70 RCM 23.60 18.02 11.41 90.92 TF-IDF 20.03 17.51 8.71 37.23 CN PLSA 26.68 20.33 12.99 80.63 CTM 29.36 21.62 14.75 86.92 RCM 31.20 23.35 15.22 88.41 TF-IDF 17.19 14.19 8.18 41.65 CV&MM PLSA 19.88 14.78 09.64 78.85 CTM 22.09 16.12 11.10 85.02 RCM 24.49 18.37 11.50 85.63 TF-IDF 17.18 14.06 7.96 38.53 Avg PLSA 17.49 13.51 8.49 78.85 CTM 21.75 16.83 11.31 85.13 RCM 24.36 19.05 12.07 85.95



Correlation between role and conformity influence The learned parameter  by RCM represents the conformity tendency for different roles. The model also learns the mean value of each individual attribute for a role, i.e., r,h . Thus we can represent each role as a vector of the mean values of different network attributes and analyze the correlation between role and conformity tendency. We select two domains, DB&DM and HP for further discussions. Figure 3 shows the correlation between a role's mean degree and its conformity probability. We discover that the correlation follows a logarithm function. When fitting the data points, we first remove the roles with a small number of related users, where the number of related users with respect to a role r is estimated by summing up the probability r i,t over all vi and t. We try different forms of functions to fit the remaining data points and select logarithm function with the largest R2 . Figure 4 shows the correlation between a role's mean clustering coef-



2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20



R



2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20



0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07



0.95

Conformity probability



1.0

Conformity probability

Clustering coefficient



1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 1.2

Clustering coefficient



0.90

AUC(%)



0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 1.2



MAP(%)



0.85

DB&DM HCI HP SE CT AI&ML CN CV&MM



DB&DM HCI HP SE CT AI&ML CN CV&MM



0.80 0.75 0.70

R



(a) MAP



(b) AUC



(a) DB&DM



(b) HP



Figure 2: Role number analysis.

1.0

Conformity probability

Conformity probability



1.0 0.8 0.6 0.4 0.2 0.00 20 40 60 80 100 120 140 160

Degree



Figure 4: The correlation between mean clustering coefficient of role and conformity probability. from social psychology to a computational model. Social influence has been studied throughly. Kempe et al. (Kempe, Kleinberg, and Tardos 2003) first proposed two fundamental diffusion models to estimate the expected influence of given seed users. Bakshy et al. (Bakshy et al. 2012) and Bond et al. (Bond et al. 2012) conducted randomized controlled trials to identify the effect of social influence. Dietz et al. (Dietz, Bickel, and Scheffer 2007) and Liu et al. (Liu et al. 2012) used topic models to learn the influential strength between papers or users. Tang et al. (Tang et al. 2009) and Tan et al. (Tan et al. 2011) used discriminative models to learn the weights of different influence factors. Gruhl et al. (Gruhl et al. 2004), Saito et al. (Kimura et al. 2011) and Goyal et al. (Goyal, Bonchi, and Lakshmanan 2010) learned the influence probabilities of the time-decayed diffusion models. However, they all focus on modeling how users influence others, and ignore the inclination of the users to be influenced.



0.8 0.6 0.4 0.2 0.00 50 100 150 200 250 300

Degree



(a) DB&DM



(b) HP



Figure 3: The correlation between mean degree of role and conformity probability. ficient and its conformity probability. We discover that one kind of roles have clustering coefficient close to 0, and the other kind of roles follows an exponential function. Since papers are publicly published, anyone could read others' papers and have almost the same opportunity to be influenced by others. Thus the phenomenon in the two figures may be explained as: when a person collaborates with more authors and the coauthors are more structurally diverse (i.e., with a small clustering coefficient), she may become more openminded and tend to accept new ideas from others. However, when the social circle of the user is restricted to a few coauthors forming a dense collaboration network, the person will be more conservative and tend not to accept other ideas.



6



Conclusion



5



Related work



Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Considerable research (Asch 1955; Bernheim 1994; Cialdini and Goldstein 2003; Kelman 1958) has been conducted on the issue of conformity in social psychology. Recently, several studies on conformity have been conducted on large social networks. For example, Li et al. (Li, Bhowmick, and Sun 2011; 2013) studied the interplay between the influence and conformity. Tang et al. (Tang, Wu, and Sun 2013) proposed a factor graph model to quantify the effects of different conformity factors. However, both the studies do not consider the problem of data sparsity. An individual's conformity cannot be estimated accurately if her historical actions are few. To overcome this problem, we assign hidden roles to users and then learn the correlation between roles and conformity tendency. Besides, to the best of our knowledge, this is the first attempt to formally connect the conformity theory



We present the first attempt to connect the conformity theory from social psychology to a computational model. We first formalize conformity theory in terms of a utility function, and validate the utility function by proving the existence of Nash equilibria. We then extend and incorporate the utility function into a probabilistic topic model that takes the role and topic distributions of users into account. Our model allows for mining the correlation between users' hidden roles and conformity tendency. Our experiments on academic research networks show an interesting result that people with higher degree and lower clustering coefficient are more likely to conform to others. In addition, our method also outperforms several baselines on the task of word usage prediction in academic papers.



ACKNOWLEDGMENTS

The work is supported by the National High-tech R&D Program (No. 2014AA015103), National Basic Research Program of China (No. 2014CB340500, No. 2012CB316006), NSFC (No. 61222212, No. 61035004), NSFC-ANR (No. 61261130588), the Tsinghua University Initiative Scientific Research Program (20121088096), a research fund supported by Huawei Technologies Co. Ltd and Beijing key lab of networked multimedia.



References

Aronson, E.; Wilson, T.; and Akert, R. 2007. Social Psychology. Prentice Hall. Asch, S. 1955. Opinions and social pressure. Bakshy, E.; Eckles, D.; Yan, R.; and Rosenn, I. 2012. Social influence in social advertising: evidence from field experiments. In EC'12, 146-161. Bernheim, B. D. 1994. A theory of conformity. Journal of Political Economy 1027(5):841-877. Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. JMLR 3:993-1022. Bond, R. M.; Fariss, C. J.; Jones, J. J.; Kramer, A. D. I.; Marlow, C.; Settle, J. E.; and Fowler, J. H. 2012. A 61million-person experiment in social influence and political mobilization. Nature 489:295-298. Cialdini, R., and Goldstein, N. 2003. Social influence: Complicance and conformity. Annual Review of Psychology 55. Danescu-Niculescu-Mizil, C.; West, R.; Jurafsky, D.; Leskovec, J.; and Potts, C. 2013. No country for old members: User lifecycle and linguistic change in online communities. In WWW'13, 307-318. Dietz, L.; Bickel, S.; and Scheffer, T. 2007. Unsupervised prediction of citation influences. In ICML'07, 233-240. Goyal, A.; Bonchi, F.; and Lakshmanan, L. V. 2010. Learning influence probabilities in social networks. In WSDM'10, 241-250. Gruhl, D.; Guha, R.; Liben-Nowell, D.; and Tomkins, A. 2004. Information diffusion through blogspace. In WWW'04, 491-501. Hofmann, T. 1999. Probabilistic latent semantic indexing. In SIGIR'99, 50-57.



Jenness, A. 1932. The role of discussion in changing opinion regarding matter of fact. Abnormal and Social Psychology 279-296. Kelman, H. C. 1958. Compliance, identification, and internalization: Three processes of attitude change. Journal of Conflict Resolution 2(1):51-60. Kempe, D.; Kleinberg, J.; and Tardos, E. 2003. Maximizing the spread of influence through a social network. In KDD'03, 137-146. Kimura, M.; Saito, K.; Ohara, K.; and Motoda, H. 2011. Learning information diffusion model in a social network for predicting influence of nodes. Intell. Data Anal. 15:633- 652. Li, H.; Bhowmick, S. S.; and Sun, A. 2011. Casino: Towards conformity-aware social influence analysis in online social networks. In CIKM'11, 1007-1012. Li, H.; Bhowmick, S. S.; and Sun, A. 2013. Cinema: conformity-aware greedy algorithm for influence maximization in online social networks. In EDBT'13, 323-334. Liu, L.; Tang, J.; Han, J.; and Yang, S. 2012. Learning influence from heterogeneous social networks. DataMKD 25(3):511-544. Sherif, M. 1935. A study of some social factors in perception. Archives of Psychology 187. Tan, C.; Lee, L.; Tang, J.; Jiang, L.; Zhou, M.; and Li, P. 2011. User-level sentiment analysis incorporating social networks. In KDD'11, 1049-1058. Tang, J.; Sun, J.; Wang, C.; and Yang, Z. 2009. Social influence analysis in large-scale networks. In KDD'09, 807-816. Tang, J.; Wu, S.; and Sun, J. 2013. Confluence: Conformity influence in large social networks. In KDD'13, 347-355.



Grounded Fixpoints

Bart Bogaerts

Department of Computer Science KU Leuven 3001 Heverlee, Belgium bart.bogaerts@cs.kuleuven.be



Joost Vennekens

Department of Computer Science Campus De Nayer, KU Leuven 2860 Sint-Katelijne-Waver, Belgium joost.vennekens@cs.kuleuven.be



Marc Denecker

Department of Computer Science KU Leuven 3001 Heverlee, Belgium marc.denecker@cs.kuleuven.be



Abstract

Algebraical fixpoint theory is an invaluable instrument for studying semantics of logics. For example, all major semantics of logic programming, autoepistemic logic, default logic and more recently, abstract argumentation have been shown to be induced by the different types of fixpoints defined in approximation fixpoint theory (AFT). In this paper, we add a new type of fixpoint to AFT: a grounded fixpoint of lattice operator O : L  L is defined as a lattice element x  L such that O(x) = x and for all v  L such that O(v  x)  v , it holds that x  v . On the algebraical level, we show that all grounded fixpoints are minimal fixpoints approximated by the well-founded fixpoint and that all stable fixpoints are grounded. On the logical level, grounded fixpoints provide a new mathematically simple and compact type of semantics for any logic with a (possibly non-monotone) semantic operator. We explain the intuition underlying this semantics in the context of logic programming by pointing out that grounded fixpoints of the immediate consequence operator are interpretations that have no non-trivial unfounded sets. We also analyse the complexity of the induced semantics. Summarised, grounded fixpoint semantics is a new, probably the simplest and most compact, element in the family of semantics that capture basic intuitions and principles of various non-monotonic logics.



1



Introduction



Motivated by structural analogies in the semantics of several non-monotonic logics, Denecker, Marek, and Truszczy nski (2000) developed an algebraic theory that defines different types of fixpoints for a so-called approximating bilattice operator, called supported, Kripke-Kleene, stable and wellfounded fixpoints. In the context of logic programming, they found that Fitting's immediate consequence operator is an approximating operator of the two-valued immediate consequence operator and that its four different types of fixpoints correspond exactly with the four major, equally named semantics of logic programs. They also identified approximating operators for default logic (DL) and autoepistemic logic (AEL) and showed that the fixpoint theory induces all main

Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



and some new semantics in these fields (Denecker, Marek, and Truszczy nski 2003). By showing that Konolige's mapping from DL to AEL preserves the approximating operator, they resolved an old research question regarding the nature of these two logics: AEL and DL are "just" two different dialects of autoepistemic reasoning (Denecker, Marek, and Truszczy nski 2011). The study of these approximating operators is called approximation fixpoint theory (AFT). It is now commonly used to define semantics of extensions of logic programs, such as logic programs with aggregates (Pelov, Denecker, and Bruynooghe 2007) and HEX logic programs (Antic, Eiter, and Fink 2013). Vennekens, Gilis, and Denecker (2006) used AFT in an algebraic modularity study for logic programming, AEL and DL. Recently, Strass (2013) showed that many semantics from Dung's argumentation frameworks (AFs) and abstract dialectical frameworks (ADFs) can be obtained by direct applications of AFT and Bogaerts et al. (2014) defined the causal logic FO(C) as an instantiation of AFT. This work suggests that fixpoint theory, despite its high level of abstraction, captures certain fundamental intuitions and cognitive principles in a range of logics and sorts of human knowledge. It is this observation that provides the basic motivation for the present study. In Section 3, we extend AFT with a new type of fixpoint: a point x in a lattice L is a grounded fixpoint of operator O : L  L if O(x) = x and for all v  L such that O(x  v )  v , it holds that x  v . In Section 4, we discuss the relation between grounded fixpoints and the other fixpoints defined by AFT. In particular, we show that all (ultimate) stable fixpoints are grounded and that all grounded fixpoints are minimal fixpoints approximated by the (ultimate) well-founded fixpoint in the bilattice. In general there are minimal fixpoints that are not grounded, and grounded fixpoints that are not stable. If the well-founded fixpoint is "exact", the well-founded fixpoint is the unique grounded and stable fixpoint of O. Grounded fixpoints have several appealing properties. First of all, a grounded fixpoint is a purely algebraical concept. As such, it can be used in all fields where AFT is applied. Secondly, a first step in the application of AFT for a lattice operator O is to choose a bilattice operator that approximates O. In contrast, grounded fixpoints are defined directly in terms of the original operator O. Thirdly, their



definition formalises and generalises well-known intuitions. In the context of logic programming, which we discuss in Section 5, the algebraic results show that grounded fixpoints induce a semantics that is slightly more "liberal" than stable semantics: all stable models are grounded (i.e., we identified a property all stable models have in common) but also every grounded fixpoint is approximated by the well-founded model; the differences collapse in case the well-founded model is two-valued. We will see that for logic programming, this semantics is simple and intuitive: we show that the grounded fixpoints can be characterised in terms of a generalised notion of unfounded set. Contrary to the more common semantics of logic programs, grounded fixpoint semantics does not rely on any form of three-valued logic: It is defined directly in terms of the (twovalued!) immediate consequence operator. The grounded fixpoint semantics is very flexible towards language extensions. Currently, much research is being conducted in order to extend stable and well-founded semantics for logic programs with new language constructs (Pelov, Denecker, and Bruynooghe 2007; Faber, Pfeifer, and Leone 2011; Marek, Niemel a, and Truszczy nski 2008; Balduccini 2013). Since the grounded fixpoint semantics is completely defined using the two-valued immediate consequence operator, it suffices to extend this operator to obtain an extended semantics; this is often trivial. These nice properties come at a cost: we show that in general, determining whether a logic program has a grounded fixpoint is P 2 -complete. However, for large classes of programs, grounded fixpoint semantics coincides with stable semantics. For those programs, we obtained a simple, concise, purely 2-valued and algebraical, extensible reformulation of the existing semantics. Due to space limitations, we do not elaborate on how our theory applies to AEL, DL or ADFs and refer to (Bogaerts, Vennekens, and Denecker 2014) for proofs. Summarised, the main contributions of this paper are as follows. We extend AFT with the notion of a grounded fixpoint, a fixpoint closely related to stable fixpoints with similar properties, but that is determined by O, not by the choice of an approximator. Applied to logic programming this yields an intuitive, purely two-valued, semantics that is easily extensible and that formalises well-known intuitions.



We call L,  a complete lattice if every subset of L has a least upper bound and a greatest lower bound. A complete lattice has both a least element  and a greatest element . An operator O : L  L is monotone if x  y implies that O(x)  O(y ). An element x  L is a prefixpoint, a fixpoint, a postfixpoint if O(x)  x, respectively O(x) = x, x  O(x). Every monotone operator O in a complete lattice has a least fixpoint, denoted lfp(O), which is also O's least prefixpoint and the limit (the least upper bound) of the increasing sequence (xi )i0 defined by * x 0 = , * xi+1 = O(xi ), * x = lub({xi | i < }), for limit ordinals .



2.2



Logic Programming



In the following sections, we illustrate our abstract results in the context of logic programming. We recall some preliminaries. We restrict ourselves to propositional logic programs, but allow arbitrary propositional formulas in rule bodies. However, AFT has been applied in a much broader context (Denecker, Bruynooghe, and Vennekens 2012; Pelov, Denecker, and Bruynooghe 2007; Antic, Eiter, and Fink 2013) and our results can also be applied in these extensions of logic programming. Let  be an alphabet, i.e., a collection of symbols which are called atoms. A literal is an atom p or the negation q of an atom q . A logic program P is a set of rules r of the form h  , where h is an atom called the head of r, denoted head(r), and  is a propositional formula called the body of r, denoted body (r). An interpretation I of the alphabet  is an element of 2 , i.e., a subset of . The set of interpretations 2 forms a lattice equipped with the order . The truth value (t or f ) of a propositional formula  in a structure I , denoted I is defined as usual. With a logic program P , we associate an immediate consequence operator (van Emden and Kowalski 1976) TP that maps a structure I to TP (I ) = {p | r  P : head(r) = p  body (r)I = t}.



3



Grounded Fixpoints



2

2.1



Preliminaries



Lattices and Operators



Let L,  be a complete lattice and O : L  L a lattice operator, fixed throughout this entire section. We start by giving the most central definition of this text, namely the notion of groundedness. Definition 3.1 (Grounded). We call x  L grounded for O if for each v  L such that O(v  x)  v , it holds that x  v . The intuition behind this concept is easiest to explain if we assume that the elements of L are sets of "facts" of some kind and the  relation is the subset relation between such sets. In this case, a point x is grounded if it contains only facts that are sanctioned by the operator O, in the sense that if we remove them from x, then the operator will add at least one of them again. The above definition captures this idea, by using a set v  L to remove all elements not in v from x. If their removal is not contradicted by O, i.e., O does not re-derive any removed element (O(x  v )  v ), then these elements cannot be part of the grounded point (x  v ).



A poset L,  is a set L equipped with a partial order , i.e., a reflexive antisymmetric, transitive relation. If S is a subset of L, then x is an upper bound, respectively a lower bound of S if for every s  S , it holds that s  x respectively x  s. An element x is a least upper bound, respectively greatest lower bound of S if it is an upper bound that is smaller than every other upper bound, respectively a lower bound that is greater than every other lower bound. If S has a least upper bound, respectively a greatest lower bound, we denote it lub(S ), respectively glb(S ). As is custom, we sometimes call a greatest lower bound a meet, and a least upper bound a join and use the related notations S = glb(S ), xy = glb({x, y }), S = lub(S ) and xy = lub({x, y }).



Proposition 3.2. If O is a monotone operator and x is grounded for O then x is a postfixpoint of O that is less than or equal to lfp(O), i.e., x  O(x) and x  lfp(O). Example 3.3. The converse of Proposition 3.2 does not hold. Consider the following logic program P : p. q  p  q. Its immediate consequence operator TP is represented by the following graph, where full edges express the order relation (to be precise, the  relation is the reflexive transitive closure of these edges) and the dotted edges represent the operator: q} 5 6 = {p, W h i {p} h c = TP is a monotone operator with least fixpoint . Also, {q } is a postfixpoint of TP since TP ({q }) =  {q }. However, {q } is not grounded since TP ({q }  {p}) = TP () = {p}  {p}, while {q }  {p}. Proposition 3.4. All grounded fixpoints of O are minimal fixpoints of O. Example 3.5. The converse of Proposition 3.4 does not hold. Consider the logic program P : p  p. q  p  q. This logic program corresponds has as immediate consequence operator TP : 6 + {p} h = In this case, {p} is a minimal fixpoint of TP , but {p} is not grounded since TP ({p}  {q }) = TP () = {q }. Proposition 3.6. A monotone operator has exactly one grounded fixpoint, namely its least fixpoint. = {p, q } W h 62 {q } s 6 {q }



denote the set of consistent elements. Elements (x, x)  Lc are called exact. We sometimes abuse notation and use the tuple (x, y ) and the interval [x, y ] interchangeably. The precision ordering on L2 is defined as (x, y ) p (u, v ) if x  u and v  y . In case (u, v ) is consistent, this means that (x, y ) approximates all elements approximated by (u, v ), or in other words that [u, v ]  [x, y ]. If L is a complete lattice, then L2 , p is also a complete lattice. AFT studies fixpoints of lattice operators O : L  L through operators approximating O. An operator A : L2  L2 is an approximator of O if it is p -monotone, and has the property that for all x, O(x)  A(x, x). Approximators are internal in Lc (i.e., map Lc into Lc ). As usual, we restrict our attention to symmetric approximators: approximators A such that for all x and y , A(x, y )1 = A(y, x)2 . Denecker, Marek, and Truszczy nski (2004) showed that the consistent fixpoints of interest (supported, stable, well-founded) are uniquely determined by an approximator's restriction to Lc , hence, sometimes we only define approximators on Lc . AFT studies fixpoints of O using fixpoints of A. The AKripke-Kleene fixpoint is the p -least fixpoint of A and has the property that it approximates all fixpoints of O. A partial A-stable fixpoint is a pair (x, y ) such that x = lfp(A(*, y )1 ) and y = lfp(A(x, *)2 ), where A(*, y )1 denotes the operator L  L : x  A(x, y )1 and analogously for A(x, *)2 . The A-well-founded fixpoint is the least precise partial Astable fixpoint. An A-stable fixpoint of O is a fixpoint x of O such that (x, x) is a partial A-stable fixpoint. This is equivalent with the condition that x = lfp(A(*, x)1 ). The A-Kripke-Kleene fixpoint of O can be constructed by iteratively applying A, starting from (, ). For the A-wellfounded fixpoint, a similar constructive characterisation has been worked out by Denecker and Vennekens (2007). In general, a lattice operator O : L  L has a family of approximators of different precision. Denecker, Marek, and Truszczy nski (2004) showed that there exists a most precise approximator, UO , called the ultimate approximator of O. This operator is defined by UO : Lc  Lc : (x, y )  ( O([x, y ]), O([x, y ])). Semantics defined using the ultimate approximator have as advantage that they only depend on O since the approximator can be derived from O. It was shown that for any approximator A, all A-stable fixpoints are UO -stable fixpoints, and the UO -well-founded fixpoint is always more precise than the A-well-founded fixpoint. We refer to UO -stable fixpoints as ultimate stable fixpoints of O and to the UO -well-founded fixpoint as the ultimate well-founded fixpoint of O. AFT and Logic Programming In the context of logic 2 programming, elements of the bilattice 2 are partial interpretations, pairs I = (I1 , I2 ) of interpretations. The pair (I1 , I2 ) approximates all interpretations I with I1  I  I2 . We are mostly concerned with consistent (or, threevalued) interpretations: tuples I = (I1 , I2 ) with I1  I2 . For such an interpretation, the atoms in I1 are true (t) in I , the atoms in I2 \ I1 are unknown (u) in I and the other atoms are false (f ) in I . If I is a three-valued interpretation, and  a formula, we write I for the standard three-valued valuation based on the Kleene truth tables (Kleene 1938). An in-



4

4.1



Grounded Fixpoints and AFT



Preliminaries: AFT



Given a lattice L, approximation fixpoint theory makes uses of the bilattice L2 . We define two projection functions for pairs as usual: (x, y )1 = x and (x, y )2 = y . Pairs (x, y )  L2 are used to approximate all elements in the interval [x, y ] = {z | x  z  z  y }. We call (x, y )  L2 consistent if x  y , that is, if [x, y ] is non-empty. We use Lc to



terpretation I corresponds to the partial interpretation (I, I ). If I = (I1 , I2 ) is a (partial) interpretation, and U  , we write I [U : f ] for the (partial) interpretation that equals I on all elements not in U and that interprets all elements in U as f , i.e., the interpretation (I1 \ U, I2 \ U ). Several approximators have been defined for logic programs. The most common is Fitting's immediate consequence operator P (Fitting 2002), a direct generalisation of TP to partial interpretations. Denecker, Marek, and Truszczy nski (2000) showed that the well-founded fixpoint of P is the well-founded model of P (Van Gelder, Ross, and Schlipf 1991) and that P -stable fixpoints are exactly the stable models of P (Gelfond and Lifschitz 1988). Contrary to classical stable and well-founded semantics, their ultimate counterparts have the nice property that they are insensitive to equivalence preserving rewritings of the bodies of rules. If two logic programs P and P have the same immediate consequence operator, then their ultimate stable (respectively ultimate well-founded) models are the same. For example consider programs P = {p  p  p} and P = {p.}. Even though the body of the rule defining p in P is a tautology, {p} is not a stable model of P (while it is a stable model of P ). However, the ultimate stable semantics treats these two programs identically. However, this property comes at a cost. Denecker, Marek, and Truszczy nski (2004) showed that deciding whether P has an ultimate stable model is 2 P -complete, while that same task is only NP-complete for classical stable models.



Corollary 4.3. If A is an approximator of O, then all Astable fixpoints are grounded fixpoints of O. Theorem 4.4. The well-founded fixpoint (u, v ) of a symmetric approximator A of O approximates all grounded fixpoints of O. Corollary 4.5. If the well-founded fixpoint of a symmetric approximator A of O is exact, then this point is the unique grounded fixpoint of O.



5



Grounded Fixpoints of Logic Programs



4.2



Grounded Fixpoints and AFT



In this section, we discuss how groundedness relates to AFT. More concretely, we show that all (ultimate) stable fixpoints are grounded and that all grounded fixpoints are approximated by the (ultimate) well-founded fixpoint. Proposition 4.1. All ultimate stable fixpoints of O are grounded. Example 4.2. The converse of Proposition 4.1 does not always hold. Consider the logic program P : p  p  q. q  q  p. This logic program has as immediate consequence operator TP : = {p, q } 6 O W h {p} hr = is grounded for TP , since the only v with TP (  v ) = TP (v )  v is itself. However, since TP ([, ]) = L \ {} and {p}  {q } = , it follows that (TP [, ]) = . Thus, lfp( TP ([*, ])) = . Therefore, is not an ultimate stable fixpoint of TP . The fact that all A-stable fixpoints are ultimate stable fixpoints (Denecker, Marek, and Truszczy nski 2004) yields: 62 {q }



In this section, we discuss grounded fixpoints in the context of logic programming. It follows immediately from the algebraical results (Corollary 4.3 and Theorem 4.4) that stable models are grounded fixpoints of the immediate consequence operator and that grounded fixpoints are minimal fixpoints approximated by the well-founded model. Grounded fixpoints can be explained in terms of unfounded sets. Intuitively, an unfounded set is a set of atoms that might circularly support themselves, but have no support from outside. Stated differently, an unfounded set of a logic program P with respect to an interpretation I is a set U of atoms such that P provides no support for the truth of any atom in U , except possibly support based on the truth of other atoms in U . Since TP maps an interpretation I to the set of atoms supported by P in I , the above intuitions are directly formalised as follows. Definition 5.1 (2-Unfounded set). Let P be a logic program, TP the corresponding direct consequence operator and I  2 an interpretation. A set U   is a 2-unfounded set of P with respect to I if TP (I [U : f ])  U = . Thus, U is a 2-unfounded set of P with respect to I if removing all elements of U from I results in a state I [U : f ] where no atom in U is supported, i.e., TP (I [U : f ]) contains no atoms from U . Definition 5.1 slightly differs from the original definition of unfounded set by Van Gelder, Ross, and Schlipf (1991) but it formalises the same intuitions. The most important difference is that we work in a two-valued setting, while van Gelder et al. defined unfounded sets in a three-valued setting. For clarity, we refer to our unfounded sets as "2-unfounded sets" and to the original definition as "GRS-unfounded sets". Our theory does not require any form of three-valued logic. In Section 6, we extend our definition to a three-valued context and show that the different notions of unfounded set are equivalent in the context of the well-founded model construction. Example 5.2. Let P be the following program:      p  q  r.  q  p.   t  s  r.   Let I be the interpretation {p, q, s, t}. Then U1 = {p, q } is an unfounded set of P with respect to I since I [U1 : f ] = {s, t} and in this structure, the bodies of rules defining p and q are false. More formally, TP (I [U1 : f ])  U1 =  U1 = . U2 = {s, t} is not a 2-unfounded set of P with respect to I since TP (I [U2 : f ])  U2 = {p, q, t}  U2 = {t} = .



In what follows, we use U c for the set complement of U , i.e., U c =  \ U . Proposition 5.3. Let P be a logic program, TP the corresponding direct consequence operator and I  2 an interpretation. A set U   is a 2-unfounded set of P with respect to I if and only if TP (I  U c )  U c . Proposition 5.3 shows that U is a 2-unfounded set if and only if its complement satisfies the condition on v in Definition 3.1! This allows us to reformulate the condition that I is grounded as follows. Proposition 5.4. A structure I is grounded for TP if and only if I does not contain any atoms that belong to a 2unfounded set U of P with respect to I . If I is a fixpoint of TP , then all sets U  I c are 2unfounded sets. We call these 2-unfounded sets trivial. With this terminology, we find: Corollary 5.5. A structure I is a grounded fixpoint of TP if and only if it is a fixpoint of TP and P has no non-trivial 2-unfounded sets with respect to I . Similarly to ultimate semantics, grounded fixpoints are insensitive to equivalence-preserving rewritings in the bodies of rules: if P and P are such that TP = TP , then the grounded fixpoints of P and P coincide. Also similar to ultimate semantics, the above property comes at a cost. Theorem 5.6. The problem "given a finite propositional logic program P , decide whether P has a grounded fixpoint" is P 2 -complete. Let us briefly compare grounded fixpoint semantics with the two most frequently used semantics of logic programming: well-founded and stable semantics. Firstly, it deserves to be stressed that the three semantics provide different formalisations of a similar intuition: a certain minimality criterion for fixpoints (which we called groundedness). Consequently it is to be expected that they often coincide. We established that for programs with a two-valued wellfounded model, the three semantics coincide. This sort of programs is common in applications for deductive databases (Datalog and extensions (Abiteboul and Vianu 1991)) and for representing inductive definitions (Denecker and Vennekens 2014). In contrast, well-founded semantics coincides only seldom with stable semantics in the context of answer set programming (ASP). We illustrated in Example 4.2 that in this case, also stable and grounded fixpoint semantics may disagree. This example is quite unwieldy, as are all such programs that we found. It led us to expect that for large classes of ASP programs, both semantics still coincide. For those programs, we have defined a an elegant, intuitive and concise reformulation of the existing semantics. It is a topic for future research to search for characteristics of ASP programs that guarantee that both semantics agree or disagree. Grounded fixpoint semantics is, to the best of our knowledge, the first purely two-valued and algebraical semantics. The well-founded semantics explicitly uses three-valued interpretations in the well-founded model construction. Stable semantics uses three-valued logic implicitly: the GelfondLifschitz reduct corresponds to an evaluation in a partial interpretation. The ultimate versions of these semantics are



purely algebraical but still refer to three-valued interpretations (replacing Kleene valuation by supervaluation). Due to this, ultimate stable and well-founded models are relatively complex to understand. Logic Programs with Abstract Constraint Atoms. The fact that grounded fixpoints semantics is two-valued and algebraical makes it not only easier to understand, but also to extend the semantics. To illustrate this, we consider logic programs with abstract constraint atoms as defined by Marek, Niemel a, and Truszczy nski (2008). An abstract constraint is a collection C  2 . A constraint atom is an expression of the form C (X ), where X   and C is an abstract constraint. The goal of such an atom is to model constraints on subsets of X . The truth value of C (X ) in interpretation I is t if I  X  C and f otherwise. Abstract constraints are a generalisation of pseudo-Boolean constraints, cardinality constraints, and much more. A deterministic logic program is a set of rules of the form1 p  a1  * * *  an  b1  * * *  bm , where p is an atom and the ai and bi are constraint atoms. The intuition behind such a rule is that p is justified if the constraints ai are satisfied while the bi are not. This intuition is captured in an extended immediate consequence operator: TP (I ) = {p | r  P : head(r) = p  body (r)I = t}. Grounded fixpoints of this operator still represent the same intuitions: an interpretation I is grounded if it contains no unfounded sets, or said differently, no atoms without external support. Thus, if it contains no set U of atoms such that TP (I [U : f ])  U = . Example 5.7. Let  be the alphabet {a, b, c, d} For every i, let Ci be the cardinality constraint {X   | |X |  i}.. Consider the following logic program P over : a. c  C4 (). b  C1 (). d  C4 ().



Any interpretation in which d holds is not grounded since for every I , C4 ()I [d:f ] = f and thus d  TP (I [d : f ]). It can easily be verified that {a, b, c} is the only grounded fixpoint of P . This example illustrates that even for complex, abstract extensions of logic programs, groundedness is an intuitive property: for any extension, a point is grounded if it contains no self-supporting atoms. Also, it often possible to derive common properties of all grounded fixpoints such as the fact that d cannot be contained in any of them. Lastly, groundedness easily extends to these rich formalisms (defining grounded fixpoints takes one line given the immediate consequence operator). This is in sharp contrast with more common semantics of logic programming (such as stable

Here, we limit ourselves to deterministic programs. In general, Marek, Niemel a, and Truszczy nski also described nondeterministic programs. We come back to this issue in Section 6.

1



and well-founded semantics) which are often hard(er) to extend to richer formalisms, as can be observed by the many different versions of those semantics that exist for logic programs with aggregates (Ferraris 2005; Son, Pontelli, and Elkabani 2006; Pelov, Denecker, and Bruynooghe 2007; Faber, Pfeifer, and Leone 2011; Gelfond and Zhang 2014).



GRS-unfounded sets (Lifschitz 2008). This again shows that many of the intuitions used in Answer Set Programming are also closely related to the notion of groundedness. Groundedness and Nondeterminism. In Section 5, we restricted ourselves to logic programs with abstract constraint atoms in the bodies of rules. As argued by Marek, Niemel a, and Truszczy nski (2008), allowing them as well in heads gives rise to a nondeterministic generalisation of the immediate consequence operator. A consistent nondeterministic operator maps every point x  L to a non-empty set O(x)  L. The definition of groundedness can straightforwardly be extended to this nondeterministic setting: a point x  L is grounded for nondeterministic operator O, if x  v for all v such that O(x  v )  v , where we define for a set X  L that X  v if x  v for every x  X . A thorough study of groundedness for nondeterministic operators is out of the scope of this paper. Other Definitions of Groundedness. The terminology "grounded" is heavily overloaded in the literature. This is not a coincidence since this term often represents similar intuitions. For example Denecker, Marek, and Truszczy nski (2002) argued that ultimate stable models satisfy "some groundedness condition" without defining this condition. We formally defined groundedness and showed in Proposition 4.1 that with this definition, their claim indeed holds. In 1988, Konolige defined notions of weak, moderate and strong groundedness in order to formalise some of his intuitions regarding "good" models of autoepistemic theories. However, as he mentions himself, the closest he got to formalising these intuitions was strong groundedness, a syntactical criterion that depends on how a theory is rewritten to a normal form. We now claim2 that our notion of groundedness formalises his intuitions, or at least, that it works for all examples he gave! In Dung's argumentation frameworks, the grounded semantics is also defined. Since this is defined as the least fixpoint of the (monotone) characteristic operator, in this case this is the unique grounded fixpoint. However, Strass (2013) showed that this does not generalise to abstract dialectical frameworks, where the grounded extension corresponds to the (ultimate) Kripke-Kleene fixpoint.



6



Discussion



Unfounded Sets. Unfounded sets were first defined by Van Gelder, Ross, and Schlipf (1991) in their seminal paper introducing the well-founded semantics. Their definition slightly differs from Definition 5.1. Definition 6.1 (GRS-Unfounded set). Let P be a logic program and I a three-valued interpretation. A set U   is a GRS-unfounded set of P with respect to I , if for each rule r with head(r)  U , body (r)I = f or body (r)I [U :f ] = f . The first difference between 2-unfounded sets and GRSunfounded sets is that GRS-unfounded sets are defined for three-valued interpretations, while we restricted our attention to (total) interpretations. Our definition easily generalises to three-valued interpretations using Fitting's operator: Definition 6.2 (3-Unfounded set). Let P be a logic program, P Fitting's immediate consequence operator and I a threevalued interpretation. A set U   is a 3-unfounded set of P with respect to I if P (I [U : f ])2  U = . This definition formalises the same intuitions as Definition 5.1: U is a 3-unfounded set if making all atoms in U false results in a state where none of them can be derived. The following proposition relates the two notions of unfounded sets. Proposition 6.3. Let P be a logic program, I a three-valued interpretation and U  . The following hold. * If U is a 3-unfounded set, then U is a GRS-unfounded set. * If I [U : f ] is more precise than I , then U is a GRSunfounded set if and only if U is a 3-unfounded set. We showed that for a certain class of interpretations, the two notions of unfounded sets coincide. Furthermore, Van Gelder et al. only use unfounded sets to define the wellfounded model construction. It follows immediately from Lemma 3.4 in (Van Gelder, Ross, and Schlipf 1991) that every partial interpretation I in that construction with GRSunfounded set U satisfies the condition in the second claim in Proposition 6.3. This means that 3-unfounded sets and GRS-unfounded sets are equivalent for all interpretations that are relevant in the original work! Essentially, we provided a new formalisation of unfounded sets that correctly formalises the underlying intuitions, and that coincides with the old definition on all interpretations used in the original work. Furthermore, our definition is simpler and translates easily to algebra. Corollary 5.5, which states that grounded fixpoints are fixpoints of TP that permit no non-trivial 2-unfounded sets, might sound familiar. Indeed, it has been shown that an interpretation is a stable model of a logic program if and only if it is a fixpoint of TP and it permits no non-trivial



7



Conclusion



In this paper, we defined a new algebraical concept, namely groundedness. We showed that grounded fixpoints behave well with respect to other fixpoints studied in approximation fixpoint theory: given an operator O and an approximator A of O, all A-stable fixpoints are grounded for O and all grounded fixpoints of O are approximated by the A-wellfounded fixpoint. Moreover, grounded fixpoints free us from the need of choosing such an approximator: they are defined directly in terms of the original lattice operator.

2 The journal version of this paper will formally describe the application of our theory to various fields, including a more detailed discussion about the different notions of groundedness, Konolige's intuitions and the relation to grounded fixpoints.



Grounded fixpoint semantics is the first purely two-valued and algebraical semantics for logic programming. Moreover, this semantics is compact, intuitive (directly based on the notion of unfounded sets) and easily extensible: as long as the (two-valued) immediate consequence operator is defined, the grounded fixpoint semantics is obtained for free. Our theory can also be applied to AEL, DL, Dung's argumentation frameworks and ADFs where it also results in a semantics with attractive properties.2 Acknowledgements This work was supported by the KU Leuven under project GOA 13/010 and by the Research Foundation - Flanders (FWO-Vlaanderen).



References

Abiteboul, S., and Vianu, V. 1991. Datalog extensions for database queries and updates. J. Comput. Syst. Sci. 43(1):62-124. Antic, C.; Eiter, T.; and Fink, M. 2013. Hex semantics via approximation fixpoint theory. In Cabalar, P., and Son, T. C., eds., LPNMR, volume 8148 of LNCS, 102-115. Springer. Balduccini, M. 2013. ASP with non-herbrand partial functions: A language and system for practical use. TPLP 13(45):547-561. Bogaerts, B.; Vennekens, J.; Denecker, M.; and Van den Bussche, J. 2014. FO(C): A knowledge representation language of causality. TPLP 14(4-5-Online-Supplement):60- 69. Bogaerts, B.; Vennekens, J.; and Denecker, M. 2014. Grounded fixpoints. Technical Report CW 677, Departement of Computer Science, Katholieke Universiteit Leuven. Denecker, M., and Vennekens, J. 2007. Well-founded semantics and the algebraic theory of non-monotone inductive definitions. In Baral, C.; Brewka, G.; and Schlipf, J. S., eds., LPNMR, volume 4483 of LNCS, 84-96. Springer. Denecker, M., and Vennekens, J. 2014. The well-founded semantics is the principle of inductive definition, revisited. In Baral, C.; De Giacomo, G.; and Eiter, T., eds., KR, 22-31. AAAI Press. Denecker, M.; Bruynooghe, M.; and Vennekens, J. 2012. Approximation fixpoint theory and the semantics of logic and answers set programs. In Erdem, E.; Lee, J.; Lierler, Y.; and Pearce, D., eds., Correct Reasoning, volume 7265 of LNCS. Springer. 178-194. Denecker, M.; Marek, V.; and Truszczy nski, M. 2000. Approximations, stable operators, well-founded fixpoints and applications in nonmonotonic reasoning. In Minker, J., ed., Logic-Based Artificial Intelligence, volume 597 of The Springer International Series in Engineering and Computer Science. Springer US. 127-144. Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2002. Ultimate approximations in nonmonotonic knowledge representation systems. In Fensel, D.; Giunchiglia, F.; McGuinness, D. L.; and Williams, M.-A., eds., KR, 177-190. Morgan Kaufmann.



Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2003. Uniform semantic treatment of default and autoepistemic logics. Artif. Intell. 143(1):79-122. Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2004. Ultimate approximation and its application in nonmonotonic knowledge representation systems. Information and Computation 192(1):84-121. Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2011. Reiter's default logic is a logic of autoepistemic reasoning and a good one, too. In Brewka, G.; Marek, V.; and Truszczy nski, M., eds., Nonmonotonic Reasoning - Essays Celebrating Its 30th Anniversary. College Publications. 111-144. Faber, W.; Pfeifer, G.; and Leone, N. 2011. Semantics and complexity of recursive aggregates in answer set programming. Artif. Intell. 175(1):278-298. Ferraris, P. 2005. Answer sets for propositional theories. In Proceedings of International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR), 119-131. Fitting, M. 2002. Fixpoint semantics for logic programming a survey. Theoretical Computer Science 278(1-2):25-51. Gelfond, M., and Lifschitz, V. 1988. The stable model semantics for logic programming. In Kowalski, R. A., and Bowen, K. A., eds., ICLP/SLP, 1070-1080. MIT Press. Gelfond, M., and Zhang, Y. 2014. Vicious circle principle and logic programs with aggregates. TPLP 14(4-5):587- 601. Kleene, S. C. 1938. On notation for ordinal numbers. The Journal of Symbolic Logic 3(4):pp. 150-155. Konolige, K. 1988. On the relation between default and autoepistemic logic. Artif. Intell. 35:343-382. Lifschitz, V. 2008. Twelve definitions of a stable model. In Garc ia de la Banda, M., and Pontelli, E., eds., ICLP, volume 5366 of LNCS, 37-51. Springer. Marek, V. W.; Niemel a, I.; and Truszczy nski, M. 2008. Logic programs with monotone abstract constraint atoms. TPLP 8(2):167-199. Pelov, N.; Denecker, M.; and Bruynooghe, M. 2007. Wellfounded and stable semantics of logic programs with aggregates. TPLP 7(3):301-353. Son, T. C.; Pontelli, E.; and Elkabani, I. 2006. An unfoldingbased semantics for logic programming with aggregates. CoRR abs/cs/0605038. Strass, H. 2013. Approximating operators and semantics for abstract dialectical frameworks. Artif. Intell. 205:39-70. van Emden, M. H., and Kowalski, R. A. 1976. The semantics of predicate logic as a programming language. J. ACM 23(4):733-742. Van Gelder, A.; Ross, K. A.; and Schlipf, J. S. 1991. The well-founded semantics for general logic programs. J. ACM 38(3):620-650. Vennekens, J.; Gilis, D.; and Denecker, M. 2006. Splitting an operator: Algebraic modularity results for logics with fixpoint semantics. ACM Trans. Comput. Log. 7(4):765-797.



Robust Subspace Clustering via Thresholding Ridge Regression

Xi Peng1 , Zhang Yi2, , and Huajin Tang1,2,

1



Institute for Infocomm Research, Agency for Science, Technology and Research (A*STAR), Singapore 138632 2 College of Computer Science, Sichuan University, Chengdu 610065, P.R. China. pangsaai@gmail.com, zhangyi@scu.edu.cn, htang@i2r.a-star.edu.sg.



In the follow analysis, lower-case bold letters represent column vectors and upper-case bold ones represent matrices. AT and A-1 denote the transpose and pseudo-inverse of the matrix A, respectively. I denotes the identity matrix. Table 1 summarizes some notations used throughout the material.



Table 1: Some used notations.

Notation n m r x  Rm c  Rn D = [d1 , d2 , . . . , dn ] Dx  D D -x Definition the size of the dictionary the dimensionality of sample the rank of a given matrix a data point the representation of x over D a given dictionary x and Dx belong to the same cluster the data points of D except Dx



Intra-subspace Projection Dominance

In this material, we provide the theoretical analyses to show that the trivial coefficients always correspond to the codes over errors. Lemmas 1-3 show that our errors-removing strategy will perform well when the p -norm is enforced over the representation, where p = {1, 2, }. Let x = 0 be a data point in the union of subspaces SD that is spanned by D = [Dx D-x ], where Dx and D-x consist of the intra-cluster and inter-cluster data points, respectively. Note that, noise and outlier could be regarded as a kind of inter-cluster data point of x. Without loss of generality, let SDx and SD-x be the subspace spanned by Dx and D-x , respectively. Hence, there are only two possibilities for the location of x, i.e., in the intersection between SDx and SD-x (denoted by x  {S|S = SDx  SD-x }), or in SDx except the intersection (denoted by x  {S|S = SDx \SD-x }).  Let c x and c-x be the optimal solutions of min c

p



Lemma 1. For any nonzero data point x in the subspace SDx except the intersection between SDx and SD-x , i.e., x  {S|S = SDx \SD-x }, the optimal solution of (1) over D is given by c which is partitioned according to the c sets Dx and D-x , i.e., c = x . Thus, we must have c-x  [c x ]rx ,1 > [c-x ]1,1 . Proof. For the nonzero data point x, suppose there exists a nonzero vector c -x such that

 x = Dx c x + D-x c-x ,



(2)



s.t. x = Dc,



(1)



over Dx and D-x , respectively. * p denotes the p -norm and p = {1, 2, }. We aim to investigate the conditions under which, for every nonzero data point x  SDx , if the p  norm of c x is smaller than that of c-x , then the coefficients over intra-subspace data points are larger than those over  inter-subspace data points, i.e., [c x ]rx ,1 > [c-x ]1,1 (intra subspace projection dominance). Here, [cx ]rx ,1 denotes the rx -th largest absolute value of the entries of c x and rx is the dimensionality of SD . In the following analysis, Lemma 1 and Lemma 3 show  [c x ]rx ,1 > [c-x ]1,1 when x  {S|S = SDx \SD-x } and x  {S|S = SDx  SD-x }, respectively. And Lemma 2 is a preliminary step toward Lemma 3.

*Corresponding Authors Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



 x - Dx c (3) x = D-x c-x ,  Since x  SDx , then x - Dx cx  SDx , i.e., D-x c -x  SDx . Since SDx  SD-x = 0 and x = 0, then c -x = 0 and  cx = 0. This contradicts the assumption c -x = 0. Then,  we must have c x = 0 and c-x = 0 which implies that   [cx ]r0 ,1 > [c-x ]1,1 . This completes the proof.



then



Lemma 2. Consider a nonzero data point x in the intersection between SDx and SD-x , i.e., x  {S|S = SDx  SD-x }. Let c , zx , and z-x be the optimal solution of min c

p



s.t. x = Dc c x c -x



(4)



over D, Dx , and D-x . c =



is partitioned accordp



ing to the sets D = [Dx D-x ]. If zx  [c x ]rx ,1 > [c-x ]1,1 .



< z-x



p,



then



Proof. (=) We prove the result using contradiction. Assume c -x = 0, then Note that, the left side and the right side of (5) correspond a data point from SDx and SD-x , respectively. Then, we must have x = Dx c (6) x + Dx z0 , and x = Dx c (7) x + D-x ze ,   c + z0 c Clearly, x and x are feasible solutions of (4) 0 ze over [Dx D-x ]. According to the triangle inequality and the condition z0 p < ze p , we have c x + z0 0  c x

p  x - Dx c x = D-x c-x .



(5)



+ z0

p



p



< c x

p



p



+ ze



p.



(8)



p



Proof. Since x  {S|S = SDx  SD-x }, we could write T T is the skinny z , where Dx = Ur0 r0 Vr x = Ur0 r0 Vr 0 0 0 SVD of Dx , r0 = diag(1 (Dx ), 2 (Dx ), * * * , r0 (Dx )), r0 is the rank of Dx , and z0 is the optimal solution of (10) 1 T over Dx . Thus, z0 = Vr0 - r0 Ur0 x. From the propositions of p-norm, i.e., z   z 1  n z  , z   z 2  n z  , and z 2  z 1   n z 2 , we have   1 T z0 p  z0 1  r0 z0 2 = r0 Vr0 - r0 Ur0 x 2 . (12) Since the Frobenius norm is subordinate to the Euclidean vector norm, we must have  1 T z0 p  r0 Vr0 - r0 Ur0 F x 2  r0 = x 2 2 (D ) + * * * +  2 (D ) 1 x x r0 where min (Dx ) = r0 (Dx ) is the smallest nonzero singular value of Dx . Moreover, x could be represented as a linear combination of D-x since it lies in the intersection between SDx and SD-x , i.e., x = D-x ze , where ze is the optimal solution of (10) over D-x . Multiplying two sides of the equation with xT , it gives x 2 = xT D-x ze . According to the H older's inequality, we have x

2 2 -1  min (Dx ) x 2



as ze p is the opc x + z0 timal solution of (4) over D-x . Then, < 0 p c c x x . It contradicts the fact that is the opti c c -x - x p p mal solution of (4) over D. (=) We prove the result using contradiction. For a nonzero data point x  {S|S = SDx  SD-x }, assume z0 p  ze p . Thus, for the data point x = x, it is possible that (4) will only choose the points from SD-x to represent x. This contradicts the assumption that c x = 0 and c -x = 0 . This completes the proof. Definition 1 (The First Principal Angle). Let  be a Euclidean vector-space, and consider the two subspaces W , V with dim(W ) := rW  V := rV . There exists a set of anW gles {i }r i=1 called the principal angles, the first one being defined as: min := min arccos T   2  ,

2



From (7), we have ze



 c -x



(13)



According to the definition of the first principal angles (Definition 1), we have DT -x x

 T = max [D-x ]T 1 x , [D-x ]2 x , * * *



 DT -x x







ze 1 ,



(14)



(9)



where [D-x ]i denotes the ith column of D-x , min is the first principal angle between SDx and SD-x , and D-x 1,2 denotes the maximum 2 -norm of the columns of D-x . Note that the smallest principal angle between any two subspaces always greater than zero, hence, cos min  [0, 1). Combining (14) and (15), it gives x hence,

2 2



 cos min D-x



1,2



x 2,



(15)



where   W and   V . Lemma 3. Consider the nonzero data point x in the intersection between SDx and SD-x , i.e., x  {S|S = SDx  SD-x }, where SDx and SD-x denote the subspace spanned by Dx and D-x , respectively. The dimensionality of SDx is rx and that of SD-x is r-x . Let c be the optimal solution of min c p s.t. x = Dc (10) over D = [Dx D-x ] and c = cording to the sets Dx and D-x . If

 then [c x ]rx ,1 > [c-x ]1,1 . Here, min (Dx ) is the smallest nonzero singular value of Dx , min is the first principal angle between Dx and D-x , D-x 1,2 is the maximum 2 norm of the columns of D-x and [c]r,1 denotes the r-th largest absolute value of the entries of c.



 cos min D-x ze

1



1,2



x



2



ze 1 ,



(16) (17)



x 2 . cos min [D-x ]1,2 From the propositions of p-norm, we have  ze Let z0

p p p,



c x be partitioned acc -x

1,2 ,







x 2 . cos min [D-x ]1,2 x 2 , cos min [D-x ]1,2



(18)



< ze



then

2



min (Dx )  r-x cos min D-x



(11) then,



-1 min (Dx ) x



<



(19)



min (Dx ) > cos min [D-x ]1,2 . (20)   It is the sufficient condition for [cx ]r0 ,1 > [c-x ]1,1 since  it implies c x = 0 and c-x = 0 (Lemma 2). This completes the proof.



It should be pointed out that the above proofs are motivated by the theoretical analysis in (Elhamifar and Vidal 2013), but they are different. (Elhamifar and Vidal 2013) provides the conditions under which SSC (1 -norm) will only chose the intra-subspace data points to represent the input when data lies onto the union of independent subspaces or disjoint subspaces, while our theoretical analyses investigate the conditions under which p -norm-based coefficients with small value correspond to the projections over the errors even though the data come from dependent subspaces, where p = 1, 2, . Moreover, the most different point between these two works may be that, we theoretically show that the effect of errors could be eliminated by removing trivial coefficients in the projection space, whereas (Elhamifar and Vidal 2013) aims to prove the success of 1 -normbased representation in the input data space.



References

Elhamifar, E., and Vidal, R. 2013. Sparse subspace clustering: Algorithm, theory, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence 35(11):2765- 2781.



IBM Research, Ireland



University of California, Irvine



Anytime Best+Depth-First Search for Bounding Marginal MAP

Radu Marinescu

IBM Research - Ireland



Junkyu Lee, Alex Ihler and Rina Dechter

University of California, Irvine



AAAI 2017 Technical Session: RU: Reasoning under Uncertainty Feb. 8th. 2017 10:00 am - 11:00 am Oral Presentation Paper 2066



IBM Research, Ireland



University of California, Irvine



Motivation and Contribution





Marginal MAP Inference

-



Probabilistic inference query





Optimal partial configuration after marginalizing hidden/latent variables in a probability distribution



- -



Complexity: NPpp complete Often it is the right task on various applications





Probabilistic conformant planning [Lee, Marinescu, Dechter, 2015] Natural language processing task [Bird, Klein, Loper, 2009] Image completion task [Xue, Li, Ermon, Gomes, Selman, 2016]















Contributions

- -



Anytime hybrid (best+depth-first) search for MMAP Improvement of anytime performance for finding upper and lower bounds





Upper-bound: estimate of optimal solution from a partial solution Lower-bound: sub-optimal solution







2



IBM Research, Ireland



University of California, Irvine



Outline





Background

- - -



Graphical model AND/OR search space & WMB heuristic Previous MMAP search algorithms







Best+Depth-First search for MMAP

- - -



LAOBF (Best-First AND/OR Search with Depth-First Lookaheads) AAOBF (Alternating Best-First and Depth-First AND/OR search) LnDFS (Learning Depth-First AND/OR search)







Experiments Conclusion

3







IBM Research, Ireland



University of California, Irvine



Background - graphical model





Graphical model

- - -







Primal graph

- -



variables domains functions



nodes are variables two nodes are connected if they appear in the same function E C







Marginal Map task

-



A



B



H



F Max and sum not commute



D



G



-



4



IBM Research, Ireland



University of California, Irvine



Background - AND/OR search space





Bucket elimination

MAX



[Dechter, 1999]







AND/OR search graph

A

0 1



[Mateescu, Dechter, 2007]



B

0 1 0



B

1



C

0 1 0



D

1 0



C

1 0



D

1 0



C

1 0



D

1 0



C

1 0



D

1



E

0 1 0



E

1



G

0 1 0



F



E

1 0 1 0



E

1



G

0 1 0



F

1



E

0 1 0



E

1



G

0 1 0



F



E

1 0 1 0



E

1



G

0 1 0



F

1



H

0



H

1 0 1



H

0



H

1 0 1



SUM





Pseudo tree

[Freuder, Quinn, 1985]



5



IBM Research, Ireland



University of California, Irvine



Background - WMB heuristics





Mini-bucket elimination [Dechter, Rish 2001]

-







Weighted Mini-bucket [Liu, Ihler, 2012]

-



"i-bound", limit on the number of variables in a single mini-bucket



Holder's inequality



MAX







WMB Moment Matching

-



MAP variables



[Liu, Ihler, 2011] [Marinescu,Ihler,Decther, 2014]



SUM

-



-



SUM variables



Mini-bucket upper bound



6



IBM Research, Ireland



University of California, Irvine



Previous MMAP search algorithms

Park, Darwiche Depth-First BnB Join-tree upper bound (relaxed variable ordering)

- depth-first search - dynamic heuristic



Marinescu, Decther, Ihler Depth-First BnB AND/OR Search WMB Heuristic

- compact AND/OR search space - more accurate WMB heuristics



Lee,Marinescu, Decther, Ihler Weighted Best-First Anytime Depth-First AND/OR WMB heuristic

- anytime solutions - infrequent solution updates - still memory intensive



2003 2009



2014 2015



2016 2017



Yuan, Hansen Depth-First BnB Incremental Join-tree upper bound

- static heuristic



Marinescu, Decther, Ihler Best-First/Recursive BF AND/OR Search WMB heuristic



Marinescu, Lee, Iihler, Decther Best+Depth-First - high quality upper/lower bounds - more frequent solution updates - memory efficiency



- BF avoids solving summation problems - very memory intensive - no anytime, return optimal solution or no solution 7



IBM Research, Ireland



University of California, Irvine



Outline





Background

- -



Graphical model AND/OR search space & WMB heuristic







Best+Depth-First search for MMAP

- - -



LAOBF (Best-First AND/OR Search with Depth-First Lookaheads) AAOBF (Alternating Best-First and Depth-First AND/OR search) LnDFS (Learning Depth-First AND/OR search)







Experiments Conclusion

8







IBM Research, Ireland



University of California, Irvine



Best+Depth-First Search





Depth-First search





Better guidance for depth-first dives using improved heuristics Frequent solution updates



Best-First search











Lower bound



Cutoff frontier of best-first search using improved lower bounds Learn accurate heuristics by depth-first lookahead



When Global UB = Global LB, Optimal Solution Discovered

9



IBM Research, Ireland



University of California, Irvine



Notations - solution tree

MAX

OR AND OR AND OR AND OR AND OR AND



partial solution tree



tip of partial solution tree



solution tree



10



IBM Research, Ireland



University of California, Irvine



Notations - basic operations

OR AND OR AND OR AND OR AND OR AND



MAX

q(n), l(n)

 



Expand(n)



Update(n)



q(n) : upper bound at n q(root) : global upper bound l(n) : lower bound at n l(root) : global lower bound : best partial solution tree (partial solution tree where OR nodes direct the child m with best q(m)



 











backup q and l values







re-direct best partial solution tree

11



IBM Research, Ireland



University of California, Irvine



LAOBF (best-first AND/OR search with depth-first lookaheads)

Best-first selection Depth-first lookahead Best-first expansion & update



  



depth-first dive at the tip of compute global lower bound cache summation subproblems



 



Select a tip node n Expand and Update n



cutoff parameter: control depth-first lookahead (at every



number of node expansions.)



12



IBM Research, Ireland



University of California, Irvine



AAOBF (alternating best-first with depth-first AND/OR search)

Depth-first greedy expansion Best-first re-direct Depth-first selection Best-first selection Depth-first re-direct Best-first expansion & update







Expand(n) and Update(n) depth-first greedy search







redirect from explicated search graph from the root with updated q and l select Expand and Update a tip node

13











IBM Research, Ireland



University of California, Irvine



LnDFS (learning depth-first AND/OR search)

Best-first update Best-first selection Depth-first expansion



Keep expanding tips nodes of Update values from tip nodes of

14



IBM Research, Ireland



University of California, Irvine



Outline





Background

- -



Graphical model AND/OR search space & WMB heuristic







Best+Depth-First search for MMAP

- - -



LAOBF (Best-First AND/OR Search with Depth-First Lookaheads) AAOBF (Alternating Best-First and Depth-First AND/OR search) LnDFS (Learning Depth-First AND/OR search)







Experiments Conclusion

15







IBM Research, Ireland



University of California, Irvine



Experiments





Anytime Algorithms

-



Presented Best+Depth-First Search





LAOBF AAOBF LnDFS Weighted Recursive Best-First AND/OR Search with Overestimation Breadth Rotate AND/OR Branch and Bound Anytime Factor Set Elimination

[Lee, Marinescu, Ihler, Dechter, 2016]











-



State-of-the-art









[Lee, Marinescu, Ihler, Dechter, 2016]







[Maua, Campos, 2012]







Memory

- - -



total 24 GB WMB-MM(i) i-bound: 20 or the largest within 4 GB caching for AND/OR search graph max 4 GB

16



IBM Research, Ireland



University of California, Irvine



Experiment





Benchmark

- - - -



derived from UAI inference competitions for MPE query randomly choose 50% of the variables as MAP variables generate 4 random MMAP instances Grid, Pedigree, Promedas domain







Problem instance parameters



Domain (#. instances) Grid (128) Pedigree (88) Promedas (100) 144,649,2500 144,649,2500 2,2 3,3 25,163,814 42,189,834



334,917,1289



334,917,1289



3,7



4,5



35,127,289



63,152,312



381,1064,1997



385,1077,2024



2,2



3,3



11,137,552



33,171,577



N: number of variables, W: constrained induced width,



F: number of functions, H: constrained pseudo tree height



K: domain size,



S: scope size

17



IBM Research, Ireland



University of California, Irvine



Experiment - individual instances





Anytime search status for individual instances



N:2500 F:2500 K:2 S:3 W:788 H:817



N:1183 F:1183 K:5 S:5 W:272 H:290



N:1675 F:1701 K:2 S:3 W:259 H:298



- search: LAOBF (lab), AAOBF (aab), LnDFS (ldt), BRAOBB (bra) - heuristic: WMB-MM (20) - memory: 24 GB Other algorithms couldn't find any solution due to memory out

18



IBM Research, Ireland



University of California, Irvine



Experiment - average solution quality





Average solution quality

- -



anytime quality of lower bound normalized by optimal solution when optimal solution is not available, used best-known solution







Result

- -



How the quality of solution improves over time LAOBF, AAOBF, LnDFS





improved upon WRBFAOO on 3 domains best on promedas domain, second worst on pedigree domain

19



-



BRAOBB





-



AFSE: worst performance on 3 domains



IBM Research, Ireland



University of California, Irvine



Experiment - average gap quality





Average gap quality

-



anytime gap (difference between upper and lower bound) normalized by upper bound (If no lower bound available, gap = 1)







Result

- -



How the gap between lower/upper bound decreases over time (gap=0 optimal) LAOBF, AAOBF, LnDFS





All similar improvements over time, especially at shorter time bounds AAOBF was overall best







-



AFSE: worst performance on 3 domains

20



IBM Research, Ireland



University of California, Irvine



Experiment - memory robustness





Memory robustness



- - - - -



How search algorithm effectively utilized the memory and improves gap within the memory limit % of instances terminated by memory limit % of instances terminated by memory limit and no solution found at all average gap computed from out of memory instances only average search time computed from out of memory instances







Result

- - -



LnDFS is the most memory robust algorithm AAOBF (LAOBF) improved memory robustness compared to WRBFAOO AFSE is the worst among 5 algorithms

21



IBM Research, Ireland



University of California, Irvine



Conclusion





Anytime Best+Depth-First search algorithms improved upon the state-of-the-art algorithms

- - -



higher quality anytime solutions tighter anytime upper bounds more effective use of memory







Future work

-



New anytime search + approximate summation inference

 



variational bounds with search probabilistic bounds from sampling



22



Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence



Bayesian Approach to Modeling and Detecting Communities in Signed Network

Bo Yang, Xuehua Zhao, and Xueyan Liu

School of Computer Science and Technology, Jilin University, Changchun, China Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, China ybo@jlu.edu.cn



Abstract

There has been an increasing interest in exploring signed networks with positive and negative links in that they contain more information than unsigned networks. As fundamental problems of signed network analysis, community detection and sign (or attitude) prediction are still primary challenges. To address them, we propose a generative Bayesian approach, in which 1) a signed stochastic blockmodel is proposed to characterize the community structure in context of signed networks, by means of explicitly formulating the distributions of both density and frustration of signed links from a stochastic perspective, and 2) a model learning algorithm is proposed by theoretically deriving a variational Bayes EM for parameter estimation and a variation based approximate evidence for model selection. Through the comparisons with state-of-the-art methods on synthetic and real-world networks, the proposed approach shows its superiority in both community detection and sign prediction for exploratory networks.



Introduction

In recent years, the study of signed networks becomes a burgeoning research area. In contrast to the extensively studied unsigned networks only encoding whether relationships exist or not, signed networks contain more information by extending the single relationship to positive and negative relationships, wherein positive ones represent to like, trust, support or collaborate and negative ones represent to dislike, distrust, oppose or compete, among others. For signed networks, community detection is of considerable importance for understanding the basic patterns of structure and dynamics. This task is trying to identify K antagonistic communities, so that most links within communities are positive while most links between communities are negative. In this sense, communities are consistent with the clusters defined in balance theory in social science (Cartwright and Harary 1956; Davis 1967), where a strongly (or weakly) balanced network can be divided into two (or K ) clusters, so that all links within clusters are positive and all links between clusters are negative. Note that, real-world signed networks are usually

Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



unbalanced due to the frustration in them, i.e., negative links within clusters and positive links between clusters. Although many methods have been proposed to address community detection since Girvan and Newman's work (Girvan and Newman 2002), however, most of them are exclusively designed for unsigned networks, which focus on link density rather than link sign to define and detect communities.Therefore, the primary techniques adopted by them cannot be directly applied to signed networks, such as modularity optimization (Newman 2004), Markov random walk (Zhou 2003), clique percolation model (Palla et al. 2005), spectral analysis (Mitrovi c and Tadi c 2009), evolutionary optimization (Pizzuti 2008), among many others. In view of this, new methods have been proposed for signed community detection. On one hand, some of them are studied from the perspective of social science. For instance, based on the social balance theory, Doreiian and Mrvar proposed a frustration-optimization based method, referred to as DM, which partitions a signed network by minimizing the sum of negative link quantity within communities and positive link quantity between communities (Doreian and Mrvar 1996). Thereafter, Larusso et al improved the same idea to partition weighted signed networks (Larusso, Bogdanov, and Singh 2010). Very similarly, Bansal et al proposed a correlation clustering method to maximize the agreement (i.e. the number of positive intra-cluster links and negative intercluster links) or to minimize the disagreement (i.e. the number of negative intra-cluster links and positive inter-cluster links) among nodes (Bansal, Blum, and Chawla 2004). On the other hand, some of them are proposed by means of generalizing the current techniques of partitioning unsigned networks as mentioned above. For examples, based on potts model Traag et al deduced an improved modularity function for signed networks and then proposed a modularity-optimization based partition algorithm PSA (Traag and Bruggeman 2009). Yang et al generalized the Markov stochastic process on unsigned network to signed network and then proposed an improved random-walk based method FEC (Yang, Cheung, and Liu 2007). Huang et al improved the clique percoltion model to detect overlapping signed communities (Huang and Qiu 2010). Anchuri et al proposed a generalized spectral method for signed network partition (Anchuri and Magdon-Ismail 2012). Very recently, multi-objective evolutionary methods have been applied to



1952



signed network decomposition, by simultaneously optimizing two objectives defined in terms of not only link density but also link sign, e.g. internal similarity versus external similarity (Liu, Liu, and Jiang 2014) and kernel k-means versus ratio-cut (Gong et al. 2014). All the aforementioned methods can be seemed as discriminative, which just focused on looking for a way to distinguish notes by clustering them into different groups, based on either predefined optimization objectives (such as modularity) or heuristics (such as random walk model). However, they are not concerned with how the real-world signed networks containing community structures are generated. Distinctly, in this work we plan to propose a generative approach. Compared with discriminative methods, generative methods are more expected because they can be applied to not only community detection but network modeling, generation, as well as link and attitude prediction. Being one important statistical network model, stochastic blockmodel (SBM) is a good generative model. As it enables us to reasonably decompose and then properly analyze an exploratory network without a priori knowledge about its intrinsic structure, SBM has attracted more and more attention since it was originally proposed by Holland and Leinhardt (Holland and Leinhardt 1981). Although various extensions of SBM have been proposed to address different tasks of network analysis, such as multiple role SBM (Airoldi et al. 2008), overlapping SBM (Latouche et al. 2011), dynamical SBM (Yang et al. 2011) and hierarchical SBM (Yang, Liu, and Liu 2012), however, to the best of our knowledge, all the existing SBMs are designed for unsigned networks and thereby incompetent for handling signed networks. In view of this, we are motivated to propose a novel generative Bayesian approach. More specifically, our main contributions are two-fold: (1) We proposed a signed stochastic blockmodel to characterize and generate the block structures of signed networks by means of explicitly formulating both link density and link sign from a stochastic perspective. (2) We proposed an effective algorithm for learning this model from exploratory networks based on variational Bayes techniques, which can automatically detect block numbers and assignments.



Given a signed network, one can deduce a latent n x K matrix Z , indicating the relationship between node and block assignment. zik = 1 if node i is assigned to block k , otherwise zik = 0. Moreover, zi follows the following multinomial distribution with a parameter : zi  M (1,  = {1 , 2 , ..., K }) Given Z , aij follows the following multinomial distribution with parameters  and : aij  M (1,  = {1 , -1 , 0 }), ziq zjl = 1 and q = l aij  M (1,  = {1 , -1 , 0 }), ziq zjl = 1 and q = l According to SSBM, one can generate a synthetic signed network with a block structure by following steps: 1) assign nodes to blocks according to . 2) generate positive and negative links between nodes within the same blocks according to . 3) generate positive and negative links between nodes belonging to different blocks according to . Accordingly, we have proofed that the log-likelihood of complete data is as follows:

n K



logp(N, Z |K ) =

i=1 q =1



ziq logq +

i<j q,l



(ziq zjl x



(2)



logM (aij ; ) + (1 - ziq zjl ) log M (aij ; )) We now describe the aforementioned SSBM in a full Bayesian framework. In the framework, we need specify the priors for the model parameters (, , ). Since p(zi |), p(aij |Z, ) and p(aij |Z, ) satisfy multinomial distribution, respectively, we select Dirichlet distribution as their conjugate prior distributions, as follows:

0 0 p(| 0 = {0 1 , ..., K }) = Dir(;  ) 0 0 0 p(| 0 = {1 , -1 , 0 }) = Dir(;  0 ) 0 0 0 p(| 0 = {0 1 , -1 , 0 }) = Dir(;  ) 0 0 where q :0 q , h: h , h:h are hyperparameters, which are interpreted as an effective pseudo-occupations of respective blocks in the prior, pseudo-observations of three types of links (positive, negative, no-link) within or between blocks in the prior, respectively. In other words, in the full Bayesian framework, parameters , , and  are regarded as random variables, the distributions of which depend on their respective hyperparameters. Being a generation of standard SBM (Snijders and Nowicki 1997), SSBM is much more flexible and it is able to depict more structural patterns of unsigned or signed networks, as defined in terms of either link density or link sign or both of them, from a stochastic perspective. For examples: (1) in the case of -1 = 0 and -1 = 0, SSBM is able to characterize either the community structure (when 1 > 1 ) or the multipartite structure (when 1 < 1 ) of unsigned networks in terms of link density; (2) in the case of -1 = 0 while 1 = 0, SSBM is able to characterize a balanced signed network in terms of link sign; (3) in the most general case of -1 = 0 while 1 = 0, SSBM is able



Signed Stochastic Blockmodel

Let A denote the adjacency matrix of a signed network N containing n nodes. aij is equal to 1 or -1 if node i is connected to node j by a positive or negative link. Otherwise, aij will be zero. The signed stochastic blockmodel (SSBM for short) of N is defined as a 4-tuple: X = (K, , , ) (1) K is the number of blocks.  is a K -dimension vector, wherein q denotes the prior probability that a node is assigned to block q .  = (1 ,-1 , 0 ) is a 3-dimension vector, in which each component denotes the probability that there is a positive link, negative link, or no link between a pair of nodes within the same block, respective. Similarly, we define  = (1 ,-1 ,0 ), in which each component denotes the probability that there is a positive link, negative link, or no link between a pair of nodes belonging to different blocks.



1953



to characterize the frustration of an unbalanced signed network in terms of both density and sign, in which there are a small fraction of negative links within communities and a small fraction of positive links between communities.



Variational Bayes SSBM Learning

Now we introduce SSBM learning algorithm (SSL for short). SSL adopts a variational Bayes EM algorithm to estimate parameters and an approximate Bayesian model evidence for model selection. We adopt such variational techniques due to two main reasons. First of all, standard EM algorithm cannot be directly used for SSBM in that the components of latent variable Z are correlated and thus the posterior distribution of Z , under the condition of data and model parameters, cannot be explicitly derived as an input required by standard EM. More specifically, component zi is correlated to others means that the computation of its posterior distribution P (zi |N, , , ) is recursively dependent on the distribution of zj for any j = i. Using variational Bayes EM, one can infer an approximate posterior distribution of Z in terms of estimated superparameters. Note that, in the literature, variational EM has ever been adopted for SBM learning (Daudin, Picard, and Robin 2008). Unlike variational EM based on point estimation, variational Bayes EM infers the distribution of Z based on the distributions of parameters instead of their point estimation values (or maximum likelihood estimation values). Therefore, compared with variational EM, variational Bayes EM is more robust and is expected to infer a better posterior distribution close to the truth from real-world networks usually containing much noise. In addition, although the Bayesian model evidence of network N (i.e. log p(N |K )) can be obtained by computing the marginal integration of log p(N, Z |K ) (see Eq. 2) over Z , however, this computation involves a summation of K n terms, which will quickly becomes prohibitively intractable. By taking the model parameters (,,) as random variables, bases on variational Bayes techniques one can readily compute a lower bound of the marginal likelihood in terms of their superparamters, as an approximation of true evidence, for model selection. Superparameter estimation The log-likelihood of N (or the marginal log-likelihood of complete data) can be decomposed into two terms: log p(N ) = L(q (*)) + KL(q (*)||p(*|N )) where L(q (*)) =

Z



In Eqs.3 and 5, KL denotes the Kullback-Leibler divergence between q (Z, , , ) and p(Z, , , |N ). To minimizing Eq.5 with respect to q (Z, , , ) is equivalent to maximizing the lower bound Eq.4. To obtain a computationally tractable algorithm, we use mean field approximation, one of the most popular forms of variational inference, in which we assume the posterior q (Z, , , ) is a fully factorized approximation. Formally, we have:

n



q (Z, , , ) = q ()q ()q ()

i=1



q (zi )



(6)



where q (), q (), q () and q (zi ) denote the posteriors of variables , ,  and Z , respectively, which will be inferred by a variational Bayes EM. Specifically, in its E-step, each distribution q (zi ) is optimized; and in its M-step, q (), q () and q () are optimized, respectively. We first derive the optimal approximation at node i. According to variational Bayes, the optimal posterior q (zi ) is: log q (zi ) = EZ \i ,,, log p(N, Z, , , ) + const = EZ \i ,, log p(N |Z, , ) + EZ \i , log p(Z |) + const

K



=

q =1



ziq

j =i



jq

h



 (aij , h)( (h ) -  (

h



h )) h ))



(7)



+

l=q



jl

h



 (aij , h)( (h ) -  (

h



+ ( (q ) -  (

k



k )) + const



where Z \i denotes Z of all nodes except node i,  (a, h) =1 * I{a=h} +0 * I{a=h} , and h  {1, -1, 0}. When y  Dir(y ; a1 , a2 , ..., aK ), Ey [log(y )] =  (aq ) -  ( aq ) where q  {1, 2, ..., K } and  (*) is Digamma function. To simplify calculations, the terms that do not depend on Zi have been absorbed into the constant. After taking the exponential of Eq.7 and normalization, the optimal approximation at node i is the following multinomial distribution: q (zi ) = M (zi ; 1, i1 , ..., iK ) (8)



(3)



where iq is the probability of node i belonging to block q , and satisfies:

n



q (Z, , , ) x log p(N, Z, , , ) d d d q (Z, , , ) q (Z, , , )x

Z



iq e (4)



 (q )- (



k



k ) j =i



ejq

K



h



 (aij ,h)( (h )- (



h



h ))



x

l=1



ejl



h



 (aij ,h)( (h )- (



h



h ))



KL(q (*)||p(*|N )) = -



(9) (5) Then, we derive the posteriors q (), q (), q () by optimizing the lower bound (see Eq.4), respectively. According



p(Z, , , |N ) log d d d q (Z, , , )



1954



n



to variational Bayes, the optimal distribution q () is: log q () = EZ,, log p(N, Z, , , ) + const = EZ log p(Z |) + log p() + const

K n



+ E logp() + E logp() -

i



Ezi logq (zi )



- E logq () - E logq () - E logq ()

n K



(10) iq log q + const



=

h



0 h - h + i<j q =1 n K



iq jq  (aij , h)  (h ) -  (

h



h)



=

q =1



0 q -1+

i=1



+



After taking the exponential of Eq.10 and normalization, we obtain the optimal approximation of q (), i.e., a Dirichlet distribution, which is the same form as its prior p().

n



0 h - h +

h K i<j q =l



iq jl  (aij , h)  (h ) -  (

h n



h )



+

q =1 n K



0 q - q +

i=1



iq ( (

K q =1 K q =1



 (q ) -  (

q



q )



q () = Dir(;  ),



q = 0 q +

i=1



iq



(11)



-

i=1 q =1



iq log iq + log ( (

h 0 h ) h h )



0 q) q )

h



K q =1 K q =1 h h



(q ) (0 q) (h ) (0 h)



In the same way, we obtain q () and q (), two Dirichlet distributions, which are the same form as their priors.

n K



+ log



(h ) 0 h (h )

h



( (



0 h) h h )



q () = Dir(;  ),



0 h = h + i<j q =1



iq jq  (aij , h) (12)

n K



According 0 h - h +

n i<j



to

n i<j



Eqs.11,12 and 13, the terms K 0 q =1 iq iq  (aij , h), h - h +

n



q () = Dir(;  ), h = 0 h + Eh -

i<j q =1



iq jq (aij , h) (13)



and 0 q - q + i=1 iq in the lower bound vanish. So, finally the low bound is: L(q (*)) = log ( ( ( (

h



K q =l iq jl  (aij , h),



( (



where Eh denotes the number of the positive, negative and no link in the network, respectively. Evidence approximation and model selection So far we have derived the approximated posteriors of model parameters and latent variables. However, the problem of automatically determining block number K has not been addressed, which is significant for exploring realworld networks, in that we usually have not a prior knowledge about K . According to Bayesian theory, an optimal model should be the one with the largest evidence. Formally, the evidence of SSBM is log P (N |K ) = log{ Z P (N, Z, , , |K )d d d }. Unluckly, the computation of SSBM evidence is intractable in that for each value of K , it involves a multiple integration over all possible values of parameters and latent variables. To tackle this issue, we plan to approximate the evidence by its lower bound, as suggested by (Hofman and Wiggins 2008). Recall Eq.3, an evidence is the sum of lower bound (Eq.4) with respect to q (*) and KL divergence (Eq.5). After the convergence of minimizing KL divergence by variational Bayes EM, q (*) is expected to be close to true posterior distribution, or in other words, the KL divergence is expected to be much smaller than the lower bound, thereby the evidence can be approximated by its lower bound with a small error, which can be seemed as the model selection criterion of SSBM. The formula of calculating the lower bound in terms of estimated posteriors of latent variables and parameters is derived as follows:

L(q (*)) =

Z



K q =1 K q =1 h



0 q) q )



K q =1 (q ) K 0 q =1 (q )



+ log



0 h )  ) h h



(h ) 0 h (h )

h (h ) 0 h (h ) n K



(14) -

i=1 q =1



+log



0 h) h h )

h



iq log iq



SSL Algorithm In summary, the algorithm of SSBM learning based on variational Bayes approach is given in Table 1, which can automatically detect the block structure of a given signed network. Next, we analyze its time complexity. Updating the posterior of Z by for loop in line 07-09 takes O(Kn2 ). Updating the posterior of  by for loop in line 10-11 takes O(Kn). Updating the posteriors of  and  by for loop in line 12-14 takes O(Kn2 ). Consequently, when K is given, the time of SSL is O(IKn2 ), where I denotes the iterations of repeat loop until convergence. Calculating the lower bound LK in 16 takes O(Kn). So, when K is unknown, the total time of SSL is O(In2 (Kmax - Kmin )2 ).



Validation

In this section, we test the proposed SSBM and SSL toward two main tasks: community detection and sign prediction.



Validation on community detection

In showing the superiority of SSBM and SSL, three representative algorithms for signed community detection are selected to compare. They are the frustration-optimization based DM (Doreian and Mrvar 1996), the random-walk based FEC (Yang, Cheung, and Liu 2007), and the modularity-optimization based PSA (Traag and Bruggeman 2009), respectively. We use both synthetic networks and



q (Z, , , )log



p(N, Z, , , ) d d d q (Z, , , )



= EZ,, logp(N |Z, , ) + EZ, logp(Z |) + E logp()



1955



i oc oe oe oe oe oi UUY UO I IIO i



i oe oe oi oi UUY UO I IIO i



i oc oe oe UUY UO I IIO i



ioe ioe ioi ioi ioi



 i



i



oi oi oi oi oi oe oe oe oe oc *



 oi oi oi oi oe oe oe oe oc o



oe oi oi oi oi oe oe oe oe oc o



i



i



O



e



e



i



(a)



(b)



(c)



(d)



Figure 1: Performance comparisons of community detection. models are used, i.e. SG(4, 300, 100, 0.8, p- , 0.2) and SG(4, 300, 100, 0.8, 0.2, p+ ), in which p- and p+ gradually increase from 0 to 1 stepping by 0.1, respectively. The two models are used to test the influence of two types of noise on the performance of community detection. For each model mentioned above, we generate 100 random networks. Fig. 1(a) shows the performance of four algorithms on balanced networks. As we can see, SSL and PSA perform the best. For all pin , the detections provided by these two algorithms are exactly the same as ground truth (i.e. NMI=1). Compared with DM and FEC, this result implies a good feature of SSL and PSA. That is, when handling balanced networks, the performance of these two algorithms will be not affected by the link density within communities. Figs. 1(b) and 1(c) show the performance of four algorithms on unbalanced networks. In Fig. 1(b), p+ is fixed and the noisy level within communities augments as p- increasing. As we see, the performance of SSL is significantly better than other three, and the detections provided by it are exactly the same as ground truth except for p- > 0.9. In Fig. 1(c), p- is fixed and the noisy level outside communities augments as p+ increasing. In this case, SSL, FEC and PSA, particularly the first two, performs much better than DM. The main reason is, as the fraction of positive links across communities (i.e. p+ ) increasing, the signed network being handled gradually turn into an unsigned network, in which community structure are dominated by link density. Compared with DM that focuses on optimizing the frustration of signs, the other three consider not only link sign but also link density when they are partitioning a network, thereby leading to a much better performance in this case. From these results, one notes that SSL performs the best when handling unbalanced networks contain different types and different levels of noise. The rationale is two told: 1) SSBM explicitly models such noise with parameters such as -1 and 1 ; and 2) SSL adopts variational Bayes to estimate the distributions rather than point values of such parameters. Fig. 1(d) shows the model selection process of SSL, in which y -axis denotes the minus evidence corresponding to different K . As an example, we just show the interval of K from 1 to 10. As we see, the evidence reaches its biggest value when K = 4, exactly the same as the truth. Modularity-optimization based methods such as PSA will suffer the problem of resolution limitation. That is, such methods tend to detect a small number of bigger commu-



Table 1: SSL Algorithm

X=SSL(N ,Kmin ,Kmax ) 01 Input: N, Kmin , Kmax 02 Output: Z 03 initialize L 04 for K = Kmin to Kmax 05 initialize , , , ; 06 repeat 07 for i = 1 to N // Update the posterior over each zi 08 for q = 1 to K 09 update iq according to Eq. 9; 10 for q = 1 to K // Update the posterior over  11 update q according to Eq. 11; 12 for h  {1, -1, 0} // Update the posterior over ,  13 update h according to Eq. 12; 14 update h according to Eq. 13; 15 until convergence 16 update LK according to Eq. 14; // Update the lower bound 17 if LK > L then L = LK ; Zp =  ; 18 calculate Z according to Zp ;



real-world networks to test the four algorithms. Since all test networks contain ground truth community structures, the NMI criterion (Kuncheva and Hadjitodorov 2004) is adopted to quantitatively measure the accuracy of community detections. Intuitively, the larger NMI, the closer to ground truth. Synthetic signed networks We first use synthetic networks to test. Although the proposed SSBM is a generation model of signed networks, for the sake of fairness, here we choose a widely used model (Yang, Cheung, and Liu 2007) to produce synthetic signed networks, which is defined as: SG(c, n, k, pin , p- , p+ ) where c is the number of communities, n is the number of nodes in each community, k is the average degree of node, pin is the probability of each node connecting other nodes in the same community. p- and p+ regulate noise levels, denoting the probabilities of negative links within communities and positive links across communities, respectively. First, we generate two types of synthetic signed networks: balanced networks and unbalanced networks. For balanced networks, the generation model is SG(4, 300, 100, pin , 0, 0), in which pin increases from 0.1 to 1 stepping by 0.1. For unbalanced networks, two



1956



I(R) IIO I



15 16 17 18 19 20 21 22 23 24 25 15 16 17 18 19 20 21 22 23 24 25 15 16 17 18 19 17 14 10 9 10 27



Figure 2: Resolution limitation test. nities. Next, we will test whether the proposed Bayesian approach is able to fix this issue. In this experiment, the network to be tested is similar to the one suggested by (Hofman and Wiggins 2008), which consists of a ring of complete cliques, as shown in Fig. 2. Each clique stands for a community. The links within cliques are positive (see solid lines), and those between cliques are negative (see dotted lines). As the number of cliques in the ring (denoted by Ktrue ) increasing, it gets more and more challenge to precisely detect them all. Fig.2 show the performance of SSL and PSA. As we see, SSL performs perfectly in all cases, much better than PSA in the cases of larger Ktrue , although PSA already takes effort to weaken the effect of resolution limitation. Real-world signed networks We use Slovene parliamentary party network (Kropivnik and Mrvar 1996), GahukuGama subtribes network (Read 1954) and monastery network (Doreian and Mrvar 1996) to further validate SSL. The three real-world networks are chosen because they all have ground truth community structures and thereby have been the benchmarks for testing the performance of signed community detection (Yang, Cheung, and Liu 2007; Doreian and Mrvar 1996). In all cases, the detections of SSL are exactly the same as the ground truth. Note that, before applying SSL to Slovene parliamentary party network, we first turn it into a binary network by setting zero as the threshold of positive and negative links.



connected and balanced signed network, where there are five communities that contain 100, 200, 300, 400 and 500 nodes, respectively. A subnetwork N is constructed by sampling links from N with a sampling rate s. N is regarded as an observed network for training and the rest links in N - N as incoming links for prediction. s takes the values 0.005, 0.01,0.02,0.03,0.05,0.07 and 0.1, alternatively. For each value, 100 subnetworks are sampled to calculate the prediction accuracy on average. Fig. 3(a) shows the performance of five algorithms. As we see, SSL, PSA and LR perform very good and stable; SSL provides the best prediction when s > 0.01.

i oc oe



i oc oe



OI ONY IIO I ONx



oe oe



OI ONY IIO I ONx



oe oe oi



oe oi oi oi oe oe U(R)1/2*  3/4(R)1/4 *



oi



oie



oi oie oi oie U(R)1/2*  * *



oi



(a)



(b)



Figure 3: Performance comparisons of sign prediction. Next, we test the five algorithms in a more challenge way by injecting different levels of noise into the abovegenerated balanced networks. In this validation, we first construct a subnetwork N with a sampling rate s = 0.1, and thereafter we change the signs of randomly selected links within or between communities with a noise rate , varying from 0.1 to 0.4 with an increment 0.03. Similarly, for each configuration we generate 100 subnetworks to calculate prediction accuracy on average. Fig. 3(b) shows the performance of five algorithms on such unbalanced networks. As we see, SSL works still better, particularly for the unbalanced networks containing more noise (i.e. > 0.25). Note that, both SSL and PSA, the two community detection based methods, perform quite good among the five competitors for both balanced and unbalanced networks. This is probably because these methods implicitly take more information, provided by the global community structure in terms of both link density and sign, into account for prediction making.



Validation on sign (or attitude) prediction

In showing the superiority of SSL for sign prediction, three representative algorithms are selected to compare. They are the balance theory based MOI (Chiang et al. 2011), the supervised learning based HOC (Leskovec, Huttenlocher, and Kleinberg 2010), and the matrix factorization based LR (Chiang et al. 2013). Distinctly, SSL predicts link signs based on community detection. Provided that we have a community structure of a network, SSL predicts the sign of an incoming link based on the following rule: the link is positive if both end nodes fall into the same community, otherwise it is negative. Based on the same idea, the aforementioned PSA is also selected to join the comparison. The fraction of correct prediction is used to measure the performance of sign prediction, which is defined as Rp = Ep /Et , where Ep is the number of links being correctly predicted and Et is the total number of links to be predicted. In this experiment, we follow the same way as suggested by (Chiang et al. 2013) to test the sign prediction performance of five algorithms, in which the learning data and testing data are generated as follows. Let N be a fully-



Conclusion

Community detection and sign prediction are important for signed network analysis. Most of the existing methods are discriminative, which are depend on either predefined optimization objectives or heuristics. Distinctly, we propose a generative Bayesian approach to addressing these tasks, in which a signed stochastic blockmodel is proposed to characterize the block structures of signed networks in terms of both link density and sign and a variational Bayes method is proposed for model learning. To the best of our knowledge, this is the first effort in the literature to generalize the current SBM to address signed networks.



1957



Acknowledgements

This work was funded by the Program for New Century Excellent Talents in University under Grant NCET-11-0204, and the National Science Foundation of China under Grants 61133011, 61373053, and 61300146.



References

Airoldi, E. M.; Blei, D. M.; Fienberg, S. E.; and Xing, E. P. 2008. Mixed membership stochastic blockmodels. Journal of Machine Learning Research 9:1981-2014. Anchuri, P., and Magdon-Ismail, M. 2012. Communities and balance in signed networks: A spectral approach. In Proceedings of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012), 235-242. IEEE Computer Society. Bansal, N.; Blum, A.; and Chawla, S. 2004. Correlation clustering. Machine Learning 56(1-3):89-113. Cartwright, D., and Harary, F. 1956. Structural balance: a generalization of heider's theory. Psychological review 63(5):277-293. Chiang, K.-Y.; Natarajan, N.; Tewari, A.; and Dhillon, I. S. 2011. Exploiting longer cycles for link prediction in signed networks. In Proceedings of the 20th ACM international conference on Information and knowledge management, 1157-1162. ACM. Chiang, K.-Y.; Hsieh, C.-J.; Natarajan, N.; Tewari, A.; and Dhillon, I. S. 2013. Prediction and clustering in signed networks: A local to global perspective. arXiv preprint arXiv:1302.5145. Daudin, J.-J.; Picard, F.; and Robin, S. 2008. A mixture model for random graphs. Statistics and computing 18(2):173-183. Davis, J. A. 1967. Clustering and structural balance in graphs. Human relations 20(2):181-187. Doreian, P., and Mrvar, A. 1996. A partitioning approach to structural balance. Social networks 18(2):149-168. Girvan, M., and Newman, M. E. 2002. Community structure in social and biological networks. Proceedings of the National Academy of Sciences 99(12):7821-7826. Gong, M.; Cai, Q.; Chen, X.; and Ma, L. 2014. Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition. Evolutionary Computation, IEEE Transactions on 18(1):82-97. Hofman, J. M., and Wiggins, C. H. 2008. Bayesian approach to network modularity. Physical review letters 100(25):258701. Holland, P. W., and Leinhardt, S. 1981. An exponential family of probability distributions for directed graphs. Journal of the american Statistical association 76(373):33-50. Huang, Z., and Qiu, Y. 2010. A multiple-perspective approach to constructing and aggregating citation semantic link network. Future Generation Computer Systems 26(3):400-407. Kropivnik, S., and Mrvar, A. 1996. An analysis of the slovene parliamentary parties network. Developments in Statistics and Methodology 209-216.



Kuncheva, L. I., and Hadjitodorov, S. T. 2004. Using diversity in cluster ensembles. In Systems, man and cybernetics, 2004 IEEE international conference on, volume 2, 1214- 1219. IEEE. Larusso, N.; Bogdanov, P.; and Singh, A. 2010. Identifying communities with coherent and opposing views. In Proc. of the 15th Annual Graduate Student Workshop in Computing. Santa Barbara: UCSB, 31-32. Latouche, P.; Birmel e, E.; Ambroise, C.; et al. 2011. Overlapping stochastic block models with application to the french political blogosphere. The Annals of Applied Statistics 5(1):309-336. Leskovec, J.; Huttenlocher, D.; and Kleinberg, J. 2010. Predicting positive and negative links in online social networks. In Proceedings of the 19th international conference on World wide web, 641-650. ACM. Liu, C.; Liu, J.; and Jiang, Z. 2014. A multiobjective evolutionary algorithm based on similarity for community detection from signed social networks. Cybernetics, IEEE Transactions on PP(99):1-1. Mitrovi c, M., and Tadi c, B. 2009. Spectral and dynamical properties in classes of sparse networks with mesoscopic inhomogeneities. Physical Review E 80(2):026123. Newman, M. E. 2004. Fast algorithm for detecting community structure in networks. Physical review E 69(6):066133. Palla, G.; Der enyi, I.; Farkas, I.; and Vicsek, T. 2005. Uncovering the overlapping community structure of complex networks in nature and society. Nature 435(7043):814-818. Pizzuti, C. 2008. Ga-net: A genetic algorithm for community detection in social networks. In Parallel Problem Solving from Nature-PPSN X. Springer. 1081-1090. Read, K. E. 1954. Cultures of the central highlands, new guinea. Southwestern Journal of Anthropology 1-43. Snijders, T. A., and Nowicki, K. 1997. Estimation and prediction for stochastic blockmodels for graphs with latent block structure. Journal of classification 14(1):75-100. Traag, V., and Bruggeman, J. 2009. Community detection in networks with positive and negative links. Physical Review E 80(3):036115. Yang, T.; Chi, Y.; Zhu, S.; Gong, Y.; and Jin, R. 2011. Detecting communities and their evolutions in dynamic social networksa bayesian approach. Machine learning 82(2):157-189. Yang, B.; Cheung, W. K.; and Liu, J. 2007. Community mining from signed social networks. Knowledge and Data Engineering, IEEE Transactions on 19(10):1333-1348. Yang, B.; Liu, J.; and Liu, D. 2012. Characterizing and extracting multiplex patterns in complex networks. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 42(2):469-481. Zhou, H. 2003. Distance, dissimilarity index, and network community structure. Physical review e 67(6):061901.



1958



Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting

Lin Chen1,2 and Hamed Hassani3 and Amin Karbasi1,2

1



Department of Electrical Engineering, 2 Yale Institute for Network Science, Yale University 3 Computer Science Department, ETH Zurich {lin.chen, amin.karbasi}@yale.edu, hamed@inf.ethz.ch



arXiv:1603.03515v2 [cs.AI] 12 Nov 2016



Abstract

In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.g., adversarial reverse engineering). We develop a general framework, called dimension coupling (DC), that 1) reduces a d-dimensional learning problem to d - 1 lowdimensional sub-problems, 2) solves each sub-problem efficiently, 3) appropriately aggregates the results and outputs a linear classifier, and 4) provides a theoretical guarantee for all possible schemes of aggregation. The proposed method is proved resilient to noise. We show that the DC framework avoids the curse of dimensionality: its computational complexity scales linearly with the dimension. Moreover, we show that the query complexity of DC is near optimal (within a constant factor of the optimum algorithm). To further support our theoretical analysis, we compare the performance of DC with the existing work. We observe that DC consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster.



1 Introduction

In contrast to the passive model of supervised learning, where all the labels are provided without any interactions with the learning mechanism, the key insight in active learning is that the learning algorithm can perform significantly better if it is allowed to choose which data points to label. This approach has found farreaching applications, including the classical problems in AI (e.g., classification (Tong and Koller 2002), information retrieval (Tong and Chang 2001), speech recognition (Hakkani-Tur, Riccardi, and Gorin 2002)) as well as the modern ones (e.g., interactive recommender systems (Karbasi, Ioannidis, and Massoulie 2012) and optimal decision making (Javdani et al. 2014)). In all the above applications, the unlabeled data are usually abundant and easy to obtain, but training labels are either time-consuming or expensive to acquire (as they require asking an expert). Throughout this paper, our objective is to actively learn an unknown halfspace H  = {x  Rd  h , x > 0}



via query synthesis (a.k.a. membership queries), where ,  denotes the standard inner product of the Euclidean space and h is the unit normal vector of the halfspace we want to learn. We would like to note that learning the halfspace H  is mathematically equivalent to learning its unit normal vector h ; therefore we focus on learning h hereinafter. In addition, it should be noted that using the kernel trick we can easily extend the halfspace learning to more complex (e.g., non-linear) decision boundaries (Shawe-Taylor and Cristianini 2004). The hypothesis space H, which consists of all possibilities of unit normal vectors, is the unit sphere S d-1 = {x  Rd  x = 1}, where  denotes the standard Euclidean norm. In active learning of halfspaces via query synthesis, the algorithm is allowed to query whether any point x in Rd resides in the true halfspace. When the algorithm queries x, the true outcome is sign(h , x)  {1, -1}. When sign(h , x) = 1, it means that x  H  ; otherwise, x  H  . We should note here that the only information we obtain from a query is the sign of the inner product rather than the value. For example, the queries of the form sign(h , ei ), where ei is the ith standard basis vector, will only reveal the sign of the ith component of h (and nothing further about its value). In the noiseless setting, we observe the true outcome of the query, i.e. sign h , x  {1, -1}. In the noisy setting, the outcome is a flipped version of the true sign with independent flip probability . That is, denoting the outcome by y we have y  {-1, 1} and Pr[y  sign h , x]   < 1 2. Since the length of the selected vector x will not affect the outcome of the query, we only query the points on the unit sphere S d-1 = {x  Rd  x = 1}. Hence, we term X = S d-1 as the query space. Given ,  > 0, we would like to seek an active learning algorithm that (i) adaptively selects vectors x1 , x2 , . . .  X , (ii) observes the (noisy) responses to each query signh , xi , (iii) and outputs, using as few queries as possible, an esti of h such that h  - h <  with probability at least mate h 1 - . Our main contribution in this paper is to develop a noise resilient active learning algorithm that has access to noisy membership queries. To the best of our knowledge, we are the first to show a near-optimal algorithm that outperforms in theory and practice the naive repe-



tition mechanism and the recent spectral heuristic methods (Alabdulmohsin, Gao, and Zhang 2015). Specifically, we develop a framework, called Dimension Coupling (DC), with the following guarantees. Its query complexity is 1 O(d(log 1 + log  )) and its computational complexity is  1 1 2 O(d(log  + log  ) ). In particular, in the noiseless setting ( = 0), both its computational complexity and query complexity are O(d log 1 ). Note that in both settings the com putational complexity scales linearly with the dimension. Moreover, the query complexity in both settings is nearoptimal. Our empirical experiments demonstrate that DC runs orders of magnitude faster than the existing methods. The rest of the paper is structured as follows. In Section 2, we start with investigating this problem in the 2-dimensional case and present an algorithm called DC2 . Then in Section 3 we generalize it to the d-dimensional case and present a general framework called DC. Empirical results are shown in Section 4. We extensively review related literature in Section 5.



Algorithm 1 DC 2 Input: orthonormal vectors e1 , e2 , estimation error at most , success probability at least 1 -  . Output: a unit vector e  which is an estimate for the normalized orthogonal projection of h onto span{e1 , e2 }.

1: Set p0 (h) to be uniform, i.e., h  S 1  p0 (h) = 1 2 . 2: for m = 1 to T, do 3: Find a vector xm  S 1 which is a solution to the fol-



4: 5: 6: 7:



lowing equation: S 1 sign x, h pm-1 (h)dh = 0. If there are multiple solutions, choose one arbitrarily. Ask from the oracle the value of sign xm , h . Based on the response obtained from the oracle, update the distribution pm-1 (h) to pm (h). end for return e  = arg maxhS 1 pT, (h).



2



DC : Solving the 2-Dimensional Problem



2



To gain more intuition before studying the general ddimensional problem, it might be beneficial to study a special case where the dimension is two. In other words, we study in this section how to learn the normalized projection of the true unit normal vector h  Rd onto span{e1 , e2 }, where e1 , e2  Rd are two orthonormal vectors and span{e1 , e2 } is the linear subspace spanned by e1 and e2 . We should note here that the underlying space is still d-dimensional (i.e., Rd ) but our goal is not to learn h per se but its normalized projection onto a 2-dimensional subspace. Formally, given two orthonormal vectors e1 , e2 we denote the (normalized) projection of h onto span{e1 , e2 } by h , i.e., h , e1  e1 + h , e2  e2 h = . (1) h , e1  e1 + h , e2  e2 2 Our objective is to find a unit vector e   span{e1 , e2 } such that e  - h < . In fact, we require the latter to hold with probability at least 1 -  . We should emphasize that noise, characterized by independent flip probability , is generally present. In the 2dimensional problem, one may propose to use the simple binary search (a detailed discussion with examples is presented in Appendix B) to find a unit vector e  that resides -close to h . To make it noise-tolerant, when the binary search algorithm queries a point, say xi , we query it R times to obtain R noisy versions of sign h , xi  and view the majority vote of the noisy versions as the true outcome (Kaariainen 2006; Karp and Kleinberg 2007; Nowak 2011). We call this method repetitive querying. However, its query complexity is O(log(1 )(log log(1 ) + log(1  )), which is suboptimal both theoretically (we will prove this bound in Appendix C) and empirically (referred to as R EPETITIVE -DC in Section 4). As a result, instead, we will present a Bayesian algorithm termed DC2 that solves this 2-dimensional problem with query complexity O(log(1 ) + log(1  )). Recall that any unit vector inside span{e1 , e2 }, e.g., h , can equivalently be



We take a Bayesian approach. In the beginning, when no queries have been performed, DC 2 assumes no prior information about the vector h . Therefore, it takes the uniform distribution on S 1 (with pdf p0 (h) = 21 ) as its prior belief  about h . After performing each query, the posterior (belief) about h will be updated according to the observation. We let pm (h) denote the (pdf of the) posterior after performing the first m queries. In this manner, DC2 runs in total of T, rounds, where in each round a specific query is selected and posed to the oracle. The number T, will be specified later (see Theorem 1). Upon the completion of round T, , the algorithm returns as its final output a vector e   S 1 that maximises the posterior pdf pT, (h). If there are multiple such maximisers, it picks one arbitrarily. We now proceed with a detailed description of DC 2 (a formal description is provided in Algorithm 1). As shown in Algorithm 1, at each round, say round m + 1, the algorithm maintains and updates the distribution pm that encodes its current belief in the true location of h . We should note here that these distributions can be stored efficiently and as a result the vector xm+1 can be computed efficiently. Indeed, (the pdf of) pm is piecewise constant on the unit circle (see Figure 1). More precisely, at any round m, there are at most 2m points u1 , u2 , , u2m that are ordered clock-wise on the unit-circle and pm is constant when restricted to each of the sectors [ui , ui+1 ). The piecewise constant property of the pdf of pm can be established by induction on m. Recall that the initial distribution p0 is uniform and thus piecewise constant. The Bayesian update step (line 5 of Algorithm 1) preserves this property when the algorithm updates the distribution pm (h) to pm+1 (h). We will show why this is true when we discuss the Bayesian update step in detail.



represented as a pair (c1 , c2 ) on the two-dimensional unit 2 circle S 1 (e.g., h = c1 e1 + c2 e2 and c2 1 + c2 = 1). To simplify 1 notation, we use a point (c1 , c2 )  S and its corresponding unit vector c1 e1 + c2 e2 interchangeably. In this setting, it is easy to see that for any x  span{e1 , e2 } sign x, h  = sign x, h  . (2)



At round m + 1, in order to find xm+1 (see line 3 of Algorithm 1), DC 2 first finds a line that passes through the centre of S 1 and cuts S 1 into two "halves" which have the same measure with respect to pm . Note that finding such a line can be done in O(m) steps because pm has the piecewise constant property. Once such a line is found, it is then easy to see that xm+1 can be any of the two points orthogonal to the line. As a result, DC 2 at round m + 1 can find xm+1 in O(m) operations. We denote the half-circle containing xm+1 by R+ and the other half by R- . We refer to Figure 1 for a schematic illustration. The key step in Algorithm 1 is the Bayesian update (line 5). Once a noisy response to the query sign xm+1 , h  is obtained (line 4)), the probability distribution pm will be updated to pm+1 in the following way. First, consider the event that the outcome of sign xm+1 , h  is +1. We have pm (sign xm+1 , h  = +1) = (1 - ) pm (R+ ) +  pm (R- ) = 1 2, and similarly pm (sign xm+1 , h  = -1) = 1 2. Therefore, by Bayes theorem we obtain the following update rules for pm+1 . If we observe that sign xm+1 , h  = +1, then for h  R+ we have and for h  R we have

-



Figure 1: Upon the completion of round m (left figure), the distribution (pdf of) pm is constant over each of the sectors [ui , ui+1 ). In the next round (right figure), in order to find xm+1 , DC2 first finds a diagonal line (red line) which separates two half-circles (R+ and R- ) that each has measure 1 2 w.r.t pm . The vector xm+1 will then be one of the two points on the unit circle that are orthogonal to this line. For updating pm to pm+1 , we note that all the points inside R+ get the same factor (either 2 or 2(1 - ) depending on the outcome of the query). The same is true for R- . Thus, pm+1 is again a piecewise constant pdf but now on 2(m + 1) sectors.



pm+1 (h) = 2(1 - )pm (h) pm+1 (h) = (2)pm (h).



Also, if we observe that sign xm+1 , h  = -1, then for h  R+ we have pm+1 (h) = (2)pm (h) and for h  R- we have pm+1 (h) = 2(1 - )pm (h).



Note that the factor of 2 here is due to the normalization. It is easy to verify that pm+1 is also a piecewise constant distribution (now on 2(m + 1) sectors; see Figure 1). 1 Theorem 1 shows that after T, = O(log 1 + log  )  2 rounds, with probability at least 1 -  , DC outputs a unit vector e   span{e1 , e2 } such that e  - h < . Also, as discussed above, the computational complexity of DC 2 is 2 + log 1 )2 ). O(T, ), i.e., O((log 1   1 1  M + max{T0 , T1 , T2 , T3 } = O(log + log ) (3)  



h . We present a detailed discussion with examples in Appendix B. A few comments are in order: The above guarantee for DC2 holds with probability one and thus the parameter  is irrelevant in the noiseless setting. Furthermore, during each round of DC 2 , the distribution pm can be represented by only two numbers (the starting and ending points of the sector Rm ), and the vector xm can be computed efficiently (it is the orthogonal vector to the midpoint of Rm ). Therefore, assuming one unit of complexity for performing the queries, DC 2 can be implemented with complexity O(T, ), i.e., O(log(1 )).



3



Dimension Coupling Based Framework



Theorem 1. (Proof in Appendix A) When the independent flip probability is , having T,



In Section 2, we devise an algorithm, called DC 2 (e1 , e2 , ,  ), that takes as input two orthonormal vectors e1 , e2 , uses noisy responses to queries of the form sign x, h , and outputs with probability at least 1 -  a vector e  with the following three properties: e   span{e1 , e2 }, e  = 1, e -

h ,e1 e1 +h ,e2 e2 h ,e1 e1 +h ,e2 e2



< .



is sufficient to guarantee that DC 2 outputs with probability at least 1 -  a vector that is within a distance  of h . 2 2 log 2 8 log   Here, we have M =  - log(4(1 , T0 = log(2(1- , T1 = -)) ))

1 8 log 8 8 4 , T2 = log(2( log(2M ) + log( log(2( ) log(2(1-)) 1-)) 1-)) 2 1- 24 log  and T3 = log2 (2(1-)) log(M ) + log( 4 ) . 



We would like to remark that when the independent flip probability  is 0 (i.e., in the noiseless case), the algorithm , DC 2 reduces to the binary search. If we let T, = log2   then DC2 outputs a vector that is within a distance  of



In other words, the unit vector e  is within a distance  to the (normalized) projection of h onto the subspace span{e1 , e2 }. In the current section, we explain a framework DC that estimates h using at most d - 1 calls to DC2 (a formal description will be given in Algorithm 2 later). Let us begin our discussion with a motivating example. Let {e1 , e2 , . . . , ed } be an orthonormal basis of Rd . Suppose d that h has the form h = d i=1 ci ei , where {ei }i=1 is an arbid trarily chosen orthonormal basis for R . We assume w.l.o.g. 2 that h is normalized (i.e., d i=1 ci = 1). Our objective is d then to learn the coefficients {ci }i=1 within a given precision by using the noisy responses to the selected sign queries. The key insight here is that this task can be partitioned in



 = DC2 (e h 12 , e 34 ) e 12 = DC (e1 , e2 )

2



tion h onto span{e1 , e2 , e3 }; finally call DC 2 (e 123 , e4 ) and   obtain an estimate for h which we denote by h.

2



e 34 = DC (e3 , e4 ) e3 e4



e1



e2



(a) Scheme 1: a balanced full binary tree



 = DC 2 (e h 123 , e4 ) e 123 = DC (e 12 , e3 )

2



Examples 1 and 2 show two possibilities of divide-andconquer schemes for a 4-dimensional problem. In fact, each scheme corresponds to a full binary tree of 4 leaf nodes. For general d, the idea is similar: We break the problem into at most d - 1 "two-dimensional" problems that each can be solved efficiently. Again, each divide-and-conquer scheme corresponds to a full binary tree of d leaf nodes. Consider the decomposition h = d i=1 ci ei . Without loss of generality, suppose that the first two leaf nodes to be combined are e1 and e2 . We can write h =

d



e4



ci e i = c 12

i=1



c1 e 1 + c2 e 2

2 c2 1 + c2



+



d



ci e i ,

i=3



(4)



e 12 = DC 2 (e1 , e2 ) e1 e2



e3



(b) Scheme 2: an unbalanced full binary tree



Figure 2: Two possible divide-and-conquer schemes for a 4dimensional problem. Each scheme can be represented by a full binary tree of 4 leaf nodes. a divide-and-conquer fashion into many smaller tasks, each involving a few dimensions. The final answer (the values of { ci } d i=1 ) will then be obtained by aggregating the answers of these subproblems. Example 1. Assume h = c1 e1 + c2 e2 + c3 e3 + c4 e4 , where ei 's are the standard basis vectors for R4 . Define c3 e 3 + c4 e 4 c1 e 1 + c2 e 2 , e34 = e12 = 2 2 2 c1 + c2 c2 3 + c4





of h onto span{e1 , e2 }. Hence, by using DC 2 (e1 , e2 , ,  ) we can obtain, with probability at least 1 -  , a good approximation e 12 (within a distance ) of this projection. Therefore, for small enough  we have h  c 12 e 12 + d i=3 ci ei . Since  h is now expressed (approximately) in terms of d - 1 orthonormal vectors {e 12 , e3 , e4 , . . . , ed }, we have effectively reduced the dimensionality of problem from d to d - 1. The idea is then to repeat the same procedure as in (4) to the newly obtained representation of h . Hence, by repeating this procedure d - 1 times in total we will reach a vector which is the final approximation of h . We present this general method in Algorithm 2.





2 where in the last step we have taken c 12  c2 1 + c2 . Now, c1 e1 +c2 e2 note that is the normalized orthogonal projection 2 2 c1 +c2



Note here that e12 is the (normalized) orthogonal projection of h onto span{e1 , e2 } and e34 is the (normalized) orthogonal projection of h onto span{e3 , e4 }. Consider the following procedure to learn h : first find out what e12 and e34 are, 2 2 and then use the relation h = c2 c2 1 + c2 e12 + 3 + c4 e34  to find h based on the orthonormal vectors e12 , e34 . By this procedure, the original "four-dimensional" problem has been broken into three "two-dimensional" problems. This procedure is illustrated in Figure 2a. We first call DC 2 (e1 , e2 ) to obtain an estimate e 12 for e12 ; then we call DC 2 (e3 , e4 ) to obtain an estimate e 34 for e34 ; finally we call  for h . DC 2 (e 12 , e 34 ) to obtain an estimate h Example 2. For another example of the 4-dimensional problem discussed in Example 1, let us consider another scheme illustrated in Figure 2b: We call DC2 (e1 , e2 ) and 2 obtain e 12 , e3 ) and 12 as an estimate for e12 ; then call DC (e obtain e 123 that estimates the normalized orthogonal projec-



Input: an orthonormal basis E = {e1 , e2 , . . . , ed } of Rd . Output: a unit vector  h which is an estimate for h . 1: for j  1 to d - 1 do 2: Replace any two vectors e and e in E with the vector DC2 (e , e , ,  ). 3: end for  be the only remaining vector in E . 4: Let h  5: return h Theorem 2. (Proof in Appendix D) For DC (outlined in Algorithm 2) and any of its divide-and-conquer scheme represented by a full binary tree, we have: 1. DC will call the two-dimensional subroutine DC2 d - 1 times. 2. Provided that the output of DC 2 is with probability 1 -  within distance  of the true value and   5 18, DC ensures an estimation error of at most 5(d - 1) with probability at least 1 -  (d - 1).



Algorithm 2 Dimension Coupling (DC)



As a result of Theorem 2, if we desire the framework DC to estimate h within distance   and with probability at least , then it is enough to fix the corresponding parameters 1-    of DC 2 to  = 5(d and  = d- . -1) 1



Theorem 2 indicates that DC requires O(d(log 1 +log 1 ))   2 1 1 queries, since each call to DC needs O(log  + log  ) queries. Recall that the computational complexity of DC2 1 2 + log  ) ). Hence, DC has computational comis O((log 1  1 1 2 plexity O(d(log  + log  ) ). As a special case, if in absence of noise, both the query complexity and time complexity of DC are O(d log 1 ). 



4 Empirical Results

In this section, we extensively evaluate the performance of DC against the following baselines: R ANDOM -S AMPLING: Queries are generated by sampling uniformly at random from the unit sphere S d-1 . U NCERTAINTY-S AMPLING: Queries are sampled uniformly at random from the orthogonal complement of w, where w is the vector learned by linear SVM. Q UERY- BY-BAGGING: The bag size is set to 20 and 1000 queries are generated at each iteration. The query with the largest disagreement is picked (Abe and Mamitsuka 1998). S PECTRAL: The version space is approximated by the largest ellipsoid consistent with all previous query-label pairs. Then, at each iteration a query is selected to approximately halve the ellipsoid (Alabdulmohsin, Gao, and Zhang 2015). R EPETITIVE -DC: In the noisy setting, one easy way to apply DC is to query each point R times and use the majority rule to determine its label; i.e., the combination of repetitive querying (Section 2) and the DC framework (Section 3). Our metrics to compare different algorithms are: a) estimation error, b) query complexity, and c) execution time. In particular, as we increase the number of queries we measure the average estimation errors and execution times for all the baselines (with 90% confidence intervals). By nature, in active learning via query synthesis, all data points and queries are generated synthetically. For all the baselines, we used the fastest available implementations in MATLAB. Noiseless setting: Figures 3a and 3b (with dimension d = 25 and 50, respectively) show that in terms of estimation error, DC outperforms all other baselines, and significantly outperforms R ANDOM -S AMPLING, U NCERTAINTYS AMPLING and Q UERY- BY-BAGGING. Note that the estimation errors are plotted in log-scales. In terms of execution times, we see in Fig. 3c that DC runs three orders of magnitude faster than other baselines. Training an SVM at each iteration for R ANDOM -S AMPLING, U NCERTAINTYS AMPLING and Q UERY- BY-BAGGING comes with a huge computational cost. Similarly, S PECTRAL requires solving a convex optimization problem at each iteration; thus its performance drastically deteriorates as the dimension increases, which makes it infeasible for many practical problems. Noisy setting: We set the noise level to  = 0.1 and compare the performance of DC against R ANDOM S AMPLING, U NCERTAINTY-S AMPLING, Q UERY- BYBAGGING, and R EPETITIVE -DC. As mentioned in (Alabdulmohsin, Gao, and Zhang 2015), and we have also observed in our experiments, S PECTRAL does not work even for small amounts of noise as it incorrectly shrinks the version space and misses the true linear separator;



therefore it is excluded here. We see again in Figures 3d and 3e (for d = 25 and 50) that DC significantly outperforms all other methods in terms of estimation error. More precisely, using the same number of queries, the estimation error of DC is around two orders of magnitude smaller than other baselines. We can also observe from these two figures that DC still runs around 100 times faster than R ANDOM -S AMPLING, U NCERTAINTY-S AMPLING, and Q UERY- BY-BAGGING. Clearly, DC has a higher computational cost than R EPETITIVE -DC, as DC performs a Bayesian update after each query. Finally, as we increase the dimension to d = 1000, R ANDOM -S AMPLING, U NCERTAINTY-S AMPLING, and Q UERY- BY-BAGGING become significantly slower. Hence, in Figure 3f we only show how the estimation error (for noise levels  = 0.01, 0.1, 0.2) decreases for DC and R EPETITIVE -DC with more queries. It can be observed from Figure 3f that consuming the same number of queries, DC can achieve an estimation error from one order (when the noise intensity is very small) to three orders of magnitude (when the noise intensity is 0.2) smaller than that of R EPETITIVE -DC.



5 Related Work

The sample complexity of learning a hypothesis was traditionally studied in the context of probably approximately correct (PAC) learning (Valiant 1984). In PAC learning theory, one assumes that a set of hypotheses H along with a set of unlabeled data points X are given, where each data point x  X is drawn i.i.d. from some distribution D. Classical PAC bounds then yield the sample complexity (i.e., the number of required i.i.d. examples) from D to output a hypothesis h  H that will have estimation error at most  with probability at least 1 -  , for some fixed ,  > 0. Here, the estimation error is defined as  = PrxD [h(x)  h (x)], where h is the unknown true hypothesis. In the realizable case of learning a halfspace, i.e., when h  Rd perfectly separates the data points into positive and negative labels, it  (d )1 i.i.d. samples one can find a linis known that with O ear separator with an estimation error . The main advantage of using active learning methods, i.e., sequentially querying data points, is to reduce the sample complexity exponential  (d log(1 )). In fact, a simple counting arfast, ideally to O gument based on sphere packing shows that any algorithm needs (d log(1 )) examples to achieve an estimation error of  (Dasgupta, Kalai, and Monteleoni 2009). For d = 2 and when the distribution is uniform over the unit sphere S 1 it is very easy to see that the halving or bi (log(1 )). By using the same halving section leads to O method, one can in principle extend the result to any dimension d. To do so, we need to carefully construct the version space (i.e., the set of hypotheses consistent with the queries and outcomes) at each iteration and then find a query that halves the volume (in the uniform case) or the density (in the general case if the distribution is known) (Dasgupta 2004). Finding such a query in high dimension is very challenging.

1  notation to ignore terms that are logarithmic or We use the O dependent on  .



100



100



104 103 Execution Time (sec.)



Estimation Error



10



-1



Estimation Error



10



-1



102 101 100 10-1 10-2



Random Uncertainty Bagging Spectral DC



10-2

Random Uncertainty Bagging Spectral DC



10-2

Random Uncertainty Bagging Spectral DC



10



-3



50



100 150 Number of Queries



200



10



-3



100



150



200 250 300 350 Number of Queries



400



450



10-3



d = 25 Dimension



d = 50



(a) Noiseless (d = 25)

103 102

1



(b) Noiseless (d = 50)

104 103



(c) Execution time (noiseless)

101

DC (noise = 0.01) DC (noise = 0.1) DC (noise = 0.2) Rep. (noise = 0.01) Rep. (noise = 0.1) Rep. (noise = 0.2)



Execution Time (sec.)



Uncertainty Bagging



Execution Time (sec.)



DC



Uncertainty



10 Estimation Error

Bagging



0



102 101 100 10-1



DC



10



10-1 10-2 10-3



100 10-1 10-2 -4 10



Random



Random



Repetitive 10-3 10-2 Estimation Error 10-1 100



10-2 -4 10



Repetitive 10-3 10-2 Estimation Error 10-1 100



10-4



0



2



4 6 Number of Queries



8



10 # 104



(d) Noisy (d = 25)



(e) Noisy (d = 50)



(f) Noisy (d = 1000)



Figure 3: Figures 3a and 3b show the estimation error in the noiseless setting as we increase the number of queries, for d = 25 and 100, respectively. Figure 3c shows the corresponding execution times. Figure 3d and 3e show the scatter plots of the execution time and the estimation error of different methods for d = 25, 50 and the noise level  = 0.1. We allow each algorithm to use a budget of 800 and 1800 queries in Figure 3d and 3e, respectively. Figure 3f presents the estimation error of DC and R EPETITIVE -DC as we increase the number of queries for d = 1000 and noise levels  = 0.01, 0.1, 0.2. One very successful approach that does not suffer from the aforementioned computational challenge is pool-based active learning (Settles 2010), where instead of ideally halving the space, effective approaximations are performed. Notable algorithms are uncertainty sampling (Lewis and Gale 1994) and query-by-committee (QBC) (Freund et al. 1997). In fact, our problem is closely related to learning homogeneous linear separators under the uniform distribution in the pool-based setting. This problem is very well understood and there exist efficient pool-based algorithms (Balcan, Broder, and Zhang 2007; Dasgupta, Kalai, and Monteleoni 2005; Dasgupta and Hsu 2008). In particular, Dasgupta et al. (Dasgupta, Kalai, and Monteleoni 2009) presented an efficient perceptron-based algorithm that achieve a near-optimal query complexity. Similar results can be obtained under log-concave distributions (Balcan and Long 2013). Most of the pool-based meth (1 ) number of unods require to have access to O labeled samples in each iteration or otherwise they perform very poorly (Balcan, Broder, and Zhang 2007; Dasgupta, Kalai, and Monteleoni 2009). This means that in order to have exponential guarantee in terms of sample complexity, we need to grow the pool size exponentially fast (note that there is no need to store all of these points). Moreover, with a few exceptions (Awasthi, Balcan, and Long 2014; Balcan, Beygelzimer, and Langford 2006) pool-based learning of linear separators in the noisy setting has been much less studied and the dependency of sample complexity on noise is not very well understood. An attractive alternative to the pool-based framework is query synthesis where we have access to membership queries (Angluin 1988)): a learner can request for any unlabeled data instance from the input space, including queries that the learner synthesizes from scratch. This way the pool size limitation is entirely eliminated. In many recent applications, ranging from automated science (King 2009), to robotics (Cohn, Ghahramani, and Jordan 1996), and to adversarial reverse engineering (Lowd and Meek 2005), query synthesis is the appropriate model. For instance, in securitysensitive applications (e.g., spam filters and intrusion detection systems) that routinely use machine learning tools, a growing concern is the ability of adversarial attacks to identify the blind spots of the learning algorithms. Concretely, classifiers are commonly deployed to detect miscreant activities. However, they are attacked by adversaries who generate exploratory queries to elicit information that in return allows them to evade detection (Nelson et al. 2012). In this work, we show how an adversary can use active learning methods by making synthetically de novo queries and thus identify the linear separator used for classification. We should emphasize that in active learning via synthesized queries the learning algorithm can query the label of any points in order to explore the hypothesis space. In the noiseless setting (if we ignore the dependency of the pool size on  (log(1 ))), one can potentially use the pool-based algoO rithms (under the uniform distribution). Our main contribution in this paper is to develop a noise resilient active learning algorithm that has access to noisy membership queries.



To the best of our knowledge, we are the first to show a near optimal algorithm that outperforms in theory and practice the naive repetition mechanism and the recent spectral heuristic methods (Alabdulmohsin, Gao, and Zhang 2015).



References

[Abe and Mamitsuka 1998] Abe, N., and Mamitsuka, H. 1998. Query learning strategies using boosting and bagging. In ICML, 1. Morgan Kaufmann Pub. [Alabdulmohsin, Gao, and Zhang 2015] Alabdulmohsin, I.; Gao, X.; and Zhang, X. 2015. Efficient active learning of halfspaces via query synthesis. In AAAI 2015. [Angluin 1988] Angluin, D. 1988. Queries and concept learning. Machine learning. [Awasthi, Balcan, and Long 2014] Awasthi, P.; Balcan, M. F.; and Long, P. M. 2014. The power of localization for efficiently learning linear separators with noise. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, 449-458. ACM. [Balcan and Long 2013] Balcan, M.-F., and Long, P. M. 2013. Active and passive learning of linear separators under log-concave distributions. In COLT, 288-316. [Balcan, Beygelzimer, and Langford 2006] Balcan, M.-F.; Beygelzimer, A.; and Langford, J. 2006. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, 65-72. ACM. [Balcan, Broder, and Zhang 2007] Balcan, M.-F.; Broder, A.; and Zhang, T. 2007. Margin based active learning. In Learning Theory. Springer. 35-50. [Cohn, Ghahramani, and Jordan 1996] Cohn, D. A.; Ghahramani, Z.; and Jordan, M. I. 1996. Active learning with statistical models. JAIR. [Dasgupta and Hsu 2008] Dasgupta, S., and Hsu, D. 2008. Hierarchical sampling for active learning. In Proceedings of the 25th international conference on Machine learning, 208-215. ACM. [Dasgupta, Kalai, and Monteleoni 2005] Dasgupta, S.; Kalai, A. T.; and Monteleoni, C. 2005. Analysis of perceptron-based active learning. In International Conference on Computational Learning Theory, 249-263. Springer. [Dasgupta, Kalai, and Monteleoni 2009] Dasgupta, S.; Kalai, A. T.; and Monteleoni, C. 2009. Analysis of perceptron-based active learning. Journal of Machine Learning Research 10(Feb):281-299. [Dasgupta 2004] Dasgupta, S. 2004. Analysis of a greedy active learning strategy. In Advances in neural information processing systems, 337-344. [Freund et al. 1997] Freund, Y.; Seung, H. S.; Shamir, E.; and Tishby, N. 1997. Selective sampling using the query by committee algorithm. Machine learning. [Hakkani-Tur, Riccardi, and Gorin 2002] Hakkani-Tur, D.; Riccardi, G.; and Gorin, A. 2002. Active learning for automatic speech recognition. In ICASSP), volume 4, IV-3904. IEEE. [Javdani et al. 2014] Javdani, S.; Chen, Y.; Karbasi, A.; Krause, A.; Bagnell, J. A.; and Srinivasa, S. 2014. Near optimal bayesian active learning for decision making. AISTAT.



[Kaariainen 2006] Kaariainen, M. 2006. Active learning in the non-realizable case. In International Conference on Algorithmic Learning Theory, 63-77. Springer. [Karbasi, Ioannidis, and Massoulie 2012] Karbasi, A.; Ioannidis, S.; and Massoulie, L. 2012. Comparison-based learning with rank nets. ICML. [Karp and Kleinberg 2007] Karp, R. M., and Kleinberg, R. 2007. Noisy binary search and its applications. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 881-890. Society for Industrial and Applied Mathematics. [King 2009] King, R. D. e. a. 2009. The automation of science. Science. [Lewis and Gale 1994] Lewis, D. D., and Gale, W. A. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval. [Lowd and Meek 2005] Lowd, D., and Meek, C. 2005. Adversarial learning. In KDD, 641-647. ACM. [Nelson et al. 2012] Nelson, B.; Rubinstein, B. I.; Huang, L.; Joseph, A. D.; Lee, S. J.; Rao, S.; and Tygar, J. 2012. Query strategies for evading convex-inducing classifiers. JMLR. [Nowak 2011] Nowak, R. D. 2011. The geometry of generalized binary search. IEEE Transactions on Information Theory 57(12):7893-7906. [Settles 2010] Settles, B. 2010. Active learning literature survey. University of Wisconsin, Madison 52(55-66):11. [Shawe-Taylor and Cristianini 2004] Shawe-Taylor, J., and Cristianini, N. 2004. Kernel methods for pattern analysis cambridge univ. Cambridge, UK. [Tong and Chang 2001] Tong, S., and Chang, E. 2001. Support vector machine active learning for image retrieval. In Proceedings of the 9th ACM international conference on Multimedia, 107-118. ACM. [Tong and Koller 2002] Tong, S., and Koller, D. 2002. Support vector machine active learning with applications to text classification. JMLR 2:45-66. [Valiant 1984] Valiant, L. G. 1984. A theory of the learnable. Communications of the ACM 27(11):1134-1142.



Appendix

Let {n , n  1} be a sequence of independent and identically distributed (iid) Bernoulli() random variables. Denote by (F , , Pr) the probability space generated by this sequence. At the m-th round of DC 2 , if m = 1 (which takes place with independent probability ) then we observe a flipped version of signxm , h . Also, if m = 0 we observe the correct version of signxm , h . Consider a query of the form signx, h . This query divides the unit circle into two parts (half-circles) depending on the sign of x, h  (see Figure 4). The two parts are: (i) Preferred part: all h such that signx, h = signx, h , and (ii) Unpreferred part: all h such that signx, h = -signx, h . The two parts can be separated by a line x that passes through the origin. We refer to Figure 4 for a schematic explanation.



A



Proof of Theorem 1



conduct the query signxm , h . As the result of the query is noisy, we have two different update rules depending on each of the following cases: (i) m = 0, i.e., we observe the correct value signxm , h . In this case, the measure pm is updated as follows pm+1 (h) = 2(1 - )pm (h) if h  Fxm , (2)pm (h) if h  Uxm .



(ii) m = 1, i.e., we observe the flipped value -signxm , h . In this case, the measure pm is updated as follows pm+1 (h) = (2)pm (h) if h  Fxm , 2(1 - )pm (h) if h  Uxm .



Consider the number T, given in (3). Our goal is to show that Pr y  S 1  d(y, h ) >  and pT, (y )  pT, (h ) < . (5) Clearly, the result of the theorem follows from (5). For better illustration, we assume w.l.o.g that h = (0, 1). Consider a point y on the right-hand side of the unit circle such  that d(y, h ) > 2 . Also, Consider points z0 , zK such that d(z0 , h ) =  4 and d(h , zK ) =  2. We now divide the sector starting with z0 and ending with zK into K = T, + 1 pints. That is, for i = 1, 2, , K we denote by zi the point   that d(h , zi ) = 4 + i 4(T, (see Figure 5). Also, for i  1, +1)



Figure 4: For any point z above the line x we have z, h  = x, h . Once we perform the query x, h , it is more likely that the (noisy) response is indeed the true value x, h . Therefore, the region above the line x is in general preferred by the query. In the figure, the sector (y, z ) is cut by the line x and the sector (z, x) is not. Also, (z, x) lies in the preferred part of the query x, h .



In this setting, we say that the query signx, h  prefers a point z if z belongs to the preferred part of the query. Otherwise, we say that the query does not prefer z. Also, we frequently use the line x rather than the query signx, h  when it causes no ambiguity. Finally, for a region A on the unit circle say that the query signx, h  cuts the region A if and only if the line x passes through region A. Otherwise, we say that the query does not cut A. If x does not cut A, then x prefers A if A is in the preferred part and does not prefer A otherwise (see Figure 4). Finally, for two points x, y we define the distance d(x, y ) to be the length of the (smaller) sector between them (see Figure 4). Clearly, we have d(x, y )  x - y 2 . At round m of DC2 a vector xm is chosen and the (noisy) outcome of signxm , h  is observed. As explained in Section 2, xm is chosen in a way that the preferred and unpreferred parts have equal measures under pm-1 , i.e., 1 pm-1 (Fxm ) = pm-1 (Uxm ) = 2 . Let us see what happens to pm (the posterior belief about h at round m) after we



Figure 5: Different regions for the proof of Theorem 1. we let the sector starting with zi-1 and ending with zi be denoted by Ai . Note that in the very beginning of the algorithm when we have uniform measure on the unit circle, each of  the regions Ai has p0 (Ai ) = 8(T (as Ai = 4(T, ). +1) , +1)  log 4((1  and consider the following events: -))

2 log

2



DC2 has in total T, rounds and in each round m it conducts a query with an associated line xm . We let M =



* E1 : There is at least M lines which separate zK from h or equivalently, there is at least M lines that cut the region (h , zK ). * E2,j (1  j  K ): The region Aj is not cut by any of the lines 1 , 2 , . . . , T, . * E3 : y such that d(y, h ) >

 2



and pT, (y )  pT, (h ).



It is easy to see that Pr K j =1 E2,j = 1 as we have T, queries and hence by the pigeon-hole principle there is always a region Aj that is not cut by any of the lines. We can write: Pr [E3 ] c = Pr [E3  E1 ] + Pr [E3  E1 ]  Pr [E3 E1 ] +

T, +1 j =1



Proof. For i  [m], define the random variable Zi as Zi  p (x ) log pi . Using the update rules of pi that we explained i (y ) above, it is easy to see that for i  1: Zi = Zi-1 + (1 -  . Also, as p0 is uniform over S 1 we have Z0 = 0. 2i ) log 1- 

1- We thus have Zm = m i=1 (1 - 2i ) log  . Hence,



c Pr [E3  E1



 E2,j ] . (6)   . 2



Pr [Zm  0]

m



= Pr log = Pr



Now using Lemma 3 (stated below), we have Pr [E3 E1 ]  Pr [E3 E1 ]  (4(1 - )) Let us now bound

c Pr [E3  E1

M 2



1- m (1 - 2i )  0  i=1 1 2

m



i 

i=1



(7)



and using the fact that E2,j = Lemma 4 that



c c Pr [E3  E1  E2,j ]  Pr [E2,j  E1 ],  4(T, +1)



 E2,j ] . We have



where the last step follows directly from the so called Chernoff bound. We note that the vector h is always a member of the preferred part of any test. As a result, at any round of DC 2 we have that h  m i=1 Fxi . Lemma 4. Consider a region A on the unit circle which does not contain h . Assume we are at round m of DC2 where a sequence of queries with associated lines x1 , x2 , . . . , xm have been conducted. We define events E1 and E2 as * E1  None of the lines xi cuts A; * E2  At most k of the lines do not prefer A, where k is an an integer. We have Pr [E1  E2 ]  k (1 + 2 ),



 (4(1 - )) 2 ,



we obtain from



and thus



c Pr [E2,j  E1 ]  (M - 1)(1 + 2 ), c Pr [E2,j  E1 ]  (T, + 1)M (1 + 2 ),



T, +1 j =1



(8)



where 1 and 2 are given in Lemma 4 with m  T, and k  M . Now, we show that the above expression is upper bounded by  2, and hence by using relations (6) and (7), we get the proof of the main theorem. The value of T0 is chosen in such a way that we have 2 log(T, + 1) log(2(1 - ))  . T, - M 4 (9)



T1 ensures that



2 8 log(2(1 - )) log  . T, - M  4 2M log(1 - 2) log(2)  . T, - M 2



(10)



T2 and ensures that



(11)



2  2 2      m - k  log(2(1 - )) - m-k log A )   , 1 = exp - 1 -    6    log ( )        and 2  2k      m - k  log(2(1 - )) + m-k log(2)   . 2 = exp - 1-  6     log(  )       Proof. We have



where



Finally, T3 ensures that 2        T - M  log(2(1 - ))    . (T, + 1)M exp - 1-   6 4   2  log        (12) Now, by plugging in (9)-(12) into the values of 1 and 2 in  . (8) we conclude that the right side of (8) is bounded by 2 Lemma 3. Let x1 , x2 , , xm be the vectors chosen by DC up to round m with Fxi and Uxi being their associated preferred and unpreferred parts (i.e. pi-1 (Fxi ) = pi-1 (Uxi ) = 1 2). Consider two points h1 , h2 such that h1  m i=1 Fxi and h2  m . We have for  > 0 that U x i i=1

2



Pr [E1  E2 ]  Pr [E2 E1 ]  where we define



k j =1



Pr [E2,j E1 ] ,



(13)



Pr [pm (x) < pm (y )]  (4(1 - )) .

m



We will now calculate Pr [E2,j E1 ] . In the beginning, p0 A . Let us puts a uniform measure on A and hence p0 (A) = 2  first investigate the dynamics of pi-1 (A) when we conduct the i-th query and condition on event E1 (i.e. given that none of the lines cut A). In this setting, we define the random variables Zi = log pi (A). At time i, assuming that the line xi does not cut A, Zi has different update rules depending on the two cases whether the line xi prefers A or does not



E2,j  Exactly j lines do not prefer A.



prefer A. (i) first case: if the line xi prefers A, then we know that either with probability 1 -  (if i = 0) we have pi (A) = 2(1 - )pi-1 (A) and with probability  (if i = 1) we have pi (A) = (2)pi-1 (A). Thus, we can write Zi = Zi-1 + Fi , where Fi  i log(2) + (1 - i ) log(2(1 - )). (ii) second case: if xi does not prefer A, then using a similar argument we obtain Zi = Zi-1 + Ui , where Ui  i log(2(1 - )) + (1 - i ) log(2). Now, in order to find an upper bound on Pr [E2,j E1 ], we assume without loss of generality that in the first m - j rounds we the lines are as in the first case and in the last j rounds the lines are as in the second case (note that any other given order of the lines is statistically equivalent to this simple order that we consider). Zm = Z0 + = log2

m-j i=1



Figure 6: An example to illustrate DC2 in the noiseless setting. In

the first round, x1 is arbitrarily chosen on S 1 . For the choice in the figure, we have sign x1 , h  = sign x1 , h  = -1. For any point h above the red line we have that sign x, h = -1 and for the points outside this half-circle the result is +1. Therefore, the distribution (pdf of) p1 is uniform on the region above the red line and is zero below it. For round m = 2 it is easy to see that the direction of x2 should be along the red line. For x2 chosen as in the figure, we have sign x2 , h  = +1 and hence at the end of the second round DC2 concludes that the vector h could uniformly be any point inside R2 . In a generic round m, any vector orthogonal to the mid-point of sector Rm-1 can be considered as a candidate for xm . For the choice in the figure, we have sign xm , h  = -1. Thus, at the end of round m, DC2 concludes that h can uniformly be any point inside Rm .



Fi +



m



Ui

i=m-j +1



Now, noting that pm (A)  1 and hence log pm (A)  0, we obtain Pr [E2,j E1 ] m-j   m    Pr log2 p0 (A) + Ui  0 Fi +   i=m-j +1 i=1    m-j m 2    F + = Pr  U  log2  i=1 i i=m-j +1 i A  

-j   m2  2   F  log 1 = Pr  i  A   i=1  



m A m-j Fi + + Ui . 2 i=1 i=m-j +1



Let us now define



and



DC2 (outlined in Algorithm 1) in the noiseless case reduces to the binary search. In this section, we explain DC 2 in the Pr [E2,j E1 ]  1 + 2 . (14) noiseless case (the binary search) with the help of a running Now, to bound 1 we obtain after some simplifications that example given in Figure 6. As we will see, after each round m - j of DC 2 the possible region that h can belong to will be 2   2 log log(2(1 - )) - m2   m - j -j A  "halved". 1 = Pr  i   x x  , 1- 2 We first note that as the initial distribution p0 is assumed  i=1   log    to be the uniform distribution on S 1 , the vector x1 (see step and by using the Chernoff bound we get 2-(a) of Algorithm 1) can indeed be any point on the unit 2   circle S 1 . Thus, DC2 chooses x1 arbitrarily on S 1 . By (2), 2 2   log ) log ( 2 ( 1 -  )) -     m - j   m-j A using the query sign x1 , h  will also give us the value of . 1  exp - 1 -   sign x1 , h . Depending on this value, it is easy to verify 6     log(  )       that only half of S 1 can possibly contain h (see Figure 6). (15) Let us denote this region by R1 . Hence, the probability disTo bound 2 we can similarly write after some simple steps tribution p1 (h) (which is our current belief about h ) is upthat dated as follows: for h  R1 we have that p1 (h) = 0, and  as all the points inside the half-circle  m-j 2j R1 are equiprobable,   log ( 2 ( 1 -  )) + log ( 2  ) m-j m-j    , we have for h  R1 that p1 (h) = 1  . In other words, at i   x 2  Pr  1 -    2  log  -j  time m = 0 the vector h could have been anywhere on the i=1+ m2  



 m-j    m   2 = Pr  Ui  0 Fi +   -j i=m-j +1 i=1+ m2    Using the union bound, we have



and using the Chernoff bound we get 2  2j      m - j  log(2(1 - )) + m-j log(2)   . 2  exp - 1-  6     log(  )       (16) We further note that both of the upper bounds on 1 and 2 decrease when we increase j . Hence, the proof of the theorem follows by letting j = k in (15) and (16), and also plugging these bounds into (13).



B



DC2 in the Noiseless Case



unit circle, but, after round m = 1 it can only belong to the half-circle R1 . Thus, after the first round, DC 2 "halves" the admissible region of h . Continuing in this theme, it is not hard to verify that (see Figure 6) at round m = 2 the value of p2 (h) is non-zero and uniform only on a region R2 which is a quarter-circle. In an inductive manner, letting Rm-1 denote the admissible region (sector) at round m - 1 (see Figure 6) and assuming that pm-1 is only non-zero and uniform on the sector Rm-1 , then xm at round m is precisely the vector that is orthogonal to the midpoint of the sector Rm-1 . Therefore, after observing the value of sign xm , h , the admissible region Rm is the better half of Rm-1 that is compatible with the observation (i.e., it contains h ). Also, Rm is again a sector and pm will be uniform on Rm and zero outside. It is also easy to see that the circular angle for the sector Rm is  . The following statement is now immediate. 2m Theorem 5. Consider DC in the absence of noise ( = 0). If , then it outputs a vector that is within we let T, = log2   a distance  of h . A few comments are in order: The above guarantee for DC 2 holds with probability one and thus the parameter  is irrelevant in the noiseless setting. Furthermore, during each round of DC2 , the distribution pm can be represented by only two numbers (the starting and ending points of the sector Rm ), and the vector xm can be computed efficiently (it is the orthogonal vector to the midpoint of Rm ). Therefore, assuming one unit of complexity for performing the queries, DC 2 can be implemented with complexity O(T, ). Finally, by using Theorem 2, we conclude that DC requires O(d log 1 ) queries with computational complexity  ) . O(d log 1 



Recall that n0 = O(log(1 )) (see Theorem 5). Plugging this into the expression of nR0 , we obtain that the query complexity of repetitive querying is O(log(1 )(log log(1 ) + log(1  )).



D Proof of Theorem 2

At each round, we replace two vectors in E , say e1 and e2 , with the output of DC 2 (e1 , e2 , ,  ); then the cardinality of E decreases by 1. Therefore, each call to DC2 will result in the cardinality of E decreasing by 1. Initially, there are d elements in E ; when the algorithm terminates, there is only one element (i.e., the final output of the algorithm) in E . Thus throughout the entire process of the algorithm, the cardinality of E decreases by d - 1; therefore, there are d - 1 calls to DC2 . If the probability of success for DC 2 is at least 1 -  , then by the union bound the probability of success of DC is at least 1 - (d - 1) . For the second part of the theorem, we prove a more general statement: Assume that we run DC with an input being an orthonormal set {e1 , e2 , . . . , et } where ei , h  Rd and t  d. We should note that the underlying space remains the d-dimensional Euclidean space Rd . We will prove that DC outputs a vector that is close to the normalized orthogonal projection of h onto span{e1 , e2 , . . . , et }. More precisely, we define t  1 ei , h ei h = i=t . i=1 ei , h 2



C



Analysis of Repetitive Querying



Firstly, we would like to compute the probability that the majority vote gives us the correct outcome. Let Ii (1  i  R) is the indicator random variable of the event that the i-th query gives us the right outcome. We know that {Ii  1  i  R} are i.i.d. Bernoulli random variables with success probability 1 - . Let SR = R i=1 Ii . By Hoeffding's inequality, we have Pr[SR  R 2] = Pr[SR - E [SR ]  R 2 - E [SR ]] = Pr[SR - E [SR ]  -R(1 2 - )]

2-)2 R



Then DC runs in d - 1 rounds, calls DC 2 d - 1 times, and out for which puts with probability at least 1 - (d - 1) a vector h   h - h  5d. In exactly similarly way as discussed above, we can conclude that DC runs in d - 1 time and uses DC 2 d - 1 times. Also, again by the union bound, with probability at least 1 - (d - 1) , all outputs of DC2 are a close estimate (within distance ) of their corresponding objective. Thus, by assuming that all the calls of DC 2 have been successful (which happens with probability at least 1 - (d - 1) ), we  - h  5(d - 1). use an inductive argument to prove that h We use induction on t. For t = 2 the result is clear. We now prove the result when t =  assuming that it holds for all t <  . Without loss of generality, assume that the algorithm calls DC 2 (e1 , e2 , ,  ) and the vectors e1 and e2 in E willl be replaced by the output of DC2 (e1 , e2 , ,  ) which we denote by e 1 . We can write h =

 i=1



 e-2(1



.



Suppose that the binary search queries n0 (distinct) points in total throughout the entire procedure. By the union bound, the probability that all n0 majority votes give us the right 2 outcome is greater than or equal to 1 - n0 e-2(1 2-) R . In order to ensure that this probability is at least 1 -  , we need log(n0  ) R . 2(1 2 - )2 Therefore the total number of queries is at least n0 R  n0 log(n0  ) . 2(1 2 - )2



ci e i = c 1 h 1+







ci e i ,

i=3 c1 e1 +c2 e2

2 c2 1 +c2



where ci = h , ei , c 1 = that h 1 =

c1 e1 +c2 e2

2 c2 1 +c2 



 2 c2 1 + c2 and h1 =



. Note



is precisely the normalized orthogonal



projection of h (and also h ) onto span{e1 , e2 }. Using the above notation, we have e 1 - h 1  .



Recall that after obtaining e 1 (the output of DC2 (e1 , e2 , ,  )), the algorithm will recursively call



DC (e 1 , e3 , e4 , . . . , e ). Suppose that the output of this   . By the assumption of the induccall is denoted by h   tion, the output h will be within the distance 5( - 2) of the normalized orthogonal projection of h onto span{e 1 , e3 , e4 , . . . , e }, which we denote by h . That is, We know that   - h  5( - 2). h



Thirdly, similarly we have h - h , e 1 e 1 +

 



ci e i

i=3 



=

i=1



h =



h , e 1 e 1 +  i=3 ci ei

2 h , e 1 2 +  i=3 ci



.



Now we will show that



h - h  5. If this is true, we will have   - h + h - h  5( - 1) h -  h  h



= c1 e1 + c2 e2 - h , e 1 e 1   = h , h  h -  h , e   e  1 1 1 1     h , h  h -  h , h 1 + h , h 1 - h , e 1 e 1 1 1 1 e 1 e      = h , h1 (h1 - e 1 ) + h , h1 - e 1 e 1 1  h 1 + h 1 -e 1 -e  2. Therefore h - (h , e 1 e 1 +  i=3 ci ei )  2   2   1 - 2  3,



ci ei - h , e 1 e 1 +



ci e i

i=3



and this completes the proof. Therefore, it suffices to show that h - h  5. 2 1 2 +  We define  = h , e i=3 ci . Firstly, we have h - h h -





= = = 



h , e 1 e 1 +  i=3 ci ei

2 h , e 1 2 +  i=3 ci 



(19)



h - (h



,e 1 e 1 +  i=3 ci ei ) 



where the last step follows from   5 18. Now, by plugging (18) and (19) into (17) we get h - h  2 + 3 = 5. Hence we have   h   - h + h - h  5( - 2) + 5 = 5( - 1). h - h



h - (h 1- + 





( - 1)h + h - (h , e 1 e 1 +  i=3 ci ei ) 





,e 1 e 1 +  i=3 ci ei ) 



. (17)



Secondly, we have 2 - 1 = = = h , e 1 2 + h , e 1 2 +

 i=3  i=3



c2 i -1 2 c2 1+ i - c

 i=3



c2 i



2 = h , e 1 2 - h , h 1  = h , e 1  - h , h 1  + h , h 1   h , e 1     1  + h , h = h , e 1 - h1   h , e 1   h  e 1 - h  e 1 + h  h 1 ( h 1 )  2,



h , e 1 2 - c 2 1



where the last step follows from h = e 1 = h 1 = 1 and e 1 - h   . Since   5 18 < 1 2 , we obtain 1    1 - 2, 1 + 2 and 1 1 1-  max  - 1, 1 -   2.  1 - 2 1 + 2 (18)



Deploying CommunityCommands: A Software Command Recommender System Case Study

Wei Li Justin Matejka Tovi Grossman George Fitzmaurice

Autodesk Research, Toronto, Canada. firstname.lastname@autodesk.com



Abstract

In 2009 we presented the idea of using collaborative filtering within a complex software application to help users learn new and relevant commands (Matejka et al. 2009). This project continued to evolve and we explored the design space of a contextual software command recommender system and completed a four-week user study (Li et al. 2011). We then expanded the scope of our project by implementing CommunityCommands, a fully functional and deployable recommender system. CommunityCommands was made available as a publically available plug-in download for Autodesks flagship software application AutoCAD. During a one-year period, the recommender system was used by more than 1100 AutoCAD users. In this paper, we present our system usage data and payoff. We also provide an in-depth discussion of the challenges and design issues associated with developing and deploying the front end AutoCAD plug-in and its back end system. This includes a detailed description of the issues surrounding cold start and privacy. We also discuss how our practical system architecture was designed to leverage Autodesks existing Customer Involvement Program (CIP) data to deliver in-product contextual recommendations to endusers. Our work sets important groundwork for the future development of recommender systems within the domain of end-user software learning assistance.



Introduction

Modern computer programs can have thousands of commands available to the user, with a general tendency to increase year after year (Baecher et al. 2000). For example, AutoCAD is a widely used software application for both 2D and 3D drafting and design. The number of commands in AutoCAD has been growing linearly and consistently over time. While the growth of commands increases a systems capabilities, the quantity can make learning the system a challenge. In particular, a users lack of

Copyright (c) 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



awareness of relevant functionality can act as a barrier to their efficiency with the system (Grossman et al. 2009, Shneiderman 1983). In a "best case scenario", a user would work with an expert next to them who could recommend commands when appropriate (Grossman et al. 2009). Indeed, this type of "over the shoulder" learning has been shown to be valuable in the workplace (Twidale 2005), yet it is obviously impractical to assume such assistance would be readily available. One promising way to address this challenge is to provide users with in-product command recommendations. Existing techniques, such as "tip-of-day" and "did you know", can expose new features, but they may be irrelevant to the user current task (Fischer 2001, Norman et al. 1986). An alternative is to provide personalized command recommendations, based on the users own history of usage. While some research has been initiated in this area (Linton and Schaefer 2000, Matejka et al. 2009), working implementations which deliver these recommendations have never been embedded within a target application. We contribute a recommender system that was released as a plug-in for AutoCAD, and has been used in real usage scenarios. During a one year period of time, over a thousand AutoCAD users downloaded and installed our CommunityCommands plugin from the official Autodesk1 website as a technology preview. In this paper, we provide an in-depth discussion of the important issues and challenges we have encountered during the development and deployment of this system. This includes many of the technical details of the recommender system itself, as well as the system architecture and implementation details required to make a real-time command recommender system hosted on a users local machine, work in practice. In particular, we

1



http://www.autodesk.com



discuss the key challenges associated with the domain of software functionality recommendations that required us to diverge from the traditional treatment of recommender system problems. This includes: cold start issue; contextual in-product real-time recommendations; and the system architecture to deliver the personal recommendations to end-users, while also protecting user privacy. In our software command recommender design, we leverage an existing Customer Involvement Program (CIP), which provides a mechanism to collect user command sequence logs anonymously. We also propose a novel architecture that pushes the item-by-item similarity matrix to each users computer. For users who have privacy and data security concerns, this push model can enable a download-only recommender being deployed to their systems. In addition to privacy concerns, CIP also provides valuable data source for solving the cold start problem. Our hope is that the presentation of these important details will set the groundwork for the future development of recommender systems within the domain of end-user software.



are not relevant to the individuals workflow will be avoided. In our previous work (Matejka et al. 2009, Li et al. 2011) we performed several offline evaluations and an online evaluation with a limited number of study participants. In this paper, we describe our deployment of Community-Commands, made available for public download and usage. We provide a detailed description of the system architecture, and report and reflect on the data which was collected from our deployment used by over 1,000 actual AutoCAD users resulting in over 55,000 command recommendations issued over a one year period of time.



Challenges of Building and Deploying a Software Command Recommender System

In this section, we describe a number of challenges that we encountered while preparing our system for public deployment. Privacy The issue of user privacy has been explored by recommender system users and researchers (Frankowski et al. 2006, Ramakrishnan et al. 2001). In many recommender systems, a central server has access to all user profiles and generates personal recommendations. This type of architecture may reveal details about the user, gained through examining their user-item relations. Some privacy research has focused on using a decentralized server architecture combined with strong algorithms to secure users data (Ahmad and Khokhar 2007, Berkovsky et al. 2007, Shokri et al. 2009), but this still requires user data to be sent to a network server. The issue of privacy is a significant concern for CommunityCommands. Customers often worry that their usage behaviors and data is being logged. For design software, such as AutoCAD, customer-generated data can be extremely sensitive. In an ideal usage situation, software users should have options and be able to control when to upload their software usage data. Cold Start The "Cold-Start" problem is a well-known issue in recommendation systems (Schein et al. 2001). For our implementation, we would have no previous data related to the individual users behavior, and thus, no information to base the recommendations on. It is also difficult to generate the required user-by-user or item-by-item similarity matrices without an existing software usage data set, which results in an inability to draw inferences to recommend items to users. Due to concerns surrounding privacy, it can be difficult to collect the usage data necessary to provide useful recommendations.



Prior Work

Collaborative filtering based recommender systems have become an important tool to help users deal with information overload and provide personalized suggestions (Hill et al. 1995, Shardanand and Maes 1995). Examples include recommending movies (Miller et al. 2003), news (Resnick et al. 1994), and books (Linden et al. 2003). However, little research has been conducted to help users learn and explore a complicated software package using a recommender system. We are aware of two such systems that have been proposed in the literature: OWL for Microsoft Office (Linton and Schaefer 2000) and CommunityCommands for Autodesk AutoCAD (Li et al. 2011). The OWL System compares a target users command frequencies to the average command frequencies of an entire user population. Based on the difference between these frequencies, OWL recommends commands that the target user should use either more or less often. OWL was designed to run within an organization, so it assumes that all users in the community should share the same command usage distribution, and in turn, use the software system in the same way. Across a broad user community, this assumption is unlikely to hold true. Users have different tasks, and preferences, so recommendations should be personalized (Mitchell and Shneiderman 1989). In contrast, CommunityCommands uses personalized collaborative filtering to produce recommendations tailored to an individual (Li et al. 2011). This adds a significant benefit over the OWL system; commands that



In-product Recommendation Another design challenge of our system is that it provides the recommendations within the product, and they are updated in real-time. This requires the recommendations to be available immediately, unlike previous software recommendation systems in which users receive periodic email updates (Linton and Schaefer 2000). Because of the in-product design and the possibility that the users might not have internet connections, the computations of recommendations must occur locally.



Customer Involvement Program

Many software applications have Customer Experience Improvement Programs2 (CEIP) or Customer Involvement Programs3 (CIP) to help collect users feedback (called CIP in the rest of paper). CIP lets users choose to send usage data to the software designers and developers, so they can get anonymous information about how their programs are being used. CIP usually gathers product usage and system configuration information, such as system memory, video card, screen resolution, and operating system details at regular intervals. This type of data is not particularly sensitive. However, in the aggregate, data items such as these give software developers a great deal of insight into what features customers are using, how well they're working, and where they could be improved.



CIP, when they execute a command, this action is recorded in the form of a (userID, commandID, timestamp) tuple in a centralized database. The voluntary nature of CIP also provides options to software users to either upload their command log to a central CIP server or keep the log on their computers. AutoCAD users can also turn off CIP anytime by clicking a menu item. Our system leverages CIP to generate command recommendations while users have the option of not revealing their personal information. Before the deployment of CommunityCommands, we used existing CIP data to solve the cold-start problem and implicitly define a rating scheme and generate item-by-item correlations.



Application Description

System architecture Based on the encouraging results of our one month user study (Li et al. 2011), we developed our recommender prototype into a plug-in for AutoCAD and released it to the public. The system runs as a palette embedded in the AutoCAD workspace, providing within-application and real-time recommendations while a user goes about their normal usage of the software (Figure 2).



Figure 2. System architecture.



Figure 1. Customer Involvement Program (CIP) Enrollment Interface in AutoCAD 2012.



In AutoCAD, command usage histories are collected as a part of the CIP data. A CIP participation window is presented to AutoCAD users during the software installation process (Figure 1). Sample data and generated reports are also presented to explain that the users privacy is still being protected. If the user agrees to participate in

2 3



http://www.microsoft.com/products/ceip/EN-US/default.mspx http://www.autodesk.com/acip/CIP_Privacy_eng.html



Here we describe our architecture design of this fully functional system for software command recommendations. The system architecture is composed of three components: the user's local machine, the CommunityCommands server, and the main AutoCAD CIP server (Figure 2). When the plug-in is installed and connected to the Internet, a 1.8 MB item-by-item similarity matrix is downloaded (pushed) to the users local machine, which is used for the item-based recommendation algorithm. The local machine collects the users command sequence, and computes the recommendations locally each time a new command is issued (using an item-based algorithm). In addition, the CommunityCommands server continuously receives command sequence logs from AutoCADs main CIP server. This allows us to generate



recommendations based on usage profiles of AutoCAD users that may not be running our plug-in. On a monthly basis, the server computes a new item-by-item similarity matrix and each users local machine downloads and replaces their existing matrix. This system architecture provides two important and unique design properties: preserving privacy and in-product recommendations. Push based recommendations For traditional recommender systems, there is no easy way to generate personalized recommendations, without some central system first receiving a user's data. In CommunityCommands, instead of uploading the users  data to the central server, the server pushes the similarity matrix to the user's local computer. Thus, we can still generate a personalized recommendation command list without ever receiving data from that user. The recommendations are still based on the personal data at the local computer, and the aggregated CIP data. In-product recommendations Recommended commands are placed in a list within the AutoCAD plug-in palette (Figure 3).



AutoCAD has a scripting language that can issue multiple commands without user input, we defer processing the recommendations and updating the UI until our threshold idle time of 0.5 seconds has been satisfied.



Figure 4. Recommended and recently used commands. Tooltip appears when mouse is hovered over the command.



Figure 3. Recommender plug-in palette is opened in AutoCAD.



Clicking on the command button executes the command. If a command in the recommendation list is used, it is immediately removed from the list and displayed in a "most recently used commands" list. Hovering over the command button causes the standard AutoCAD tooltip to appear, and dwelling longer reveals an extended tooltip with additional usage information (Figure 4). During our development process, we found it critical to be minimally disruptive to the computational resources needed by the main application. Under normal usage, computation of recommendations is unnoticeable to the user, so we compute the recommendations after an individual command has been executed. However, we have to delay the recommender computation if we observe a rapid succession of command usage. In addition, since



Training before recommending To further address the cold start problem, the plug-in begins in a training period, where commands are logged, but no recommendations are presented. Determining the right length of this training period is difficult - we wanted the recommendations to start as soon as possible, but only after we reliably know what commands the user is already aware of. To minimize the time needed for training, we ran a pilot test by analyzing data from 27 users (Li et al. 2011). On a daily interval, we measured the rate at which new commands were used (had not been previously observed for that user), across a period of 4 weeks (Figure 5). The data showed that the rate of using "new commands" levels off quickly. For example, after 8 days, 50% of users had less than 3 new commands per day. However, because users will have different daily usage rates, this public released recommender exits the training phase when the user performs less than 3 new commands on two consecutive days. To ensure enough data has indeed been collected, it also requires that the training phase was active for at least 10 usage days, or, until at least 200 commands have been captured.



Figure 5. New command adoption rates based on 27 users.



During this training phase, we display a message to the user, and use the pallet to provide access to recently used



commands. This gives the users some value, while waiting for the recommendations to begin (see Figure 6).



The inverse user frequency (iuf), a measure of the general importance of the command, is based on the percentage of total users that use it: | | |{ where: | |: total number of users in the community |* +|: Number of users who use ci. - A high rating in cf-iuf is obtained when a command is used frequently by a particular user, but is used by a relatively small portion of the overall population. For each user uj, we populate the command vector Vj such that each cell, Vj(i), contains the cf-iuf value for each command ci, and use these vectors to compute user similarity. Rather than matching users based on their command usage, our item-based collaborative filtering algorithm matches the active users commands to similar commands. Similar to user-based approach, each cell, Vi(j), contains the cf-iuf value for each user uj. In our released recommender, we applied item-based approach and customized our suggested commands based on active users short term preference (session-based command history) to generate contextual in-product real-time recommendations (Li et al. 2011). Novelty evaluation metrics Command recommendation is a top-N recommendation problem, which identifies a set of N commands that will be of interest to a user (Karypis 2001, Herlocker et al. 2004). We consider good recommendations to be those where the user was not previously familiar with the command, but after seeing the suggestion, will use it. As such, we were required a metric that would indicate usefulness and novelty. To do so, we developed a k-tail evaluation which dynamically measures the usefulness of an algorithm based on the sequential information in a users command log (Matejka et al. 2009). Here, we propose to approximate a command recommendations novelty factor using its binomial probability. We call this the binomial novelty indicator (BNI). To evaluate the novelty of the recommendations, we compute the probability that a command, which was correctly predicted by the recommender, would appear in the testing set by random chance. We do this by using the binomial probability formula, based on a commands overall frequency across the entire user community: ( ) ( ) ( ) With those two metrics we can compute the cf-iuf as }|



Figure 6. Recommender training phase UI



Use of AI Technology

As alluded to in our review of the related work, there are a number of unique considerations to address in developing a collaborative filtering system for software commands. Ratings Standard collaborative filtering algorithms work by viewing a dataset as a rating matrix. These ratings are either captured implicitly, for example, through purchase records and browsing histories, or explicitly, by asking users to rate the items. We need to map users command history onto a rating matrix. One approach is to allow a user to give explicit ratings for each command. This approach would not utilize the users historical data and would thus suffer from the cold start problem (Schein et al. 2001). Moreover, an explicit rating system would be impractical, since software application users will be focused on their primary task, not on rating the functions which they use. In addition, research has shown that users may be reluctant to provide explicit ratings (Shardanand and Maes 1995). As such, the implicit acquisition of user preferences of software commands is more favorable in practice. Our method uses the command frequency to imply the rating for the user (Li at el 2011). To model how important a command is to a particular user within a community, and to suppress the overriding influence of commands that are being used frequently and by many users, we have adapted tf-idf (Jones 1972) into a command frequency, inverse user frequency (cf-iuf) rating function. We first take the command frequency (cf) to give a measure of the importance of the command ci to the particular user uj.





where nij is the number of occurrences of the considered command for user uj, and the denominator is the number of occurrences of all commands for user uj.



where P(k) is the probability of a specific command C executed exactly k times in a commands sequence of length l, and p is the overall probability of C being executed in the dataset. The cumulative distribution function for k can be expressed as: ( ) ( ) ( )



F(l,l,p) represents the chance of seeing command C at least once. So we define the binomial novelty indicator (BNI) as: ( ) ( ) ( )



recommendations into their regular workflows. Figure 7 shows the number of recommended commands being used by the users who have moved past the training phase. The figure contains the recommended commands being used at least once, three times, ten times and twenty times. We call those commands adopted recommendations or useful recommendations. On average, 21.4 recommendations were used by users at least once. 14 new recommendations were used by users more than three times, 9.6 for 10 times and 7.3 for 20 times.



This gives us an explicit measurement as to the likelihood a recommended command would have appeared in the sequence by chance. For example, consider a command A that has a frequency of 0.036 and a command B that has a frequency of 0.002, across all users, and a testing set with 13000 commands. We compute that there is a 95% chance that A appears in the testing set once or more, and a 3% chance that B appears once or more. If the recommender predicts both A and B correctly, we can be reasonably certain that the user more likely knew A than B. Comparing this across all correctly recommended commands, we can get a measurement of how novel, overall, the commands that a recommender algorithm generates are. Thus, we combine BNI with k-tail offline evaluation by computing the mean of BNI for every unique command in RT, where l is the length of T. Our deployed recommender uses both k-tail and BNI to select collaborative filtering algorithms and tuning parameters.



Figure 7. Recommended command adoption



Figure 8. Total adopted useful recommendations over deployed time. The horizontal axis shows the percentage of time passed.



Application Use and Payoff

Overall Usage We report how long the CommunityCommands plugin was deployed on the users system. This deployment time was calculated using the time stamps of the first and last time the user ran the recommender. During the one year period after we released this recommender system, approximately 1100 AutoCAD users downloaded and installed the plugin. 983 users used the plug-in for at least one day. 709 users used the plug-in for more than 30 days. On average, the plug-in was installed at the users computer for more than two months (69.8 days). We also observed that most users who have very short usage times did not pass their training phase before they uninstalled or disabled the plugin. Recommendation adoption Our hope is that users of the recommender system would start using the recommended commands. We hope they not only try the command a few times, but adopt the



Figure 8 shows the distribution of the total adopted recommendations over time. Here we assume all users start at the same time and spend the same amount time using the system. This figure shows 50% recommendation adoptions happened during the first 19% of the entire period of system usage time. CommunityCommands only recommend commands that had never been executed in the users command history. But there may be commands used by the user before the installation of the plug-in. As such, some of these adopted commands may have already been known to the user. CIP Enrollment CIP is a key component for solving the users privacy concerns and cold-start problem. A large group of users (71.3%) who downloaded the CommunityCommands plugin enrolled in CIP. This of course means that 28.7% of users did not enroll into CIP, mostly due to privacy and technical concerns. As such, our system needs to work for both user groups.



Command usage visualization To help visualize the data which was collected during our deployment, we developed Personal Software Usage DNA diagrams for the users of our plug-in. These diagrams are generated by looking at the command usage patterns of each individual user. By ordering the commands based on the communitys overall usage, and coloring them based on the individuals usage, we can see commands that an individual is using more (or less) often than the community as a whole. By looking at how densely the individual row is filled in, we can also see if the individual uses a lot, or relatively few commands.



Conclusion and Future Work

Based on our experiences, we believe that recommender systems have a rich future for use within software applications. We have provided a detailed treatment of the issues surrounding the development of a command recommender system and the architecture used for its deployment. Our hope is that this research will serve as groundwork and inspiration for future efforts in this area. We have shown that collaborative filtering algorithms can identify commands that will be useful to a user. This leads us to believe that such systems could also be used to recommend higher-level task flows and relevant tutorial materials. The item-based collaborative filtering provides relevant and novel recommendations. It aggregates user-item relations into item-item relations. When combined with the system architecture we proposed here, the item-based algorithm can also preserve user's privacy, which is a desirable feature for many business applications. Certain software applications, including AutoCAD, have a main version, but also "parallel" customized versions for specific user groups. By using collaborative filtering technology, we will be able to recommend customized software features to the appropriate user groups. For example, AutoCAD has vertical versions for mechanical engineers, electric engineers, civil engineers and architects. Recommending commands commonly used by civil engineering to architects, when those commands fit the current workflow, could increase the diversity and novelty of current recommendations. Another issue is related to software upgrades. In ecommerce situations, when new products or services emerge, the interest of customers and the temporal feature of the ratings in collaborative filtering may change. Previous work (Ding and Li 2005) has used a time weighted item-by-item correlation to track concept drifting. It would be interesting to apply this same idea to help introduce new commands in each release of a software package to the users and allow the newer and



Figure 9. Legend of software usage DNA diagram



Figure 9 presents the information included in each DNA diagram. A red command name means that the command was recommended but was removed by the user from the recommendation list. A green command name means that it was normally showed in the recommendation list. The brightness of the command background represents the usage frequency of that command. So a green command on a bright background is a strongly adopted recommendation. Figure 10 shows the 17 most active users DNA diagram, with the top user enlarged. In the future, it could be interesting to present these Personal Software Usage DNA diagram to the end users, to encourage usage reflection and further command adoption.



Figure 10. Software usage DNA diagrams from the 17 most active users.



potentially more efficient commands to be recommended. In summary, the novel contribution of our work is the description of system architecture that has allowed us to embed a software command recommender system within a target application, during real usage situations. Software command/feature recommendation opens a new domain for recommender system research. Many interesting problems arise which open up areas for future work.



international conference management. 247-254.



on



Information



and



knowledge



Linden, G., Smith, B. and York, J. (2003). Amazon.com Recommendations: Item-to-Item Collaborative Filtering. IEEE Internet Computing. 7(1):76-80. Linton, F. and Schaefer, H.-P. (2000). Recommender Systems for Learning: Building User and Expert Models through Long-Term Observation of Application Use. User Modeling and UserAdapted Interaction. 10(2-3):181-208. Matejka, J., Li, W., Grossman, T. and Fitzmaurice, G. (2009). CommunityCommands: Command Recommendations for Software Applications. Proceedings of the 22nd Symposium on User Interface Software and Technology.193-202. Miller, B. N., Albert, I., Lam, S. K., Konstan, J. A. and Riedl, J. (2003). MovieLens unplugged: experiences with an occasionally connected recommender system. Proceedings of the 8th international conference on Intelligent user interfaces. 263-266. Mitchell, J. and Shneiderman, B. (1989). Dynamic versus static menus: an exploratory comparison. SIGCHI Bull. 20(4):33-37. Norman, D. A. and Draper, S. W., User Centered System Design; New Perspectives on Human-Computer Interaction. 1986: L. Erlbaum Associates Inc. 526. Schein, A., Popescul, A., Ungar, L., Pennock, D. (2001). Generative Models for Cold-Start Recommendations. the 2001 SIGIR Workshop on Recommender Systems. Ramakrishnan, N., Keller, B. J., Mirza, B. J., Grama, A. Y. and Karypis, G. (2001). Privacy Risks in Recommender Systems. IEEE Internet Computing. 5(6):54-62. Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P. and Riedl, J. (1994). GroupLens: an open architecture for collaborative filtering of netnews. Proceedings of the 1994 ACM conference on Computer supported cooperative work. 175-186. Shardanand, U. and Maes, P. (1995). Social information filtering: algorithms for automating "word of mouth". Proceedings of the SIGCHI conference on Human factors in computing systems. 210-217. Shneiderman, B. (1983). Direct Manipulation: A Step Beyond Programming Languages. Computer. 16(8):57-69. Shokri, R., Pedarsani, P., Theodorakopoulos, G. and Hubaux, J.P. (2009). Preserving privacy in collaborative filtering through distributed aggregation of offline profiles. Proceedings of the third ACM conference on Recommender systems. 157-164. Twidale, M. B. (2005). Over the Shoulder Learning: Supporting Brief Informal Learning. Comput. Supported Coop. Work. 14(6):505-547. Li, W., Matejka, J., Grossman, T., Konstan, J. A. and Fitzmaurice, G. (2011) Design and Evaluation of a Command Recommendation System for Software Applications. ACM Trans. Comput.-Hum. Interact.,18(2):6:1-6:35



Acknowledgement

We would like to thank Joseph A. Konstan for providing valuable suggestions and participating discussions during this project development.



References

Ahmad, W. and Khokhar, A. (2007). An Architecture for Privacy Preserving Collaborative Filtering on Web Portals. Proceedings of the Third International Symposium on Information Assurance and Security. 273-278. Baecker, R., Booth, K., Jovicic, S., McGrenere, J. and Moore, G. (2000). Reducing the gap between what users know and what they need to know. Universal Usability-2000. 17-23. Berkovsky, S., Eytani, Y., Kuflik, T. and Ricci, F. (2007). Enhancing privacy and preserving accuracy of a distributed collaborative filtering. Proceedings of the 2007 ACM conference on Recommender systems. 9-16. Ding, Y. and Li, X. (2005). Time weight collaborative filtering. Proceedings of the 14th ACM international conference on Information and knowledge management. 485-492. Fischer, G. (2001). User Modeling in Human\&ndash;Computer Interaction. User Modeling and User-Adapted Interaction. 11(12):65-86. Frankowski, D., Cosley, D., Sen, S., Terveen, L. and Riedl, J. (2006). You are what you say: privacy risks of public mentions. Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 565-572. Grossman, T., Fitzmaurice, G. and Attar, R. (2009). A Survey of Software Learnability: Metrics, Methodologies and Guidelines. ACM CHI conference on Human Factors in Computing Systems. 10 pages. Herlocker, J. L., Konstan, J. A., Terveen, L. G. and Riedl, J. T. (2004). Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22(1):5-53. Hill, W., Stead, L., Rosenstein, M. and Furnas, G. (1995). Recommending and evaluating choices in a virtual community of use. Proceedings of the SIGCHI conference on Human factors in computing systems. 194-201. Jones, K. S. (1972). A statistical interpretation of specificity and its application in retrieval. Journal of Documentation. 60(5):10. Karypis, G. (2001). Evaluation of Item-Based Top-N Recommendation Algorithms. Proceedings of the tenth



Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)



Fast Nonsmooth Regularized Risk Minimization with Continuation

Shuai Zheng, Ruiliang Zhang, James T. Kwok

Department of Computer Science and Engineering Hong Kong University of Science and Technology Hong Kong {szhengac, rzhangaf, jamesk}@cse.ust.hk  with the much faster O(1/ T ) and O(1/T ) rates, respectively (Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2013). Recently, Shamir and Zhang (2013) recovered these rates by using a polynomial-decay averaging scheme on the SGD iterates. However, a major drawback is that it does not exploit properties of the regularizer. For example, when used with a sparsity-inducing regularizer, its solution obtained may not be sparse (Duchi and Singer 2009). Nesterov (2005b) proposed to smooth the nonsmooth objective so that it can then be efficiently optimized. This smoothing approach is now popularly used for nonsmooth optimization. However, the optimal smoothness parameter needs to be known in advance. This restriction is later avoided by the (batch) excessive gap algorithm (Nesterov 2005a). In the stochastic setting, Ouyang and Gray (2012) combined Nesterov's smoothing with SGD. Though these methods achieve the fastest known convergence rates in the batch and stochastic settings respectively, they assume a Lipschitz-smooth regularizer, and nonsmooth regularizers (such as the sparsity-inducing regularizers) cannot be used. Recently, based on the observation that the training set is indeed finite, a number of fast stochastic algorithms are proposed for both smooth and composite optimization problems (Schmidt, Roux, and Bach 2013; Johnson and Zhang 2013; Xiao and Zhang 2014; Mairal 2013; Defazio, Bach, and Lacoste-Julien 2014). They are based on the idea of variance reduction, and attain comparable convergence rates as their batch counterparts. However, they are not applicable when both the loss and regularizer are nonsmooth. To alleviate this, Shalev-Shwartz and Zhang (2014) suggested running these algorithms on the smoothed approximation obtained by Nesterov's smoothing. However, as in (Nesterov 2005b), it requires a careful setting of the smoothness parameter. Over-smoothing deteriorates solution quality, while under-smoothing slows down convergence. The problem of setting the smoothness parameter can be alleviated by continuation (Becker, Bobin, and Cand es 2011). It solves a sequence of smoothed problems, in which the smoothing parameter is gradually reduced from a large value (and the corresponding smoothed problem is easy to solve) to a small value (which leads to a solution closer to that of the original nonsmooth problem). Moreover, solution of the intermediate problem is used to warm-start the next smoothed problem. This approach is also similar to that



Abstract

In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of O(1/T 2 ) on strongly convex problems, and O(1/T ) on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art.



Introduction

In regularized risk minimization, one has to minimize the sum of an empirical loss and a regularizer. When both are smooth, it can be easily optimized by a variety of solvers (Nesterov 2004). In particular, a popular choice for big data applications is stochastic gradient descent (SGD), which is easy to implement and highly scalable (Kushner and Yin 2003). For many nonsmooth regularizers (such as the 1 and nuclear norm regularizers), the corresponding regularized risks can still be efficiently minimized by the proximal gradient algorithm and its accelerated variants (Nesterov 2013). However, when the regularizer is smooth but the loss is nonsmooth (e.g., the hinge loss and absolute loss), or when both the loss and regularizer are nonsmooth, proximal gradient algorithms are not directly applicable. On nonsmooth problems, SGD can still be used, by simply replacing the gradient with subgradient. However, the information contained in the subgradient is much less informative (Nemirovski and Yudin 1983), and convergence is hindered. On general  convex problems, SGD converges at a rate of O(log T / T ), where T is the number of iterations; whereas on strongly convex problems, the rate is O(log T /T ). In contrast, its smooth counterparts converge

Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



2393



of gradually changing the regularization parameter in (Hale, Yin, and Zhang 2007; Wen et al. 2010; Mazumder, Hastie, and Tibshirani 2010). Empirically, continuation converges much faster than the use of a fixed smoothing parameter (Becker, Bobin, and Cand es 2011). However, the theoretical convergence rate obtained in (Becker, Bobin, and Cand es 2011) is only for one stage of the continuation algorithm (i.e., on the smoothed problem with a particular smoothing parameter), while the convergence properties for the whole algorithm are not clear. Recently, Xiao and Zhang (2012) obtained a linear convergence rate for their continuation algorithm, though only for the special case of 1 -regularized least squares regression. In this paper, we consider the general nonsmooth optimization setting, in which both the loss and regularizer may be nonsmooth. The proposed continuation algorithm can be flexibly used with a variety of existing batch/stochastic solvers in each stage. Theoretical analysis shows that the proposed algorithm, with this wide class of solvers, achieves the rate of O(1/T 2 ) on strongly convex problems, and O(1/T ) on general convex problems. These are the fastest known rates for nonsmooth optimization. Note that these rates are for the whole algorithm, not just one of its stages as in (Becker, Bobin, and Cand es 2011). Experiments on nonsmooth classification and regression models demonstrate that the proposed algorithm outperforms the state-of-the-art. Notation. For x, y  Rd , x

d 2



Other examples in machine learning include popular regularizers such as the 1 , total variation (Becker, Bobin, and Cand es 2011), overlapping group lasso, and graph-guided fused lasso (Chen et al. 2012). Minimization of the smooth (and convex) g  can be performed efficiently using first-order methods, including the so-called "optimal method" and its variants (Nesterov 2005b) that achieve the optimal convergence rate.



Nesterov Smoothing with Continuation

Consider the following nonsmooth minimization problem min P (x)  f (x) + r(x),

x



(5)



where both f and r are convex and nonsmooth. In machine learning, x usually corresponds to the model parameter, f is the loss, and r the regularizer. We assume that the loss f on a set of n training samples can be decomposed as n 1 f (x) = n i=1 fi (x), where fi is the loss value on the ith sample. Moreover, each fi can be written as in (1), i.e.,  fi (x) = f i (x) + maxuU [ Ai x, u - Q(u)]. One can then apply Nesterov's smoothing, and P (x) in (5) is smoothed to   (x) = f P  (x) + r (x),  where f  (x) =

1 n n i=1 uU



(6)



 f i (x) and (7)



=



d i=1



x2 i is its



2 -norm,



  f i (x) = fi (x) + max [ Ai x, u - Q(u) -  (u)] .



x 1 = i=1 |xi | is its 1 -norm, and x, y is the dot product between x, y . Moreover, f denotes the subdifferential of a nonsmooth function f , if f is differentiable, then f denotes its gradient. I is the identity matrix.



As for r, we assume that it is "simple", namely that its prox1 x - * 2 + r(x) for imal operator, proxr (*)  arg minx 2 any  > 0, can be easily computed (Parikh and Boyd 2014).



Related Work

Consider nonsmooth functions of the form g (x) = g (x) + max[ Ax, u - Q(u)],

uU



Strongly Convex Objectives

In this section, we assume that P is -strongly convex. This strong convexity may come from f (e.g., 2 -regularized hinge loss) or r (e.g., elastic-net regularizer) or both. Assumption 1. P is -strongly convex, i.e., there exists  > 2 0 such that P (y )  P (x) +  T (y - x) +  2 y - x 2 ,   d P (x) and x, y  R . The proposed algorithm is based on continuation. It proceeds in stages, and a smoothed problem is solved in each stage (Becker, Bobin, and Cand es 2011). The smoothness parameter is gradually reduced across stages, so that the smoothed problem becomes closer and closer to the original one. In each stage, an iterative solver M is used to solve the smoothed problem. It returns an approximate solution, which is then used to warm-start the next stage. In stage s, let the smoothness parameter be s , the s (x), x = arg minx P s (x), smoothed objective in (6) be P s and x s be the solution returned by M. As M is warms ( started by x s-1 , the error before running M is P xs-1 ) -   Ps (xs ). At the end of stage s, we assume that the error is reduced by a factor of s . The expectation E below is over the stochastic choice of training samples for a stochastic solver. For a deterministic solver, this expectation can be dropped. s ( s (x )  s (P s ( Assumption 2. EP xs ) - P xs-1 ) - s   Ps (x )), where s  (0, 1).

s



(1)



where g  is convex, continuously differentiable with L p Lipschitz-continuous gradient, U  R is convex, A  Rpxd , and Q is a continuous convex function. Nesterov (2005b) proposed the following smooth approximation: (x) + max [ Ax, u - Q(u) -  (u)] , g  (x) = g

uU



(2)



where  is a smoothness parameter, and  is a nonnegative  -strongly convex function. For example, consider the hinge loss g (x) = max(0, 1 - T x), where x is the linear model parameter, and (zi , yi ) yi zi is the ith training sample with yi  {1}. Using  (u) = 1 2 2 u 2 , g can be smoothed to (Ouyang and Gray 2012)  T x1 yi zi  0  T T 1 - yi zi x - 2 y i zi x < 1 -  . (3) g  (x) =  1 (1 - y z T x)2 otherwise i i 2

T x| can be smoothed to Similarly, the 1 loss g (x) = |yi - zi   T T yi - zi x  yi - zi x - 2  T T -(yi - zi x) - 2 yi - zi x < - . (4) g  (x) =  1 (y - z T x)2 otherwise i i 2



2394



Table 1: Examples of non-accelerated and accelerated solvers. Note that Prox-GD and APG are batch solvers while the others are stochastic solvers. Here,  and p are parameters related to the stepsize, and are fixed across stages. In particular,   (0, 0.25) and  p) satisfies (1 - 4)s - 4 > 0, and p  (0, 1) and satisfies s > p(2+ 1-p . Accelerated Prox-SVRG has Ts = O ( s log(1/s )) only when a sufficiently large mini-batch is used.

Prox-GD (Nesterov 2013) Prox-SVRG (Xiao and Zhang 2014) SAGA (Defazio, Bach, and Lacoste-Julien 2014) MISO (Mairal 2013) APG (Schmidt, Roux, and Bach 2011) Accelerated Prox-SVRG (Nitanda 2014) Ts 4s log(1/s )  (1-4 )s -4 (s + 4) 3n 3s s n +1  s log(2/s )  1 s (1-2 s p) log -

2+p



non-accelerated







ns s



accelerated



p 1-p



log(2/s ) 1 log s - p

2+p



1 (1-4 )s -4 1 s 1 s



(s ) log(1/s )



1-p



 2 (1-p)



a 4  9 n 1



b 0 4 3n 0 0 0



c 0 0 0 0 0 0



We consider two types of solvers, which differ in the number of iterations (Ts ) it takes to satisfy Assumption 2. 1. Non-accelerated solvers: Ts = as (s ) + b(s ) + c;  2. Accelerated solvers: Ts = a s (s ) + b(s ) + c. Here, s is the condition number of the objective, a, b, c  0 are constants not related to s and (s ). Moreover,  satisfies (i) (s ) > 0 and non-increasing for s  (0, 1); (ii) (s ) is not related to s . Note that when s is large (as is typical when the smoothed problem approaches the original problem), non-accelerated solvers need a larger Ts than accelerated solvers. Table 1 shows some non-accelerated and accelerated solvers popularly used in machine learning. Algorithm 1 shows the proposed procedure, which will be called CNS (Continuation for NonSmooth optimization). It is similar to that in (Becker, Bobin, and Cand es 2011), which however does not have convergence results. Moreover, a small but important difference is that Algorithm 1 specifies how Ts should be updated across stages, and this is essential for proving convergence. Note the different update options for non-accelerated and accelerated solvers. Algorithm 1 CNS algorithm for strongly convex problems. 1: Input: number of iterations T1 and smoothness parameter 1 for stage 1, and shrinking parameter  > 1. 2: Initialize: x 0 . 3: for s = 1, 2, . . . do s  smooth P with smoothing parameter s ; 4: P s (x) by running M for Ts itera5: x s  minimize P tions; 6: s+1 = s / ; 7: Option I (non-accelerated solvers): Ts+1 =  Ts ; 8: Option II (accelerated solvers): Ts+1 =  Ts ; 9: end for 10: Output: x s . The following Lemma shows that when T1 is large enough, error reduction can be guaranteed across all stages. Lemma 1. For both non-accelerated and accelerated solvers, if T1 is large enough such that 1  1/ 2 , then s  1/ 2 for all s > 1.



If 1 is known, a sufficiently large T1 can be obtained from Table 1; otherwise, we can obtain T1 by ensur1 ( 1 ( 1 ( ing P x1 )  P x0 )/ 2 , which then implies P x1 ) -   2 1 ( 1 (x ))/ . 1 (x )  (P x0 ) - P P

1 1



Convergence when Non-Accelerated Solver is used Let x = arg minx P (x), and Du = maxuU  (u). The following Lemma shows that if x is an -accurate solution of s (i.e., P s (x) - P s (x )  ), it is also an ( + s Du )the P s accurate solution of the original objective P . s (x ) - s Du  P (x) - P (x )  s (x) - P Lemma 2. P s    Ps (x) - Ps (x ) + s Du .

s



Since Lemma 2 holds for any x, it also holds in expectas (x ) - s Du  EP ( s ( xs ) - P xs ) - P (x )  tion, i.e., EP s    x s ) - Ps ( x s ) +  s D u . EPs ( Theorem 1. Assume that T1 in Algorithm 1 is large enough so that 1  1/ 2 . When non-accelerated solvers are used, EP ( xS )-P (x ) 

S s=1 S



s (P ( x0 )-P (x ))+O



1 Du , (8) T



where S is the number of stages, T = s=1 Ts , and S 2  = O (1 /T ) . s s=1 The first term on the RHS of (8) reflects the cumulative decrease of the objective after S stages, while the second term is due to smoothing. The condition 1  1/ 2 is used to obtain the O(1/T ) rate in the last term of (8). If we instead require that 1  1/ , it can be shown that the rate will be  slowed further to  to O(log T /T ); if 1  1/  , it degrades c O(1/ T ). On the other hand, if 1  1/ with c > 2, the rate will not be improved. Corollary 1. Together with Lemma 1, we have EP ( xS ) - P (x )  P ( x0 ) - P ( x ) +O  2S 1 Du T , (9)



where 1/ 2S = O(1/T 2 ). Existing stochastic algorithms such as SGD, FOBOS and RDA have a convergence rate of O(log T /T ) (Rakhlin, Shamir, and Sridharan 2012; Duchi and Singer 2009; Xiao 2009), while here we have the faster O(1/T ) rate. Recent



2395



works in (Shamir and Zhang 2013; Ouyang and Gray 2012) also achieve a O(1/T ) rate. However, Shamir and Zhang (2013) use stochastic subgradient, and do not exploit properties of the regularizer (such as sparsity). This can lead to inferior performance (Duchi and Singer 2009; Xiao 2009; Mazumder, Hastie, and Tibshirani 2010). On the other hand, (Ouyang and Gray 2012) is restricted to r  0 in (5). Next, we compare with the case where continuation is not used (i.e., s is a constant). Equivalently, this corresponds to setting  = 1 in Algorithm 1. Proposition 1. When continuation is not used, let   (0, 1) be the error reduction factor at each stage, and  > 0 be the fixed smoothing parameter. When either an accelerated or non-accelerated solver is used, EP ( xS ) - P (x )S (P ( x0 ) - P (x ))+(1+ S )Du .(10) Proposition 2. Assume that the two terms on the RHS of (9) and (10) are equal to  and (1 - ) , respectively, where  > 0 and > 0. Let 1 =  = 1/ 2 in (8) and (10). Assume that Algorithm 1 needs a total of T iterations to obtain an -accurate solution, while its fixed-s variant takes T iterations. Then,

T  S - 1  -1 a a1  1 2 + b  1 2 +c , 1 2 + b 1 2

1 2



Proposition 3. With the same assumptions in Proposition 2,  S  1 1  -1 + b +c , a 1  T  2  2  -1   2S + 1 1 1  T  S a + b + c , 1 + C  S +1 +  S 2 2 where S, C are as defined in Proposition 2.



General Convex Objectives

When P is not strongly convex, we add to it a small 2 term (with weight s ). We then gradually decrease s and s simultaneously to approach the original problem. The revised procedure is shown in Algorithm 2. Algorithm 2 CNS algorithm for general convex problems. 1: Input: number of iterations T1 , smoothness parameter 1 and strong convexity parameter 1 for stage 1, and shrinking parameter  > 1. 2: Initialize: x 0 . 3: for s = 1, 2, . . . do s  smooth P with smoothing parameter s ; 4: P s (x) + s x 2 by running M for 5: x s  minimize P 2 2 Ts iterations; 6: s+1 = s / ; s+1 = s / ; 7: Option I (non-accelerated solvers): Ts+1 =  2 Ts ; 8: Option II (accelerated solvers): Ts+1 =  Ts ; 9: end for 10: Output: x s . We assume that there exists R > 0 such that x 2  R, s 2  and x s 2  R for all s. Define Hs (x) = Ps (x) + 2 x 2 , and let x = arg min H ( x ) . The following assumption is x s s similar to that for strongly convex problems. Assumption 3. EHs ( xs ) - Hs ( x xs-1 ) - s )  s (Hs (  Hs (xs )), where s  (0, 1). Theorem 3. Assume that T1 in Algorithm 2 is large enough so that 1  1/ 2 . When non-accelerated solvers are used,

EP ( xS )-P (x ) 

S



T S



 2S + 1 1 + C  S +1 +  S



+c ,



where S 1-

2S







log

K ,



 (P ( x0 )-P (x ))



/ log



, C



=



 +1  S +1 + S



and K is a constant,



T and T are usually dominated by the a1 (1/ 2 ) term, and T is roughly S times that of T . This is also consistent with empirical observations that continuation is much faster than fixed smoothing (Becker, Bobin, and Cand es 2011). Convergence when Accelerated Solver is used Theorem 2. Assume that T1 in Algorithm 1 is large enough so that 1  1/ 2 . When accelerated solvers are used, EP ( xS )-P (x) 

S s=1 S s=1 S



s (P ( x0 )-P (x ))+O



1 Du . T2



Ts , and s=1 s = O(1/T 4 ). where T = As the s 's for non-accelerated and accelerated solvers S are different, the s=1 s term here is different from that in Theorem 1. Moreover, the last term is improved from O(1/T ) in Theorem 1 to O(1/T 2 ) with accelerated solvers. This is also better than the rates of existing stochastic algorithms (O(log T /T ) in (Duchi and Singer 2009; Xiao 2009) and O(1/T ) in (Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2013; Ouyang and Gray 2012)). Besides, the black-box lower bound of O(1/T ) for strongly convex problems (Agarwal et al. 2009) does not apply here, as we have additional assumptions that the objective is of the form in (1) and the number of training samples is finite. Though the (batch) excessive gap algorithm (Nesterov 2005a) also has a O(1/T 2 ) rate, it is limited to r  0 in (5). As in Proposition 2, the following shows that if continuation is not used, the algorithm is roughly S times slower.



s

s=1



P ( x0 )-P (x )+ +O



1 x 0 2 ,



2 2



+O



1 R2 /2  T



1 D u  T



(11)



where



S s=1



1 s = O( T ). For accelerated solvers,

S



EP ( xS )-P (x ) 



s

s=1



P ( x0 )-P (x )+ +O



1 x 0 2 ,



2 2



+O



1 R2 /2 T



1 D u T



(12)



where



 For non-accelerated solvers, the O(1/ T ) convergence rate in (11) is only as good as that obtained in (Xiao 2009; Duchi and Singer 2009; Ouyang and Gray 2012; Shamir and



S s=1



s = O( T12 ).



2396



Zhang 2013). Hence, they will not be studied further in the sequel. However, for accelerated solvers, the O (1/T ) con vergence rate in (12) is faster than the O(1/ T ) rate in (Xiao 2009; Duchi and Singer 2009; Ouyang and Gray 2012; T Shamir and Zhang 2013) and the O( T12 + log T ) rate in (Orabona, Argyriou, and Srebro 2012). The O(1/T ) convergence rate is also obtained in (Nesterov 2005a; 2005b), but again only for r  0 in (5). When continuation is not used, the following results are analogous to those obtained in the previous section. Proposition 4. Let x 0 = 0. When continuation is not used, let  be the error reduction factor at each stage. When either an accelerated or non-accelerated solver is used, EP ( xS ) - P (x )





archive, and is a subset of the Million Song data set. We use the hinge loss for classification, and 1 loss for regression. Both can be smoothed using Nesterov's smoothing (to (3) and (4), respectively). As for the regularizer, we use the 2 2 1. elastic-net regularizer r(x) = 1 x 1 +  2 x 2 (Zou and Hastie 2005), and problem (5) is strongly convex; and 2. 1 regularizer r(x) = 1 x 1 , and (5) is (general) convex. Here, 1 , 2 are tuned by 5-fold cross-validation. Obviously, all losses and regularizers are convex but nonsmooth. We use mini-batch for all methods. The mini-batch size b is 50 for rcv1, and 100 for YearPredictionMSD. Table 3: Data sets used in the experiments. rcv1 YearPredictionMSD #train 20,242 463,715 #test 677,399 51,630 #features 47,236 90







 (P ( x0 ) - P (x ))  +(1 + S )Du + R2 . (13) 2

S







Proposition 5. Let x 0 = 0. Suppose that the three terms on the RHS of (13) are equal to  ,  and  , respectively, where , ,  > 0 and  +  +  = 1. Let 1 =  = 1/ 2 in (12) and (13). Assume that Algorithm 2 (with accelerated solver) needs a total of T iterations to obtain an -accurate solution, while its fixed-s variant takes T iterations. Then, T  S - 1  -1 a  a 1  1 2 + b 1 2 1 2 +c , 1 2

1 2



T S where S 1-



 2S + 1 1 + C ( + 1)2  + log

 1+

S



+ b / log



+c , , C =



 (P ( x0 )-P (x ))



 +1 ( +1)2



2S



-



 +1 ( +1)2



2S



K 1



and K is a constant.



A summary of the convergence results is shown in Table 2. As can be seen, the convergence rates of the proposed CNS algorithm match the fastest known rates in nonsmooth optimization, but CNS is less restrictive and can exploit the composite structure of the optimization problem. Table 2: Comparison with the fastest known convergence rates for nonsmooth optimization problem (1). The fastest known batch solver is restricted to r  0, while the fastest known stochastic solver does not exploit properties of r. strongly convex yes no batch solver 1/T 2 1/T stochastic solver 1/T  1/ T CNS (batch/stochastic) non-accel. accel. 1/T 1/T 2  1/ T 1/T



The following stochastic algorithms are compared: 1. Forward-backward splitting (FOBOS) (Duchi and Singer 2009), a standard baseline for nonsmooth stochastic composite optimization. 2. SGD with polynomial-decay averaging (Poly-SGD) (Shamir and Zhang 2013), the state-of-art for nonsmooth optimization. 3. Regularized dual averaging (RDA) (Xiao 2009): This is another state-of-the-art for sparse learning problems. 4. The proposed CNS algorithm: We use proximal SVRG (PSVRG) (Xiao and Zhang 2014) as the underlying non-accelerated solver, and accelerated proximal SVRG (ACC-PSVRG) (Nitanda 2014) as the accelerated solver. The resultant procedures are denoted CNS-NA and CNSA, respectively. We set 1 = 0.01,  = 2, and T1 = n/b . Empirically, this ensures 1  1/ 2 (in Theorems 1 and 3) on the two data sets. Note that FOBOS, RDA and the proposed CNS can effectively make use of the composite structure of the problem, while Poly-SGD cannot. For each method, the stepsize is tuned by running on a subset containing 20% training data for a few epochs (for the proposed method, we tune 1 ). All algorithms are implemented in Matlab.



Strongly Convex Objectives

Figure 1 shows convergence of the objective and testing performance (classification error for rcv1 and 1 -loss for YearPredictionMSD). The trends are consistent with Theorem 1. CNS-A is the fastest (with a of O(1/T 2 )). This is followed by CNS-NA and Poly-SGD, both with O(1/T ) rate (from Theorem 1 and (Shamir and Zhang 2013)). The slowest are FOBOS and RDA, which converge at a rate of O(log T /T ) (Duchi and Singer 2009; Xiao 2009). Figure 2 compares with the case where continuation is not used. Two fixed smoothness settings,  = 10-2 and  = 10-3 , are used. As can be seen, they are much slower (Propositions 2 and 3). Moreover, a smaller  leads to slower convergence but better solution, while a larger  leads to



Experiments

Because of the lack of space, we only report results on two data sets (Table 3) from the LIBSVM archive: (i) the popularly used classification data set rcv1; and (ii) YearPredictionMSD, the largest regression data in the LIBSVM



2397



10 -2



10 0



10 -1



10 0



10 -3



10 -1



10 -2



10 -1



objective minus best



objective minus best



10 -4



10 -2



objective minus best



objective minus best



10 -2



10 -3



10 -5



10 -3



10 -3



10 -6



10 -4



10 -4



10 -4



10 -7



0



10



20



30



40



50



10 -5



0



10



20



30



40



50



10 -5



0



10



20



30



40



50



10 -5



0



10



20



30



40



50



CPU time (s)



CPU time (s)



CPU time (s)



CPU time (s)



3.9 3.895 3.89 3.885



0.22 0.2 0.18



4.6



0.24 0.22



4.55



0.2 0.18



test error (%)



test loss



test loss



3.88 3.875 3.87 3.865 3.86 3.855 3.85 0 10 20 30 40 50



test error (%)



0.16 0.14 0.12 0.1 0.08 0.06



4.5



0.16 0.14 0.12



4.45



4.4



0.1 0.08



0



10



20



30



40



50



4.35



0



10



20



30



40



50



0.06



0



10



20



30



40



50



CPU time (s)



CPU time (s)



CPU time (s)



CPU time (s)



rcv1.



YearPredictionMSD.



rcv1.



YearPredictionMSD.



Figure 1: Objective (top) and testing performance (bottom) vs CPU time (in seconds) on a strongly convex problem.



Figure 3: Objective (top) and testing performance (bottom) vs CPU time (in seconds) on a general convex problem.

10 -2 10 0



faster convergence but worse solution. This is also consistent with Proposition 1, as using a fixed  only allows convergence to the optimal solution with a tolerance of (1 + S )Du . Moreover, a smaller  leads to a larger condition number, and convergence becomes slower.

10 -2 10 0



10 -1



objective minus best



10 -3



objective minus best

0 10 20 30 40 50



10 -2



10 -3



10 -4



10 -4



10 -5



10 -5



0



10



20



30



40



50



CPU time (s)



CPU time (s)



(a) rcv1.



(b) YearPredictionMSD.



10 -3



10 -1



objective minus best



10 -4



objective minus best



10 -2



Figure 4: Effect of continuation (general convex problem).



10 -5



10 -3



10 -6



10 -4



10 -7



0



10



20



30



40



50



10 -5



Conclusion

0 10 20 30 40 50



CPU time (s)



CPU time (s)



(a) rcv1.



(b) YearPredictionMSD.



Figure 2: Effect of continuation (strongly convex problem).



General Convex Objectives

We set 1 in Algorithm 2 to 10-5 for rcv1, and 10-7 for YearPredictionMSD. As can be seen from Figure 3, the trends are again consistent with Theorem 3. CNS-A is the fastest (O(1/T ) convergence rate), while the others all have  a rate of O(1/ T ) (Duchi and Singer 2009; Xiao 2009; Shamir and Zhang 2013). Also, RDA shows better performance than FOBOS and Poly-SGD. Recall that Poly-SGD outperforms FOBOS and RDA on strongly convex problems. However, on general convex problems, Poly-SGD is the worst as its rate is only as good as others, and it does not exploit the composite structure of the problem. Figure 4 compares with the case where continuation is not used. As in the previous section, CNS-NA and CNS-A show faster convergence than its fixed-smoothing counterparts.



In this paper, we proposed a continuation algorithm (CNS) for regularized risk minimization problems, in which both the loss and regularizer may be nonsmooth. In each of its stages, the smoothed subproblem can be easily solved by either existing accelerated or non-accelerated solvers. Theoretical analysis establishes convergence results on the whole continuation algorithm, not just one of its stages. In particular, when accelerated solvers are used, the proposed CNS algorithm achieves the rate of O(1/T 2 ) on strongly convex problems, and O(1/T ) on general convex problems. These are the fastest known rates for nonsmooth optimization. However, CNS is advantageous in that it allows the use of a regularizer (unlike the fastest batch algorithm) and can exploit the composite structure of the optimization problem (unlike the fastest stochastic algorithm). Experiments on nonsmooth classification and regression models demonstrate that CNS outperforms the state-of-the-art.



Acknowledgments

This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614513).



2398



References

Agarwal, A.; Wainwright, M. J.; Bartlett, P. L.; and Ravikumar, P. K. 2009. Information-theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, 1-9. Becker, S.; Bobin, J.; and Cand es, E. J. 2011. NESTA: A fast and accurate first-order method for sparse recovery. SIAM Journal on Imaging Sciences 4(1):1-39. Chen, X.; Lin, Q.; Kim, S.; Carbonell, J. G.; Xing, E. P.; et al. 2012. Smoothing proximal gradient method for general structured sparse regression. Annals of Applied Statistics 6(2):719-752. Defazio, A.; Bach, F.; and Lacoste-Julien, S. 2014. SAGA: A fast incremental gradient method with support for nonstrongly convex composite objectives. In Advances in Neural Information Processing Systems, 2116-2124. Duchi, J., and Singer, Y. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research 10:2899-2934. Hale, E.; Yin, W.; and Zhang, Y. 2007. A fixed-point continuation method for 1 -regularized minimization with applications to compressed sensing. Technical Report CAAM TR07-07, Rice University. Johnson, R., and Zhang, T. 2013. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, 315-323. Kushner, H. J., and Yin, G. 2003. Stochastic Approximation and Recursive Algorithms and Applications, volume 35. Springer Science & Business Media. Mairal, J. 2013. Optimization with first-order surrogate functions. In Proceedings of the 30th International Conference on Machine Learning. Mazumder, R.; Hastie, T.; and Tibshirani, R. 2010. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research 11:2287- 2322. Nemirovski, A., and Yudin, D. 1983. Problem Complexity and Method Efficiency in Optimization. Wiley. Nesterov, Y. 2004. Introductory Lectures on Convex Optimization, volume 87. Springer. Nesterov, Y. 2005a. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization 16(1):235-249. Nesterov, Y. 2005b. Smooth minimization of non-smooth functions. Mathematical Programming 103(1):127-152. Nesterov, Y. 2013. Gradient methods for minimizing composite functions. Mathematical Programming 140(1):125- 161. Nitanda, A. 2014. Stochastic proximal gradient descent with acceleration techniques. In Advances in Neural Information Processing Systems, 1574-1582. Orabona, F.; Argyriou, A.; and Srebro, N. 2012. PRISMA: Proximal iterative smoothing algorithm. Preprint arXiv:1206.2372.



Ouyang, H., and Gray, A. G. 2012. Stochastic smoothing for nonsmooth minimizations: Accelerating SGD by exploiting structure. In Proceedings of the 29th International Conference on Machine Learning, 33-40. Parikh, N., and Boyd, S. 2014. Proximal algorithms. Foundations and Trends in Optimization 1(3):127-239. Rakhlin, A.; Shamir, O.; and Sridharan, K. 2012. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, 449-456. Schmidt, M.; Roux, N. L.; and Bach, F. R. 2011. Convergence rates of inexact proximal-gradient methods for convex optimization. In Advances in Neural Information Processing Systems, 1458-1466. Schmidt, M.; Roux, N. L.; and Bach, F. 2013. Minimizing finite sums with the stochastic average gradient. Preprint arXiv:1309.2388. Shalev-Shwartz, S., and Zhang, T. 2014. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In Proceedings of the 31st International Conference on Machine Learning, 64-72. Shamir, O., and Zhang, T. 2013. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Proceedings of the 30th International Conference on Machine Learning, 71-79. Wen, Z.; Yin, W.; Goldfarb, D.; and Zhang, Y. 2010. A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation. SIAM Journal on Scientific Computing 32(4):1832-1857. Xiao, L., and Zhang, T. 2012. A proximal-gradient homotopy method for the 1 -regularized least-squares problem. In Proceedings of the 29th International Conference on Machine Learning, 839-846. Xiao, L., and Zhang, T. 2014. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization 24(4):2057-2075. Xiao, L. 2009. Dual averaging method for regularized stochastic learning and online optimization. In Advances in Neural Information Processing Systems, 2116-2124. Zou, H., and Hastie, T. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B 67(2):301-320.



2399



SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions

Han Xiao, Minlie Huang, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China bookman@vip.163.com; {aihuang,zxy-dcs}@tsinghua.edu.cn



Abstract



arXiv:1604.04835v1 [cs.CL] 17 Apr 2016



Knowledge graph embedding represents the entities and relations as numerical vectors, and then knowledge analysis could be promoted as a numerical method. So far, most methods merely concentrate on the fact triples that are composed by the symbolic entities and relations, while the textual information which is supposed to be most critical in NLP could hardly play a reasonable role. For this end, this paper proposes the method SSP which jointly learns from the symbolic triples and textual descriptions. Our model could interact both two information sources by characterizing the correlations, by which means, the textual descriptions could make effects to discover semantic relevance and offer precise semantic embedding. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines on the tasks of knowledge graph completion and entity classification.



methods have been proposed, such as TransE (Bordes et al., 2013), PTransE (Lin et al., 2015a), KG2E (He et al., 2015) etc. The translation-based methods as a key branch of embedding models, adopt the principle of translating the head entity to the tail one by a relation-specific vector, mathematically speaking h + r = t. As Fig.1 shows, in the knowledge graph, the entities such as h, t have textual descriptions, which contain much extra semantic information about knowledge triples.



Figure 1: Example of entity descriptions. Despite the success of triple embedding, there are still two reasons why textual descriptions would be necessary in this task: discovering semantic relevance and offering precise semantic expression. Firstly, the semantic relevance between entities is capable to recognize the true triples, which are difficult to be inferred only with fact triples. For example, the triple (Anna Roosevelt, Parents, Franklin Roosevelt), indicates "Franklin Roosevelt" is the parent of "Anna Roosevelt". There is almost no hint to infer this fact from other symbolic triples. In contrast, there are many keywords such as "Roosevelt" and "Daughter of the President" in the textual description of the head entity, which match the keywords "Roosevelt" and "President" in that of the tail one. These matched keywords imply this fact triple by offering extra semantic relevance. Specifically, we measure the possibility of a



1



Introduction



Knowledge graph provides an effective basis for NLP in many tasks such as question answering, web search and semantic analysis. In order to provide a numerical computation framework for knowledge graph, which could be leveraged by the numerical deep learning, knowledge graph embedding represents the entities and relations in a continuous low-dimensional vector space. More specifically, a basic fact in knowledge graph is usually represented as a symbolic triple (h, r, t), where h, r, t are the representation vectors of the head entity, the relation and the tail entity, respectively. To this end, a lot of embedding



triple by projecting the loss onto a hyperplane that represents the semantic relevance between entities. Thus, it is always possible to accept a fact triple so long as the projection of the loss vector onto the semantic hyperplane is sufficiently small. Secondly, precise semantic expression could promote the discriminative ability between two indistinguishable triples. For an instance, when we query about the profession of "Daniel Sturgeon", there are two possible options that "politician" and "lawyer". It's hard to distinguish if only focusing on the symbolic triples. However, the textual description of "Daniel Sturgeon" is full of politics-related keywords such as "Democratic Party", "State Legislature" etc. and even the word "Politician" also appears in this description. The textual descriptions help to refine the topic of "Daniel Sturgeon" in a more precise way from the social celebrities to the government officers, which makes the true answer "politician" more preferable. Formally, even though the loss vectors of the two facts are almost equal-length, after respectively projected onto the "politician" and "lawyer" related semantic hyperplanes, the losses are distinguished reasonably. In this way, precise semantic expression refines the embedding.



Figure 2: Simple illustration of TransE and SSP where h + r - t is the loss vector. The existing embedding methods with textual semantics such as DKRL (Xie et al., 2016) and "Jointly" (Zhong et al., 2015), have achieved much success. But there are still one important disadvantage, that is the weak-correlation modeling issue, indicating current models could hardly characterize the strong correlations between texts and triples. In the DKRL, for a triple, the embedding vector of the head entity is translated to that of the tail one as well as possible, where the encoded texts and entities are concatenated as the embedding vectors. Besides, "Jointly" model generally attempts to make the embeddings of the corresponding entity and text proximal. Both are



first-order constraints. It's noteworthy that triple embedding is always the main procedure and textual descriptions must interact with triples for better embedding. Only in this way, the semantic effects could make more senses. Actually, the strong correlation, that the high-order restriction, would interact texts and triples to complement each other in a more semantics-specific way than simple constraints. For the above example of "Daniel Sturgeon", the textual descriptions imply two candidate answers "Banker" and "Politician". Thus, only by considering both the triples and texts, we could come out with the true answer. Therefore, we focus on the stronger semantic interaction by projecting triple embedding onto a semantic subspace such as hyperplane, as shown in Fig.2. Mathematically, the quadratic constraint is adopted to model the strong correlation, thus the embedding topologies are sufficiently semanticsspecific. We evaluate the effectiveness of our model Semantic Subspace Projection (SSP) on two tasks that are knowledge graph completion and entity classification, for three benchmark datasets that are the subsets of Wordnet (Miller, 1995) and Freebase (Bollacker et al., 2008). Experimental results on real-world datasets show that our model consistently outperforms the other baselines with an extensive improvement. Contributions. We propose a knowledge graph embedding method SSP which models the strong correlations between the symbolic triples and the textual descriptions by performing the embedding in a semantic subspace as hyperplane. Besides, our method outperforms all the baselines on the tasks of knowledge graph completion and entity classification, which justifies our effectiveness.



2



Related Work



We have surveyed the related literature and categorized the embedding methods into two branches: Translation-Based Embedding that is only triplespecific and "Text-Aware" Embedding that involves textual descriptions. 2.1 Triple-Specific Embedding



TransE (Bordes et al., 2013) is a seminal work for this branch, which translates the head entity to the tail one by the relation vector, or h + r = t. Naturally, the scale of the loss vector is the score function, which measures the plausibility of triples



and a smaller score is better. The following variants transform entities into different subspaces to play different roles. TransH (Wang et al., 2014b) utilizes the relation-specific hyperplane to lay the entities. TransR (Lin et al., 2015b) applies the relation-related matrix to rotate the embedding space. Similar researches also contain TransA (Xiao et al., 2015), TransD (Ji et al., 2015) and TransM (Fan et al., 2014). Further researches take extra structural information into embedding. PTransE (Lin et al., 2015a) is a path-based model, simultaneously considering the information and confidence level of the path in the knowledge graph. (Wang et al., 2015) incorporates the rules to restrict the embeddings for the complex relation types such as 1-N, N-1 and N-N. SSE (Guo et al., 2015) aims at discovering the geometric structure of embedding topologies then based on these assumptions, designs a semantically smooth score function. Also, KG2E (He et al., 2015) involves probabilistic analysis to characterize the uncertain concepts of knowledge graph. There are also some pioneering work such as SE (Bordes et al., 2011), LFM (Jenatton et al., 2012), NTN (Socher et al., 2013) and RESCAL (Nickel et al., 2011) etc. 2.2 Embedding with Textual Information



3



Methodology



In this section, we first introduce our model and discuss the details. Furthermore, we provide two different insights to address the ability of our model. We also list some notations: all the symbols h, t indicate the head and tail entity, respectively. h (or t) is the embedding of the entity from the triples, sh (or st ) is the semantic vector generated from the texts, and d is the dimension of embedding. The data involved in our model are the knowledge triples and the textual descriptions of entities. In experiments, we adopt the "entity descriptions" of Freebase and the textual definitions of Wordnet as textual information. Actually, any related textual data could be involved in our method. 3.1 Model Description



Previous analysis in the introduction suggests to characterize the strong correlation between triples and texts. For the purpose of interacting the symbolic triples and textual descriptions, this paper attempts to restrict the embedding procedure of a specific triple in the semantic subspace. Specifically, we leverage a hyperplane with normal vector . s = S (sh , st ) as the subspace, where S : R2d  Rd is the semantic composition function discussed in  3.2 , and sh , st is the head-specific and tailspecific semantic vectors. The score function in the translation-based methods is ||h + r - t||2 2 , which means the triple embedding focuses on the loss vector . e = h + r - t . According to our motivation, assuming e is length-fixed, the target is to maximize the component inside the hyperplane, which is ||e - s es||2 2 . In detail, the component of the loss in the normal vector direction is (s es), then the other orthogonal one, that is inside the hyperplane, is (e - s es). Naturally, the total loss vector should also be punished. For this end, we introduce a factor  to balance the two parts, stated as:

2 fr (h, t) = -||e - s es||2 2 + ||e||2



"Text-Aware" Embedding, that indicates representing knowledge graph with textual information, generally dates back to NTN (Socher et al., 2013). NTN makes use of entity name and embeds an entity as the average word embedding vectors of the name. (Wang et al., 2014a) attempts to align the knowledge graph with the corpus then jointly conduct knowledge embedding and word embedding. However, the necessity of the alignment information limits this method both in performance and practical application. Thus, (Zhong et al., 2015) proposes "Jointly" method that only aligns the freebase entity to the corresponding wiki-page. DKRL (Xie et al., 2016) extends the translation-based embedding methods from the triple-specific one to the "Text-Aware" model. More importantly, DKRL adopts a CNNstructure to represent words, which promotes the expressive ability of word semantics. Generally speaking, by further jointing knowledge and texts, DKRL keeps the state-of-the-art performance of this branch.



where  is a suitable hyper-parameter. Moreover, a smaller score means more plausible. For clarity, the definitions of the symbols are boxed. Notably, the projection part in our score function is negative, so more projection means less loss.



3.2



Semantic Vector Generation



There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei, 2012) such as LSA, LDA, NMF (Stevens et al., 2012) and word embedding such as CBOW (Mikolov et al., 2013b), Skip-Gram (Mikolov et al., 2013a). More concretely, this paper adopts the topic model, treating each entity description as a document and then obtains the topic distribution of document as the semantic vector of entity. The entities are usually organized by the topic in knowledge base, for an example of the "entity type" in Freebase. Therefore, we conjecture that the topic model could be more competent. Notably, the word embedding would also work well though maybe not better. Given the pre-trained semantic vectors, our model fixes them and then optimizes the other parameters. We call this setting Standard (short as Std.). The reason why we could not adapt all the parameters, is the training procedure would refill the semantic vectors and flush the semantics out. For the purpose of jointly learning the semantics and the embeddings, we conduct the topic model and the embedding model, simultaneously. In this way, the symbolic triples also pose a positive effect on the textual semantics and we call this setting Joint. As each component of semantic vector indicates the relevant level to a topic, we suggest the semantic composition should take the addition form: S (sh , st ) = sh + st ||sh + st ||2 2



hyperplane, implying the head and tail entity also lay on it, as the beginning and ending point. Thus, there exists the important restriction, that the entities co-occur in a triple should be embedded in the semantic space composed by the associated textual semantics. This restriction is implemented as a quadric form to characterize the strong correlation between texts and triples, in other words, to interact both the information sources. A strong interaction between the textual descriptions and symbolic triples complements each other in a more semantics-specific form, which guarantees the semantic effects. More concretely, the embeddings are decided in the training procedure not only by triples but also by semantics, based on which, our embedding topologies are semantically different from the other methods. Comparatively, the firstorder constraints could hardly reach so far, making an unsatisfactory usage of textual semantics. 3.4 Semantic Perspective



where the normalization is applied to make a normal vector. Since the largest components represent the topics, the addition corresponds to the topic union, making the composition indicate the whole semantics. For example, when sh = (0.1, 0.9, 0.0) and st = (0.8, 0.0, 0.2), the topic of the head entity is #2 and that of the tail is #1, while the composition is s = (0.45, 0.45, 0.10), corresponding to the topic of #1,#2, or we say the topic union of both entities. 3.3 Correlation Perspective



Specifically, our model attempts to lay the loss . h - t onto the hyperplane, where h = h + r is the translated head entity. Mathematically, if a line lies on a hyperplane, so do all the points of this line. Correspondingly, the loss lays on the



There are two semantic effects for textual descriptions: discovering semantic relevance and offering precise semantic expression. Our model characterizes the strong correlations with a semantic hyperplane, which is capable of taking the advantages of both two semantic effects. Firstly, according to the correlation perspective, the entities which are semantically relevant, approximately lay on a consistent hyperplane. Therefore, the loss vector between them (h - t) is also around the hyperplane. Based on this geometric insight, when a head entity matches a negative tail, the triple is far from the hyperplane, making a large loss to be classified. Conversely, even if a correct triple makes much loss, the score function after projected onto the hyperplane could be relatively smaller (or better). By this mean, the semantic relevance achieved from the texts, promotes embedding. For instance, the fact triple (Portsmouth Football Club, Locate, Portsmouth) could hardly be inferred only within the triple embedding. It ranks 11,549 out of 14,951 by TransE in link prediction, which means a totally impossible fact. But the keywords "Portsmouth", "England", and "Football" occur many times in both the textual descriptions, making the two entities semantically relevant. Unsurprisingly, after the semantic projection, the case ranks 65 out of 14,951 in our model, which is regarded as nearly true.



Secondly, all the equal-length loss vectors in TransE are equivalent in term of discrimination, which leads to the weak distinction. However, with textual semantics, the distinguishing ability could be strengthened in our model. Specifically, the equal-length loss vectors are measured with the projection onto the corresponding semantic hyperplanes, which makes a reasonable division of the losses. For an instance of the query about which film "John Powell" contributes to, there are two candidate entities, that the true answer "Kung Fu Panda" and the negative one "Terminator Salvation". Without textual semantics, it's difficult to discriminate, thus the losses calculated by TransE are 8.1 and 8.0, respectively, leading to a bad classification. Diving into the textural semantics, we discover, "John Powell" is much relevant to the topic of "Animated Films", which matches that of "Kung Fu Panda" and does not for the other. Based on this fact, both the query and the true answer lie in the "Animated Films"directed hyperplane, whereas the query and the negative one do not co-occur in the corresponding associated semantic hyperplane. Thus, the projected loss of the true answer could be much less than that of the false one. Concretely, the losses in our model are 8.5 and 10.8, respectively, which are sufficient for discrimination. Notably, the scales of scores in TransE and our model are different as usual and only relative scores make sense. 3.5 Objectives & Training



max( * , 0) is the hinge loss. The false triples are sampled with "Bernoulli Sampling Method" as introduced in (Wang et al., 2014b) and the method selects the negative samples from the set of {(h , r, t)|h  E }  {(h, r, t )|t  E }  {(h, r , t)|r  R} We initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio, 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al., 2012). The stochastic gradient descent algorithm (SGD) is adopted in the optimization. For the topic-related objective, we take the advantage of the NMF Topic Model (Stevens et al., 2012), which is both simple and effective. Then we re-write the target as an inner-product form with the L2 -loss, stated as: Ltopic =

eE, wDe



(Ce,w - se w)2 s e  0, w  0



(2) (3)



There are two parts in the objective function, which are respectively embedding-specific and topic-specific. To balance both the two parts, a hyper-parameter  is introduced. Overall, the total loss is: L = Lembed + Ltopic (1)



Notably, there is only the first part in the Standard setting where  = 0 in fact. In term of the embedding-related objective, the rank-based hinge loss is applied, which means to maximize the discriminative margin between the golden triples and the negative ones:

embed



where E is the set of entities, and De is the set of words in the description of entity e. Ce,w is the times of the word w occurring in the description e. se is the semantic vector of entity e and w is the topic distribution of word w. Similarly, SGD is applied in the optimization. Theoretically, our computation complexity is comparable to TransE, as O( x O(T ransE )), and the small constance  is caused by the projection operation and topic calculation. In practice, TransE costs 0.28s for one round in Link Prediction and our model costs 0.36s in the same setting. Generally, TransE is most efficient among all the translation-based methods, while our method could be comparable to TransE in running time, justifying the efficiency of our model.



4

4.1



Experiments

Datasets & General Settings



L



=

(h, r, t)   (h , r , t )  



[fr (h , t ) - fr (h, t) +  ]+



where  is the set of golden triples and  is that of the negative ones.  is the margin, and [ * ] =



Our experiments are conducted on three public benchmark datasets that are the subsets of Wordnet and Freebase. About the statistics of these datasets, we strongly suggest the readers to refer to (Xie et al., 2016) and (Lin et al., 2015b). The entity descriptions of FB15K and FB20K are the same as DKRL (Xie et al., 2016), each of which is a small part of the corresponding wiki-page. The



textual information of WN18 is the definitions that we extract from the Wordnet. Notably, for the zero-shot learning, FB20K is involved, which is also built by the authors of DKRL. 4.2 Knowledge Graph Completion



This task is a benchmark task, a.k.a "Link Prediction", which concerns the performance to infer the missing element of the triple (entity/relation) when given the other two elements. Many NLP tasks could benefit from Link Prediction, such as relation extraction (Hoffmann et al., 2011). Evaluation Protocol. The same protocol used in previous studies, is adopted. First, for each testing triple (h, r, t), we replace the tail t (or the head h) with every entity e in the knowledge graph. Then, a probabilistic score of this corrupted triple is calculated with the score function fr (h, t). By ranking these scores in the ascending order, we then get the rank of the original triple. The evaluation metrics are the average of the ranks as Mean Rank and the proportion of testing triple whose rank is not larger than 10 (as HITS@10). This is called "Raw" setting. When we filter out the corrupted triples that exist in the training, validation, or test datasets, this is the"Filter" setting. If a corrupted triple exists in the knowledge graph, ranking it ahead the original triple is also correct. To eliminate this effect, the "Filter" setting is more preferred. In both settings, a higher HITS@10 and a lower Mean Rank mean better performance. Table 1: Evaluation results of Knowledge Graph Completion (Entity) on FB15K and WN18. FB15K Mean Rank HITS@10 TransE TransH Jointly DKRL(BOW) DKRL(ALL) SSP (Std.) SSP (Joint) WN18 TransE TransH SSP (Std.) SSP (Joint) 210 212 167 1 200 181 154 163 263 401 204 168 119 87 39 1 113 91 77 82 251 338 193 156 48.5 45.7 51.7 1 44.3 49.6 57.1 57.2 75.4 73.0 81.3 81.2 66.1 64.4 77.3 1 57.6 67.4 78.6 79.0 89.2 82.3 91.4 93.2



Implementation. As the datasets are the same, we directly report the experimental results of several baselines from the literature. We have attempted several settings on the validation dataset to get the best configuration. Under the "bern." sampling strategy, the optimal configurations of our model SSP are as follows. For WN18, embedding dimension d = 100, learning rate  = 0.001, margin  = 6.0, balance factor  = 0.2 and for SSP(Joint)  = 0.1. For FB15K, embedding dimension d = 100, learning rate  = 0.001, margin  = 1.8, balance factor  = 0.2 and for SSP(Joint)  = 0.1. We train the model until convergence. Table 2: Evaluation results of Knowledge Graph Completion (Relation) on FB15K. FB15K Mean Rank HITS@10 TransE TransH DKRL(BOW) DKRL(ALL) SSP (Std.) SSP (Joint) 2.91 8.25 2.85 2.41 1.58 1.87 2.53 7.91 2.51 2.03 1.22 1.47 69.5 60.3 65.3 69.8 69.9 70.0 90.2 72.5 82.7 90.8 89.2 90.9



Results Evaluation results are reported in Tab.1 and Tab.2. Note that "Jointly" refers to (Zhong et al., 2015). We observe that: 1. SSP outperforms all the baselines in all the tasks, yielding the effectiveness of our models and the correctness of our theoretical analysis. Specifically, SSP(Joint) improves much more than SSP(Std.) for jointly learning the textual semantics and symbolic triples. 2. DKRL and "Jointly" model only consider the first-order constraints, which interact the textual and symbolic information, unsatisfactorily. By focusing on the strong correlation, SSP outperforms them. Notice that the "Jointly" model involves much more extra corpus to produce the result, but SSP also has an extensive advantage against it. Though TransH is also a hyperplanebased method, SSP adopts the hyperplane in a semantics-specific way rather than a simple relation-specific form. Thus, SSP outperforms TransH extensively. 3. TransE could be treated as missing textual descriptions, and DKRL(BOW) could approx-



Mean Rank



HITS@10



1 This method involves much more extra text corpus, thus it's unfair to directly compare with others.



imately be conjectured as missing symbolic triples. SSP (Joint) improves 12.4% against TransE while 20.9% against DKRL(BOW), illustrating the triple embedding is always the key point and the interactions between both the information sources play a critical role. 4.3 Entity Classification This task is essentially a multi-label classification, focusing on predicting entity types, which is crucial and widely used in many NLP & IR tasks (Neelakantan and Chang, 2015). The entity in Freebase always has types, for instance, the entity types of "Scots" are Human Language, Rosetta Languoid. We adopt the same datasets as DKRL, for the details of which, we suggest the readers to refer to (Xie et al., 2016). Overall, this is a multi-label classification task with 50 classes, which means for each entity, the method should provide a set of types rather one specific type. Evaluation Protocol In the training, we use the concatenation of semantic vector and embedding vector (se , e) as entity representation, which is the feature for the front-end classifier. For a fair comparison, our front-end classifier is also the Logistic Regression as DKRL in a one-versus-rest setting for multi-label classification. The evaluation is following (Neelakantan and Chang, 2015), which applies the mean average precision (MAP) that is commonly used in multi-label classification. Specifically, for an entity, if the methods predict a rank list of types and there are three correct types that lie in #1, #2, #4, the MAP is calculated as 1/1+2/2+3/4 . For FB20K, the methods could only 3 make use of the descriptions. Obviously, FB20K is a zero-shot scenario which is really hard. Table 3: Evaluation results of Entity Classification Metrics FB15K FB20K TransE BOW DKRL(BOW) DKRL(ALL) NMF SSP (Std.) SSP (Joint) 87.8 86.3 89.3 90.1 86.1 93.2 94.4 57.5 52.0 61.9 59.6 67.4



of several baselines from the literature. We have attempted several settings on the validation dataset to get the best configuration. Under the "bern." sampling strategy, the optimal configurations of our model SSP are as follows. For FB15K,embedding dimension d = 100, learning rate  = 0.001, margin  = 1.8, balance factor  = 0.2 and for SSP(Joint)  = 0.1. For FB20K, embedding dimension d = 100, learning rate  = 0.001, margin  = 1.8, balance factor  = 0.2 and for SSP(Joint)  = 0.1. We train the model until convergence. Results The evaluation results are listed in Tab.3. Notably, TransE are only triple-specific and SSP (Std.) performs as well as the NMF in the zero-shot learning. thus they are out of the ability for FB20K. The observations are as following: 1. Overall, our method SSP yields the best accuracy, justifying the effectiveness of our method and the correctness of our theories. 2. Compared to NMF and TransE, the results prove the critical declaration that the interactions between triples and texts make more sense and neither of them could hardly produce an effective method. The promotion of SSP (Joint) in FB20K demonstrates the triple embedding also has a positive effect on textual semantics, and on the other side, illustrates our effectiveness in the zero-shot learning. 3. Compared to TransE, the improvements illustrate the effectiveness of semantic integration. Compared to DKRL, the promotion demonstrates the important role of strong correlation. 4.4 Semantic Relevance Analysis



Implementation. As the datasets are the same, we directly report the experimental results



One of the actual effects of the semantic relevance is to recognize the true triples that could not be inferred by just focusing on the symbolic triples. Hence, we make a statistic analysis of the results in Link Prediction, reported in the Tab.4. The cell in the table is under the condition of the column and the row. For example, the cell in the second column and the second row means there are 601 triples, which are ranked inside the top 100 in SSP, yet outside the top 500 in TransE. Note that SSP(S.) indicates the standard setting, SSP(J.) means the joint setting and E is short for TransE. The statistic matrix addresses there



are many triples could benefit from the semantic relevance offered by textual descriptions. The experiments also justify the theoretical analysis about the semantic relevance and demonstrate the effectiveness of our models. We also study some specific cases. The triple (Luis Miguel Gallego Basteri, Music Artist Genre, Ballad) even transforms from the rank 1,602 in TransE to the rank 8 in SSP. Diving into the textual details, we discover the two entities share the relevant topics of "Cross-Country" and "Music". For another triple that (Laura Elizabeth Dern, Film Performance, Alice Doesn't Live Here Anymore), it ranks from 1,137 in TransE to 77 in SSP, because both the entities share the relevant semantics as "Best Actress" and "Academy Award". Table 4: Statistics of the results of Link Prediction SSP(S.)#100 SSP(J.)#100 E#500 E#1000 E#2000 E#3000 E#5000 4.5 601 275 80 32 3 672 298 89 39 3



Figure 3: Precise semantic expression analysis for SSP (Std.). The x-axis indicates the score difference, where a bigger value means better. The y-axis means the proportion of the corresponding triple pairs. The gray part indicates that both the TransE and SSP make mistakes while the colored part means that SSP makes it correct but TransE fails. Notably, it is similar for Joint setting. likely to be semantically similar. To conduct the case study, we obtain the top three entities closest to a given one. For the entity Peoria that is a city, SSP (Std.) comes up with Jolie, Newark and Springfield, all of which are cities, while TransE comes up with Blue Ridge Mountain, U.S.A and Grammy Award, all of which are city-irrelevant. For another entity of Indian National Football Team, SSP (Std.) comes up with Atalanta B.C., C.C& Football Club and A.E.K. Athens F.C., all of which are football clubs, but TransE comes up with Brendan Coyle, Dayton and Akiva D. Kiv, which are respectively an actor, a city and a writer. Comparatively, the embedding topologies in our model are obviously semantics-specific, which justifies that our model fuses the symbolic triples with the textual semantics, effectively.



Precise Semantic Expression Analysis



As discussed previously, precise semantic expression leads to better discrimination. To justify this argument, we have collected the negative triples by link prediction, which are scored sightly better than the golden ones by TransE, and then plotted the SSP score difference between each corresponding pair of the negative and golden triples as Fig.3 shows. All these triples are mistaken by TransE, but with precise semantic expression, our model correctly distinguishes 82.0% (Std.) and 83.2% (Joint) of them. In the histogram, the right part indicates that SSP makes it correct and the left means both the SSP and TransE mistakes. Bigger x-axis direction means involving stronger semantics. The experiments prove the theoretical analysis about the precise semantic expression and demonstrate the effectiveness of our models. 4.6 Embedding Topologies Analysis



5



Conclusion



As the "Correlation Perspective" suggests, our embedding topologies are semantics-specific. Thus the adjacent entities, which are more probably to co-occur in the same subspace, are



In this paper, we propose the knowledge graph embedding model SSP, which jointly learns from the symbolic triples and textual descriptions. SSP could interact the triples and texts by characterizing the strong correlations, by which means, the textual descriptions could make more effects to discover semantic relevance and offer precise semantic expression. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines.



References

[Blei2012] David M Blei. 2012. Probabilistic topic models. Communications of the ACM, 55(4):77-84. [Bollacker et al.2008] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247- 1250. ACM. [Bordes et al.2011] Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of knowledge bases. In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence. [Bordes et al.2013] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pages 2787-2795. [Fan et al.2014] Miao Fan, Qiang Zhou, Emily Chang, and Thomas Fang Zheng. 2014. Transitionbased knowledge graph embedding with relational mapping properties. In Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, pages 328-337. [Glorot and Bengio2010] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics, pages 249-256. [Guo et al.2015] Shu Guo, Quan Wang, Bin Wang, Lihong Wang, and Li Guo. 2015. Semantically smooth knowledge graph embedding. In Proceedings of ACL. [He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015. Learning to represent knowledge graphs with gaussian embedding. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 623-632. ACM. [Hoffmann et al.2011] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541-550. Association for Computational Linguistics. [Jenatton et al.2012] Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, and Guillaume R Obozinski. 2012. A latent factor model for highly multirelational data. In Advances in Neural Information Processing Systems, pages 3167-3175.



[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge graph embedding via dynamic mapping matrix. [Lin et al.2015a] Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015a. Modeling relation paths for representation learning of knowledge bases. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. [Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015b. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. [Mikolov et al.2013a] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013a. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119. [Mikolov et al.2013b] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In HLTNAACL, pages 746-751. [Miller1995] George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41. [Neelakantan and Chang2015] Arvind Neelakantan and Ming-Wei Chang. 2015. Inferring missing entity type instances for knowledge base completion: New dataset and methods. arXiv preprint arXiv:1504.06658. [Nickel et al.2011] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 809-816. [Socher et al.2013] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926-934. [Stevens et al.2012] Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 952-961. Association for Computational Linguistics. [Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014a. Knowledge graph and text jointly embedding. In EMNLP, pages 1591-1601. Citeseer.



[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014b. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1112-1119. [Wang et al.2015] Quan Wang, Bin Wang, and Li Guo. 2015. Knowledge base completion using embeddings and rules. In Proceedings of the 24th International Joint Conference on Artificial Intelligence. [Xiao et al.2015] Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. 2015. TransA: An adaptive approach for knowledge graph embedding. arXiv preprint arXiv:1509.05490. [Xie et al.2016] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge graphs with entity descriptions. [Zhong et al.2015] Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. 2015. Aligning knowledge and text embeddings by entity descriptions. In Proceedings of EMNLP, pages 267- 272.



Online Bandit Learning for a Special Class of Non-convex Losses

National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China 2 Department of Computer Science, the University of Iowa, Iowa City, IA 52242, USA 3 Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA {zhanglj, zhouzh}@lamda.nju.edu.cn, tianbao-yang@uiowa.edu, rongjin@cse.msu.edu

1



Lijun Zhang1 and Tianbao Yang2 and Rong Jin3 and Zhi-Hua Zhou1



Abstract

In online bandit learning, the learner aims to minimize a sequence of losses, while only observing the value of each loss at a single point. Although various algorithms and theories have been developed for online bandit learning, most of them are limited to convex losses. In this paper, we investigate the problem of online bandit learning with non-convex losses, and develop an efficient algorithm with formal theoretical guarantees. To be specific, we consider a class of losses which is a composition of a non-increasing scalar function and a linear function. This setting models a wide range of supervised learning applications such as online classification with a non-convex loss. Theoretical analysis shows e (poly (d)T 2/3 ) regret that our algorithm achieves an O bound when the variation of the loss function is small. To the best of our knowledge, this is the first work in online bandit learning that does not rely on convexity.



Introduction

Online decision-making has become a popular learning paradigm in many disciplines such as Artificial Intelligence, Economics and Control Theory (Saha and Tewari 2011). At each round of online learning, the learner chooses a decision from a given set, and an adversary responds with a loss function that decides the cost of decisions. The performance of an online learning algorithm is measured by the regret, which is the difference between the total cost of the decisions it chooses, and the cost of the optimal decision chosen in hindsight. According to the amount of information revealed to the learner, online learning can be classified into two categories (Cesa-Bianchi and Lugosi 2006): i) full information setting where the learner observes the entire cost function, and ii) bandit setting where only the cost of the selected action is available. In the past decades, there have been tremendous progresses made in online bandit learning, ranging from multiarmed bandit (Robbins 1952; Auer et al. 2003), online linear optimization with bandit feedback (Awerbuch and Kleinberg 2004; Dani, Kakade, and Hayes 2008; Abernethy, Hazan, and Rakhlin 2008), to online convex optimization with bandit feedback (Flaxman, Kalai, and McMahan 2005;

Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



Saha and Tewari 2011; Agarwal, Dekel, and Xiao 2010). A major limitation of the previous work is that most of them are restricted to convex losses. The drawback of using convex losses has been revealed by several recent studies. In (Calauz enes, Usunier, and Gallinari 2012), the authors show it is impossible to find any convex loss that is calibrated with the standard evaluation metrics for ranking. Similarly, for multi-label learning, no convex loss is consistent with the popular ranking loss (Gao and Zhou 2011). The success of deep learning also indicates the significance of using non-convex losses (Hinton, Osindero, and Teh 2006). The resurge of non-convex losses in machine learning motives us to investigate online bandit learning with nonconvex loss functions. In particular, we consider the following learning protocol. * At the t-th round, the learner submits a point xt 2 Rd with kxt k2  1, and simultaneously an oblivious adversary selects a vector ut and a non-increasing scalar function ft : R 7! R that assesses the consistency between the ground truth ut and the answer xt by ft (x> t ut ). * Instead of revealing the loss function ft (hut , *i) directly, the adversary only provides the learner ct 2 R whose expectation is the cost ft (u> t xt ), i.e., Et 1 [ c t ] = ft ( u > (1) t xt ), where Et 1 [*] is the expectation conditioned on the randomness until round t 1. Notice that the above protocol generalizes many machine learning tasks. Taking the online classification as an example, we can set ut = yt zt , where zt 2 Rd is an instance, yt 2 {1} is the assigned class label, and ft (*) can be any loss function such as the ramp loss (Ertekin, Bottou, and Giles 2011). We emphasize that in our setting both ut and ft (*) are unknown to the learner. More importantly, unlike most online learning that assume the loss function to be convex, in this study, ft (*) can be non-convex. This relaxation makes our problem significantly more challenging than most online learning problems, including the traditional online bandit learning. Following the convention in online learning, our goal is to generate a sequence of answer vectors x1 , . . . , xT , that leads to a small regret defined below T T X X regret = ft ( u > x ) min ft ( u > t t t x).

t=1 kx k2  1 t=1



We present a simple algorithm for online bandit learning with non-convex losses. It is computationally efficient and achieves a non-trivial regret bound under appropriate conditions. Our approach follows the standard explorationexploitation framework for bandit learning (Awerbuch and Kleinberg 2004; McMahan and Blum 2004). In an exploration round, the algorithm submits a random vector in order to obtain an unbiased estimate of ut , and updates the current solution based on the bandit feedback. In an exploitation round, it submits the current solution in order to incur a small loss. Under the assumption that ft (*) is non-increasing and Lipschitz continuous, we are able to bound the regret by the number of iterations and the variation of the target vectors {ut }T specific, the regret bound takes the form t=1 . To be p e (poly (d)T 2/3 + T VT ),1 where VT is the variation of vecO tors u1 , . . . , uT . Thus, the proposed algorithm achieves an e ((poly (d)T 2/3 ) regret bound if VT  O(T 1/3 ). O



and Proutiere 2014), the regret could be further improved. The online linear optimization problem with bandit feedback was first introduced by Awerbuch and Kleinberg (2004) who obtained a O(d3/5 T 2/3 ) regret bound against an oblivious adversary. Later, McMahan and Blum (2004) achieved an O(poly (d)T 3/4 ) regret bound against an adaptive adversary. In (Dani, Kakade, and Hayes 2008; Abernethy, Hazan, and Rakhlin 2008), the regret bound was improved p to O(poly (d) T ), where the dependence on T is optimal (Bubeck, Cesa-Bianchi, and Kakade 2012).



Online Non-convex Optimization

Several heuristic approaches have been developed for online learning with non-convex loss in the full information setting, such as the online version of the concave-convex procedure (Ertekin, Bottou, and Giles 2011; Gasso et al. 2011). However, none of them are equipped with a formal regret bound. One exception is the online submodular p minimization (Hazan and Kale 2012) that achieves O( dT ) and O(dT 2/3 ) regret bounds in the full information and bandit settings, respectively. But these algorithms rely on the specific property of submodular function (i.e., the Lov asz extension is convex), and thus cannot be applied to the problem considered here.



Related Work



In this section, we briefly review the related work in online convex and non-convex optimizations.



Online Convex Optimization

In the full information setting, online convex optimization has been extensively studied (Kivinen, Smola, and Williamson 2002; Zhang et al. 2013). Zinkevich (2003) shows that a p simple online gradient descent algorithm achieves an O( T ) regret bound for convex and Lipschitz continuous functions. When the loss function is strongly convex, the regret bound can be improved to O(log T ) (Hazp an, Agarwal, and Kale 2007). Both the O( T ) and O(log T ) regrets bounds, for convex and strongly convex loss functions respectively, are known to be minimax optimal (Abernethy et al. 2009). Compared to the full information setting, the regret bound for the bandit setting is usually worse and has an explicit dependence on the dimensionality d. The current best-known e ( d 2 / 3 T 2 / 3 ) , O ( d 2 / 3 T 2 /3 ) , regret bounds are O(dT 3/4 ), O p and O(d T ) for convex, convex-and-smooth, strongly convex, and strongly-convex-and-smooth functions, respectively (Flaxman, Kalai, and McMahan 2005; Saha and Tewari 2011; Agarwal, Dekel, and Xiao 2010). Notice that when the learner is allowed to query the loss function at multiple points, the regret can be improved to match its counterpart in the full information setting (Agarwal, Dekel, and Xiao 2010). In the bandit setting, there are two special cases that are well-studied: multiarmed bandit and online linear optimization with bandit feedback. In the first problem, we assume there are K arms, and a gambler pulls one of them to receive a reward in each round. Auer et al. (2003) p prove that e the gambler's regret can be bounded by O( KT ), which is optimal up to logarithmic factors (Audibert and Bubeck 2009). Furthermore, if the reward function has some structural properties, such as Lipschitz (Magureanu, Combes,

1 e notation to hide constant factors as well as polyWe use the O logarithmic factors in d and T .



An Efficient Algorithm for Online Bandit Learning

We first describe the proposed algorithm for online bandit learning with non-convex losses, and then state its theoretical guarantees.



The Algorithm

Algorithm 1 summarizes the key steps of the proposed algorithm. We maintain two sequences of vectors during the learning process: the answer vectors xt and the auxiliary vector wt . We initialize the answer vector x1 to be a random normalized vector, and the auxiliary vector w1 to be 0. At each iteration t, we generate a Bernoulli random variable Zt with Pr(Zt = 1) =  to determine whether to explore or exploit. When Zt = 0, we will simply submit the answer vector xt as the solution, and make no update. When Zt = 1, we will first compute a normalized Gaussian random vector vt /kvt k2 and submit it as the answer. Based on the received feedback ct , we update the auxiliary vector and the answer vector by wt+1 = wt ct wt+1 vt and xt+1 = . kvt k2 kwt+1 k2



We note that for the sake of simplicity, Algorithm 1 follows the early studies of online bandit learning that separate the exploration steps from the exploitation steps (Awerbuch and Kleinberg 2004; McMahan and Blum 2004). This is different from the more recent strategy for explorationexploitation (Flaxman, Kalai, and McMahan 2005; Abernethy, Hazan, and Rakhlin 2008) that usually combines exploration and exploitation into a single step by adding random perturbation to the submitted solutions. We will exam-



Algorithm 1 An Efficient Algorithm for Online Bandit Learning Input: step size  and number of trials T 1: Set  = T 1/3 2: Initialize x1 as any random normalized vector and w1 = 0 3: for t = 1, 2, . . . , T do 4: Sample binary random variable Zt with Pr(Zt = 1) =  . 5: Sample a random vector vt from an Gaussian distribution N (0, Id ) vt 6: Submit the solution x0 Zt ) x t t = Zt kvt k2 + (1 7: Receive ct from the adversary t ct 8: Update the auxiliary vector wt+1 = wt kZ v t k2 v t 9: Update the answer vector xt+1 = wt+1 /kwt+1 k2 10: end for ine in the future the second strategy for online bandit learning with non-convex losses. Finally, we would like to point out that following the idea of discretizing the decision space (Dani, Kakade, and Hayes 2008), our problem can be reduced to the multiarmed bandit and solved by existing methods (Auer et al. 2003). However, this strategy is inefficient because the number of arms is exponential in the dimensionality d, and the regret bound may also have a high dependence on d. In contrast, our algorithm is very efficient and the regret bound only has a polynomial dependence on d.



where (*) is the Dirac delta function. When d 2, we have (Cho 2009) ( d 3 (d 2) p (1 z 2 ) 2 , for 1 < z < 1 h( z ) = (3) ( d2 1 )  0, otherwise where (*) is the Gamma function. The following proposition, which is inspired by the recent developments in one-bit compressive sensing (Plan and Vershynin 2013; Zhang, Yi, and Jin 2014), provides the key observation for our analysis. Proposition 1. We have  Zt ct Et 1 vt =  u, t = 1, . . . , T kvt k2 where = Z

1



(4)



f (z )h(z )z d z.

1



(5)



From our assumption that f (*) is non-increasing, it is easy to verify that 0. We note that will have a strong dependence on d. Generally speaking, we have = poly (d 1 ). For instance, when f (z ) = z , we have = E[z 2 ] = d 1 (Cho 2009). Proposition 1 shows that in the exploration step, our algorithm is able to find an unbiased estimate of u, up to a scaling factor. Based on this observation, we obtain the following regret bound. Theorem 1. Assume 8 !3 9 r < = T (d + 1) T max e, log log T . : ; regret    1 4B log + 1 + 4B 



The Main Results

Besides the basic assumption in (1), we further make the following assumptions in our analysis. * ft (*) is non-increasing and L-Lipschitz continuous. * Both ct and ft (*) are upper bounded by a constant B . That is,

t2 [T ]



(6)



Set  = T 1/3 in Algorithm 1. Then, with a probability 1  , we have 3L r T (d + 1) !



sup |ct |  B, and



sup

t 2 [ T ] ,kx k2  1



|ft ( u > t x)|  B.



(2)



log



+ 1 T 3.



2



* The target vectors ut 's are of unit length, i.e., kut k2 = 1, t = 1, . . . T . * The adversary is oblivious, meaning that both ut 's and ft 's are fixed. As a starting point, we first analyze the regret bound for the simplest case when all the target vectors are the same, and then move to the general case. Regret Bound for a Single Target Vector We first consider the simplest case when u1 = u2 = * * * = uT = u and f1 = f2 = * * * = fT = f . Define h(z ) as the probability density function (PDF) of the inner product of random unit vectors in Rd . When d = 1, it is easy to verify h( z ) = 1 ( (z 2 1) + (z + 1)),



To simplify the presentation, we assume the horizon T is known so that we can choose  = T 1/3 in the algorithm. This limitation could be addressed by the well-known "doubling trick" (Cesa-Bianchi and Lugosi 2006). Theorem 1 ime ( 1 T 2/3 ) regret bound, plies our algorithm achieves an O which is even better than the regret bound in the general online convex optimization with bandit feedback (Flaxman, Kalai, and McMahan 2005). From the discussion in (Dani and Hayes 2006; Abernethy, Hazan, and Rakhlin 2008), we also know that (T 2/3 ) regret bound is unavoidable if any algorithm ignores the feedback received during exploitation. We finally note that although the regret bound in Theorem 1 does not have an explicit dependence on d, its dependence on d comes from . We now extend the simple case to a slightly more complicated scenario where a different loss function is used. In this case, we have the following proposition.



Proposition 2. We have  Zt ct Et 1 vt = kvt k2 where

t



t  u,



t = 1, . . . , T



=



Following almost the same analysis for Theorem 1, we obtain the following theorem to bound the regret. Theorem 2. Suppose T and  satisfy the conditions in Theorem 1. With a probability 1  , we have regret    1 4B log + 1 + 4B  where  = min

t 2 [T ] t.



Z



1



ft (z )h(z )z d z.

1



3L 



r



log



T (d + 1)



+1 T3



!



2



When VT  O(T 1/3 ), the additional p the total variation p term T (VT + 12 VT ) is on the order of T 2/3 . Furthermore, if we assume a small variation for each iteration, that  t k2 will be lower is, Vt  O(t1/3 ) for all t 2 [T ], each ku bounded by some constant,2 and thus 1/T is upper bounded e ( 1 T 2/3 ) by some constant. As a result, we still have an O regret bound. On the other hand, the regret bound becomes trivial when VT = (T ), which is significantly worse than the previous results on variation based regret bound (Hazan and Kale 2010). This is because we are dealing with nonconvex optimizations and therefore the gradient does not provide an universal lower bound for the entire function. Thus, we cannot utilize the analysis for convex functions, but only rely on the assumption that ft (*) is non-increasing and Lipschitz continuous. Finally, it is straightforward to extend the above result to the case when a different loss function is used, by introducing  defined in Theorem 2.



The only difference between Theorems 2 and 1 is that in Theorem 1 is replaced with  , the smallest one among { t }T t=1 . Regret Bound for the General Case We now consider the t more general case where each ut is a different vector. Let u be the average of vectors u1 , . . . , ut , i.e., t = u 1 t

t X i=1



Analysis

We here present the proofs of main theorems. The omitted proofs are provided in the supplementary material.



Proof of Theorem 1

Under the assumption u1 = u2 = * * * = uT = u, we have regret

T X t=1



ut . =



Similar to Theorem 1, we have the following regret bound for the general case when a single loss function is used. Theorem 3. Suppose T and  satisfy the conditions in Theorem 1. Then, with a probability 1  , we have   T X 1  t 1 k2 regret 4B log + 1 + 2L kut u  t=2 ! r 2 3L T (d + 1) + 4B log + 1 T 3, T where  t k2 . is defined in (5) and T = min ku

t 2 [T ]



f (u> x0 t)



T min f (u> x)

kxk1



(2)



  >   T X u vt > = Zt f f (u xt ) kvt k2 t=1  T  X + f (u> xt ) min f (u> x)

t=1 T X t=1



(8)



2B



Zt +

1



To bound the second term in the regret bound, we need the following inequality (Hazan and Kale 2010) v u T T T uX X X 2 t  t 1 k2   T k2 kut u  k u u k +12 ku t u t T 2 2 2.

t=2 t=1 t=1



Then, we discuss how to bound 1 and 2 . According to the Multiplicative Chernoff Bound (Angluin and Valiant 1979) provided in the supplementary, we have with a probability at least 1 

1



| {z }



T  X



kxk1



f (u> xt )



|



t=1



{z



kxk1



 min f (u> x) . }



2



From (7) and Theorem 3, we have   q p 1 regret 4B log + 1 + 2L T (VT + 12 VT )  ! r 2 3L T (d + 1) + 4B log +1 T3 T where VT =

T X t=1



(7)



 2E[



1]



+ 2 log



1 1 = 2 T + 2 log .  



(9)



To bound 2 , we introduce the vector-valued martingaledifference sequence

i



=



The following lemma follows immediately from the Freedman's inequality for matrix martingales (Tropp 2011).

2  t k2  c, then We can prove it by contradiction. Suppose ku we must have Vt (1 c)2 t = (t).



Zi ci vi kvi k2



 u, i = 1, . . . , T.



(10)



kut



 T k2 u 2.



Lemma 1. With a probability 1

t X i=1 i 2



, we have, for any t 2 [T ],  t ( ) r



Proof of Lemma 1

We first state the Freedman's inequality for matrix martingales below. Theorem 4. (Tropp 2011, Corollary 1.3) Let k * k be the spectral norm of a matrix, which returns its largest singular value. Consider a matrix martingale {Yi : i = 0, 1, 2, . . . } whose values are matrices with dimension d1  d2 . Let {Xi : i = 1, 2, 3, . . .} be the difference sequence, and assume that the difference sequence is uniformly bounded: kXi k  R almost surely i = 1, 2, 3, . . . . Define two predictable quadratic variation processes for this martingale: Wcol,t := Wrow,t := Then, for all Pr kYt k

t X i=1 t X i=1



where t ( ) =



4B T (d + 1) T (d + 1) log + B 2 t log . 3 From the assumption that f (*) is non-increasing and LLipschitz continuous, we have the following lemma. Lemma 2. We have f (x> min f (x> u) t u) kxk1 ( 2B, if t = 1; Pt 1  2L , otherwise. i=1 i  (t 1)

2



Based on Lemmas 1 and 2, we have with a probability at least 1 , T 2 L X t 1 ( )  2 B + 2  t=2 t 1 =2B +

T 1 8LB T (d + 1) X 1 log 3  t t=1 s T 1 2LB 2 T (d + 1) X 1 p + log  t t=1



Ei Ei



> 1 [ Xi Xi ] ,



> 1 [ Xi Xi ] ,



t = 1, 2, 3, . . . .



0 and



2



> 0,



(11)



(6)



 2B +



6LB T (d + 1) log log T  s 4LB 2T T (d + 1) + log , 



where we use the following inequalities Z T T X 1 1 1 + dt = 1 + log t|T 1 = log T + 1, and t t t =1 t=1 Z T T X p p 1 1 p 1+ p dt = 1 + 2 t|T 1. 1 =2 T t t t=1 t=1



and max{kWcol,t k, kWrow,t k}  2   2 /2  (d1 + d2 ) exp . 2 + R /3   2 /2 By setting = exp 2 +R /3 , Theorem 4 implies that with a probability at most , r 2R d1 + d2 d1 + d2 kY t k log + 2 2 log and 3 max{kWcol,t k, kWrow,t k}  2 . We then introduce several facts that will be used in our analysis. Let  be a random vector. Based on Jensen's inequality, we have kE[ ]k2  E[k k]2 . (12)



Combining (8), (9) and (11), we have with a probability at least 1  , regret   1 6LB T (d + 1) 4B  T + log + 1 + log log T   s 4LB 2T T (d + 1) + log    2 1 6LB 1 T (d + 1) 3 =4B T + log + 1 + T 3 log log T  r 6LB T (d + 1) 2 + log T3 r   (6) 2 1 12LB T (d + 1) 2 3 4B T + log + 1 + log T 3. 



From the property of positive semidefinite (PSD) matrices, we have   E ( E [ ])( E [ ])> = E[ > ] E[ ]E[ ]> => E[ > ] E[ ]E[ ]>  (13) > E[ > ]  kE[ > ]k, where  is the largest eigenvector of the PSD matrix E[ > ] E[ ]E[ ]> . Furthermore, it is easy to verify that   E ( E [ ])> ( E [ ]) (14) =E[ >  ] E[ ]> E[ ]  E[ >  ]. Notice that the spectral norm of a vector is its 2 -norm.



We bound the 2 -norm of the k i k2 = Zi c i  vi kvi k2 (4), (12) Zi ci  vi kvi k2

(2)



i



as follows u

2



Zi ci vi kvi k2

2



+ k  uk2  Zi ci + Ei 1 vi kvi k2 2



According to the procedure in Algorithm 1, we have t 1 t 1 X X Zi ci (10) wt = vi =  (t 1)u + i. kv i k2 i=1 i=1 Then, we have kwt

2



 (t



1)uk2  u

2



We then bound the two predictable quadratic variation processes as follows

t X



2B, i = 1, . . . , T.



,



wt  (t 1)







Ei Ei Ei



> 1[ i i ]



= 



Zi ci v i + u kv i k2 i=1  t (4), (13) X Zi c2 i >  Ei 1 2 vi vi k v k i 2 i=1

1 t X



t X



i=1 t X i=1



1



" "



Zi ci v i + u kv i k2



 



Zi ci v i + u kvi k2 Zi ci v i + u kvi k2



> # > #



Following a simple geometric argument, we have t 1 X wt 2 u  i kw t k  (t 1) i=1 2

2 (15)



t 1 X 1  (t 1) i=1



t 1 X i=1



i 2 i 2



.



) f (x> t u)



kxk1



min f (x> u) 



Proof of Theorem 2



t 1 X 2L  (t 1) i=1



i 2



.



(2)



  B 2 t,



Ei Ei

t X i=1



> 1[ i i]



=



(4), (14)



i=1 t X i=1



1



"

1



Zi ci vi +  u kvi k2 

(2)



> 



Zi ci vi +  u kvi k2



#







Ei



2 Zi c2 i   B t.



In this case, we define the vector-valued martingaledifference sequence as Zi ci vi i = i  u, i = 1, . . . , T. kvi k2 It is easy to verify that Lemma 1 still holds and Lemma 2 become the following one. Lemma 3. We have ft ( x > min ft (x> u)  t u) kxk1 ( 2B, if t = 1; Pt 1 2L , otherwise. i=1 i   (t 1)

2



The rest proof is the same as that for Theorem 1.



Then, based on Theorem 4, for each t 2 [T ], we have with a probability at least 1 r t X 4B d+1 d+1  log + B 2 t log . i 3 i=1

2



We complete the proof by taking the union bound over t = 1, . . . , T .



Proof of Lemma 2

When t = 1, it is clear that f (x> 1 u)

kxk1



min f (x> u)  2B.



(2)



In the following, we discuss the case when t 2. From our assumption that f (*) is a non-increasing function, we have f (x> t u)

kxk1



In this paper, we study the problem of online bandit learning with non-convex losses, and assume the loss function is a composition of a non-increasing scalar function and a linear function. Following the idea of exploration and exploitation, we develop an efficient algorithm which achieves e (poly (d)T 2/3 ) regret bound under appropriate conditions. O One limitation of the current work is that the regret bound only holds against an oblivious adversary. In the future, we will investigate how to extend our results to the adaptive adversary. There are also many open problems for bandit learning with non-convex losses, such as under what condition there exists a Hannan-consistent algorithm and what is the lower bound. We will leave these questions for future investigations. This research was supported by NSFC (61333014, 61321491), the Collaborative Innovation Center of Novel Software Technology and Industrialization, NSF (IIS1251031), ARO (W911NF-11-1-0383), and Baidu Fund (181315PO5760).



Conclusion and Future Work



min f (x> u) = f (x> t u)



f (u> u).



Since f (*) is L-Lipschitz continuous, we further have f (x> t u) f ( u > u )  L| x > t u uk2 = L  Lkxt wt kwt k u> u| u

2



Acknowledgments



.



(15)



Abernethy, J.; Agarwal, A.; Bartlett, P. L.; and Rakhlin, A. 2009. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory. Abernethy, J.; Hazan, E.; and Rakhlin, A. 2008. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning, 263-274. Agarwal, A.; Dekel, O.; and Xiao, L. 2010. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning, 28-40. Angluin, D., and Valiant, L. 1979. Fast probabilistic algorithms for hamiltonian circuits and matchings. Journal of Computer and System Sciences 18(2):155-193. Audibert, J.-Y., and Bubeck, S. 2009. Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory. Auer, P.; Cesa-Bianchi, N.; Freund, Y.; and Schapire, R. E. 2003. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing 32(1):48-77. Awerbuch, B., and Kleinberg, R. D. 2004. Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In Proceedings of the 36th Annual ACM Symposium on Theory of Computing, 45-53. Bubeck, S.; Cesa-Bianchi, N.; and Kakade, S. M. 2012. Towards minimax policies for online linear optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory. Calauz enes, C.; Usunier, N.; and Gallinari, P. 2012. On the (non-)existence of convex, calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems 25, 197-205. Cesa-Bianchi, N., and Lugosi, G. 2006. Prediction, Learning, and Games. Cambridge University Press. Cho, E. 2009. Inner product of random vectors. International Journal of Pure and Applied Mathematics 56(2):217-221. Dani, V., and Hayes, T. P. 2006. Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary. In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithm, 937-943. Dani, V.; Kakade, S. M.; and Hayes, T. P. 2008. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems 20, 345-352. Ertekin, S.; Bottou, L.; and Giles, C. L. 2011. Nonconvex online support vector machines. IEEE Transactions on Pattern Analysis and Machine Intelligence 33(2):368-381. Flaxman, A. D.; Kalai, A. T.; and McMahan, H. B. 2005. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms, 385- 394. Gao, W., and Zhou, Z.-H. 2011. On the consistency of multilabel learning. In Proceedings of the 24th Annual Conference on Learning Theory, 341-358.



References



Gasso, G.; Pappaioannou, A.; Spivak, M.; and Bottou, L. 2011. Batch and online learning algorithms for nonconvex neyman-pearson classification. ACM Transactions on Intelligent Systems and Technology 2(3):28:1-28:19. Hazan, E.; Agarwal, A.; and Kale, S. 2007. Logarithmic regret algorithms for online convex optimization. Machine Learning 69(2-3):169-192. Hazan, E., and Kale, S. 2010. Extracting certainty from uncertainty: regret bounded by variation in costs. Machine Learning 80(2-3):165-188. Hazan, E., and Kale, S. 2012. Online submodular minimization. In Journal of Machine Learning Research, volume 13, 2903-2922. Hinton, G. E.; Osindero, S.; and Teh, Y.-W. 2006. A fast learning algorithm for deep belief nets. Neural Computation 18(7):1527-1554. Kivinen, J.; Smola, A. J.; and Williamson, R. C. 2002. Online learning with kernels. In Advances in Neural Information Processing Systems 14, 785-792. Magureanu, S.; Combes, R.; and Proutiere, A. 2014. Lipschitz bandits: Regret lower bounds and optimal algorithms. In Proceedings of the 27th Conference on Learning Theory, 975-999. McMahan, H. B., and Blum, A. 2004. Online geometric optimization in the bandit setting against an adaptive adversary. In Proceedings of the 17th Annual Conference on Learning Theory, 109-123. Plan, Y., and Vershynin, R. 2013. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory 59(1):482-494. Robbins, H. 1952. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society 58(5):527-535. Saha, A., and Tewari, A. 2011. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 636-642. Tropp, J. A. 2011. Freedman's inequality for matrix martingales. Electronic Communications in Probability 16:262- 270. Zhang, L.; Yi, J.; Jin, R.; Lin, M.; and He, X. 2013. Online kernel learning with a near optimal sparsity bound. In Proceedings of the 30th International Conference on Machine Learning (ICML). Zhang, L.; Yi, J.; and Jin, R. 2014. Efficient algorithms for robust one-bit compressive sensing. In Proceedings of the 31st International Conference on Machine Learning (ICML). Zinkevich, M. 2003. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, 928- 936.



Stochastic Optimization for Kernel PCA

Lijun Zhang1,2 and Tianbao Yang3 and Jinfeng Yi4 and Rong Jin5 and Zhi-Hua Zhou1,2

National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, China 3 Department of Computer Science, the University of Iowa, Iowa City, USA 4 IBM Thomas J. Watson Research Center, Yorktown Heights, USA 5 Alibaba Group, Seattle, USA {zhanglj, zhouzh}@lamda.nju.edu.cn, tianbao-yang@uiowa.edu, jinfengy@us.ibm.com, jinrong.jr@alibaba-inc.com

2 1



Abstract

Kernel Principal Component Analysis (PCA) is a popular extension of PCA which is able to find nonlinear patterns from data. However, the application of kernel PCA to largescale problems remains a big challenge, due to its quadratic space complexity and cubic time complexity in the number of examples. To address this limitation, we utilize techniques from stochastic optimization to solve kernel PCA with linear space and time complexities per iteration. Specifically, we formulate it as a stochastic composite optimization problem, where a nuclear norm regularizer is introduced to promote low-rankness, and then develop a simple algorithm based on stochastic proximal gradient descent. During the optimization process, the proposed algorithm always maintains a low-rank factorization of iterates that can be conveniently held in memory. Compared to previous iterative approaches, a remarkable property of our algorithm is that it is equipped with an explicit rate of convergence. Theoretical analysis shows that the solution of our algorithm converges to the optimal one at an O(1/T ) rate, where T is the number of iterations.



the low-rank structure, the approximator can be easily stored and manipulated. The major limitation of approximate approaches is that there always exists a non-vanishing gap between their solution and that found by eigendecomposing K directly. Iterative approaches (Kim, Franz, and Sch olkopf 2005) use partial information of K in each round to estimate the top eigenvectors, and thus do not need to keep the entire matrix in memory. With appropriate initialization, the solution of iterative approaches will converge to the groundtruth asymptotically. However, there is no guarantee of the convergence rate or the global convergence property for general initial conditions. Inspired by the recent progresses in stochastic optimization (Avron et al. 2012; Rakhlin, Shamir, and Sridharan 2012), we develop a novel iterative algorithm for kernel PCA that has a solid convergence guarantee. The staring point is the following observation: Since only the top eigensystems of K are used in kernel PCA, it is sufficient to find a low-rank matrix K from which we can recover the top eigensystems of K . In this paper, we choose K as the low-rank matrix obtained by applying Singular Value Shrinkage (SVS) operator (Cai, Cand es, and Shen 2010) to K . Thus, the problem becomes how to estimate K without constructing K explicitly. To this end, we formulate the SVS operation as a stochastic composite optimization problem, and develop an efficient algorithm based on Stochastic Proximal Gradient Descent (SPGD). The advantage of the stochastic formulation is that only a low-rank estimate of K is needed during the optimization process. Since the SVS operation is applied in each iteration, all the iterates are prone to be low-rank. Furthermore, the low-rankness of iterates in turn makes the SVS operation very efficient. As a result, in each iteration, both space and time complexities are linear in n. By exploiting the strong convexity of the objective, we prove that the last iterate of SPGD converges to K at an O(1/T ) rate, where T is the number of iterations. It implies we can simply take the last iterate as the final solution, and thus avoid the averaging operation in the traditional algorithms (Hazan and Kale 2011; Rakhlin, Shamir, and Sridharan 2012). Finally, we examine the empirical performance of the proposed algorithm on two benchmark data sets.



Introduction

Principal Component Analysis (PCA) is a powerful dimensionality reduction method that has been widely used in various applications including data mining, information retrieval, and pattern recognition (Duda, Hart, and Stork 2000). While the classical PCA is limited to identifying linear structures, kernel PCA, a non-linear extension of PCA, has been proposed for extracting non-linear patterns from data (Sch olkopf, Smola, and M uller 1998). The key idea is to map the data into a kernel-induced Hilbert space, where dot product between points can be computed efficiently through the kernel evaluation. Given a set of n training examples, kernel PCA needs to perform eigendecomposition of the n x n kernel matrix K . As it takes O(n2 ) space to store K and O(n3 ) time to eigendecompose it, kernel PCA is prohibitively expensive for big data, where n is very large. Existing studies for reducing the computational cost of kernel PCA can be classified into two categories: approximate and iterative. Approximate approaches (Lopez-Paz et al. 2014) construct a low-rank approximator of the kernel matrix, and use its eigensystems as an alternative. Due to

Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



Related Work

In this section, we briefly review the related work on kernel PCA and stochastic optimization.



Kernel PCA

The basic idea of kernel PCA is to map the input data into a Reproducing Kernel Hilbert Space (RKHS) induced by a kernel function, and perform PCA in that space (Sch olkopf, Smola, and M uller 1998). Let H be a RKHS with a kernel function (x, y) = (x) (y), x, y  Rd , where  : Rd  H is a possibly nonlinear feature mapping. For the sake of simplicity, we assume the data are centered, i.e., n i=1 (xi ) = 0. The covariance matrix in H is given by n 1 C= n i=1 (xi )(xi ) . We now have to find the eigenvalues   0 and eigenvectors v  H \ {0} satisfying C v = v. Since all solutions v with  = 0 lie within the span of (x1 ), . . . , (xn ), we can represent v as v = u, where  = [(x1 ), . . . , (xn )] and u  Rn . As a result, it is equivalent to consider the following problem 1  u = u. (1) n Let K  Rnxn be the kernel matrix with Kij = (xi , xj ) for i, j = 1 . . . , n. Multiplying both sides of (1) by  , we obtain K 2 u = nK u, which can be simplified to the eigenvalue problem K u = nu (Sch olkopf, Smola, and M uller 1998, Appendix A). Let (ui , i ) be the i-th eigenvector and eigenvalue pair of K , with normalization i ui 2 2 = 1. Then, the i-th eigenvector of C is given by vi = ui , which has unit length as indicated below vi vi = ui  ui = ui K ui = i ui

2 2 2



that aim to reduce its cost in testing. In particular, sparse kernel PCA (Tipping 2001) has been proposed to express each eigenvector vi in terms of a small number of training examples. It was later extended to online setting (Honeine 2012), where training examples arrive sequentially. Finally, we note that it is always possible to cast the problem of kernel PCA as a special case of linear PCA, which can be solved efficiently by online algorithms designed for linear PCA. To do this, we simply treat columns of K as feature vectors, evaluate them sequentially, and pass them to online algorithms for linear PCA. In this way, we can find the top eigensystems of K 2 , from which we can derive the top eigensystems of K . However, this kind of approaches suffers from one of the following limitations. 1. Some online PCA algorithms, such as the generalized Hebbian algorithm, are only able to find top eigenvectors. But for kernel PCA, we need both top eigenvectors and eigenvalues. 2. Many online algorithms for linear PCA, such as capped MSG (Arora, Cotter, and Srebro 2013) and incremental SVD (Brand 2006), lack formal theoretical guarantees. 3. Although certain online algorithms are equipped with regret bounds (Warmuth and Kuzmin 2008), the difference between the eigenvectors returned by online algorithms and the ground-truth remains unclear.



Stochastic Optimization

Stochastic optimization refers to the setting where we can only access to the stochastic gradient of the objective function (Hazan and Kale 2011; Zhang, Mahdavi, and Jin 2013). For general Lipschitz continuous convex functions, Stochastic Gradient Descent (SGD) exhibits the unimprovable O(1/ T ) rate of convergence (Nemirovski and Yudin 1983). For strongly convex functions, some variants of SGD (Hazan and Kale 2011; Rakhlin, Shamir, and Sridharan 2012; Zhang et al. 2013) achieve the optimal O(1/T ) rate (Agarwal et al. 2012). Recently, a special case of stochastic optimization, namely Stochastic Composite Optimization (SCO), has received significant interest in optimization and learning communities (Ghadimi and Lan 2012; Lin, Chen, and Pe na 2014; Zhang et al. 2014). In SCO, the objective function is given by the summation of non-smooth and smooth stochastic components (Lan 2012). The most popular non-smooth components are the 1 -norm regularizer for vectors and the nuclear norm regularizer for matrices, which enforce sparseness and low-rankness, respectively. Although the generic algorithms designed for stochastic optimization can also be applied to SCO, by replacing gradient with subgradient, they can not utilize the structure of the objective function to generate sparse or low-rank intermediate solutions. The specialized algorithms for SCO are all built upon Stochastic Proximal Gradient Descent (SPGD), where the power of the non-smooth term is preserved (Lan 2012; Ghadimi and Lan 2012; Chen, Lin, and Pe na 2012; Lin, Chen, and Pe na 2014). A major limitation of existing algorithms for SCO is that they did not treat memory as a limited resource. If we apply them to the SCO problem considered in this paper, the



= 1.



Generally speaking, it takes O(dn ) time to calculate K , O(n2 ) space to store, and O(n3 ) time to eigendecompose it. Thus, the vanilla procedure described above becomes computationally expensive when n is large. Approximate approaches for kernel PCA (Achlioptas, Mcsherry, and Sch olkopf 2002; Ouimet and Bengio 2005; Zhang, Tsang, and Kwok 2008; Lopez-Paz et al. 2014) adopt matrix approximation techniques, such as the Nystr om method (Williams and Seeger 2001; Drineas and Mahoney 2005), to construct a low-rank approximator of K , and then perform eigendecomposition of this low-rank matrix. For approximate approaches, there is a dilemma between the space complexity and the accuracy of their solution. The smaller the memory, the larger the approximation error, and vice visa. On the other hand, iterative approaches can find an accurate solution with a small memory, at the cost of a longer time. The most popular iterative approach for kernel PCA is the Kernel Hebbian Algorithm (KHA) (Kim, Franz, and Sch olkopf 2005; G unter, Schraudolph, and Vishwanathan 2007), which is a kernelized version of the generalized Hebbian algorithm designed for linear PCA (Sanger 1989). Similar to the algorithm proposed here, KHA is also a stochastic approximation algorithm. However, due to the non-convexity of its formulation, there is no global convergence guarantee for KHA. While the work referenced above focus on reducing the cost of kernel PCA during training, there are some studies



memory complexity is still O(n2 ). We do find a heuristic algorithm (Avron et al. 2012) in the literature which combines truncated SVD with SGD to control the space complexity. But it relies on the assumption that the objective value can be evaluated easily, which unfortunately does not hold in our case. That is the reason why we choose the basic SPGD instead of more advanced methods to optimize our problem and establish a novel convergence guarantee for SPGD.



is an unbiased estimate of K with rank at most k . If a symmetric matrix is desired, we can set   k k n  = Kij eij + eij Kij  2k j =1 j =1 which is an unbiased estimate of K with rank at most 2k . 2. When the kernel matrix K is generated by a shiftinvariant kernel, such as the Gaussian kernel and the Laplacian kernel. We can construct  by the random Fourier features (Rahimi and Recht 2008). Let (x, y ) be the shift-invariant kernel with Fourier representation (x, y) = p(w) exp(j w (x - y))dw



Algorithm

We first formulate kernel PCA as a SCO problem, then develop the optimization algorithm, next discuss implementation issues, and finally present the theoretical guarantee.



Reformulation of Kernel PCA

Denote the eigendecomposition of the kernel matrix K by U U , where U = [u1 , . . . , un ],  = diag[1 , . . . , n ], and 1  2  * * * n . To train kernel PCA, it is sufficient to find a low-rank matrix K from which the top eigensystems of K can be recovered. The ideal low-rank matrix would be k the truncated SVD of K , i.e., i=1 i ui ui for some integer k > 0. However, the truncated SVD operation is nonconvex, making it difficult to design a principled algorithm. Instead, we consider the low-rank matrix K obtained by applying the Singular Value Shrinkage (SVS) operator to K with threshold  (Cai, Cand es, and Shen 2010), 1 i.e., K = D [K ] = (i - )ui ui .

i:i >



where p(w) is a density function. Let w be a Fourier component randomly sampled from p(w), and let a(w) and b(w) be the feature vectors generated by w, i.e., a(w) =[cos(w x1 ), . . . , cos(w xn )] , b(w) =[sin(w x1 ), . . . , sin(w xn )] . By drawn k independent samples from p(w), denoted by w1 , . . . , wk , we construct  as = 1 k

k



a(wi )a(wi ) + b(wi )b(wi )

i=1



From the above expression, we observe that eigenvectors of K with nonzero eigenvalues are the top eigenvectors of K . Furthermore, nonzero eigenvalues of K are the top eigenvalues of K minus . As a result, we can recover the top eigensystems of K (with eigenvalues larger than ) from the eigendecomposition of K . In the following, we formulate the SVS operation as a SCO problem. First, it is well-known K is the optimal solution to the following convex composite optimization problem 1 min Z -K 2 (2) F + Z  n x n 2 Z R where *  is the nuclear norm of matrices. Let  be a lowrank random matrix which is an unbiased estimate of K , i.e., K = E[ ]. We list examples of such random matrices below. 1. For general kernel matrix K , we can construct  by sampling its rows or columns randomly. Let {i1 , . . . , ik } be a set of random indices sampled from [n] uniformly, Kij be the ij -th column of K , eij be the ij -th canonical base. Then, k n Kij eij = k j =1

For a matrix X  Rmxn with singular value decomposition U V , where  = diag[1 , . . . , min(m,n) ], D [X ] is given by D [X ] = U D []V and D [] = diag max(0, 1 - ), . . . , max(0, min(m,n) - ) .

1



which is an unbiased estimate of K with rank at most 2k . 3. For dot product kernels such as the polynomial kernel, we can generate the random matrix  in a similar way (Kar and Karnick 2012). Then, we rewrite Z - K 2 F in (2) as Z -K = Z =E

2 F 2 F



= Z

2 F



2 F



- 2 tr(Z K ) + K

2 F



2 F 2 F



- 2 tr(Z E[ ]) + E[  + K

2 F]



Z -



- E[



2 F] +  2 F]



K



- E[ 



2 F]



Since K 2 F - E[  (2) is equivalent to min nxn



is a constant term with respect to Z ,



1 E Z - 2 (3) F + Z  2 Z R a standard SCO problem with a nuclear norm regularizer.



Optimization by Stochastic Proximal Gradient Descent (SPGD)

At this point, one may consider applying existing algorithms for stochastic optimization to the problem in (3). Unfortunately, we find that all the previous algorithms can not be applied directly due to the high space complexity or unrealistic assumptions, as explained below. 1. The generic algorithms for stochastic optimization (Nemirovski et al. 2009; Hazan and Kale 2011; Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2012) are built up SGD, and thus cannot utilize the structure of (3) to enforce low-rankness. Furthermore, those algorithms return the average of iterates as the final solution, which could be full-rank.



2. Although the specialized algorithms for SCO can generate low-rank iterates based on SPGD, they need to keep track of the averaged iterates as an auxiliary variable (Chen, Lin, and Pe na 2012; Lin, Chen, and Pe na 2014) or as the final solution (Lan 2012; Ghadimi and Lan 2012). Thus, the space complexity is still O(n2 ). 3. Although the heuristic algorithm in (Avron et al. 2012) is able to make the space complexity linear in n, it needs to evaluate the objective value in each iteration, which is impossible for the SCO problem in (3). Furthermore, it is designed for general SCO problems and thus cannot exploit the strong convexity of (3). Due to the above reasons, we develop a new algorithm to optimize (3), which is purely based on SPGD and takes its last iterate as the final solution. Denote by Zt the solution at the t-th iteration. In this iteration, we first sample a random matrix t  Rnxn , and it is easy to verify that Zt - t is an unbiased estimate of the gradient of 1 Z - 2 F . 2E Then, we update the current solution by the SPGD, which is essentially a stochastic variant of composite gradient mapping (Nesterov 2013) Zt+1 1 Z - Zt 2 F + t Z - Zt , Zt - t + t  Z 2 Z Rnxn 1 2 = argmin Z - [(1 - t )Zt + t t ] F + t  Z  2 n x n Z R = argmin =Dt  [(1 - t )Zt + t t ] where t > 0 is the step size. Let Zt+1 = (1 - t )Zt + t t . The SVS operation applies a soft-thresholding rule to the singular values of Zt+1 , effectively shrinking them toward zero. In particular, singular values of Zt+1 that are below the threshold t  vanish, and thus Zt+1 tends to be a lowrank matrix. Let ZT +1 be the final solution obtained after T iterations. If ZT +1 is symmetric, we will eigendecompose ZT +1 and obtain its eigensystems {(ui , i )}k i=1 with nonzero eigenvalues. Otherwise, we will use the eigensystems of (ZT +1 + ZT +1 )/2 instead of ZT +1 . Note that (ZT +1 + ZT +1 )/2 is symmetric and always more close to K than ZT +1 , since 1 (ZT +1 + ZT +1 ) - K 2 F 1 1  ZT +1 - K F + ZT +1 - K 2 2 = ZT +1 - K F .





Algorithm 1 A Stochastic algorithm for Kernel PCA Input: The number of trials T , and the regularization parameter  1: Initialize Z1 = 0 2: for t = 1, 2, . . . , T do 3: Sample a random matrix t 4: t = 2/t 5: Zt+1 = Dt  [(1 - t )Zt + t t ] 6: end for 1 7: Calculate the nonzero eigensystems of 2 (ZT +1 + k ZT +1 ): {(ui , i )}i=1 8: return {(ui , i + )}k i=1



and 1n is a n-dimensional vector of all ones. If  is an unbiased estimate of K , then it is easy to verify  +  where 1 1 1 (1n  1n )1n 1n - 1n 1n  -  1n 1n 2 n n n is an unbiased estimate of K + . To find the top eigensystems of K + , we just need to replace the random matrix  in our algorithm with  +  and all the rest is the same. =



Implementation Issues

In this section, we discuss how to ensure all the iterates are represented in low-rank factorization form and how to accelerate the SVS operation by utilizing this fact. First, the random matrices t can always be represented by t = t t , where t , t  Rnxat are two rectangular matrices with at n. Now, suppose Zt is also represented by Zt = Ut Vt , where Ut , Vt  Rnxbt are two rectangular matrices with bt n.2 Then, Zt+1 = Dt  (1 - t )Ut Vt + t t t can be solved efficiently according to Lemma 3.4 of (Avron et al. 2012). Specifically, we introduce two matrices Xt , Yt  Rnx(at +bt ) such that  Xt =[ 1 - t Ut , t t ],  Yt =[ 1 - t Vt , t t ], and Zt+1 = Dt  [Xt Yt ]. Next, we perform a reduced QR decomposition (Golub and Van Loan 1996) of Xt = QX RX and Yt = QY RY , and find the SVD of RX RY = U V . Define Ut = QX U and Vt = QY V . It is easy to verify that Ut Vt is the SVD of Xt Yt , from which Zt+1 can be calculated trivially, and represented in the from of Zt+1 = Ut+1 Vt+1 . From the above discussion, it is clear that the space complexity is O(n(at + bt )) in each iteration. The running time is dominated by calculating t , which takes O(ndat ) time, and QR decompositions, which take O(n(at + bt )2 ) time. In summary, the time complexity is O(n[dat + (at + bt )2 ]). Thus, both space and time complexities are linear in n.



F



Finally, we return {(ui , i + )}k i=1 as the top eigensystems of K . The above procedure is summarized in Algorithm 1. Although we assume that data are centered in RKHS, our algorithm can be immediately extend to the general case. If data are uncentered, kernel PCA (Sch olkopf, Smola, and M uller 1998) needs the top eigensystems of K + , where = 1 1 1 (1 K 1n )1n 1n - 1n 1n K - K 1n 1n n2 n n n



Theoretical Guarantee

The following theorem shows that with a high probability, ZT +1 converges to K , the optimal solution to (3), at an O(1/T ) rate.

2



At least, we can represent Z1 in this form since Z1 = 0.



Theorem 1 Assume the Frobenius norm of the random matrix  is upper bounded by some constant C > 0. By setting t = 2/t, with a probability at least 1 -  , we have ZT +1 - K 

2 F



 8 2 log2 T C max rt + C 2 8 + 6 log T  t[T ] log log T =O T



The random matrix in SKPCA is constructed by random Fourier features (Rahimi and Recht 2008). The experiments are done on two benchmark data sets: Mushrooms (Chang and Lin 2011) and Magic (Frank and Asuncion 2010), which contain 8, 124 and 19, 020 examples, respectively. We choose those two medium-size data sets, because they can be handled by Baseline and thus allow us to compare different methods quantitatively. For all the experiments, we repeat them 10 times and report the averaged result.



where rt is the rank of Zt . Note that the O(1/T ) convergence rate matches the lowerbound of stochastic optimization of strongly convex functions (Agarwal et al. 2012). Our result differs from previous studies of SPGD (Rosasco, Villa, and V u 2014) in the sense that we prove a high probability bound instead of an expectation bound. Although a similar result has been proved for SGD (Rakhlin, Shamir, and Sridharan 2012), this is the first time such a guarantee is established for SPGD. The proof of this theorem relies on the recent analysis of SGD (Rakhlin, Shamir, and Sridharan 2012) and concentration inequalities (Bartlett, Bousquet, and Mendelson 2005; Cesa-Bianchi and Lugosi 2006). Due to space limitations, details are provided in the supplementary material.



Experimental Results

We first examine the convergence rate of SKPCA. We run SKPCA with four different combinations of the parameter  and the number of random Fourier components k . In Fig. 1(a), we report the normalized recover error Zt - 2 K 2 F /n with respect to the number of iterations t on the Mushrooms data set. For comparison, we also plot the curve of 0.03/t. From the similarity among those curves, we believe the proposed algorithm achieves the O(1/T ) rate. As can be seen, the two curves of k = 5 (or k = 50) almost overlap with each other. That is probably because on this data set  is not the dominating term in the upper bound given in Theorem 1. On the other hand, the convergence rate highly depends on the number of Fourier components k . The curves of k = 50 converge significantly faster than those of k = 5. The reason is that the larger k is, the closer  and K are, and the smaller the constant C in Theorem 1 is. Then, we check the rank of the intermediate iterate Zt , denoted by rank(Zt ), which determines the computational complexity of the t-th round. Fig. 1(b) plots rank(Zt ) as a function of t, which first increases and then converges to certain constant. The rank of the target matrix K is 158 when  = 1 and 55 when  = 10. As can be seen, rank(Zt ) is just a constant factor larger than rank(K ). To compare different methods, we use the top 50 eigensystems returned by each algorithm to construct a rank-50 approximator of K , denoted by K 50 , and report the approximation error K 50 - K F /n in Fig. 1(c). In order to fit the figure, the training time of Baseline was divided by 2. The result returned by Baseline is optimal, but it takes a longer time and a much larger memory. Although Nystr om is able to find a good solution, it cannot further reduce the approximation error. In comparison, SKPCA is able to refine its solution continuously and outperforms Nystr om after 10 seconds. Finally, we note that SKPCA is much faster than KHA. Experimental results on the Magic data set are provided in Fig. 2, which exhibits similar behaviors. On this data set, The rank of the K is 89 when  = 10 and 17 when  = 100. The training time of Baseline was divided by 20 in Fig. 2(c).



Experiments

In this section, we perform several experiments to examine the performance of our method.



Experimental Setting

We compare our stochastic algorithm for kernel PCA (SKPCA) with the following methods. 1. Baseline (Sch olkopf, Smola, and M uller 1998), which calculates the kernel matrix K explicitly and eigendecomposes it. 2. Approximation based on the Nystr om method (Drineas and Mahoney 2005; Zhang, Tsang, and Kwok 2008), which uses the Nystr om method to find a low-rank approximator of K , and eigendecomposes it. 3. Kernel Hebbian Algorithm (KHA) (Kim, Franz, and Sch olkopf 2005), which is an iterative approach for kernel PCA. In order to run SKPCA, we need to decide the value of the parameter  in (3), which in turn determines the number of eigenvectors used in kernel PCA. To minimize the generalization error, we would like to find a  such that eigenvalues of K that are smaller than it fall quickly (Shawe-Taylor et al. 2005). However, it is infeasible to calculate eigenvalues of K for large n, so we will use eigenvalues of a small kernel matrix K of m examples to estimate . Note that eigenvalues of K/n and K/m both converges to those of the integral operator (Braun 2006). Although the optimal step size of KHA in theory is 1/t, we found it led to very slow convergence, and thus set it to be 0.05 as suggested by (Kim, Franz, and Sch olkopf 2005). We choose the Gaussian kernel (xi , xj ) = exp( xi - xj 2 /(2 2 )), and set the kernel width  to the 20-th percentile of the pairwise distances (Mallapragada et al. 2009).



Conclusions

In this paper, we have formulated kernel PCA as a stochastic composite optimization problem with a nuclear norm regularizer, and then develop an iterative algorithm based on the stochastic proximal gradient descent algorithm. The main advantages of our method are i) both space and time complexes are linear in the number of samples; and ii) it is guaranteed to converge at an O(1/T ) rate, where T is the num-



4



x 10



-3



Zt - K



2



Rank(Zt )



200 150 100 50



= = = =



1, k = 5 1, k = 50 10, k = 5 10, k = 50



Approximation Error



3

2 2 F /n



= = = =

0 .03 t



1, k = 5 1, k = 50 10, k = 5 10, k = 50



0.06



300

0.05



250



0.04 0.03 0.02 0.01 0 0 10



Basline (1/2) Nystrom KHA SKPCA



1



0



50



100 t

2 2 F /n



150



200



50



100 t



150



200



20 30 40 50 Training Time (s)



60



70



(a) Zt - K



versus t



(b) rank(Zt ) versus t



(c) Approximation error for K



Figure 1: Experimental Results on the Mushrooms data set. To fit the figure, the training time of Baseline was divided by 2 in (c).

4 x 10

-3



2 2 F /n



150

= = = = 10, k = 5 10, k = 50 100, k = 5 100, k = 50



Approximation Error



3



= = = =

0 .05 t



10, k = 5 10, k = 50 100, k = 5 100, k = 50



200

0.05 0.04 0.03 0.02 0.01 0



Zt - K



2



Rank(Zt )



100



Basline (1/20) Nystrom KHA SKPCA



1



50



0



50



100 t

2 2 F /n



150



200



50



100 t



150



200



0



20



40 60 Training Time (s)



80



100



(a) Zt - K



versus t



(b) rank(Zt ) versus t



(c) Approximation error for K



Figure 2: Experimental Results on the Magic data set. To fit the figure, the training time of Baseline was divided by 20 in (c). ber of iterations. Experiments on two benchmark data sets illustrate the efficiency and effectiveness of the proposed method. scent for nuclear norm regularization. In Proceedings of the 29th International Conference on Machine Learning, 1231- 1238. Bartlett, P. L.; Bousquet, O.; and Mendelson, S. 2005. Local rademacher complexities. The Annals of Statistics 33(4):1497-1537. Brand, M. 2006. Fast low-rank modifications of the thin singular value decomposition. Linear Algebra and its Applications 415(1):20-30. Braun, M. L. 2006. Accurate error bounds for the eigenvalues of the kernel matrix. Journal of Machine Learning Research 7:2303-2328. Cai, J.-F.; Cand es, E. J.; and Shen, Z. 2010. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization 20(4):1956-1982. Cesa-Bianchi, N., and Lugosi, G. 2006. Prediction, Learning, and Games. Cambridge University Press. Chang, C.-C., and Lin, C.-J. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology 2(3):27:1-27:27. Chen, X.; Lin, Q.; and Pe na, J. 2012. Optimal regularized dual averaging methods for stochastic optimization. In Ad-



Acknowledgments

This research was supported by NSFC (61333014, 61321491), NSF (IIS-1463988, IIS-1545995), and MSRA collaborative research project (FY14-RES-OPP-110).



References

Achlioptas, D.; Mcsherry, F.; and Sch olkopf, B. 2002. Sampling techniques for kernel methods. In Advances in Neural Information Processing Systems 14, 335-342. Agarwal, A.; Bartlett, P. L.; Ravikumar, P.; and Wainwright, M. J. 2012. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory 58(5):3235-3249. Arora, R.; Cotter, A.; and Srebro, N. 2013. Stochastic optimization of pca with capped msg. In Advances in Neural Information Processing Systems 26, 1815-1823. Avron, H.; Kale, S.; Kasiviswanathan, S.; and Sindhwani, V. 2012. Efficient and practical stochastic subgradient de-



vances in Neural Information Processing Systems 25, 404- 412. Drineas, P., and Mahoney, M. W. 2005. On the nystr om method for approximating a gram matrix for improved kernel-based learning. Journal of Machine Learning Research 6:2153-2175. Duda, R. O.; Hart, P. E.; and Stork, D. G. 2000. Pattern Classification. Wiley-Interscience Publication. Frank, A., and Asuncion, A. 2010. UCI machine learning repository. Ghadimi, S., and Lan, G. 2012. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization 22(4):1469-1492. Golub, G. H., and Van Loan, C. F. 1996. Matrix computations, 3rd Edition. Johns Hopkins University Press. G unter, S.; Schraudolph, N. N.; and Vishwanathan, S. V. N. 2007. Fast iterative kernel principal component analysis. Journal of Machine Learning Research 8:1893-1918. Hazan, E., and Kale, S. 2011. Beyond the regret minimization barrier: an optimal algorithm for stochastic stronglyconvex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, 421-436. Honeine, P. 2012. Online kernel principal component analysis: A reduced-order model. IEEE Transactions on Pattern Analysis and Machine Intelligence 34(9):1814-1826. Kar, P., and Karnick, H. 2012. Random feature maps for dot product kernels. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics, 583-591. Kim, K. I.; Franz, M. O.; and Sch olkopf, B. 2005. Iterative kernel principal component analysis for image modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(9):1351-1366. Lan, G. 2012. An optimal method for stochastic composite optimization. Mathematical Programming 133:365-397. Lin, Q.; Chen, X.; and Pe na, J. 2014. A sparsity preserving stochastic gradient methods for sparse regression. Computational Optimization and Applications 58(2):455-482. Lopez-Paz, D.; Sra, S.; Smola, A. J.; Ghahramani, Z.; and Sch olkopf, B. 2014. Randomized nonlinear component analysis. In Proceedings of the 31st International Conference on Machine Learning. Mallapragada, P. K.; Jin, R.; Jain, A. K.; and Liu, Y. 2009. Semiboost: Boosting for semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 31(11):2000-2014. Nemirovski, A., and Yudin, D. B. 1983. Problem complexity and method efficiency in optimization. John Wiley & Sons Ltd. Nemirovski, A.; Juditsky, A.; Lan, G.; and Shapiro, A. 2009. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization 19(4):1574- 1609.



Nesterov, Y. 2013. Gradient methods for minimizing composite functions. Mathematical Programming 140(1):125- 161. Ouimet, M., and Bengio, Y. 2005. Greedy spectral embedding. In Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics, 253-260. Rahimi, A., and Recht, B. 2008. Random features for largescale kernel machines. In Advances in Neural Information Processing Systems 20, 1177-1184. Rakhlin, A.; Shamir, O.; and Sridharan, K. 2012. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, 449-456. Rosasco, L.; Villa, S.; and V u, B. C. 2014. Convergence of stochastic proximal gradient algorithm. ArXiv e-prints arXiv:1403.5074. Sanger, T. D. 1989. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks 2(6):459-473. Sch olkopf, B.; Smola, A.; and M uller, K.-R. 1998. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation 10(5):1299-1319. Shamir, O., and Zhang, T. 2012. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. ArXiv e-prints arXiv:1212.1824. Shawe-Taylor, J.; Williams, C. K. I.; Cristianini, N.; and Kandola, J. 2005. On the eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory 51(7):2510-2522. Tipping, M. E. 2001. Sparse kernel principal component analysis. In Advances in Neural Information Processing Systems 13, 633-639. Warmuth, M. K., and Kuzmin, D. 2008. Randomized online pca algorithms with regret bounds that are logarithmic in the dimension. Journal of Machine Learning Research 9:2287- 2320. Williams, C., and Seeger, M. 2001. Using the nystr om method to speed up kernel machines. In Advances in Neural Information Processing Systems 13, 682-688. Zhang, L.; Yang, T.; Jin, R.; and He, X. 2013. O(log T ) projections for stochastic optimization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning. Zhang, W.; Zhang, L.; Hu, Y.; Jin, R.; Cai, D.; and He, X. 2014. Sparse learning for stochastic composite optimization. In Proceedings of the 28th AAAI Conference on Artificial Intelligence, 893-899. Zhang, L.; Mahdavi, M.; and Jin, R. 2013. Linear convergence with condition number independent access of full gradients. In Advance in Neural Information Processing Systems 26, 980-988. Zhang, K.; Tsang, I. W.; and Kwok, J. T. 2008. Improved nystr om low-rank approximation and error analysis. In Proceedings of the 25th International Conference on Machine Learning, 1232-1239.



Deep Contextual Networks for Neuronal Structure Segmentation

Hao Chen ,  , Xiaojuan Qi , , Jie-Zhi Cheng , Pheng-Ann Heng , 





Department of Computer Science and Engineering, The Chinese University of Hong Kong  School of Medicine, Shenzhen University, China  Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China



Abstract

The goal of connectomics is to manifest the interconnections of neural system with the Electron Microscopy (EM) images. However, the formidable size of EM image data renders human annotation impractical, as it may take decades to fulfill the whole job. An alternative way to reconstruct the connectome can be attained with the computerized scheme that can automatically segment the neuronal structures. The segmentation of EM images is very challenging as the depicted structures can be very diverse. To address this difficult problem, a deep contextual network is proposed here by leveraging multi-level contextual information from the deep hierarchical structure to achieve better segmentation performance. To further improve the robustness against the vanishing gradients and strengthen the capability of the back-propagation of gradient flow, auxiliary classifiers are incorporated in the architecture of our deep neural network. It will be shown that our method can effectively parse the semantic meaning from the images with the underlying neural network and accurately delineate the structural boundaries with the reference of low-level contextual cues. Experimental results on the benchmark dataset of 2012 ISBI segmentation challenge of neuronal structures suggest that the proposed method can outperform the state-of-the-art methods by a large margin with respect to different evaluation measurements. Our method can potentially facilitate the automatic connectome analysis from EM images with less human intervention effort.



Introduction

In neuroscience, the neuronal circuit reconstruction, also termed as connectome, from biological images can manifest the interconnections of neurons for more insightful functional analysis of the brain and other nervous systems (Sporns, Tononi, and K otter 2005; Laptev et al. 2012). For instance, the 2D serial high resolution Electron Microscopy (EM) imaging is commonly used for the visualization of micro neural circuits and hence is a very informative imaging tool for the connectome analysis. In this paper, we focus

Authors contributed equally. Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.





on the widely used serial section Transmission Electron Microscopy (ssTEM) images for neuronal structure segmentation (Cardona et al. 2010). To illustrate the image complexity, a 2D example of original ssTEM image and corresponding segmentation by expert are illustrated in Figure 1. It can be found that the neuronal structures depicted in the ssTEM images are very complex and hence require the further segmentation of each structure to elucidate the interconnection relation. However, this is a non-trivial task. The ssTEM images can depict more than tens of thousands of neurons where each neuron may have thousands of synaptic connections. Thus, the size of ssTEM images is usually formidably large in a terabyte scale. Accordingly, the extremely complicated interconnections of neuronal structures and sheer image volume are far beyond the human capability for annotation, as the manual labeling of all neuronal structures may take decades to finish (White et al. 1986; Bock et al. 2011; Wu 2015). In this case, automatic segmentation methods are highly demanded to assist the parsing of the ssTEM images into concrete neurological structures for further analysis (Seung 2011). However, as can be observed in Figure 1, the segmentation problem for the neuronal structures can be very challenging in threefold. First, the image deformation during the acquisition may blur the membrane boundaries between neighboring neurons as shown in Figure 1 (left). Second, the variation of neuron membrane in terms of image contrast and membranal thickness can be very large. Particularly for the thickness, it can range from solid dark curves to grazed grey swaths (Jurrus et al. 2010). Third, the presence of intracellular structures makes edge detection and region growing based methods ineffective for the identification of neuron membrane. Some confounding micro-structures may also mislead the merging of regions or incorrect splitting of one region into several sections. Meanwhile, the imaging artifacts and image alignment errors can impose difficulties on the design of effective segmentation algorithm as well.



Related Work

Because of the anisotropic nature of ssTEM data, most previous methods were devised under the framework of initial 2D membrane detection and latter 3D linking process (Jurrus et al. 2010). Although considerable progress has been witnessed over the last decade, earlier studies achieved an in-



Figure 1: Left: the orginal ssTEM image. Right: the corresponding segmentation annotation (individual components are denoted by different colors).



ferior accuracy of segmentation and often failed to suppress the intracellular structures effectively by utilizing the handcrafted features, e.g., radon and ray-like features (Kumar, V azquez-Reina, and Pfister 2010; Mishchenko 2009; Laptev et al. 2012; Kaynig, Fuchs, and Buhmann 2010). Recently, deep neural networks with hierarchical feature representations have achieved promising results in various applications, including image classification (Krizhevsky, Sutskever, and Hinton 2012), object detection (Simonyan and Zisserman 2014; Chen et al. 2015) and segmentation (Long, Shelhamer, and Darrell 2014). In terms of EM segmentation, Ciresan et al. (2012) employed the deep convolutional neural network as a pixel-wise classifier by taking a square window centered on the pixel itself as input, which contains contextual appearance information. This method achieved the best performance in 2012 ISBI neuronal structure segmentation challenge. A variant version with iterative refining process has been proposed to withstand the noise and recover the boundaries (Wu 2015). Besides, several methods worked on the probability maps produced by deep convolutional neural networks as a post-processing step, such as learning based adaptive watershed (Uzunbas , Chen, and Metaxsas 2014), hierarchical merge tree with consistent constraints (Liu et al. 2014) and active learning approach for hierarchical agglomerative segmentation (Nunez-Iglesias et al. 2013), to further improve the performance. These methods refined the segmentation results with respect to the measurements of rand error and warping error (Jain et al. 2010) with significant performance boost in comparison to the results of (Ciresan et al. 2012). However, the performance gap between the computerized results and human neuroanatomist annotations can be still perceivable. There are two main drawbacks of previous deep learning based studies on this task. First, the operation of sliding window scanning imposes a heavy burden on the computational efficiency. This must be taken into consideration seriously regarding the large scale neuronal structure reconstruction. Second, the size of neuronal structure can be very diverse in EM images. Although, classification with single size sub-window can achieve good performance, it may produce unsatisfactory results in some regions where the size of contextual window is set inappropriately.



In order to tackle the aforementioned challenges, we propose a novel deep contextual segmentation network to demarcate the neuronal structure in EM stacks. This approach incorporates the multi-level contextual information with different receptive fields, thus it can remove the ambiguities of membranal boundaries in essence that previous studies may fail. Inspired by previous studies (Ciresan et al. 2012; Lee et al. 2014), we further make the model deeper than (Ciresan et al. 2012) and add auxiliary supervised classifiers to encourage the back-propagation flow. This augmented network can further unleash the power of deep neural networks for neuronal structure segmentation. Quantitative evaluation was extensively conducted on the public dataset of 2012 ISBI EM Segmentation Challenge (Ignacio et al. 2012), with rich baseline results for comparison in terms of pixel- and object-level evaluation. Our method set the state-of-the-art record, which outperformed those of other methods on all evaluation measurements. It is also worth noting that our results surpassed the annotation by neuroanatomists on the measurement of warping error.



Method

Deeply Supervised Contextual Network

In this section, we present a deeply supervised contextual network for neuronal structure segmentation. Inspired by recent studies of fully convolutional networks (FCN) (Long, Shelhamer, and Darrell 2014; Chen et al. 2014), which replace the fully connected layers with all convolutional kernels, the proposed network is a variant and takes full advantage of convolutional kernels for efficient and effective image segmentation. The architecture of the proposed method is illustrated in Figure 2. It basically contains two modules, i.e., down-sampling path with convolutional and maxpooling layers and upsampling path with convolutional and deconvolutional layers. Noting that we upsampled the feature maps with the backwards strided convolution in the upsampling path, thus we call them as deconvolutional layers. The downsampling path aims at classifying the semantical meanings based on the high level abstract information, while the upsampling path reconstructing the fine details such as boundaries. The upsampling layers are designed by taking full advantage of the different feature maps in hierarchical layers. The intuition behind this is that global or abstract information from higher layers helps to resolve the problem of what (i.e., classification capability) and local information from lower layers helps to resolve the problem of where (i.e., localization accuracy). Finally, these multi-level contextual information are fused together with a summing operation. The probability maps are generated by inputting the fused map into a softmax classification layer. Specifically, the architecture of neural network contains 16 convolutional layers, 3 max-pooling layers for downsampling and 3 deconvolutional layers for upsampling. The convolutional layers along with convolutional kernels (3 x 3 or 1 x 1) perform linear mapping with shared parameters. The max-pooling layers downsample the size of feature maps by the max-pooling operation (kernel size 2 x 2 with a



3



64



64 max-pool 2x2 conv 3x3 64 128 128 128 256 256 256 256 512 512 512 up-conv classifier conv 1x1 fusion



Input



1202



1202



1202



1202



602



602



602



480x480



480x480



480x480



2402



2402



4802



2402



4802



4802

4802 4802



4802

4802 C2



4802

4802 C3

Softmax



C1



Figure 2: The architecture of the proposed deep contextual network. stride 2). The deconvolutional layers upsample the size of feature maps by the backwards strided convolution (Long, Shelhamer, and Darrell 2014) (2k x 2k kernel with a stride k , k = 2, 4 and 8 for upsampling layers, respectively). A non-linear mapping layer (element-wise rectified linear activations) is followed for each layer that contains parameters to be trained (Krizhevsky, Sutskever, and Hinton 2012). In order to alleviate the problem of vanishing gradients and encourage the back-propagation of gradient flow in deep neural networks, the auxiliary classifiers C are injected for training the network. Furthermore, they can serve as regularization for reducing the overfitting and improve the discriminative capability of features in intermediate layers (Bengio et al. 2007; Lee et al. 2014; Wang et al. 2015). The classification layer after fusing multi-level contextual information produces the EM image segmentation results by leveraging the hierarchical feature representations. Finally, the training of whole network is formulated as a per-pixel classification problem with respect to the ground-truth segmentation masks, as shown following: L(X ; ) =  ( 2

2 ||Wc ||2 2 + ||W ||2 )- c



  



4802



weight. Finally, the parameters  = {W, Wc } of deep contextual network are jointly optimized in an end-to-end way by minimizing the total loss function L. For the testing data of EM images, the results are produced with an overlap-tile strategy to improve the robustness.



Importance of Receptive Field

In the task of EM image segmentation, there is a large variation on the size of neuronal structures. Therefore, the size of receptive field plays a key role in the pixel-wise classification given the corresponding contextual information. It's approximated as the size of object region with surrounding context, which is reflected as the intensity values within the window. As shown in Figure 3, different regions may depend on a different window size. For example, the cluttered neurons need a small window size for clearly separating the membranes between neighboring neurons, while a large size is required for neurons containing intracellular structures so as to suppress the false predictions. In the hierarchical structure of deep contextual networks, these upsampling layers have different receptive fields. With the depth increasing, the size of receptive field is becoming larger. Therefore, it can handle the variations of reception field size properly that different regions demand for correct segmentation while taking advantage of the hierarchical feature representations.



wc c (x, (x)) -

c xX xX



 (x, (x))



(1)



where the first part is the regularization term and latter one including target and auxiliary classifiers is the data loss term. The tradeoff of these two terms is controlled by the hyperparameter . Specifically, W denotes the parameters for inferring the target output p(x; W ),  (x, (x)) denotes the cross entropy loss regarding the true label (x) for pixel x in image space X , similarly c (x, (x)) is the loss from cth auxiliary classifier with parameters Wc for inferring the output, the parameter wc denotes the corresponding discount



Morphological Boundary Refinement

Although the probability maps output from the deep contextual network are visually very good, we observe that the membrane of ambiguous regions can sometimes be discontinued. This arises partially from the averaging effect of probability maps, which are generated by several trained models. Therefore, we utilized an off-the-shelf watershed algorithm (Beucher and Lantuejoul 1979) to complete the



602



2 2 2



2 2 2



2 2 2



dataset took about three hours using a standard PC with a 2.50 GHz Intel(R) Xeon(R) E5-1620 CPU and a NVIDIA GeForce GTX Titan X GPU.



Qualitative Evaluation

Two examples of qualitative segmentation results without morphological boundary refinement can be seen in Figure 4. We can see that our method can generate visually smooth and accurate segmentation results. As the red arrows shown in the figure, it can successfully suppress the intracellular structures and produce good probability maps that classify the membrane and non-membrane correctly. Furthermore, by utilizing multi-level representations of contextual information, our method can also close gaps (contour completion as the blue arrows shown in Figure 4) in places where the contrast of membrane is low. Although there still exist ambiguous regions which are even hard for human experts, the results of our method are more accurate in comparison to those generated from previous deep learning studies (Stollenga et al. 2015; Ciresan et al. 2012). This evidenced the efficacy of our proposed method qualitatively.



Figure 3: Illustration of contextual window size. Left: the original ssTEM image. Right: manual segmentation result by an expert human neuroanatomist (black and white pixels denote the membrane and non-membrane, respectively). contour. The final fusion result pf (x) was produced by fusing the binary contour pw (x) and original probability map p(x) with linear combination: pf (x) = wf p(x) + (1 - wf )pw (x) (2)



The parameter wf is determined by obtaining the optimal result of rand error on the training data in our experiments.



Quantitative Evaluation and Comparison

In the 2012 ISBI EM Segmentation Challenge, the performance of different competing methods is ranked based on their pixel and object classification accuracy. Specifically, the 2D topology-based segmentation evaluation metrics include rand error, warping error and pixel error (Ignacio et al. 2012; Jain et al. 2010), which are defined as following: Rand error: 1 - the maximal F-score of the foregroundrestricted rand index (Rand 1971), a measure of similarity between two clusters or segmentations. For the EM segmentation evaluation, the zero component of the original labels (background pixels of the ground truth) is excluded. Warping error: a segmentation metric that penalizes the topological disagreements (object splits and mergers). Pixel error: 1 - the maximal F-score of pixel similarity, or squared Euclidean distance between the original and the result labels. The evaluation system thresholds the probability maps with 9 different values (0.1-0.9 with an interval 0.1) separately and return the minimum error for each segmentation metric. The quantitative comparison of different methods can be seen in Table 1. Noting that the results show the best performance for each measurement across all submissions by each team individually. More details and results are available at the leader board1 . We compared our method with the state-of-the-art methods with or without post-processing separately. Furthermore, we conducted extensive experiments with ablation studies to probe the performance gain in our method and detail as following. Results Comparison without Post-Processing Preliminary encouraging results were achieved by IDSIA team (Ciresan et al. 2012), which utilized a deep convolutional neural network as a pixel-wise classifier in a sliding

Please refer to the leader board for more details: http://brainiac2.mit.edu/isbi_challenge/ leaders-board

1



Experiments and Results

Data and Preprocessing

We evaluated our method on the public dataset of 2012 ISBI EM Segmentation Challenge (Ignacio et al. 2012), which is still open for submissions. The training dataset contains a stack of 30 slices from a ssTEM dataset of the Drosophila first instar larva ventral nerve cord (VNC), which measures approximately 2x2x1.5 microns with a resolution of 4x4x50 nm/voxel. The images were manually annotated in the pixellevel by a human neuroanatomist using the software tool TrakEm2 (Cardona et al. 2012). The ground truth masks of training data were provided while those of testing data with 30 slices were held out by the organizers for evaluation. We evaluated the performance of our method by submitting results to the online testing system. In order to improve the robustness of neural network, we utilized the strategy of data augmentation to enlarge the training dataset (about 10 times larger). The transformations of data augmentation include scaling, rotation, flipping, mirroring and elastic distortion.



Details of Training

The proposed method was implemented with the mixed programming technology of Matlab and C++ under the opensource framework of Caffe library (Jia et al. 2014). We randomly cropped a region (size 480 x 480) from the original image as the input into the network and trained it with standard back-propagation using stochastic gradient descent (momentum = 0.9, weight decay = 0.0005, the learning rate was set as 0.01 initially and decreased by a factor of 10 every two thousand iterations). The parameter of corresponding discount weight wc was set as 1 initially and decreased by a factor of 10 every ten thousand iterations till a negligible value 0.01. The training time on the augmentation



Slice #06



Slice #13



Figure 4: Examples of original EM images and segmentation results by our method (the darker color of pixels denotes the higher probability of being membrane in neuronal structure). Table 1: Results of 2012 ISBI Segmentation Challenge on Neuronal Structures

Group name ** human values ** CUMedVision (Our) DIVE-SCI IDSIA-SCI optree-idsia (Uzunbas , Chen, and Metaxsas 2014) motif (Wu 2015) SCI (Liu et al. 2014) Image Analysis Lab Freiburg (Ronneberger, Fischer, and Brox 2015) Connectome PyraMiD-LSTM (Stollenga et al. 2015) DIVE IDSIA (Ciresan et al. 2012) INI MLL-ETH (Laptev et al. 2012) CUMedVision-4(C3) CUMedVision-4(C2) CUMedVision-4(C1) CUMedVision-4(with C) CUMedVision-4(w/o C) CUMedVision-6(with C) CUMedVision-4(with fusion) Rand Error 0.002109173 0.017334163 0.017841947 0.018919792 0.022777620 0.026326384 0.028054308 0.038225781 0.045905709 0.046704591 0.047680695 0.048314096 0.060110507 0.063919883 Warping Error 0.000005341 0.000000000 0.000307083 0.000616837 0.000807953 0.000426483 0.000515747 0.000352859 0.000478999 0.000462341 0.000374222 0.000434367 0.000495529 0.000581741 Pixel Error 0.001041591 0.057953485 0.058436986 0.102692786 0.110460288 0.062739851 0.063349324 0.061141279 0.062029263 0.061624006 0.058205303 0.060298549 0.068537199 0.079403258 0.060940140 0.061248112 0.102325669 0.058372960 0.062864362 0.059902422 0.057953485 Rank 1 2 3 4 5 6 7 8 9 10 11 12 13



0.043419035 0.000342178 0.046058434 0.000421524 0.258966855 0.001080322 0.035134666 0.000334167 0.040492503 0.000330353 0.040406591 0.000000000 0.017334163 0.000188446 There are total 38 teams participating this challenge till Sep 2015.



window way. The best results were obtained by averaging the outputs from 4 deep neural network models. Different from this method by training the neural network with different window sizes (65 and 95) separately, our approach integrates multi-size windows (i.e., different receptive fields in upsampling layers) into one unified framework. This can help to generate more accurate probability maps by leveraging multi-level contextual information. The Image Analysis Lab Freiburg team (Ronneberger, Fischer, and Brox 2015) designed a deep U-shaped network by concatenating features from lower layers and improved the results than those of (Ciresan et al. 2012). This further demonstrated the effectiveness of contextual information for accurate segmentation. However, with such a deep network (i.e., 23 convolutional layers), the back-propagation of gradient flow may be a potential issue and training took a long time (about 10



hours). Instead of using the convolutional neural network, the PyraMiD-LSTM team employed a novel parallel multidimensional long short-term memory model for fast volumetric segmentation (Stollenga et al. 2015). Unfortunately, a relatively inferior performance was achieved by this method. From Table 1, we can see that our deep segmentation network (with 6 model averaging results, i.e., CUMedVision6(with C)) without watershed fusion achieved the best performance in terms of warping error, which outperformed other methods by a large margin. Notably it's the only result that surpasses the performance of expert neuroanatomist annotation. Our submitted entry CUMedVision-4(with C) on averaging 4 models (the same number of models as (Ciresan et al. 2012)) achieved much smaller rand and warping errors than the results of other teams also employing deep learning methods without sophisticated post-processing steps, such



as DIVE, IDSIA, and Image Analysis Lab Freiburg. This corroborates the superiority of our approach by exploring multilevel contextual information with auxiliary supervision. Results Comparison with Post-Processing In order to further reduce the errors, we fused the results from watershed method as illustrated in the method section, which can reduce the rand error dramatically while increasing the warping error unfortunately. This is reasonable since these two errors consider the segmentation evaluation metric from different aspects. The former one could penalize even slightly misplaced boundaries while the latter one disregards non-topological errors. Different from our simple postprocessing step, the SCI team post-processed the probability maps generated by the team DIVE and IDSIA with a sophisticated post-processing strategy (Liu et al. 2014). The post-processed results were evaluated under the team name of DIVE-SCI and IDSIA-SCI, respectively. Although it utilized a supervised way with hierarchical merge tree to achieve structure consistency, the performance is relatively inferior compared to ours, in which only an unsupervised watershed method was used for post-processing. In addition, our method also outperformed other methods with sophisticated post-processing techniques including optreeidsia and motif by a large margin. This further highlights the advantages of our method by exploring multi-level contextual information to generate probability maps with better likelihood. We released the probability maps including training and testing data of our method for enlightening further sophisticated post-processing strategies2 . Ablation Studies of Our Method In order to probe the performance gain of our proposed method, extensive ablation studies were conducted to investigate the role of each component. As illustrated in Table 1, compared with methods using single contextual information including CUMedVision-4(C3/C2/C1), the deep contextual model harnessing the multi-level contextual cues achieved significantly better performance on all the measurements. Furthermore, we compared the performance with (CUMedVision-4(with C)) and without (CUMedVision-4(w/o C)) the injection of auxiliary classifiers C , the rand error and pixel error from method with C were much smaller while the warping error with C is competitive compared to the method without C . This validated the efficacy of auxiliary classifiers with deep supervision for encouraging back-propagation of gradient flow. By fusing the results from the watershed method, we achieved the result with rand error 0.017334, warping error 0.000188, and pixel error 0.057953, which outperforms those from other teams by a large margin. To sum up, our method achieved the best performance on different evaluation measurements, which demonstrates the promising possibility for read-world applications. Although there is a tradeoff with respect to different evaluation metrics, the neuroanatomists can choose the desirable results based on the specific neurological requirements.

2 Results: http://appsrv.cse.cuhk.edu.hk\ %7Ehchen/research/2012isbi_seg.html



Computation Time Generally, it took about 0.4 seconds to process one test image with size 512 x 512 using the same configuration of training. Taking advantage of fully convolutional networks, the computation time is much less than previous studies (Ciresan et al. 2012; Wu 2015) utilizing a sliding window way, which caused a large number of redundant computations on neighboring pixels. With new imaging techniques producing much larger volumes (terabyte scale) that contain thousands of neurons and millions of synapses, the automatic methods with accurate and fast segmentation capabilities are of paramount importance. The fast speed and better accuracy of our method make it possible for large scale image analysis.



Conclusion

In this paper we have presented a deeply supervised contextual neural network for neuronal structure segmentation. By harnessing the multi-level contextual information from the deep hierarchical feature representations, it can have better discrimination and localization abilities, which are key to image segmentation related tasks. The injected auxiliary classifiers can help to encourage the back-propagation of gradient flow in training the deep neural network, thus further improve the segmentation performance. Extensive experiments on the public dataset of 2012 ISBI EM Segmentation Challenge corroborated the effectiveness of our method. We believe the promising results are a significant step towards automated reconstruction of the connectome. In addition, our approach is general and can be easily extended to other biomedical applications. Future work will include further refining the segmentation results with other sophisticated post-processing techniques (Uzunbas , Chen, and Metaxsas 2014; Liu et al. 2014; Nunez-Iglesias et al. 2013) and investigating on more biomedical applications. Acknowledgements This work is supported by National Basic Research Program of China, 973 Program (No. 2015CB351706) and a grant from Ministry of Science and Technology of the People's Republic of China under the Singapore-China 9th Joint Research Program (No. 2013DFG12900). The authors also gratefully thank the challenge organizers for helping the evaluation.



References

Bengio, Y.; Lamblin, P.; Popovici, D.; Larochelle, H.; et al. 2007. Greedy layer-wise training of deep networks. Advances in neural information processing systems 19:153. Beucher, S., and Lantuejoul, C. 1979. Use of watersheds in contour detection. In International Conference on Image Processing. Bock, D. D.; Lee, W.-C. A.; Kerlin, A. M.; Andermann, M. L.; Hood, G.; Wetzel, A. W.; Yurgenson, S.; Soucy, E. R.; Kim, H. S.; and Reid, R. C. 2011. Network anatomy and in vivo physiology of visual cortical neurons. Nature 471(7337):177-182. Cardona, A.; Saalfeld, S.; Preibisch, S.; Schmid, B.; Cheng, A.; Pulokas, J.; Tomancak, P.; and Hartenstein, V. 2010. An integrated micro-and macroarchitectural analysis of the



drosophila brain by computer-assisted serial section electron microscopy. PLoS biology 8(10):2564. Cardona, A.; Saalfeld, S.; Schindelin, J.; Arganda-Carreras, I.; Preibisch, S.; Longair, M.; Tomancak, P.; Hartenstein, V.; and Douglas, R. J. 2012. Trakem2 software for neural circuit reconstruction. PloS one 7(6):e38011. Chen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and Yuille, A. L. 2014. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062. Chen, H.; Shen, C.; Qin, J.; Ni, D.; Shi, L.; Cheng, J. C.; and Heng, P.-A. 2015. Automatic localization and identification of vertebrae in spine ct via a joint learning model with deep neural networks. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015. Springer. 515-522. Ciresan, D.; Giusti, A.; Gambardella, L. M.; and Schmidhuber, J. 2012. Deep neural networks segment neuronal membranes in electron microscopy images. In Advances in neural information processing systems, 2843-2851. Ignacio, A.-C.; Sebastian, S.; Albert, C.; and Johannes, S. 2012. 2012 ISBI Challenge: Segmentation of neuronal structures in EM stacks. http://brainiac2.mit.edu/ isbi_challenge/. Jain, V.; Bollmann, B.; Richardson, M.; Berger, D. R.; Helmstaedter, M. N.; Briggman, K. L.; Denk, W.; Bowden, J. B.; Mendenhall, J. M.; Abraham, W. C.; et al. 2010. Boundary learning by optimization with topological constraints. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 2488-2495. IEEE. Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.; Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093. Jurrus, E.; Paiva, A. R.; Watanabe, S.; Anderson, J. R.; Jones, B. W.; Whitaker, R. T.; Jorgensen, E. M.; Marc, R. E.; and Tasdizen, T. 2010. Detection of neuron membranes in electron microscopy images using a serial neural network architecture. Medical image analysis 14(6):770-783. Kaynig, V.; Fuchs, T. J.; and Buhmann, J. M. 2010. Geometrical consistent 3d tracing of neuronal processes in sstem data. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2010. Springer. 209-216. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097-1105. Kumar, R.; V azquez-Reina, A.; and Pfister, H. 2010. Radonlike features and their application to connectomics. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, 186-193. IEEE. Laptev, D.; Vezhnevets, A.; Dwivedi, S.; and Buhmann, J. M. 2012. Anisotropic sstem image segmentation using dense correspondence across sections. In Medical Image



Computing and Computer-Assisted Intervention-MICCAI 2012. Springer. 323-330. Lee, C.-Y.; Xie, S.; Gallagher, P.; Zhang, Z.; and Tu, Z. 2014. Deeply-supervised nets. arXiv preprint arXiv:1409.5185. Liu, T.; Jones, C.; Seyedhosseini, M.; and Tasdizen, T. 2014. A modular hierarchical approach to 3d electron microscopy image segmentation. Journal of neuroscience methods 226:88-102. Long, J.; Shelhamer, E.; and Darrell, T. 2014. Fully convolutional networks for semantic segmentation. arXiv preprint arXiv:1411.4038. Mishchenko, Y. 2009. Automation of 3d reconstruction of neural tissue from large volume of conventional serial section transmission electron micrographs. Journal of neuroscience methods 176(2):276-289. Nunez-Iglesias, J.; Kennedy, R.; Parag, T.; Shi, J.; Chklovskii, D. B.; and Zuo, X.-N. 2013. Machine learning of hierarchical clustering to segment 2d and 3d images. PloS one 8(8):08. Rand, W. M. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association 66(336):846-850. Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Convolutional networks for biomedical image segmentation. arXiv preprint arXiv:1505.04597. Seung, H. S. 2011. Neuroscience: towards functional connectomics. Nature 471(7337):170-172. Simonyan, K., and Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Sporns, O.; Tononi, G.; and K otter, R. 2005. The human connectome: a structural description of the human brain. PLoS Comput Biol 1(4):e42. Stollenga, M. F.; Byeon, W.; Liwicki, M.; and Schmidhuber, J. 2015. Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation. arXiv preprint arXiv:1506.07452. Uzunbas , M. G.; Chen, C.; and Metaxsas, D. 2014. Optree: a learning-based adaptive watershed algorithm for neuron segmentation. In Medical Image Computing and ComputerAssisted Intervention-MICCAI 2014. Springer. 97-105. Wang, L.; Lee, C.-Y.; Tu, Z.; and Lazebnik, S. 2015. Training deeper convolutional networks with deep supervision. arXiv preprint arXiv:1505.02496. White, J.; Southgate, E.; Thomson, J.; and Brenner, S. 1986. The structure of the nervous system of the nematode caenorhabditis elegans: the mind of a worm. Phil. Trans. R. Soc. Lond 314:1-340. Wu, X. 2015. An iterative convolutional neural network algorithm improves electron microscopy image segmentation. arXiv preprint arXiv:1506.05849.



Journal of Biogeography (J. Biogeogr.) (2006) 33, 2120-2135



ORIGINAL ARTICLE



Recent advance of white spruce (Picea glauca) in the coastal tundra of the  bec, eastern shore of Hudson Bay (Que Canada)

Marco Caccianiga* and Serge Payette



NSERC Northern Research Chair, Centre tudes nordiques, Universite  Laval, Que bec d'e City, Canada G1K 7P4



ABSTRACT



Aim The species-specific response of tree-line species to climatic forcing is a crucial topic in modelling climate-driven ecosystem dynamics. In northern  bec, Canada, black spruce (Picea mariana) is the dominant species at the tree Que line, but white spruce (Picea glauca) also occurs along the maritime coast of Hudson Bay, and is expanding along the coast and on lands that have recently emerged because of isostatic uplift. Here we outline the present distribution, structure, dynamics and recent spread of white spruce from the tree line up to its northernmost position in the shrub tundra along the Hudson Bay coast. We aimed to obtain a minimum date of the arrival of the species in the area and to evaluate its dynamics relative to recent climate changes. Location White spruce populations and individuals were sampled along a latitudinal transect from the tree line to the northernmost individual in the shrub tundra along the Hudson Bay coast and in the Nastapoka archipelago in northern  bec and Nunavut, Canada (5606-5632 N). Que Methods White spruce populations were mapped, and the position, dimension, growth form and origin (seed or layering) of every individual recorded. Tree-ring analyses of living and dead trees allowed an estimation of the population structure, past recruitment, growth trends and growth rate of the species. A macrofossil analysis was performed of the organic horizon of the northernmost white spruce stands and individuals. Radiocarbon dates of white spruce remains and organic matter were obtained. The rate of isostatic uplift was assessed by radiocarbon dating of drifted wood fragments. Results The first recorded establishment of white spruce was almost synchronous at all sites and occurred around ad 1660. Spruce recruitment was rather continuous at the tree line, while it showed a gap in the northern shrub tundra during the first decades of the 19th century. A vigorous, recent establishment of seedlings was observed in the shrub tundra; only wind-exposed, low krummholz (stunted individuals) did not show any sexual regeneration. A period of suppressed growth occurred from the 1810s to the 1850s in most sites. A growth increase was evident from the second half of the 19th century and peaked in the 1880s and the 20th century. A shift from stunted to tree growth form has occurred since the mid19th century. No sample associated with white spruce remains gave a date older than 300 14C years bp [calibrated age (cal.) ad 1430-1690]. Main conclusions White spruce probably arrived recently in the coastal tundra of Hudson Bay due to a delayed post-glacial spread. The arrival of the species probably occurred during the Little Ice Age. The established individuals survived by layering during unfavourable periods, but acted as nuclei for sexual recruitment almost continuously, except in the northernmost and most exposed sites. Warmer periods were marked by strong seedling recruitment and



*Correspondence: Marco Caccianiga, Dipartimento di Biologia, Sezione Botanica  di Milano, Via Celoria Sistematica, Universita 26, 20133 Milano, Italy. E-mail: marco.caccianiga@cen.ulaval.ca Present address: Dipartimento di Biologia,  di Sezione Botanica Sistematica, Universita Milano, Via Celoria 26, 20133 Milano, Italy.



2120



www.blackwellpublishing.com/jbi doi:10.1111/j.1365-2699.2006.01563.x



 2006 The Authors Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra



a shift to tree growth form. Unlike white spruce, black spruce showed no evidence of an ongoing change in growth form and sexual recruitment. Ecological requirements and recent history of tree-line species should be taken into account in order to understand the present dynamics of high-latitude ecosystems. Keywords Forest tundra, isostatic uplift, macrofossil analysis, Picea glauca, shrub tundra,  bec, tree colonization, tree line, tree-ring analysis, white spruce. subarctic Que



INTRODUCTION The position of the arctic tree line and its ecological context has always been considered a major indicator of past and present climatic conditions (Lamb, 1985; Cwynar & Spear, 1991; Payette & Lavoie, 1994; Gamache & Payette, 2005; Lloyd, 2005). The species-specific response of tree-line species to climatic forcing is a crucial topic in modelling the pathways of ecosystem dynamics linked to changing climatic conditions, and has sometimes been overlooked. Different ecological requirements, life-history traits and recent history may induce divergent responses in closely related species living in the same ecological context, such as the two dominant boreal spruce species in North America, white spruce, Picea glauca (Moench) Voss, and black spruce, Picea mariana (Mill.) B.S.P. In eastern Canada, white spruce is particularly abundant along the maritime coasts, while black spruce is the dominant species inland and reaches the northernmost latitudes (Payette, 1993). This pattern may be due to a higher tolerance by white spruce to humid weather conditions, and to a lower recurrence  gin, 1999). of fire on the coast (Payette, 1993; Ricard & Be White spruce reaches the tree line (the limit of normal spruce trees generally  2.5 m tall) in two different areas, along the Hudson Bay coast and in northern Labrador. This distribution is probably associated with post-glacial spreading around the slowly decaying ice sheet, which may still be in progress (Richard et al., 1982; Ritchie & MacDonald, 1986; Dyke & Prest, 1987; Ritchie, 1987). In northern Labrador, white spruce reached its northern limit between 4500 and 3800 bp (Payette, 1993) with a dispersal route located mainly along the coast (Ritchie & MacDonald, 1986; Payette, 1993). No data are available about the date of arrival of the species along the eastern Hudson Bay coast: this population is probably the result of an independent dispersal route of problematic origin (Gajewski & Garralla, 1992; Payette, 1993; Gajewski et al., 1996). In this area, white spruce responded to recent climate warming and fire occurrence with an increase in population density, rather than a shift in the altitudinal tree limit (Payette & Filion, 1985). A similar pattern has been described in central and western Canada (Scott et al., 1987; Szeicz & MacDonald, 1995); in Alaska this pattern was often accompanied by a forest-to-tundra shift in both altitude and latitude (Suarez et al., 1999; Lloyd & Fastie, 2002, 2003; Lloyd, 2005).



Contrasting patterns of white spruce dynamics have been observed in interior and coastal northern Labrador (S.P., unpublished data). White spruce is expanding along the maritime coasts (Payette, 1993), but no progression of the species inland has been observed so far. The species is currently colonizing lands  gin et al., 1993; recently emerged because of isostatic uplift (Be  gin, 1999), although with several local regeneration Ricard & Be failures due to unfavourable soil conditions (Marr, 1948). In many forest-tundra sites, white spruce shows normal tree growth and is surrounded by stunted black spruce clones (Payette & Filion, 1985). At its northern limit of distribution across North America, white spruce shows a contrasting response to warmer temperatures, as its establishment may be limited by both the negative influence of lichens on seed regeneration (Scott et al., 1987; Scott & Hansell, 2002); and drought stress induced by warmer temperatures, as reported in Alaska (Barber et al., 2000; Lloyd & Fastie, 2003). Whereas it has been shown that white spruce density increased markedly south of the tree line during the 20th century (Payette & Filion, 1985), it is not known when the species arrived and expanded in the region corresponding to the area between its current position at the tree line and the northernmost position of the species in the coastal tundra. The main goals of this paper are to outline the present distribution, structure, dynamics and recent spread of white spruce from the tree line up to its northernmost position in the shrub tundra along the Hudson Bay coast. In doing so, it is possible to obtain a minimum date of the arrival of the species in the area, and to evaluate its dynamics relative to recent changes in climate. To fulfil our goals we have used a retrospective approach based on tree-ring analysis of living, dead and subfossil trees, supplemented by a macrofossil analysis of the organic horizon of the northernmost white spruce stands and individuals, both at the tree line and in the shrub tundra. METHODS Study area The study was carried out along the east shore of Hudson Bay  langer islands, two islands belonging to the and on Ross and Be  bec and Nunavut Nastapoka Archipelago, in northern Que 2121



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



M. Caccianiga and S. Payette



Inukjuak

58N



TUNDRA



T+FT

HUDSON BAY

Umiujaq

56



FOREST-TUNDRA



Kuujjuarapik



LICHEN WOODLAND

JAMES BAY



CLOSED FOREST



Figure 1 Location of the study area and the main ecological domains. Dashed line, northern range limit of white spruce; dotted line, northern range limit of black spruce (both from Payette, 1993).



(Canada), between 5606 and 5632 N (Fig. 1). The area is part of the Canadian Precambrian Shield, with uniform, lowelevated morphology. Hudson Bay coast is subjected to a fast isostatic rebound of c. 1-1.3 m per century (Allard & Tremblay, 1983). The study area corresponds to the northern limit of the forest-tundra and the adjacent treeless tundra populated by scattered white spruce individuals and black spruce krummholz. The forest-tundra consists of small forest stands in the lowlands and on protected slopes, and extensive tundra on the hills. The Hudson Bay coast belongs to the oceanic domain of the forest-tundra characterized by the occurrence of white spruce (Payette, 1983, 1993). The other tree species occurring in the study area is black spruce, which becomes dominant in the interior, where extensive forests are distributed in the lowlands. The tree limit dips southwards approaching the Hudson Bay and runs almost parallel to the coast because of the negative influence of the cold water body of Hudson Bay (Lescop-Sinclair & Payette, 1995). Site description Sampling was carried out during summer 2003 and 2004, along a latitudinal transect from forests and open groups of trees at the tree line in the forest-tundra to isolated krummholz 2122



(deformed spruce trees generally  2.5 m tall) in the tundra, up to the northernmost white spruce individual. The distribution limits were assessed by an extensive helicopter survey. Sixteen sites were sampled (Fig. 2; Table 1), each consisting of a white spruce population with dimensions ranging from a single spruce clone to a few tens of individuals. A black spruce clone (site EN1) was also sampled on Ross Island. Some isolated individuals and small populations within the range limit were not sampled. Black spruce individuals were sampled when present within a given area occupied by a white spruce population. At each site, a detailed description of every tree was given. Stem height, basal diameter, growth form, occurrence of cones, scars, dead branches and frost damage on needles were recorded. The snowpack level was inferred based on the position of reddish and yellowish needles, stem anomalies (e.g. base of branchless stems above a dense mass of living foliage) and mean height of shrub cover (mostly dwarf birch, Betula glandulosa Michx.). This evidence usually indicated the multiyear thickness of snow cover (Lavoie & Payette, 1992). The diameter and circumference of spruce clones and the number of supranival (above snow) stems were also recorded. For the largest populations consisting of several individuals (sites EB1, EB9 and EB21), the position of each tree, each



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra



5630' N



Umiujaq EB20 EB19 EB10-13 EB2 EB3-4 EB5



5630' N



Hudson Bay



EB1



5615' Ross Island



EB7 EB8-EN1



5615'



Lac Guillaume-Delisle

EB9



Belanger Island



EB 21 0 10 km 7700' W 7630' 7600'



Figure 2 Sampling sites along the Hudson Bay coast and on the Nastapoka Islands.



Table 1 Details of sites sampled

Maximum spruce height (cm) 400 350 300 160 230 220 650 150 100 90 150 120 500 100 Snow thickness (cm) 40-250 40-110 40-120 50 < 40 70-110 70-110 90-230 < 50 < 50 < 50 < 50 70 70



Site EB1 EB2 EB3 EB4 EB5 EB7 EB8 EB9 EB10 EB11 EB12 EB13 EB19 EB20 EB21 EN1



Latitude (N) 5623 5630 5630 5630 5628 5614 5614 5609 5631 5632 5633 5634 5631 5632 5607 5614



Longitude (W) 7631 7632 7632 7632 7631 7645 7645 7644 7632 7632 7632 7632 7632 7632 7646 7645



Altitude (m a.s.l.) 74 4.8 7 7 5 20 24 60 10 8 9 9 5.2 6.5 3.5 12



Life form High krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Tree-high krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Tree Krummholz



Cones X X X



Seedlings X X X



Tree establishment 1661 1693 1663 1763 1771 1756 1959 1666 Not dated 1780 1751 1719 1660 1778 1897 Not sampled



X



X



X X X X



X



seedling and each dead stump was mapped with an infrared theodolite (0.001-m precision; Leica T1010). In site EB1, the largest white spruce stand, mapping was performed along a 10m-wide belt transect across the whole population. At site EB21,  langer Island, the rate of isostatic uplift was located on Be assessed by radiocarbon dating of drifted wood fragments distributed along a linear altitudinal gradient from the sea level up to 10.75 m a.s.l. We assumed the influence of storm surge, waves and sea ice as constant through the investigated time



span. Dating was performed at the 14C laboratory of the Centre  tudes nordiques (Universite  Laval, Que  bec City, Canada). d'e The measured radiocarbon age was calibrated using calib 4.4 (Stuiver et al., 2003). Age structure and tree-ring analysis At each site a core was taken from every living tree, using a Pressler increment borer at the base of the trunk. 2123



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



M. Caccianiga and S. Payette Cross-sections were taken from dead trees at the lowest position possible along the trunk. In the whole area, 195 trees were sampled. Trees that showed rotten wood were discarded and sometimes more than one sample was taken from a tree. Samples showing reaction wood were not measured, but were taken into account in the evaluation of population dynamics. Samples were air-dried and finely sanded. Annual tree-ring width was measured with a Velmex micrometer at 40* with a precision of 0.01 mm. Two radii were measured for each crosssection. Cross-dating of samples was performed using light rings (rings with exceptionally few latewood cells; Filion et al., 1986). High-frequency (> 75%) light-ring years occurred in 1686, 1784, 1816, 1817, 1853, 1912, 1956, 1965 and 1969, respectively. A total of 177 seedlings and saplings too small to be cored were dated by counting the number of internodes (whorls) along the stem. A tree-ring width chronology was built for each site. In two sites (sites EB1 and EB9) where a large number of individuals were available, highly correlated individual curves (r > 0.6) were retained and the subsample signal strength (SSS; Wigley et al., 1984) was calculated. This test defines the minimum number of samples representative of the whole series and consequently the significant length of the chronology. Only the part of the series with a SSS value > 0.85 was considered representative. In the remaining sites, consisting of isolated clones (EB5, EB7, EB8, EB11-13, EB19 and EB20) or very small populations (EB2, EB3 and EB21), the mean tree-ring width for each site was calculated after discarding samples with rotten and reaction wood and poor correlation values (r < 0.4); the SSS was not calculated for these sites. Tree-ring width curves were considered instead of index chronologies to allow better comparison between growth rates at the different sites. Statistics and correlation values were calculated using the program cofecha (Holmes, 1983). Long-term growth trends were evaluated by linear regression, calculated on 50-year periods: 1659-1700, 1701-50, 1751-1800, 1801-50, 1851- 1900, 1901-50, 1951-2003 (Lloyd & Fastie, 2002). Macrofossil analysis Fourteen soil monoliths were recovered from 10 sites. The soil samples consisted of the topmost organic (O and A) horizons accumulated over coarse, well drained mineral sediments. These soil monoliths were used for their strategic position beneath spruce clones and individuals and the centre of spruce populations, and because of the absence of peat deposits in the study area. The soil samples were kept in the freezer before analysis. Soil subsamples were taken at every centimetre, treated with 5% KOH, and searched for spruce macrofossils. Spruce needles from the lowest level were extracted using 850- and 425-lm mesh sieves and identified based on resin duct morphology, as suggested by Weng & Jackson (2000). Resin ducts were observed longitudinally where possible. When this observation was not possible (as for charred needles), three cross-sections were performed to assess the continuity of the resin ducts. If two continuous resin ducts 2124 with constant diameter were observed but the needle was not complete, the identification referred to `Picea mariana uncertain'. If no resin ducts could be observed and the needle was not complete, the identification referred to `Picea glauca uncertain'. If no evidence could be observed, the sample was classified as unidentifiable. Where available, 100 needles were identified for each 1-cm slice; otherwise all available needles were analysed. The identification was performed upwards along each soil monolith until recent, well preserved needles corresponding to the present species composition at the soil surface. For site EB3, the identification of spruce needles was performed on the whole soil profile, as both spruce species occurred at the same time, and as a charcoal layer with identifiable charred needles could be observed. The whole plant macrofossil assemblage was analysed in the soil monolith of EB3. Plant macrofossils were extracted using 850-, 425-, 180- and 125-lm mesh sieves and identified under a stereomicroscope. The lowest needles and charcoal fragments (when present) were sampled and dated by 14C accelerator mass spectrometry (AMS) dating at Beta Analytic, Inc. (Miami, FL, USA). Charred needles were chosen for the dating of charcoal layers in order to date spruce that were probably alive when the fire occurred. Where no charcoal or needles were available for AMS dating, organic matter was taken from the base of the profile and dated by conventional radiocarbon  tudes nordiques. dating at the 14C laboratory of the Centre d'e RESULTS Site description The northernmost white spruce was recorded near the village of Umiujaq (site EB20, 5632 N; Fig. 2). Along the coast, the tree line occurred between 5620 N and 5623 N (EB1). High krummholz were observed until 5624 N. In the Nastapoka  langer Island (site EB9, Islands, the tree line is located on Be 5609 N), whereas only scattered, stunted spruce were observed on Ross Island (EB7 and EB8). Seedlings were recorded until the latitude of 5630 N (EB2), but mature cones were observed on the ground under the canopy of the northernmost individual at Umiujaq.



Hudson Bay coast

Site EB1 was located at the tree line and corresponded to an open spruce population comprising 150-200 spruce individuals. The largest individuals were c. 4 m tall. All showed supranival stems with damaged foliage (red needles, dead twigs) and, in a few cases, dead stems above the snowpack. The inferred snow level ranged from 40 to c. 250 cm at the centre of the tree population, and was strongly influenced by the presence of spruce. A vigorous establishment of seedlings was observed on one side of the stand on moist ground (Fig. 3). The directional establishment could be also due to the predominantly westerly wind blowing from the sea (LescopSinclair & Payette, 1995). All living trees were < 100 years old.



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra



Figure 3 Map of the white spruce population of site EB1 (Hudson Bay coast). Data on stem establishment in 50-year classes, except after 1950. The average snow level, estimated from tree growth forms and patterns of foliage damage, is also indicated.



Some trees that died in the 1980s and 1990s were c. 100 years old; two others were much older: one of these could not be dated because of rotten wood, the other was established in 1661 and died in the 1920s or early 1930s. North of the tree line, a small clone with no regeneration and established in the 18th century was sampled in an open, wind-exposed tundra site (EB5). Located 3.5 km north of EB5, site EB3 was occupied by a large clone and three dead stumps. The older stem of the clone, still alive, was established in 1727; two of the dead stumps were established in 1663 and 1693, respectively, and died after 1785 and 1825, respectively. Eleven seedlings and saplings were recorded, including eight spruce < 10 years old, one spruce > 20 years old, and two spruce c. 30 years old. A dead clone established in the 18th century, which died in the 1930s, was also sampled nearby (EB4). The northernmost limit of seed regeneration was located in site EB2. The site was occupied by a clone 14 m in diameter and three smaller clones nearby. Three of the 11 stems of the main clone were cut by Inuit. The older stem started to grow in 1693, but a bigger and probably older stem could not be dated because of rotten wood. Ten seedlings and saplings < 20 years old, originated by sexual reproduction, were also located close to the main clone. About 500 m north of site EB2, sites EB10-13 were composed of stunted clones growing in wind-exposed tundra conditions. The EB10 clone was dead, whereas the others were in a regressive stage with dead wood in the central part of the clone; the sampled stems died in the 1880s and 1920s. Two clones bore mature and open cones, but no seedling was found. Clones EB11, EB12 and EB13 were all established during the 18th century, while EB10 could not be dated. Sites EB19 and EB20 corresponded to the northernmost sites. EB19 was occupied by a living individual established in



1936 and two dead stumps established in 1660 and before 1666, respectively. EB20, the northernmost white spruce individual along the Hudson Bay coast, was a clone consisting of four stems, two alive and developed in the 20th century, two dead and established in the 18th century.



Nastapoka Islands

 langer Island and corresponded to Site EB9 was located on Be the tree line on the Nastapoka Islands. It was composed of a large white spruce population located in a flat area with longlasting snow cover, and surrounded by stunted black spruce clones. The white spruce population consisted of about 15 living tree individuals, 22 clones, 71 seedlings and 65 dead stumps (Fig. 4). Tree individuals were single-stemmed trees, originated from seeds, with no evidence of vegetative reproduction (layering), 3-6.5 m tall. Clones consisted of groups of stems originated by layering; they were up to 7.5 m long and 6 m wide, more than 20 m in circumference and including up to 20 stems. The inferred snow level in the site varied from 130 to 200 cm within the clones, and from 30 to 110 cm among the sparse tree individuals. The overall age structure (Fig. 5) showed that living trees were all < 100 years old, whereas most of the stems from the clones were slightly older (established at the end of the 19th century), except for one established in 1711, which died in 1992. Most dead individuals showed a stunted growth form: they were established in the 18th century and died during the 20th century. Four were established in the 17th century, with the oldest established in 1666. The oldest individuals were mostly located at the centre of the stand (Fig. 4). The changing growth forms among the different spruce generations are shown through the relationship between age and diameter (Fig. 6). Dead stems showed a 2125



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



M. Caccianiga and S. Payette



Figure 4 Map of the white spruce popula langer Island). Open tion of site EB9 (Be circles indicate the position and dimension of living individuals and clones (seedlings excluded). Symbols for date of stem establishment and average snow level as in Fig. 3.



45 40 35



lndividuals (n)



30 25 20 15 10 5 0

0 0 0 0 0 93 19 0 60 -1 97 19 0 80 -1 99 20 0 00 -2 01 0 0 0 0 0 0 0 0 75 77 79 87 0 67 69 71 73 81 83 85 -1 -1 -1 -1 89 -1 -1 -1 -1 -1 -1 -1 -1 -1 91 40 60 80 60



Years



Figure 5 Age structure of the white spruce  langer Island) population at site EB9 (Be (establishment dates in 10-year classes). White bars, dead spruce; black bars, living spruce.



60



80



00



20



00



20



40



80



00



17



17



17



18



16



16



17



17



18



18



18



18



19



19



20



-1



markedly lower growth rate than living trees and clones. Only two individuals established in 1899 and 1929, respectively, and thus belonging to a younger generation, showed a growth rate close to that of living individuals. Stunted black spruce clones nearby were up to 60 m in circumference, corresponding to an inferred age of c. 1000 years (Laberge et al., 2000).  langer Island and is Site EB21 is situated along the shores of Be occupied by a young spruce population colonizing the emergent coast due to rapid isostatic uplift; the population consisted of 45 tree individuals and 35 seedlings (Fig. 7). The oldest spruce was 106 years old. The rate of isostatic uplift assessed by the radiocarbon dates of drifted wood (Fig. 8) indicates that the area 2126



occupied by the oldest spruce trees emerged from the sea between cal. ad 1800-1940, thus suggesting a fast colonization by white spruce on newly emerged substrates. Moreover, seedlings and saplings almost reached the present tide line (Fig. 7). Spruce could be observed only on the recently emerged substrates, while the uppermost terrains representing older shorelines were devoid of white spruce individuals. Only scattered black spruce krummholz occupied the surrounding hills. The position of white spruce seedlings indicated that the main spruce colonization of the area was occurring seaward (Fig. 7). Only one single stunted black spruce seedling was observed, at 7.5 m above the tide line.



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra

50 45 40 35 30 25 20 15 10 5 0 0 50 100 150 200



12 10

Altitude (m)

Living stems Dead stems



Diameter (cm)



8 6 4 2 0



250



300



0



100



200



Age (years)



300 400 500 600 Age (cal years BP)



700



800



Figure 6 Age-diameter relationship among different spruce  langer Island). Dead tree individuals generations in site EB9 (Be indicated by arrows were established in the late 19th-20th century.



 langer Island (site Figure 8 Curve of isostatic rebound on Be EB21) as assessed by radiocarbon dating of drifted wood fragments. Error bars indicate 2r calibrated age (age intervals including the 95% probability distribution).



 langer Island, was occupied by open Ross Island, north of Be tundra with scattered spruce clones. Sites EB7 and EB8 corresponded to two white spruce clones established around 1750 and in the 20th century, respectively. The black spruce clone of site EN1 could not be dated, but its circumference reached 15 m, corresponding to an inferred age of c. 250- 350 years (Laberge et al., 2000). Age structure and tree-ring analysis The oldest living stem was 276 years old (site EB3). The oldest  langer individual was a 281-year-old tree established on Be Island (EB9) in 1711, which died in 1992. No tree was present before ad 1660, and the first establishment of white spruce was almost synchronous at all sites (Table 1). The number of individuals established per decade (Fig. 9) was almost continuous in the southern part of the study area, while it showed



a marked gap in the northern part during the first decades of the 19th century. Spruce establishment occurred throughout the whole area up to the northernmost latitude during the 18th century (except the 1740s and 1750s), particularly in the 1780s and 1790s, and in the 1660s, when the first occurrence of spruce was recorded. An overall increase of spruce establishment occurred during the 20th century, followed by a decrease in the 1970s and another increase in the 1980s and 1990s. However, the survival rate of seedlings and saplings should be taken into account to make a realistic comparison with the recruitment of the older cohorts (Fig. 9). Only one black spruce seedling has been observed in the whole study area, at  langer Island. A high mortality rate was site EB21 on Be observed at the end of the 20th century in the southernmost sites; at the species limit, mortality occurred all through the 20th century (Fig. 9).



Figure 7 Map of white spruce population of  langer Island). Symbols for date site EB21 (Be of establishment as in Fig. 3. Dashed line, uppermost limit of present drifted wood; solid line, present tide line. The position of ancient drifted wood fragments and their respective 14C dates and altitude above sea level are also reported.

Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



2127



M. Caccianiga and S. Payette

(a)

60 >5630' 5625'-5630' 50 5620'-5625' 5610'-5620' 40 <5610'



lndividuals (n)



30



20



10



0



(b) lndividuals (n)



1660 1680 1700 1720 1740 1760 1780 1800 1820 1840 1860 1880 1900 1920 1940 1960 1980 2000



0 10 20



Figure 9 Number of spruce established (a) and dead (b) per decade along the latitudinal gradient in the study area. Individuals established by seed and by layering are both included.



2 (mm) 1 0 1 (mm) 0



EB20

1 (mm) 0



EB19



EB13

1



EB12

1 (mm) 0 1 (mm) 0 2 (mm) 1 0



(mm)



EB11 EB2



1 (mm) 0



EB3 1 EB5

0 (mm)



EB1

0 2



EB9

2 (mm) 1 0



1 (mm) 0



EB21



1655



1705



1755



1805



1855



1905



1955



2005



Figure 10 Ring-width series. Curves are arranged along the latitudinal gradient, from tree and high krummholz stands (EB21-EB9 and EB1 respectively, bottom) to the northernmost individual (EB20, top). The interrupted period of the curve of sites EB5 and EB19 is not covered by spruce samples. Grey-shaded areas in the chronologies of sites EB1 and EB9 indicate the part with SSS < 0.85, where samples are not sufficiently abundant to be representative of the whole population.



Tree-ring width series from all sites were ordered along a latitudinal gradient (Fig. 10), except for sites EB4, EB7 and EB8, which were excluded from the analysis. Mean annual tree-ring widths were calculated for sites EB2, EB3, EB5, EB11, EB12, EB13, EB19, EB20 and EB21. The two radii from site EB11 were retained, even if poorly correlated (r 1/4 0.27), as the 2128



low r value was probably due to their short period of overlapping. Mean tree-ring width ranged from 0.14 mm (EB11) to 1.04 mm (EB21) (Table 2) with an overall decreasing trend with latitude, except for the low values of the exposed krummholz of sites EB5 and EB11-13 and the high value of the northernmost individual from site EB20, consisting of



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra

Table 2 Statistics of the tree-ring width chronologies. Sites are arranged along the latitudinal gradient.

Trees (n) 3 30 28 2 3 4 1 2 1 2 3 Radii (n) 5 47 34 3 6 6 2 4 2 4 4 Mean ring width (mm) 1.04 0.70 0.64 0.25 0.33 0.31 0.14 0.20 0.25 0.32 0.54 Mean r 0.39 0.60 0.66 0.72 0.57 0.46 0.27 0.51 0.66 0.55 0.45 Time series 1899-2003 1669-2002 1685-2002 1770-2002 (gap 1859-90) 1795-1986 1663-2002 1791-1888 1753-1925 1713-1885 1659-2003 (gap 1915-36) 1778-2003 Significant time series (SSS > 0.85)



Site EB21 EB9 EB1 EB5 EB2 EB3 EB11 EB12 EB13 EB19 EB20



SD 0.483 0.320 0.285 0.123 0.117 0.137 0.054 0.109 0.134 0.199 0.238



Sensitivity 0.27 0.19 0.29 0.25 0.20 0.21 0.21 0.24 0.23 0.25 0.24



Autocorrelation 0.59 0.80 0.75 0.85 0.69 0.80 0.61 0.79 0.82 0.84 0.74



1713-2002 1885-2002



Table 3 Growth trends from 1659 to present



Site EB21 EB9 EB1 EB5 EB3 EB2 EB11 EB12 EB13 EB19 EB20



1659-1700



1701-50



1751-1800



1801-50



1851-1900



1901-50 )0.113 0.091 0.124 )0.141 )0.675 )0.534 0.233 )0.727 0.051



1951-present )0.300 )0.060 )0.754 )0.200 )0.724 )0.160



0.505 )0.355 0.307



)0.699 )0.621 )0.155



0.244



0.081



0.440 0.461 )0.452 0.096 0.418 )0.580 )0.447 0.042 )0.481 )0.067



)0.215 )0.776 )0.308 )0.011 )0.618 )0.320 )0.780 )0.143 0.591 0.284



0.842 0.726 0.77 0.036 0.391 )0.192 0.474 0.859 0.740 )0.507



0.813 0.470



Standardized regression coefficients (b) of 50-year time periods are reported for each site. Significant values (P < 0.05) are in bold type.



relatively young stems in a more protected site. Site EB21 was occupied by young individuals showing fast growth. The overall growth trend of this site was close to that of the adjacent site EB9. Fast growth was observed in the late 17th and the late 18th centuries, when an overall positive growth trend could be observed (Table 3). Periods of suppressed growth occurred in the early 18th century and from the 1810s (starting with an abrupt decrease in 1816) to the 1850s in almost every site. In site EB20 recovery had already started in the late 1820s, and the overall growth trend of the period 1801-50 is positive (Table 3). During this period, spruce growth of the most exposed krummholz (sites EB5 and EB11-13) was reduced almost to zero, with slow or no recovery. A ring-width increase was evident everywhere in the second half of the 19th century, except in site EB20 where the maximum growth was reached in the 1850s. Ring width peaked in the 1880s in site EB3 and in the 20th century in sites EB9, EB1 and EB19, the latter showing a severe growth suppression in the 1940s and 1950s. The period after 1951 was marked by a positive growth trend in the northernmost sites (EB19 and EB20); elsewhere a stable or



even significant downward trend (EB1 and EB3) was apparent (Table 3). Macrofossil analysis Spruce needles were recorded throughout the soil profiles in seven out of 14 monoliths (sites EB1, EB2, EB3, EB7, EB9, EB20, EN1). Their identification reflected the present species composition (see Appendix S1 in Supplementary Material) except for site EB3, where both black spruce and white spruce needles were observed (Fig. 11). Black spruce needles were slightly more frequent in the lowest layers. Charcoal fragments were found only in site EB3 and consisted of charred wood and spruce needles. Charred needles were mostly white spruce needles, although a few black spruce needles were recorded (Fig. 11). Macrofossils from site EB3 (Fig. 11) showed the alternation of levels rich in spruce remains with levels of plant remains from open tundra conditions such as B. glandulosa, Empetrum nigrum L. subsp. hermaphroditum (Lange ex  cher, and Vaccinium spp. Remains of Cornus Hagerup) Bo spp. (possibly Cornus suecica L.) occur in the lowest layers. 2129



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



2130



M. Caccianiga and S. Payette



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Figure 11 Macrofossil concentration diagram of site EB3. Black bars, number of macrofossils per cm3; open bars, percentage of Picea mariana needles calculated on the total number of needles examined in each 1-cm slice. Criteria for assessing uncertain attribution are given under Methods.



Advance of white spruce in the coastal tundra

Table 4 Radiocarbon dates from the sampled sites along the Hudson Bay coast and on the Nastapoka Islands

Laboratory number UL-2813 UL-2825 Beta-190595 Beta-190597 Beta-190599 Beta-190598 Beta-190594 UL-2777 UL-2778 UL-2779 Beta-199326 Beta-190596 UL-2849 UL-2850 UL-2851 UL-2852 UL-2855 UL-2859 UL-2860 UL-2864 UL-2865 UL 2866 UL 2868 Sample number EB1(43)-21-20 EB2 EB3 Bottom EB3 Charcoal EB3-10cm EB7-9-8cm EB9-2-8-7cm EB938-45-12-11 EB938-45-7-6 EB938-45-6-5 EB20-10-9 EN1-11-10cm Bel-2 Bel-9 Bel-5 Bel-6 Bel-3 Bel-4 Bel-1a Bel-1b Bel-7 Bel-10 Bel-8 Depth (cm) 20-21 22-23 23-24 17-18 10-11 8-9 7-8 11-12 6-7 5-6 9-10 10-11 Age (14C years bp) 280  80 300  90 110  40 140  40 90  40 104.1 124.1 1060 110 10 103.1 400 300 580 400 370 150 270 110 140 460 730 380                   0.5 pMC* 0.6 pMC* 90 90 90 0.4 pMC* 40 60 60 60 60 40 50 50 40 40 40 40 Age (calibrated years) 2r ad 1440-1690; ad 1720-1810; 1840-80; 1920-50 ad 1430-1690; ad 1720-1810; 1840-80; 1920-50 ad 1670-1780; ad 1800-1950 ad 1660-1950 ad 1680-1770; ad 1800-1940; ad 1950-60 Modern Modern ad 770-1190 ad 1660-1950 ad 1670-1780; ad 1800-1940; ad 1950 Modern ad 1430-1530; ad 1560-1630 ad 1450-1670 ad 1300-1430 ad 1430-1640 ad 1440-1640 ad 1665-1784 ad 1480-1680 ad 1800-1940 ad 1668-1781; ad 1796-1895 ad 1401-94 ad 1216-1305 ad 1440-1530



Site EB1 EB2 EB3 EB3 EB3 EB7 EB9 EB9 EB9 EB9 EB20 EN1 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21



Location Hudson Bay coast Hudson Bay coast Hudson Bay coast Hudson Bay coast Hudson Bay coast Ross Island  langer Island Be  langer Island Be  langer Island Be  langer Island Be Umiujaq Ross Island  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be



Material Organic matter Organic matter Needles (unidentified) Charcoal (Picea glauca) Needles (P. glauca) Needles (P. glauca) Needles (P. glauca) Organic matter Organic matter Organic matter Needles (P. glauca) Needles (unidentified) Wood Wood Wood Wood Wood Wood Wood Wood Wood Wood Wood



Age (14C years bp) 1/4 conventional radiocarbon age  SE. 2r calibrated age: age intervals including the 95% probability distribution. *pMC, percentage of modern carbon: the result indicates that the sample was living after ad 1950.



No sample associated with white spruce remains gave a date older than 300 14C years bp (cal. ad 1430-1690; site EB2); charred white spruce needles gave a date of 140 14C years bp (cal. ad 1660-1950; site EB3) (Table 4). A date of 400 14C years bp (cal. ad 1430-1530) was obtained for unidentifiable needles found under a black spruce clone (site EN1, Ross Island) where only black spruce needles were recovered from the soil sample (Appendix S1). No spruce needles or other spruce remains were present at the level dated 1060 14C years bp (cal. ad 770- 1190) at site EB9. Radiocarbon dating of the lowest white spruce needles at this site gave a modern age (Table 4). DISCUSSION White spruce dynamics White spruce expanded throughout the study area over the past 400-500 years. In particular, all tree-line populations increased significantly during the past 100 years. Seedlings have been found up to c. 4 km south of the northernmost position of the species in the coastal tundra. A recently established spruce (established 1936) was found close to the species limit, and the northernmost individual was bearing



apparently fertile cones. The establishment of new individuals has been almost continuous during the past 300 years at the tree line, but no regeneration occurred during the 19th century in the northern part of the study area dominated by tundra communities. The downward colonization of the newly emerged shorelines was fast and effective on favourable substrates, as already observed along the Hudson Bay coast  gin et al., 1993). However, the absence of white spruce (Be  langer Island suggested the from the ancient shorelines on Be past occurrence of unfavourable periods, or the recent arrival of the species at the site. The first establishment of white spruce was almost synchronous from the current position of the tree line to the species limit. Further establishment has been effective mostly in sites already colonized by single spruce, with few recently created populations all in the southern part of the study area:  langer Island (established at the end of the 19th EB21 on Be century), and the clone EB8 on Ross Island. Even recently expanding populations (e.g. site EB1) showed the remains of one or few older `founder' individuals, usually located in the inner part of the stand. The existing populations thus seem to have originated from single, scattered individuals, with a progressive recruitment of seeds towards the periphery of the 2131



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



M. Caccianiga and S. Payette stand (sometimes asymmetrically due to prevalent wind, e.g. EB1) during favourable periods. Even during such periods, seedlings could establish only on moist, protected sites, with a positive feedback effect on further recruitment (EB1, EB9, EB21). This pattern is consistent with the observations of Scott et al. (1987) in forest-tundra sites at Churchill, Manitoba. However, the possibility should be taken into account that isolated young populations, not originating from a single individual, have been overlooked. The process of expansion at the present tree line was accompanied by a marked change of growth forms. Trees established in the second half of the 19th century replaced stunted individuals already present since the 17th century. Most of those ancient individuals died in the 19th or the 20th century. The expansion trend of white spruce along the Hudson Bay coast has been linked to recent climate warming, effects of which could also be observed through the effectiveness of postfire recruitment (Payette & Filion, 1985). The present study focused mostly on the northernmost sites rather than on the forest stands where such recruitment was apparent. Only one site showed charcoal, suggesting a low fire frequency in the scattered spruce populations along the coast. The fire affected a pre-existing white spruce clone, but the occurrence of black spruce remains below the charcoal layer may indicate a black spruce stand surrounding the white spruce clone: a post-fire decline of black spruce clones could be assumed. Tree-ring data confirmed the increase in radial growth affecting the tree and high-krummholz populations in the late 1800s, after a marked period of suppressed growth. These trends were already documented for white spruce in eastern Canada (Parker et al., 1981; Jacoby, 1983; Jacoby et al., 1988) and could be linked to global climate events such as the last peak of the Little Ice Age (Grove, 1988) and the subsequent warming (Tett et al., 1999; Houghton et al., 2001; Parmesan & Yohe, 2003). However, the spruce population at the tree line showed a significant downward trend in the late 20th century (EB9 and EB1) (Table 3). White spruce krummholz have registered such changes in a contrasting way. More exposed individuals from sites EB11-13 hardly survived the unfavourable period of the 1820-50s, and died shortly after. We can presume the same story for the clone at site EB10, which could not be dated. A significant upward growth trend in the late 19th century (Table 3) failed to result in ring widths comparable with those of the other sites. The individual from EB12 showed an ephemeral recovery in the early 20th century, but without a long-term trend, and this was rapidly followed by death. Larger krummholz in sites EB2 and EB3 showed a slight recovery after the 1860s, a peak in the 1880s but with no real increase in ring width, and a downward trend in the 20th century. EB19 and EB20, although the northernmost individuals, showed overall faster growth and were less affected by the cold conditions of the early 1800s. The contrasting growth pattern in the past 50 years between the northernmost sites and those situated at lower latitude is noteworthy. In our sites this pattern was probably affected by the young age of the stems covering the last part of the series in the northernmost sites, and by the characteristics of each site. The old clone of site EB3, situated well north of the tree line, showed a downward trend and was already declining at the beginning of the 20th century (Table 3). The response to climate by scattered tree islands appeared to be influenced strongly by exposure to wind and subsequent snow cover, which can overrule the influence of the latitudinal gradient (Fig. 12). A positive feedback between tree island dimension and snow cover has been described for white spruce islands (Payette & Filion, 1985; Scott & Hansell, 2002). However, the declining pattern of the young population of site EB1 seemed to reflect a long-term trend, also indicated by the high mortality of the 1980s and 1990s (Fig. 9). Species-specific response to climate White spruce responded to recent, favourable climatic conditions through active seed recruitment of new individuals, particularly in the northernmost forest sites and in foresttundra (Payette & Filion, 1985; Scott et al., 1987; Szeicz & MacDonald, 1995; MacDonald et al., 1998; Lloyd et al., 2005). Present data show the same behaviour at the tree limit and in shrub-tundra sites, and almost to the species limit. A similar pattern, with continuous although low levels of seedling establishment, was observed for white spruce in the shrub tundra at Tuktoyatuk (north-western Canada), the northernmost conifer population in North America (McLeod & Henry, 2002).



7



1*2 1 0*8 0*6 0*4 0*2 0

EB21 South Snow height EB9 EB1 EB5 EB3 EB2 EB11 EB12 EB13 EB19 EB20 North Ring width



Tree/snow height (m)



6 5 4 3 2 1 0



Ring width (mm)



Sites

Tree height



Figure 12 Trends of maximum tree height, inferred snow level and mean ring width of study sites positioned according to latitudinal gradient.



2132



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra In contrast, black spruce seed recruitment in the same area was almost absent. Black spruce tree-line displacements across  bec were driven by seedling establishment only in northern Que the southern forest-tundra (Gamache & Payette, 2005) and by layering and change of growth forms in the northern foresttundra and in shrub-tundra (Lescop-Sinclair & Payette, 1995; Gamache & Payette, 2004, 2005). The different behaviour of the two species may be due to their reproductive traits. In particular, semi-serotinous cones of black spruce retain seeds for a certain number of years in the absence of fire (Viereck & Johnston, 1990), thus delaying the response to favourable climatic conditions (Gamache & Payette, 2005). Furthermore, the phenotypic plasticity of black spruce, although rendering it able to withstand harsh climatic conditions, causes the establishment of stunted growth forms with reduced cone  gin & Filion, production and effective post-fire recruitment (Be 1999; Sirois, 2000; Gamache & Payette, 2004; Lloyd et al., 2005). On the other hand, the more constant seed release of white spruce cones (Nienstaedt & Zasada, 1990) renders this species more able to respond readily to favourable climatic conditions, despite its lower morphological plasticity. Also, white spruce was probably favoured by its recent arrival after a constant decline of pre-existing black spruce stands due to the failure of post-fire regeneration (Payette & Gagnon, 1985; Asselin & Payette, 2005). In the absence of changes in the fire regime, white spruce will probably continue its process of expansion through seedling establishment, increase of population density and a northward shift of the tree growth form. Warmer climate could enhance the inception of arborescent growth form and seedling establishment in the more exposed sites where the magnitude of climate warming has been insufficient until now. The latter process could also involve black spruce clones in a northward and coastward process (Lescop-Sinclair & Payette, 1995; Gamache & Payette, 2005). Higher fire frequency could cause a further decline of black spruce if it occurs before the inception of sexual reproduction. With both species fully able to reproduce sexually, fire would probably favour black spruce regeneration, or at least limit the inception of a long-term dominance of white spruce (Lloyd et al., 2005). Arrival of white spruce on the eastern coast of Hudson Bay None of the tree-ring and macrofossil analyses performed showed any evidence of the occurrence of white spruce along the Hudson Bay coast before the 17th century: tree-ring data and 14C dates gave converging results. Black spruce was certainly present in the area long before that date, as shown by 14 C dates, clone dimension and extensive literature data  guin, 1987; Gajewski & Garralla, 1992; Payette, (Allard & Se 1993 and references therein; Gajewski et al., 1996; Asselin & Payette, 2005). The peculiarities of the Hudsonian white spruce population, and the uncertainties about its origin and date of arrival, have already been emphasized (Payette, 1993). The present data suggest a recent and ongoing arrival of the species. Its establishment probably derived from scattered individuals already present on the sites, traces of which are difficult to find in the absence of surviving or recently dead individuals. Given the position and altitude of all study sites, most white spruce individuals are established on soil surfaces generally < 2000 years old, assuming a general rate of land  langer emergence of c. 1-1.3 m per century according to our Be Island curve and Allard & Tremblay's (1983) curve. All treeline stands and tundra tree islands have in common the same overall structure of living and dead spruce, whatever their location and altitude along the coast. With these facts in mind, it seems likely that the arrival of white spruce occurred during the Little Ice Age, a period well defined in eastern Canada between the end of the 1500s and the late 1800s. The arrival of the species to its northern limit during a climatically unfavourable period is noteworthy, as well as its failure of further expansion during the subsequent warmer periods. Even if the colonizing event probably took place during a favourable period of the Little Ice Age, the observed pattern of establishment is probably the outcome of a migrational lag rather than the result of climatic limitation. Also, we must emphasize the fact that the lack of evidence for white spruce presence before 1660 does not prove that the species was not present in the study area earlier. It is noteworthy that spruce needles from 420-520 cal. years bp preserved in well drained soil conditions, and organic matter from 760-1180 cal. years bp, could be found not associated with white spruce remains. It is also possible that some evidence could have been lost by the rapid decomposition of dead wood at the soil surface and needles in the soil in the maritime climate along the coast of Hudson Bay. However, the preservation of wood remains depends on the date of death, rather than the date of establishment. We found that well preserved individuals died in the late 18th to early 19th centuries (Fig. 9); given the longevity of the species, trees established 200-250 years before could have been found, if present. CONCLUSIONS The dynamic behaviour of white spruce on the eastern coast of Hudson Bay appears to be the result of a complex ecological history. The species probably arrived recently in the area due to a delayed post-glacial spread along the Hudsonian pathway (Payette, 1993). A colonizing event brought white spruce to its latitudinal limit in tundra areas during a favourable climatic moment during the Little Ice Age. The established individuals survived by layering during unfavourable periods, but acted as nuclei for sexual recruitment almost continuously, except in the northernmost and more exposed sites. Warmer periods were marked by strong seedling recruitment and consequently increased density of populations, with no or minor latitudinal shift. A shift of the tree limit driven by change of growth form was observed, but even small krummholz were able to reproduce sexually. The recovery to tree growth form of white spruce was faster than that of black spruce. Small black spruce krummholz surround arborescent populations of white spruce 2133



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



M. Caccianiga and S. Payette at the tree line with no evidence of an ongoing change of growth forms. The response of white spruce to the present climatic conditions is different from that of black spruce in similar ecological contexts. Species-specific life-history traits appear critical in understanding the differential responses of both species to climate change, and these responses should be taken into account in modelling the behaviour of tree-line ecosystems to climatic forcing. ACKNOWLEDGEMENTS This study has been supported financially by the Natural Sciences and Engineering Research Council of Canada  bastien Cyr, Ann(NSERC). We express our gratitude to Se  lisabeth Robert and Simon Thibault for , E Catherine Laliberte assistance in macrofossil analysis. The assistance of Ann Delwaide in the field and the laboratory was much appreciated. REFERENCES  guin, M.K. (1987) The Holocene evolution of Allard, M. & Se permafrost near the tree-line, on the eastern coast of  bec). Canadian Journal of Earth Hudson Bay (northern Que Sciences, 24, 2206-2222. Allard, M. & Tremblay, G. (1983) La dynamique littorale des   ne. Zeitschrift fu  r Geomoriles Manitounuk durant l'Holoce phologie. N.F., 47, 61-95. Asselin, H. & Payette, S. (2005) Late Holocene opening of the  bec, Canada. Global forest-tundra landscape in northern Que Ecology and Biogeography, 14, 307-313. Barber, V., Juday, G.P. & Finney, B.P. (2000) Reduced growth of Alaskan white spruce in the twentieth century from temperature-induced drought stress. Nature, 405, 668-673.  gin, C. & Filion, L. (1999) Black spruce (Picea mariana) Be architecture. Canadian Journal of Botany, 77, 664-672.  gin, Y., Be  rube  , D. & Gre  goire, M. (1993) Downward Be migration of coastal conifers as a response to recent land  bec. Quaternary emergence in eastern Hudson Bay, Que Research, 40, 81-88. Cwynar, L.C. & Spear, R.W. (1991) Reversion of forest to tundra in the central Yukon. Ecology, 72, 202-212. Dyke, A.S. & Prest, V.K. (1987) Late Wisconsinan and Holoographie physique cene history of the Laurentide ice sheet. Ge et Quaternaire, 41, 237-263. Filion, L., Payette, S., Gauthier, L. & Boutin, Y., (1986) Light rings in subarctic conifers as a dendrochronological tool. Quaternary Research, 26, 272-279. Gajewski, K. & Garralla, S. (1992) Holocene vegetation histories from three sites in the tundra of northwestern  bec, Canada. Arctic and Alpine Research, 24, 329-336. Que Gajewski, K., Garralla, S. & Milot-Roy, V. (1996) Postglacial vegetation at the northern limit of lichen woodland in  bec. Ge ographie physique et Quaternaire, northwestern Que 50, 341-350. Gamache, I. & Payette, S. (2004) Height growth response of tree line black spruce to recent climate warming across the 2134 forest-tundra of eastern Canada. Journal of Ecology, 92, 835- 845. Gamache, I. & Payette, S. (2005) Latitudinal response of subarctic tree lines to recent climate changes in eastern Canada. Journal of Biogeography, 32, 849-862. Grove, J.M. (1988) The Little Ice Age. Methuen, London. Holmes, R.L. (1983) Computer-assisted quality control in treering dating and measurement. Tree-Ring Bulletin, 43, 69-78. Houghton, J.T., Ding, Y., Griggs, D.J., Noguer, M., van der Linden, P.J. & Xiaosu, D. (2001) Climate change 2001: the scientific basis. Contribution of Working Group I to the third assessment report of the Intergovernmental Panel on Climate Change (IPCC). Cambridge University Press, Cambridge, UK. Jacoby, G.C. (1983) A dendroclimatic study in the foresttundra ecotone on the east shore of Hudson Bay. Nordicana, 47, 95-99. Jacoby, G.C., Ivanciu, I.S. & Ulan, L.D. (1988) A 263-year record of summer temperature for northern Quebec reconstructed from tree-ring data and evidence of a major climatic shift in the early 1800s. Palaeogeography, Palaeoclimatology, Palaeoecology, 64, 69-78. Laberge, M.-J., Payette, S. & Bousquet, J. (2000) Life span and biomass allocation of stunted black spruce clones in the subarctic environment. Journal of Ecology, 88, 584-593. Lamb, H.F. (1985) Palynological evidence for postglacial change in the position of tree limit in Labrador. Ecological Monographs, 55, 251-258. Lavoie, C. & Payette, S. (1992) Black spruce growth forms as a record of a changing winter environment at treeline, Quebec, Canada. Arctic and Alpine Research, 24, 40-49. Lescop-Sinclair, K. & Payette, S. (1995) Recent advance of the arctic treeline along the eastern coast of Hudson Bay. Journal of Ecology, 83, 929-936. Lloyd, A.H. (2005) Ecological histories from Alaskan tree lines provide insight into future change. Ecology, 86, 1687-1695. Lloyd, A.H. & Fastie, C.L. (2002) Spatial and temporal variability in the growth and climate response of treeline trees in Alaska. Climatic Change, 52, 481-509. Lloyd, A.H. & Fastie, C.L. (2003) Recent changes in treeline forest distribution and structure in interior Alaska. Ecoscience, 10, 176-185. Lloyd, A.H., Wilson, A.E., Fastie, C.L. & Landis, R.M. (2005) Population dynamics of black spruce and white spruce near the arctic tree line in the southern Brooks Range, Alaska. Canadian Journal of Forest Research, 35, 2073-2081. MacDonald, G.M., Szeicz, J.M., Claricoates, J. & Dale, K.A. (1998) Response of the central Canadian treeline to recent climatic changes. Annals of the Association of American Geographers, 88, 183-208. Marr, J.W. (1948) Ecology of the forest-tundra ecotone on the east coast of Hudson Bay. Ecological Monographs, 18, 117- 144. McLeod, T.K. & Henry, G.H.R. (2002) Site-specific patterns in establishment of Picea glauca (white spruce) at its northern range limits, NWT, Canada. 6th International Conference on



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



Advance of white spruce in the coastal tundra  tudes nordiques, Dendrochronology Abstracts, Centre d'e  Laval, Que  bec City, Canada. Universite Nienstaedt, H. & Zasada, J.H. (1990) Picea glauca. Silvics of North America. Volume 1. Conifers, Agriculture Handbook 654 (ed. by R.M. Burns and B.H. Honkala). pp. 204-226. US Department of Agriculture, Forest Service, Washington, DC, USA. Parker, M.L., Josza, L.A., Johnson, S.G. & Bramhal, P.A. (1981) Dendrochronological studies of the coasts of James Bay and Hudson Bay. Climatic Change in Canada. Parts 1 and 2. Syllogeus 33 (ed. by C.R. Harrington), pp. 129-188. National Museum of Canada, Ottawa, Canada. Parmesan, C. & Yohe, G. (2003) A globally coherent fingerprint of climate change impacts across natural systems. Nature, 421, 37-42. Payette, S. (1983) The forest-tundra and present tree-lines of  bec-Labrador peninsula. Tree-line ecology. the Northern Que bec Tree-line Conference. Proceedings of the Northern Que Nordicana, 47, 3-23. Payette, S. (1993) The range limit of boreal tree species in  bec-Labrador: an ecological and palaeoecological interQue pretation. Review of Palaeobotany and Palynology, 79, 7-30. Payette, S. & Filion, L. (1985) White spruce expansion at the tree line and recent climatic change. Canadian Journal of Forest Research, 15, 241-254. Payette, S. & Gagnon, R. (1985) Late Holocene deforestation and tree regeneration in the forest-tundra of Quebec. Nature, 313, 570-572. Payette, S. & Lavoie, C. (1994) The arctic tree line as a record of past and recent climatic changes. Environmental Reviews, 2, 78-90.  gin, Y. (1999) Le de  veloppement d'une pessie  re Ricard, B. & Be e  pinette blanche et a  lichens sur la co  te en e  mersion rapide a  bec subarctique. Ge ographie de la Baie d'Hudson au Que physique et Quaternaire, 53, 351-364.  ge de Richard, P.J.H., Larouche, A. & Bouchard, M.A. (1982) N  glaciation finale et histoire postglaciaire de la vegetation la de  bec. Ge ographie dans la partie centrale du Nouveau-Que physique et Quaternaire, 36, 63-90. Ritchie, J.C. (1987) Postglacial Vegetation of Canada. Cambridge University Press, Cambridge, UK. Ritchie, J.C. & MacDonald, G.M. (1986) The patterns of postglacial spread of white spruce. Journal of Biogeography, 13, 527-540. Scott, P.A. & Hansell, R.I.C. (2002) Development of white spruce tree islands in the shrub zone of the forest-tundra. Arctic, 55, 238-246. Scott, P.A., Hansell, R.I.C. & Fayle, D.C.F. (1987) Establishment of white spruce populations and responses to climatic change at the treeline, Churchill, Manitoba, Canada. Arctic and Alpine Research, 19, 45-51. Sirois, L. (2000) Spatiotemporal variation in black spruce cone and seed crops along a boreal forest-tree line transect. Canadian Journal of Forest Research, 30, 900-909. Stuiver, M., Reimer, P.J. & Reimer, R.W. (2003) CALIB 4.4. http://www.calib.org. Suarez, F., Binkley, D., Kaye, M.W. & Stottlemyer, R. (1999) Expansion of forest stands into tundra in the Noatak National preserve, northwest Alaska. Ecoscience, 6, 465- 470. Szeicz, J.M. & MacDonald, G.M. (1995) Recent white spruce dynamics at the subarctic alpine treeline of north-western Canada. Journal of Ecology, 83, 873-885. Tett, S.F.B., Stott, P.A., Allen, M.R., Ingram, W.J. & Mitchell, J.F.B. (1999) Causes of twentieth-century temperature change near the earth's surface. Nature, 399, 569-572. Viereck, L.A. & Johnston, W.F. (1990) Picea mariana. Silvics of North America. Volume 1. Conifers, Agriculture Handbook 654 (ed. by R.M. Burns and B.H. Honkala). pp. 227-237. US Department of Agriculture, Forest Service, Washington, DC, USA. Weng, C. & Jackson, S.T. (2000) Species differentiation of North American spruce (Picea) based on morphological and anatomical characteristics of needles. Canadian Journal of Botany, 78, 1367-1383. Wigley, T.M.L., Briffa, K.R. & Jones, P.D. (1984) On the average value of correlated time series, with applications in dendroclimatology and hydrometeorology. Journal of Climate and Applied Meteorology, 23, 201-213. SUPPLEMENTARY MATERIAL The following supplementary material is available for this article online from http://www.Blackwell-Synergy.com: Appendix S1 Identification of spruce needles found in the soil samples.



BIOSKETCHES  di Marco Caccianiga obtained a PhD at the Universita Milano, working on primary succession on alpine glacier forelands. This project was part of his postdoctoral research at  Laval concerning the recent dynamics of subarctic Universite white spruce. His research interests focus on vegetation dynamics in climatically limited environments. Serge Payette is a professor of plant ecology and palaeo partement de Biologie and Centre d'e  tudes ecology at the De  Laval. As chairman of the NSERC nordiques of Universite Northern Research Chair on subarctic forest ecology, he studies the relationships between boreal and subarctic ecosystems, climate and disturbances at various temporal and spatial scales.



Editor: Glen MacDonald



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd



2135



Loss-calibrated Monte Carlo Action Selection

Ehsan Abbasnejad Justin Domke Scott Sanner

NICTA & ANU scott.sanner@nicta.com.au



NICTA & ANU ANU & NICTA ehsan.abbasnejad@anu.edu.au justin.domke@nicta.com.au



Abstract

Bayesian decision-theory underpins robust decisionmaking in applications ranging from plant control to robotics where hedging action selection against state uncertainty is critical for minimizing low probability but potentially catastrophic outcomes (e.g, uncontrollable plant conditions or robots falling into stairwells). Unfortunately, belief state distributions in such settings are often complex and/or high dimensional, thus prohibiting the efficient application of analytical techniques for expected utility computation when real-time control is required. This leaves Monte Carlo evaluation as one of the few viable (and hence frequently used) techniques for online action selection. However, loss-insensitive Monte Carlo methods may require large numbers of samples to identify optimal actions with high certainty since they may sample from high probability regions that do not disambiguate action utilities. In this paper we remedy this problem by deriving an optimal proposal distribution for a loss-calibrated Monte Carlo importance sampler that bounds the regret of using an estimated optimal action. Empirically, we show that using our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by conventional loss-insensitive samplers.



a1 u a2



p



q  Figure 1: Motivation for loss-calibration in Monte Carlo action

selection. (Top) Utility u( ) for actions a1 and a2 as a function of state  . (Middle) A belief state distribution p() for which the optimal action arg maxa{a1 ,a2 } Ep [u( , a)] should be computed. (Bottom) A potential proposal distribution q ( ) for importance sampling to determine the optimal action to take in p( ).



Introduction

Bayesian decision-theory (Gelman et al. 1995; Robert 2001; Berger 2010) provides a formalization of robust decisionmaking in uncertain settings by maximizing expected utility. Formally, a utility function u( , a) quantifies the return of performing an action a  A = {a1 , . . . , ak } in a given state  . When the true state is uncertain and only a belief state distribution p( ) is known, Bayesian decision-theory posits that an optimal control action a should maximize the expected utility (EU)  U (a) = E[u( , a)] = u( , a)p( )d . (1) where by definition, the optimal action a is a = arg max U (a).

a



belief distribution p( ) may be complex (e.g., highly multimodal) and/or high-dimensional thus prohibiting the application of analytical methods to evaluate the EU integral of (1). Practitioners often resort to the use of Monte Carlo methods to compute an approximate (but unbiased) expectation using n samples from p( ):

1 n

n



[u( i , a)],

i=1



 i  p.



(3)



(2)



In real-world settings such as robotics (Thrun 2000), the

Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



Unfortunately, na ive application of Monte Carlo methods for optimal action selection often proves to be inefficient as we illustrate in Figure 1. At the top we show two utility functions for actions a1 and a2 as a function of univariate state  on the x-axis. Below this, in blue, we show the known state belief distribution p(). Here, it turns out that U (a1 ) > U (a2 ). Unfortunately, if we sample from p() to compute a Monte Carlo expected utility for each of a1 and a2 , we find ourselves sampling frequently in the region where u( , a1 ) and u( , a2 ) are very close, but where suboptimal a2 is marginally better than a1 . An intuitive remedy to this problem is provided by an im-



portance sampling approach (see e.g. (Geweke 1989)) where we sample more heavily in regions as indicated by distribution q () and then reweight the Monte Carlo expectation to provide an unbiased estimate of U (a). Formally, the theory of importance sampling tells us that since  u( , a)p( ) U (a) = q ( )d , (4) q ( ) we can draw samples from q to compute an (unbiased) estimate of U (a) as n (a) = 1 U n

n



i=1



u( i , a)p( i ) , q ( i )



 i  q.



(5)



This leaves us with one key question to answer in this paper: How can we automatically derive a q ( ) to increase the probability that the optimal action a is selected for a finite set of n samples? Answering this question is important for real-time applications of Bayesian decision-theoretic approaches where efficiency and optimality are two key operating criteria. To this end, in the subsequent section we derive an optimal proposal distribution for a loss-calibrated Monte Carlo importance sampler. To do this, we first show connections between regret and the probability of non-optimal action selection and then further connect an upper bound on the latter to the variance of our expected utility estimator. We are then able to derive an optimal proposal distribution q that minimizes this variance through the calculus of variations. We evaluate our loss-calibrated Monte Carlo method in two domains. We first examine synthetic plant control examples building on those of (Lacoste-Julien, Huszar, and Ghahramani 2011), who were also motivated by losscalibration in Bayesian decision theory, albeit not in the case of Monte Carlo methods as we focus on in this work. We also demonstrate results in a Bayesian decision-theoretic robotics setting with uncertain localization motivated by the work of (Thrun 2000). In both empirical settings and in states with up to 100 dimensions, we demonstrate that using our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by conventional loss-insensitive samplers to achieve the same level of accuracy. This suggests a new class of loss-calibrated Monte Carlo samplers for efficient online Bayesian decision-theoretic action selection.



Since the samples are drawn randomly from q , a n is a random variable and so is its expected utility U ( an ). As such, we use E, P and V henceforth to denote the expectation, probability and variance operators. We emphasize that all random variables are determined by q . In principle, we would like to select the distribution q to minimize regret, i.e. maximize the true EU of the estimated action a n . As this is challenging to do directly, we proceed in three steps: 1. We establish a connection between regret and the probability of non-optimal action selection in Theorem 1. 2. Since calculating the probability of selecting the nonoptimal action is intractable to be directly minimized, we derive an upper bound in Theorem 3, based on the variance of the difference of estimated utilities. 3. Theorem 5 shows how to calculate the distribution q to minimize this bound.



Minimizing regret

To find the optimal estimated action a n with fewer samples, we wish to select q that minimizes the regret. Formally we define this as

min

q



( an )



=



E [U (a ) - U ( an )] .



(7)



Direct minimization of Equation 7 is difficult, hence we bound it with the probability of selecting a non-optimal action instead. Tightening this bound with respect to q will lead to a practical strategy. It is detailed in the following theorem. Theorem 1 (Regret bounds). For the optimal action a and its estimate a n the regret as defined in Equation 7, is bounded as  P [a = a n ]  ( an )   P [a = a n ] , where  = U (a ) - maxa mina A U (a ). P [a = a n ] U (a ) +

aA\{a }  A\{a } 



(8)



U (a ) and  = U (a ) -



Proof. We know E [U ( an )] is equal to P [a = a n ] U (a) P [a = a n ] min U (a )

aA\{a } a A a A



P [a = a n ] U (a ) +



=P [a = a n ] U (a ) + P [a = a n ] min U (a ). This is equivalent to stating that ( an )   P [a = a n ] after some manipulation. Similarly, we have that

E [U ( an )]  P [a = a n ] U (a ) + P [a = a n ]

a A\{a }



Loss-calibrated Monte Carlo Importance Sampling

In many applications of decision theory, sampling is the most time-consuming step. Since we know these samples are ultimately used to estimate high-utility actions, we are interested in guiding the sampler to be more efficient for this task. Here, we will pick a distribution q to draw samples from, which are in turn used to select the action that maximizes the EU. The estimated optimal action a n from n samples is defined as n (a). a n = arg max U (6)

a



max



U (a )



which leads to ( an )  P [a = a n ] . The bound is very intuitive: minimizing the probability of the estimated optimal action a n being non-optimal will lead to a bound on the regret. Clearly, for two actions we have  = . Thus, in the two-action case, minimizing the probability of selecting a non-optimal action is equivalent to maximizing the expected utility of the selected action. With more actions, these objectives are not equivalent, but we can see that the difference is controlled in terms of  and .



Minimizing the probability of non-optimal action

We now turn to the problem of minimizing P [a = a n ]. In the following, Lemma 2 provides a bound on the probability of non-optimal action. Further details and another view of the same problem with slightly better bounds is provided in supplementary material. In the subsequent lemma, we upper bound the indicator function with a smooth and convex upper bound that will be easier to minimize. The use of surrogate function for minimizing indicator has also been used in similar problems (see e.g. (Bartlett, Jordan, and McAaliffe 2006)). Lemma 2. For an optimal action a and its estimate a n obtained from sampling, we have t > 0,

P [a = a n ] 

a=a a =a







E



n (a) - U n (a ) + 1 t U



2



.



The critical feature of Equation 9 is that all terms on the RHS other than the variance are constant with respect to the sampling distribution q . Thus, this theorem suggests that a reasonable surrogate to minimize the regret in Equation 7 and consequently maximize the expected utility of the estimated optimal action is to minimize the variance of the difference of the estimated utilities. This result is quite intuitive -- if we have a low-variance estimate of the differences of utilities, we will tend to select the best action. This is aligned with the importance sampling literature where it is well known that the optimal distribution to sample from is the one that minimizes the variance (Rubinstein 1981; Glasserman 2004). Our analysis shows the variance of the function that has to be minimized is of a particular form that depends on the difference of the utilities (rather than each utility independently).



Proof. Firstly, decompose P [a = a n ] as

P[a = a n ] 

a=a a=a a =a



Optimal q

We established that to find the optimal proposal distribution q  (i.e. optimal q ), we minimize the sum of variances obtained from Theorem 3. Since a is unknown, we sum over all actions in A, rather than just A\{a }. Since everything except variance in Equation 9 is independent of q , we formulate the objective    min V Un (a) - Un (a ) s.t. q ( )d = 1.

q aA a A\{a}



n (a) > U n (a )] P[U n (a) > U n (a ) E I U

a=a a =a



=



.



Applying the inequality that I[v > 0]  (tv + 1)2 gives the result.



The next theorem then relates the value inside the above expectation to the variance, thus bonding the probability of incorrect action selection by the sum of variances. Theorem 3 (Upper bound on the probability of non-optimal actions). We have the following upper bound on the probability of non-optimal action selection for k actions in set A, n (a) obtained true expected utility U (a) and its estimation U from finite samples:

P [a = a n ] 

a=a a =a



1 + 2t U (a) - U (a )

2



(10) Here, the constraint on q is to ensure the resulting solution is a proper probability distribution. The following theorem provides the solution to the optimization problem in Equation 10 that we are interested in. Theorem 5. Let A = {a1 , . . . , ak } with non-negative utilities. The optimal distribution q  ( ) is the solution to problem in Equation 10 and has the following form:

q  ( )  p( )

aA a A\{a}



(u( , a) - u( , a ))2 .



(11)



n (a) - U n (a ) + t2 U (a) - U (a ) + t2 V U



, (9)



Proof. Expand the quadratic in Lemma 2 and use E[X 2 ] = n (a) - U n (a )] = U (a) - U (a ). V[X ] + E[X ]2 and E[U In general, one would like to select the constant t to minimize this bound. As this depends on the variance, which is a function of q and the number of samples n, this is difficult to do analytically. However, we expect the variance to decrease as n increases, so we instead derive a value for t that leads to an asymptotically tight upper bound. Lemma 4. For the bounds detailed in Theorem 3, if the variance term is zero, the value of t that minimizes the upper bound is a=a a =a U (a ) - U (a) t = 2. a=a a =a (U (a) - U (a )) Proof. In general, the value of t minimizing 2ta + t b is t = -a/b.

2



Proof. Since we know V[X ] = E[X 2 ] - E[X ]2 , for computing the objective in Equation 10 the second expectation 2 becomes (U (a) - U (a )) and is independent of q , then we only need to minimize E

n (a) - U n (a ) U

2



. Consider this



value for a particular pair (a, a ). Denoting ( i , a, a ) = u( , a) - u( , a ), this is equal to  q ( 1,...,n )  1 = 2 n

n



1 n

n



n



i=1



( i , a, a )p( i ) q ( i )



2



d 1,...,n



( i , a, a )( j , a, a )p( i )p( j ) q ( i )q ( j ) i=1 j =1 x q ( 1 , . . . ,  n )d 1,...,n .



Since all the samples are independent, q ( 1 , . . . ,  n ) = q ( 1 ) . . . q ( n ). Now if i = j , it is easy to see that q vanishes and those terms become independent of q . If i = j however, only one of the terms in the denominator cancels out with the



joint. Also because the sum is over similar terms, we have n times the same expression, leading to the Lagrangian of

1 n 

aAa A\{a}



( , a, a )2 p( )2 d +  q ( )



 q ( )d  - 1 .



Taking the derivative with respect to q ( ), we have that

- 1 n ( , a, a )2 p( )2 +=0 q ( )2



aA a A\{a}



which concludes the theorem since n only induces a proportionality constant. This is quite intuitive - the samples  will be concentrated on regions where p( ) is large, and the difference of utilities between the actions is large, which is precisely the intuition that motivated our work in Figure 1. This will tend to lead to the empirically optimal action being the true one, i.e. that a n approaches a . In practice, the normalization constants for p and q are likely to be unknown, meaning that direct use of Eq. 5 is impossible. However, there are well-known self-normalized variants that can be used in practice with p()  p ( ) and q ( )  q ( ), namely p ( i ) i  q . q ( i ) i=1 i=1 (12) This simply means that for the case of unnormalized p  and 1 terms cancel, the summed utility values q  and letting the n have to now be reweighted by the slightly more complex p ( j ) n ( i ) value of p j =1 q q ( i ) ( j ) . Furthermore, as it is hard to directly sample q , we must resort to Markov Chain Monte Carlo (MCMC) methods (Neal 1993), e.g. Metropolis-Hastings (MH). This disregards an important aspect, namely that the samples we obtain for q are not truly independent. Rather, the number of effective samples are affected by the mixing rate of the Markov chain. Our derivation above does not account for these mixing rates, which could be important in many applications. For this reason, our experiments will distinguish between two settings: First, one can run an extremely long chain, and subsample from this, approximating nearly independent samples as in the derivation above, which we call Subsampled MC. Second, one can run a single Markov chain, as would be typical in practice, which we call Sequential MC. u( i , a) n (a) = 1 U n

n



each action. In case direct sampling is not possible we use Metropolis-Hastings MCMC by initializing the chain at a random point and using a Normal distribution centered at the current sample with isotropic covariance optimally tuned so that around 23% of samples are accepted (Roberts, Gelman, and Gilks 1997). In each experiment n samples are generated 200 times and the mean of the percentage of times the true optimal action is selected is reported. We include two diagnostics for MCMC samplers: in the first one (Subsampled MC) we have generated a large chain of 100000 samples and selected random subsamples to compute the best action using the empirical expected utilities n (a). Since samples drawn from Markov chains are typU ically correlated, this diagnosis will help ensure samples are independent. In the second diagnostic (Sequential MC) we draw samples sequentially from a single Markov Chain started at a random point to calculate the expected utilities for selecting the best action.



Power-plant Control

We consider a power plant where the temperature of the generator has to be maintained in a safe range following the example from (Lacoste-Julien, Huszar, and Ghahramani 2011); the only actions available to achieve this are to turn the generator on or off. Suboptimal action choices that keep the generator on in high temperatures or turn it off in unnecessary cases when the temperature is safe and no maintenance is required lead to financial loss for the power plant and should be avoided. For this problem, we can model the distribution of the temperature and use a high utility for cases where a safe action of turning the generator on or off is taken, formally,

u( , a = on/off) = Ha La ca,1 <  (d) < ca,2 otherwise

(d) (d)



p ( i ) q ( i )



1 n



n



(13)



where  (d) is the d-th dimension of the temperature, Ha , La (for action a  on/off) is the reward for the given ac(d) (d) tion a in the temperature intervals defined by ca,1 and ca,2 . We use three distinct one dimensional utilities for simu(1) (1) (1) lations: (i) c(1) on,1 = 15, con,2 = 20, coff,1 = 15, coff,2 =

21, Hon = 6.5, Hoff = 5, Lon = 1.5, Loff = 4; (ii) con,1 =

(1) 45, con,2 (1) (1)



=



(1) 50, coff,1



3, Lon = 1.5, Loff =



(1) = 35, coff,2 (1) 2.5; (iii) con,1



= 40, Hon = 6.5, Hoff = = 5, con,2 = +, coff,1 =

(1) (1)



-, coff,2 = +, Hon = 5, Hoff = 3, Lon = 2.5, Loff = 3.



Applications

As discussed earlier, many applications require optimal actions to be selected efficiently given known (but complex) p and u. In this section we provide applications and evaluate how well the samples drawn from p and q  compare. In these simulations we are interested in finding the optimal action, i.e., the one that maximizes the expected utility, with the minimum number of samples. As such we generate samples from the true distribution p and the proposed optimal distribution q  (obtained from Theorem 5 as per the application's specifications) and compute the expected utilities for



Corresponding to each utility, the following three distributions are used to demonstrate how the samples drawn from p and q  obtained from Theorem 5 perform in selecting the optimal action:

(i) p( ) = 0.7  N ( (1) ; 3, 7) + 0.3  N ( (1) ; 12, 2) where N ( (1) ; ,  2 ) is a normal distribution with mean  and variance  2 ; (ii) p( ) = 0.05  N ( (1) ; 3, 1) + 0.2  N ( (1) ; 6, 1) + 0.05  N ( (1) ; 10, 3) + 0.3  N ( (1) ; 15, 2) + 0.05  N ( (1) ; 20, 7) + 0.1N ( (1) ; 25, 2)+0.05N ( (1) ; 30, 3)+0.2N ( (1) ; 40, 5) (iii) a log-normal distribution p( ) = Log-N ( (1) ; 0, 1).



u( , a = on/off)

6 Utility Value 5 4 3 2 -10 0 10  20 30

0.035



distribution of p and q 

% Optimal Action Selected 100 90 80 70 60 0



Subsampled MC

% Optimal Action Selected 100 90 80 70 60 0



Sequential MC



u( , on) u( , off)

Probability



0.05 0.04 0.03 0.02 0.01 -10 0 10  20 30



p q* 500 1000 1500 Markov Chain Length 2000



500 1000 1500 No. Subsamples



2000



% Optimal Action Selected



0.03 Probability 0.025 0.02 0.015 0.01 0.005



Utility Value



5 4 3 2 0

5



80 60 40 20 0 100 80 60 40 20 0 500 1000 1500 No. Subsamples 2000



% Optimal Action Selected



6



u( , on) u( , off)



100



100 80 60 40 20 0 100 80 60 40 20 0 p q* 500 1000 1500 Markov Chain Length 2000 500 1000 1500 Markov Chain Length p q* 2000



20 



40



60

u( , on) u( , off) Probability 0.12 0.1 0.08 0.06 0.04 0.02



0



20 



40



60



% Optimal Action Selected



Utility Value



4.5 4 3.5 3 20 40 60  80 100



20



40



60 



80



100



500 1000 1500 No. Subsamples



2000



Figure 2: Power-plant simulations: the step-valued utility function (as in Equation 13) in the first column, the true distribution p (in blue) and q  (in red) in the second column and in the third and forth columns the result of performing Subsampled MC and Sequential MC (as described in the text) are shown. In the two right-hand columns, note that q  achieves the same percentage of optimal action selection performance as p in a mere fraction of the number of samples. In Figure 2, the utility functions are shown in the first column (with black indicating the utility of on as detailed in Equation 13) and the temperature distributions (p in blue as described above and q  in red) in the second column are shown. In the third and fourth columns the result of performing Subsampled MC and Sequential MC of the Metropolis-Hastings sampler for selecting the best action is shown such that the x-axis represents the number of samples and the y-axis shows the percentage of times the correct optimal action is selected. Here, in general, we observe that a significantly smaller number of samples from q  is needed to select the best action in comparison to the number of samples from p required to achieve the same performance. To investigate the performance of samples from p and q  in higher dimensions, we use a d-dimensional Gaussian mixture corresponding to temperatures at each point in the plant as p() = N (; 10, ) + N (; 20, ) where 10 and 20 are d-dimensional vectors with constant value 10 and 20 as the mean and i,j = 5 + I[i = j ] and i,j = 3 + 7I[i = j ] as d x d covariance matrix. In addition, the utility function in d) (d) (d) Equation 13 is specified with c( on,1 = 23, con,2 = 25, coff,1 =

20, coff,2 = 22, Hon = 50d, Hoff = 13, Lon = 1.1, Loff = 1.5 log(d). In Figure 3 for d  {2, 4, 10, 20, 50, 80, 100}, we

(d)



ficient. In fact, for a 100-dimensional bimodal Gaussian we are unable to find the optimal action using only 200 samples from p, which should be contrasted with the significantly improved performance given by sampling from q  .



100 % Optimal Action Selected 80 60 40 20 0 % Optimal Action Selected p q*



% Optimal Action Selected



100 80 60 40 20 0 p q*



0



20



40 60 Dimension



80



100



0



20



40 60 Dimension



80



100



(a) Subsampled MC



(b) Sequential MC



observe that in an average of 100 runs of the MCMC with 200 samples, as the dimensions increase using q  is more ef-



Figure 3: Performance of the decision maker in selecting the best action as the dimension of the problem increases in the power-plant. Note that at 100 dimensions, p is unable to select the optimal action whereas q still manages to select it a fraction of the time (and would do better if more samples were taken).



Assuming a deterministic transition dynamics model

 = T ( , a) and denoting (T ( x , a), T ( y , a)) as the loca-



tion of the robot after taking action a from state  (that is, moving from the current location in the direction of the the selected action heading) and Rr the set induced by region r, we use the following utility function:

  H u( , a) = L  M (a) Environment's Map

% Optimal Action Selected % Optimal Action Selected 100 90 80 70 60 0 500 1000 1500 No. Subsamples 2000 100 90 80 70 60 0 500 1000 1500 Markov Chain Length p q* 2000



(T ( x , a), T ( y , a))  Rcharger (T ( x , a), T ( y , a))  Rstair , otherwise



(14)



(b) Subsampled MC



(c) Sequential MC



where L < M < H (in our experiments: L = 1, M = 10, H = 400) and a  {forward, backward, right, left}. Using distribution q  from Theorem 5 as illustrated in Figure 4a, the samples from q  (in red) concentrated on the charger's location which has higher utility value compared to the samples from p (in blue) that are from the mode of the distribution. As shown in Figure 4b and 4c, using distribution q  and running the same diagnostics as the previous experiment we see significant improvement in selection of the optimal action, requiring only a fraction of the samples of p to achieve the same optimal action selection percentage.



Figure 4: A robot's internal map showing the samples taken from its true belief distribution p (two modes are shown in blue, the second one is slightly obfuscated by the robot) and the optimal sampling distribution q  derived by our loss calibrated Monte Carlo importance sampler in 4a. In 4b and 4c we see the performance (in terms of percentage of optimal action selected) of our loss-calibrated sampling method using q  leads to near immediate detection of the optimal action in only a few samples.



Conclusion and Future Work

We investigated the problem of loss-calibrated Monte Carlo importance sampling methods to improve the efficiency of optimal Bayesian decision-theoretic action selection in comparison to conventional loss-insensitive Monte Carlo methods. We derived an optimal importance sampling distribution to minimize the regret bounds on the expected utility for multiple actions. This, to the best of our knowledge, is the first result linking the utility function for actions and the optimal distribution for Monte Carlo importance sampling in Bayesian decision theory. We drew connections from regret to the probability of selecting non-optimal actions and from there to the variance. We showed using an alternative distribution as derived in Theorem 5 will sample more heavily from regions of significance as identified by their sum of utility differences. Empirically, we showed that our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by lossinsensitive samplers in synthetic examples of up to 100 dimensions and robotics-motivated applications. Future work should investigate the extension of the novel results in this work to the case of (a) continuously parameterized actions (Alessandro, Restelli, and Bonarini 2007), (b) imprecise utility functions (e.g, when the return of a state is not known precisely, but can be sampled) (Boutilier 2003), (c) uncontrollable sampling (where the utility partially depends on auxiliary variables that cannot be directly sampled from) and (d) applications in active learning and crowdsourcing (Beygelzimer, Dasgupta, and Langford 2009). Furthermore, the bounds obtained here are not tight in the multiaction setting and can be improved in future work. Altogether, this work and the many avenues of further research it enables suggest a new class of state-of-the-art loss-calibrated Monte Carlo samplers for efficient online Bayesian decision-theoretic action selection.



Robotics

Another application where sampling has been commonly used is localization in robotics (Thrun 2000). It is a risksensitive-decision making problem where the robot is rewarded for navigating to the charger in order to maintain its power but wrong actions may lead to catastrophic incidents like falling down the stairs or crashing into obstacles. Due to minimal resources on-board a robot and the nature of the real-time localization problem, it is crucial for the robot to be able to select the optimal action rapidly, yet safely. The state of the robot is the combination of its coordinates on a map and its heading direction. In our example for these experiments, we use a three dimensional Gaussian belief state distribution with two locations in a room intended to model that a robot's belief update has been confused by symmetries in the environment: one mode is at the robot's true location and the other at the opposite end of the room. In this experiment, we consider a map as shown in Figure 4a where there is a flat in-door environment that the robot can move by selecting one of the four actions forward, backward, right or left. This action will lead to a movement step in robot from the current point on map with the heading direction towards the selected action. In doing so however, the robot has to avoid the stairs (low utility region) and select the charging source (high utility region).



Acknowledgements

NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.



References

Alessandro, L.; Restelli, M.; and Bonarini, A. 2007. Reinforcement learning in continuous action spaces through sequential monte carlo methods. In Advances in Neural Information Processing Systems. Bartlett, P.; Jordan, I. M.; and McAaliffe, J. D. 2006. Convexity, classification, and risk bounds. Journal of the American Statistical Association 101(473):138-156. Berger, J. 2010. Statistical Decision Theory and Bayesian Analysis. Springer, 2nd edition. Beygelzimer, A.; Dasgupta, S.; and Langford, J. 2009. Importance weighted active learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, 49-56. New York, NY, USA: ACM. Boutilier, C. 2003. On the foundations of expected expected utility. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI'03, 285-290. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. Gelman, A.; Robert, C.; Chopin, N.; and Rousseau, J. 1995. Bayesian Data Analysis. CRC press. Geweke, J. 1989. Bayesian inference in econometric models using monte carlo integration. Econometrica 57(6):1317- 1339. Glasserman, P. 2004. Monte Carlo Methods in Financial Engineering. Applications of Mathematics. Springer, 1st edition. Lacoste-Julien, S.; Huszar, F.; and Ghahramani, Z. 2011. Approximate inference for the loss-calibrated bayesian. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), volume 15, 416-424. Neal, R. M. 1993. Probabilistic inference using markov chain monte carlo methods. Technical report, University of Toronto, University of Toronto. Robert, C. 2001. The Bayesian Choice. Springer Texts in Statistics. Springer, 2nd edition. Roberts, G. O.; Gelman, A.; and Gilks, W. R. 1997. Weak convergence and optimal scaling of random walk metropolis algorithms. The Annals of Applied Probability 7(1):110- 120. Rubinstein, R. Y. 1981. Simulation and the Monte Carlo Method. John Wiley & Sons, Inc., 1st edition. Thrun, S. 2000. Probabilistic algorithms in robotics. AI Magazine 21:93-109.



From: AAAI-88 Proceedings. Copyright (c)1988, AAAI (www.aaai.org). All rights reserved.



Robust Operative Diagnosis sthesis Space as Problem Solving in a

Kathy W. Abbott NASA Langley Research Center Hampton, Virginia 23665-5225



Abstract

The lack of robustness in current diagnostic systems is an important research issue because it has two major consequences: inability to diagnose novel faults and inability to diagnose more than one type of fault. This paper describes an approach that formulates diagnosis of physical systems in operation (operative diagnosis) as problem solving in a hypothesis space. Such a formulation increases robustness by: (1) incremental hypotheses construction via dynamic inputs, since the fault propagation results in changes in symptoms over time; (2) reasoning at a higher level of abstraction to construct hypotheses, albeit less specific ones, when specific knowledge is not available; and (3) partitioning the space by grouping fault hypotheses according to the type of physical system representation and problem solving techniques used in their construction. The approach was implemented for aircraft subsystems and evaluated on eight actual aircraft accident cases involving engine faults, with very promising results.



%



Introduction



The lack of robustness in current diagnostic systems is an important research issue because it has two major consequences: inability to diagnose novel faults and inability to diagnose more than one type of fault. For example, most current approaches to diagnosis depend on compiled, specific knowledge about the associations between symptoms and faults. However, when novel faults occur for which there is no specific associational knowledge, approaches that depend on such knowledge are inadequate. When the diagnosis is done for physical systems in operation (operuti-ue diagnosis), it is even more important to diagnose novel faults because the cost of inappropriate responses may be high. The purpose of operative diagnosis is to facilitate continued, safe operation, rather than identifying the part to repair. Moreover, identifying the eflects of the fault on the status of the physical system is equally as important as identifying the cause of the fault. In operative diagnosis, determining system status is often a dynamic process, as the effect of the fault propagates while the system continues to operate. Therefore, the operative nature of the diagnosis affects the reasoning in two ways: the need to reason about dynamic inputs and to generate system status. Another important consideration is that testing for



additional information is limited because of the need for safe, continued operation. Limited testing means that information available to discriminate hypotheses is less than sometimes desired. Moreover, sensed parameters are not available for every component in the system, and these sensor readings are sampled at (usually fixed) intervals. The set of symptoms may change because of fault propagation, and some changes may be undetected between samples. Much research has been done in diagnosis. Several of these approaches diagnose known faults where the effect of the fault propagates. For example, [Fagan et ad., 1984; Patil, 1987; Weiss et al., 1978; Pan, 19831 address diagnosis of known faults. Although these and other research efforts address the problem in much depth, they do not address novel faults. The fragility of these systems motivated several current approaches that use deep models in the diagnosis process [Fink and Lusth, 1987; Davis, 1985; Hamilton e$ ad., 19861. These model-based approaches generate hypotheses that identify the cause of the problem, (e.g., the faulty component), but not the system status. While this may be sufficient in cases where all the diagnostician needs to do is identify the part to replace, it assumes that no other parts need to be replaced or repaired as a result of the fault. Additionally, because they only use functional models, they cannot diagnose failures where one component damages another physically-adjacent component. Their capability to use multiple physical system representations is limited or nonexistent. Diagnosing some faults requires multiple representations [Davis, 19851, although even Davis' approach cannot generate system status or combine representations. This paper describes an approach that views diagnosis as problem solving in a hypothesis space. This view enables an improvement in robustness through incremental hypothesis updates, and the abstraction and partitioning of the hypotheses in the space. Incremental hypothesis updates enable diagnosis of dynamic fault behavior caused by fault propagation. Within the hypothesis space, the approach uses specific associational knowledge when available. However, when novel faults occur, the diagnostic problem solver uses abstraction of the individual hypotheses to provide a diagnosis, albeit a less specific diagnosis. The hypothesis space is partitioned into fault classes, grouping faults into different classes if their behavior requires different problem solving techniques or representations to diagnose them. Other diagnostic approaches can be viewed as diagnosing a subset of the classes included here. This approach was implemented in a computer program called Draphys (aagnostic Reasoning About Physical Systems) and demonstrated in the domain of aircraft sub-



Abbott



369



3.2



Diagnosis



of Known



Faults



Fall Compressor Combustor Turbines



Figure 1: Aircraft Turbofan Engine. systems; specifically, an aircraft turbofan engine and hydraulic subsystem. The approach was evaluated using actual aircraft accident cases involving engine faults, with very promising results.



Draphys uses compiled associational knowledge to diagnose known, commonly occurring faults. For the aircraft domain, the fault-symptom associations were obtained by interviewing domain experts (pilots and engine designers) and by examining actual fault cases. They were implemented in a rule-based system that permits the temporal functions defined by Allen [Allen, 19841 as part of the rules. These rules can adequately capture the sequence of symptoms as described by the experts, but have representational limitations as discussed in [Abbott et al., 19871. These include the awkwardness of expressing all the propagation behavior over time that could occur for any one fault, as well as temporal duration. The major question addressed below is what to do if the fault is one whose symptoms do not correspond to the associational knowledge. This question is of great interest, since novel faults appear to be very difficult for humans to diagnose.



3.3



Graceful egradation Abstraction



Via



2



Hypotheses Diagnosis



in Operative



In operative diagnosis, the diagnosis is done to assist in continued operation of the system under consideration. Each element of the diagnosis problem space is a hypothesis that describes the cause of the fault and the current system status. In Draphys, a hypothesis includes: the fault type, the cause or source of the problem, the propagation path, and the system status. The fault type is either single fault or multiple independent faults. The source of the fault is the physical component that is broken. The specific cause describes how that component is broken. The propagation path describes the order in which the fault affected the components. The system status describes the components affected by the fault and their operational status. The operational status of an affected component is either definitely aflected by the failure when symptom information justifies it, or possibly u$ected when there is reason to believe that the component might be affected but symptom information cannot confirm or refute it.



3

3.1



Diagnokis as Problem Solving in a Hypothesis Space

Aircraft Subsystem Diagnosis



Inflight diagnosis of aircraft subsystems is an example of operative diagnosis. The aircraft subsystems diagnosed by Draphys are two turbofan engines, two fuel subsystems, and a hydraulic subsystem. A schematic of the engine used in later examples is shown in figure 1. The input to the diagnosis system is a set of qualitative sensor values that identify which sensors are abnormal and how they are abnormal, e.g., fuel flow is high. A fault monitor generates these symptoms by comparing the sensor readings to expected values computed from a numerical simulation model of, for example, the engine. Schutte [Schutte and Abbott, 19861 describes the fault monitor.



Much of the related research views graceful degradation in the presence of novel faults as reasoning with deep models. In such a view, it is the eficiency of the reasoning process that degrades. The approach presented here does not view graceful degradation as an issue of degraded efficiency, but an issue of degraded specificity. If the diagnostic system cannot identify exactly what the fault is, it can still generate useful diagnostic information, even if that information is less specific than desired. Before presenting the approach, it is useful to explore what information should be abstracted and why. If the goal of the diagnostician is to select a remedial action to take in response to the fault, information should be generated to support that selection. During the interviews of experts, they described default actions that they would take if they did not recognize the fault or if there were multiple hypotheses. This action was generally a conservative response to the fault. For example, if the pilot knew he had a compressor failure, but did not know how the fan was broken, he would shut down the engine. However, if he knew it was an eroded compressor blade, he might reduce the throttle on that engine. Thus he had an action associated with the general class of compressor failures that was (potentially) d ifferent from the action associated with the specific compressor fault. Motivated by this and other examples, a structured way of forming general categories of faults with associated default actions was identified. In the aircraft domain, these categories are defined as the components in the physical system, as exemplified above. When novel faults occur, diagnostic reasoning takes place at a higher level of abstraction. Hypotheses are produced that identify what component is faulty, without identifying how the component is broken. The operational status of the component that is abstracted (e.g., abnormal rather than low pressure), so Draphys uses this abstraction is called status abstraction. two such levels, shown in figure 2. Since the diagnostic reasoning at the higher abstraction level is designed to identify the component that is faulty, the symptoms can be abstracted as well. Although it is



370



Common Sense Reasoning



HYPOTHESIS



1 OF 2



HYPOTHESIS



2 OF 2



Current Symptoms: N 1 Abnormal Fault Type: Single Fault Propagation Path And Component Status:



Current Symptoms: Nl Abnormal Fault Type: Single Fault Propagation Path And Component Status:



Propagation Type: Functional



Propagation Type: Functional



abstracts1



to



Responsible abstracts to abstracts to a



Component



Definitely Affected



Figure 3: Hypotheses Resulting From a Symptom in Ni. would be symptomatic to reflect this. The turbines would not be extracting energy, so the fan and compressor would not turn as fast since they derive their power from the turbines. Thus the faulty response is perpetuated. For this fault, suppose that the first symptom that Draphys detects is in Ni. Since Ni is an engine parameter, Draphys is able to localize the fault to the engine subsystem. Each component in the engine subsystem is then proposed as a candidate responsible component. For each proposed responsible component, Draphys generates a fault hypothesis by qualitatively simulating the fault propagation behavior. For example, when Draphys proposes the fan as the responsible component, it uses a model of the engine and its functional interconnections to determine that the high-pressure compressor and the Ni sensor functionally depend on the fan. Knowing these interconnections, Draphys then attempts to continue simulating the propagation of the failure to these functionally dependent components. In this example, it checks whether the fault' s effect has reached the high-pressure compressor by examining the symptoms to see if N2 is symptomatic. If it is, then Draphys assumes that the failure affects the high-pressure compressor, and continues the process from there. If N2 is not symptomatic, as in this example, simulated propagation halts on this path. Draphys then explores all remaining functional propagation paths. After exhausting all paths, the hypothesis is tested for validity. Draphys does the same process for each candidate component. In this example, two valid hypotheses are generated, shown in figure 3. The first is that the fan is the responsible component, and the second is that the Ni sensor failed. A fault in either component could result in the current symptoms. Extending this example illustrates the incremental updating of hypotheses. Assume that a short time after the Ni symptom was first detected and diagnosed, a symptom in N2 is detected. Draphys then tries to extend the propagation path of all the valid hypotheses to explain the new symptoms by continuing the qualitative simulation from the end of the propagation path in the old hypotheses. For instance, in one valid hypothesis propagation stopped at the fan, because the next component on this functional propagation path was the high-pressure compressor. Since earlier there was no symptom in N2, Draphys assumed that the compressor was unaffected. Now that there is a



Figure 2: The Two Levels of Status Abstraction in Drapb



necessary at the lower level to identify how the symptomatic sensor compares with its expected value (e.g., high or low), it is not necessary to make this distinction at the higher level. It is only necessary to identify that the value of the sensor is abnormal. This is also status abstraction, but it is the parameter value status that is abstracted. Figure 2 also illustrates the relationship between the specific fault hypotheses and the corresponding symptoms. The reasoning at the higher level of abstraction is a generate-and-test process. When symptoms first appear, the generator localizes the fault in a component hierarchy, resulting in a set of candidate components that might be the source of the problem. It then constructs fault hypotheses by simulating fault propagation from each of the candidates. Each resulting hypothesis is then tested to determine if it is valid; that is, if it accounts for all the current symptoms. Often this generate-and-test process results in multiple valid hypotheses. If new symptoms arrive as time progresses, the generator incrementally updates the old hypotheses to determine whether they can account for the new symptoms. If they can, the generator retains them. Otherwise, it prunes them. An example will clarify this process. Suppose the fault is a fan failure. In such a failure, the first sensor affected would be Ni. Since the fan would not compress air properly, the effect of that failure would propagate to the high-pressure compressor and thus to N2. It would then propagate to the combustor since the under-compressed air would not ignite as efficiently. Therefore, the expanding gases resulting from combustion would not turn the turbines as rapidly as it normally would. EGT and EPR



Abbott



371



HYPOTHF,SlS 1 OF 1

Current Symptoms: N2 Abnormal Fault Type: Single Fault Propaqation Path And Component Status:



kIYPOTHFSIS

Current Symptoms:



1 OF I.

:.:.:.X.X.&$+ Functional 4 Physical Propagation Propagation



Ni Abnormal N2 Abnormal Hydraulic Pressure Abnormal



Responsible Component Defintely Affected Possibfy Affected



Faulf Type: Single Fauft



Propagation



Type: Functional



Figure 4: Hypothesis Remaining After a Symptom in N2.

Propagation Type: Hybrid



symptom in N2, Draphys updates the system status for this hypothesis and continues the simulated propagation. The resulting hypothesis accounts for all symptoms. It is the only member of the set of old valid hypotheses that can do so, since a sensor failure in N1 could not result in functional propagation that would account for the symptom in Nz. Figure 4 shows this remaining hypothesis.

3.3.1 Using Multiple Representations Physical System



Figure 5: Composed Hypothesis Explaining a Fan Blade Separation.



The reasoning based on the functional model is sufficient for the faults that propagate along functional dependency links, but not all faults do. Suppose that the fan blade broke off and damaged a hydraulic line in the wing to which the engine was attached. The monitor detects symptoms in N1 and in the hydraulic pressure sensor. Draphys cannot explain these symptoms by simulating functional propagation, because there is no functional relation$hip between I these components. A physical proximity relationship does exist. Therefore, by knowing that the fan is physically adjacent to the wing containing the hydraulic line, Draphys can identify propagation from the engine to the wing. This represents another class of faults, since it requires a different representation (physical rather than the functional structure). This type of fault is analogous to Davis' bridge fault [Davis, 19851. The reasoning process used is the same as described with faults that propagate functionally, except that the models used in localization and simulation are based on physical structure rather that functional structure. The component hierarchy used for localization groups components according to physical location rather than functional relationships. The simulation model used is a specialized model of physical proximity. This specialized model includes directional information in representing these physical proximity relationships. For instance, it is possible for the fan blade to break off and damage the hydraulic line, but not vice versa. Unfortunately, reasoning with a single representation is not sufficient. Once a fan blade separation has caused damage in both the engine and in the hydraulic system,



the effect of the fault will propagate functionally in both subsystems. The initial propagation was physical, but subsequent propagation was functional. Therefore, explaining the current fault behavior requires models of both physical and functional structure. Draphys diagnoses hybrid fault propagation by composing the simple hypotheses that describe the single type of propagation, as illustrated in figure 5. Faults involving physical damage illustrate that some known faults are more appropriately represented at the higher level of abstraction. The reasoning described for diagnosing physical damage could be compiled into specific rules, but doing so may not improve ability to take remedial action. Moreover, physical damage can occur so many different ways that a large number of specific rules would result, possibly inhibiting their timely retrieval.

3.3.2 Partitioning the Hypothesis Space



So far, four fault classes were described that require different problem solving techniques or different physical system representations. Figure 6 includes these four fault classes, and shows the partitioning of the hypothesis space. The present implementation of Draphys diagnoses all fault classes shown except for multiple faults. The fault classes are examined in order of likelihood and correspond to a depth-first, left-to-right traversal of the space as shown.



4



Evaluation



Draphys was evaluated by reconst rutting actual civil transport aircraft accident cases and using their symptoms as input [a; b; c; d; e; f; g; h]. Each accident was an enginerelated failure that resulted in the loss of life and property. Four of the eight accident cases were used to guide the design and construction of Draphys. The remaining four were set aside for evaluation purposes. All eight were reconstructed by an objective party and presented as input



372



Common Sense Reasoning



Stage 1 Hvbothesis 1. Turbine Blade Separation l l . Turbine Blade Separation 2. Flamaout



Stage 2 Hypothesis 1. Fan 2. Compressor 3. Combustor 4. Turbine



l



2. Fan Failure



1. Turbine Blade Separation 1. Turbine Blade Separation



` l.Fan



3. Fan Failure 4. Foreign Object Ingestion



l



1. Fan Fan Compressor Combustor Turbine



* 1. 2. 3. 4. 1. Turbine Blade Separation 2. Flameout 1. Fuel System Failure 2. Flameout

l



5. Water



Ingestion



1. Combustor 2. Turbine



6. Engine Separation



` 1. Engine



- Fan



Figure 6: Hypothesis Space Partitioning.



7. Turbine



Disk



Separation



1. Turbine Blade Separation



l



1. Combustor 2. Turbine



Each level of abstraction was invoked for to Draphys' . each case to determine the diagnosis success of the associational rules at the specific level and the generate-andtest at the higher abstraction level. The physical system model used contained approximately 40 components and 100 interconnections. A brief summary of the resulting hypotheses (without system status) is shown in table 1. A successful diagnosis was defined as one in which the correct hypothesis was among the set of valid hypotheses. This definition was used because Draphys may generate several valid hypotheses for a particular set of symptoms. It may be impossible to isolate to one hypotheses with the sensor information available, even for a human expert. Moreover, since Draphys does not yet include any representation of uncertainty, the valid hypotheses cannot be ordered by likelihood. Using this criterion for success, seven of the eight accident cases were successfully diagnosed. Of the seven successes, two were diagnosed using the associational rules at the specific level of abstraction. All seven of the successes were diagnosed at the higher abstraction level. Of these seven cases, five involved physical damage. In each of the five cases, functional propagation resulted from the physical damage. No physical damage cases were diagnosed successfully by the associational rules at the specific abstraction level. The accident case that was not successfully diagnosed was not a structural fault. It involved massive water ingestion into the engine during a heavy rainstorm, leading to engine failure. Modifying Draphys to diagnose this failure would require modeling the inputs to a device as a potential source of the fault, which may be a desirable enhancement. Why did this approach work so well? The credit for success lies mainly with two aspects of the approach: the

` 1 am indebted to Paul Schutte for reconstructing the accident cases and doing the initial evaluation as described in [Schutte et aI., 19871.



8. Bearing Failure



1. Turbine Blade Separation 2. Flameout



l l . Compressor *correct diannosis



Table 1: Summary of Accident Case Diagnoses. symptoms detected and the models used. Symptoms provided by the monitor identify abnormal sensor readings as soon as they occur (or the first sample thereafter). This detects symptoms sooner than current operational systems, which alert the operator when a sensor exceeds its total normal operating range. The physical system models used must represent the behavior of the faulted system for the fault class being diagnosed. For example, the functional model represents a model of the normal system, but is at a high enough level of abstraction that it represents behavior under many fault conditions as well. In contrast, the model of physical structure only includes directional proximity information for possible physical damage, thus it does not model normal behavior. Including all nondirectional proximity relationships may be much less efficient and might not incorporate domain knowledge known from the device design about how internal physical damage might occur. In addition to appropriate representations, the ability to combine the physical and functional models was also important.



This paper presented an approach that views diagnosis as problem solving in a hypothesis space. With this view, robustness is improved through reasoning about fault propagation, permitting incremental hypothesis construction; status abstraction of the individual hypotheses, and partitioning of the hypothesis space to group fault hypotheses according to representation and problem solving technique. Incremental hypothesis construction based on fault propagation behavior can be used to discriminate hypotheses, particularly when symptoms change over time. How-



Abbott



373



ever, this requires that the detection process identify when sensor readings become abnormal, not just when they exceed the normal operating range. Abstraction of hypotheses is useful when actions are associated with the general fault categories represented by the abstract hypotheses. The approach of using different abstraction levels for diagnosing novel faults is appropriate when specific hypotheses are most desirable, but abstract hypotheses are better than nothing. Moreover, some known faults are more appropriately represented at the higher level of abstraction, such as, physical damage. This is the caSe when more specific hypotheses do not improve ability to take remedial action or the increase in number of specific hypotheses would inhibit their timely retrieval. Partitioning the fault space is appropriate when different problem solving techniques or representations are required to diagnose different classes of faults. In this approach, differentfault classes and their corresponding diagnostic techniques and representations were identified. One of these classes involved diagnosis of faults which propagate within multiple representations, which no other current approach can do. Evaluation of this approach revealed that diagnostic capability depends on the available physical system models and the fault propagation behavior that they can represent.



4lst Mechanical sium, Ott 1986.



Failure



Preventions



Group



Sympo-



[Pan, 19831 Y-C. Pan. Qualitative



Reasonings With DeepLevel Mechanism Models for Diagnosis of Dependent PhD thesis, University of Illinois, 1983. Failures.



[Patil, 19871 R. Patil. A case study on evolution of system building expertise: medical diagnosis. In Grimson and Patil, editors, AI in the 1980s and Beyond, MIT Press, 1987. [Schutte and Abbott, 19861 P. Schutte and K. Abbott. An artificial intelligence approach to onboard fault monitoring and diagnosis for aircraft applications. In

AIAA Guidance, ence, 1986. Navigation, and Control Confer-



[Schutte et al., 19871 P. Schutte, K. Abbott, M. Palmer, and W. Ricks. An evaluation of a real time fault diagnosis expert system for aircraft applications. In

Proceedings of the 26th IEEE and Control, 1987. Conference on Decision



[Weiss et al., 19781 S. M. Weiss, C. Kulikowski, S. Amarel, and A. Safir. A model-based method for computeraided medical decision making. Artificial Intelligence, 11:145-172, 1978.



Acknowledgements

The research described here is part of the author' s dissertation research at Rutgers University. I thank my advisor, Professor Lou Steinberg, and Professors Chris Tong, Don Smith, and Chuck Schmidt for guidance and suggestions. Peter Friedland, Paul Schutte, and George Steinmetz also commented on a draft of this paper.



bl



tional Transportation 9.



Aircraft Accident Report: United Airlines, Inc., Boeing 737-222, N9005U, Philadelphia International Airport, Philadelphia, Pennsylvania, July, 19, 1970. Na-



Safety Board. NTSB-AAR-72-



bl



Board. NTSB-AAR-75-2.



Aircraft Accident Report: National Airlines, Inc., DC-lo-lo, NGONA, Near AIbuquerque, New Mexico, November 3, 1973. National Transport ation Safety Aircraft Accident Report: Overseas National Airways, Inc., Douglas DC-10-30, Nl032F, John F. Kennedy International Airport, Jamaica, New York, November 12, 1975. National Transportation Safety Board.



References

[Abbott et al., 19871 K. Abbott, P. Schutte, M. Palmer, and W. Ricks. Faultfinder: a diagnostic expert system with graceful degradation for onboard aircraft applications. In 14th International Symposium on Aircraft Integrated Monitoring Systems, Friedrichshafen, West Germany, September 1987. [Allen, 19841 J. Allen. Towards a general theory of action and time. Artificial Intelligence, 23, 1984. [Davis, 19651 Randall Davis. Diagnostic reasoning based on structure and function. In Daniel G. Bobrow, editor, Qualitative Reasoning About Physical Systems, The MIT Press, 1985. [Fagan et al., 19841 L. Fagan, J. Kunz, E. Feigenbaum, and J. Osborn. Extensions to a rule-based formalism for a monitoring task. In B. Buchanan and E. Shortliffe, editors, Rule-Based Expert Systems, AddisonWesley, 1984. [Fink and Lusth, 19871 P. K. Fink and J. C. Lusth. Expert systems and diagnostic expertise in the mechanical and electrical domains. IEEE Transactions on Systems, Man, and Cybernetics, SMC-17(3), 1987. [Hamilton et al., 19861 T. Hamilton, D. Simmons, and R. Carlson. HELIX: an engine monitoring system. In



[cl



NTSB-AAR-76-19.



PI



Aircraft DC-9-31, 78-3.



Accident Nl335U,



Report: Southern Airways New Hope, Georgia, April4,



Inc., 1977.



National Transport ation Safety Board. NTSB-AAR-



kl



Aircrafl Accident Report: American Airlines Inc., DC-lo-lo, NllOAA, Chicago-O' Hare International Airport, Chicago, Illinois, May 25, 1979. National



Transportation Safety Board. NTSB-AAR-79-17.



M



Aircraft Incident Report: Northwest Airlines 79, McDonnell Douglas DC-10-40, Nl43US, Leesburg, Virginia, January 31, 1981. National Transportation



Safety Board. NTSB-AAR-81-10.



kl



portation Safety Board. NTSB-AAR-82-3.



Aircraft Accident Report: Air Florida Airlines, Inc., McDonnell-Douglas, Inc. DC-lo-JOCF, NlOl TV, Miami, Florida, September 22, 1981. National TransAircraft Accident Report: Eastern Airlines Flight 935, Lockheed L-1011-384, N309EA, Near Colts Neck, New Jersey, September 22, 198.2. National Transportation



bl



Safety Board. NTSB-AAR-82-5.



374



Common Sense Reasoning



Visibility Induction for Discretized Pursuit-Evasion Games

Ahmed Abdelkader and Hazem El-Alfy

Department of Engineering Mathematics and Physics Faculty of Engineering, Alexandria University Alexandria 21544, EGYPT {abdelkader,helalfy}@alexu.edu.eg



Abstract

We study a two-player pursuit-evasion game, in which an agent moving amongst obstacles is to be maintained within "sight" of a pursuing robot. Using a discretization of the environment, our main contribution is to design an efficient algorithm that decides, given initial positions of both pursuer and evader, if the evader can take any moving strategy to go out of sight of the pursuer at any time instant. If that happens, we say that the evader wins the game. We analyze the algorithm, present several optimizations and show results for different environments. For situations where the evader cannot win, we compute, in addition, a pursuit strategy that keeps the evader within sight, for every strategy the evader can take. Finally, if it is determined that the evader wins, we compute its optimal escape trajectory and the corresponding optimal pursuit trajectory.



Introduction

We consider the problem of target tracking, that is planning the motion of a mobile robot (pursuer) as it tracks a target (evader) moving amongst obstacles. We use the term target tracking to mean following that target or, more precisely, maintaining its visibility. This terminology is common within the robotic planning community (Hsu, Lee, and Rong 2008) as opposed to the broader notion of tracking, known in the computer vision literature, which refers to the identification of paths of different targets. Several applications of that problem are suggested in the literature (Hsu, Lee, and Rong 2008; Bhattacharya and Hutchinson 2008; Murrieta-Cid et al. 2007). Those cover surveillance and security in sensitive or restricted areas, providing home care by watching over children or elderly people and monitoring the performance of human workers. The studied problem is part of the more general visibilitybased pursuit-evasion problems (LaValle 2006). In those problems, the task of a pursuer is to compute a path which guarantees finding an evader that might be hiding in an environment. Obviously, the evader might "sneak" between different hiding places as the pursuer is traveling making a single pursuer unable to solve the problem. The natural extension becomes that of finding the minimum number of pursuers needed to eliminate any hiding places for one or

Copyright c 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



more evaders under different constraints of knowledge of the environment and information about other players' locations (Gerkey, Thrun, and Gordon 2004; Kolling and Carpin 2010; Klein and Suri 2011; Borie, Tovey, and Koenig 2011). Once the evader is found, the related problem that arises is that of maintaining its visibility which is known as target tracking. An early attempt at that problem is presented in (LaValle et al. 1997) for fully predictable targets (optimal tracking paths found offline) and partially predictable targets (best next step found online). Recently, there has been interest in a variant of that problem in which a pursuer, tracking an unpredictable evader, loses immediately if its view of the evader is obstructed by an obstacle (Bhattacharya and Hutchinson 2008; Murrieta-Cid et al. 2007). The problem of deciding which player wins for any pair of initial positions has been shown to be NP-complete (Murrieta-Cid et al. 2008). In this paper, we are interested in the latter problem of deciding which player wins the pursuit-evasion game described above. We develop an algorithm, using a mesh discretization approach, to decide the outcome of the game. For any input pair of players' positions, the algorithm decides whether the evader can win by moving in such a way to break the line of sight with the pursuer at any time instance or otherwise, the pursuer wins by maintaining the visibility of the evader throughout the game. Using a grid to discretize the surveyed environment has the advantages of being independent of the geometry and layout of the obstacles as well as being computationally feasible. Grid discretization is often used in Artificial Intelligence to solve problems in AI planning (Ishida and Korf 1995). In AI, the focus is on solving the problem of finding and (or) catching the evader using algorithms that search the discretized state space along with heuristics to speed up the process. The problem of deciding if a solution exists is only approached theoretically and is often intractable. Recently, a polynomial time algorithm that decides which player wins and in how many steps has been developed (Hahn and MacGillivray 2006). Mathematical results from graph theory are used to present bounds on the time complexity of the problem that can be generalized, in theory, to the case of more players. A realization of such algorithms in real world problems is still not practical. In contrast, our approach presents a grid-based solution that can be applied in practice to a robotics path planning problem. It enriches the



literature by linking to existing research in that area, modeling realistic constraints of bounded speeds, different players' speeds and limited sensor range. In the robotics motion planning area, two approaches are used. Using the terminology in (LaValle 2006), these are combinatorial approaches that find exact solutions through a continuous space and sampling-based approaches that divide the space (probabilistically or deterministically) into discrete regions and find paths through these regions. The simplest form of deterministically dividing the space (a.k.a cell decomposition) is with a grid of fixed resolution. The main advantage of this approach is its simple implementation in contrast to combinatorial methods, many of which are impractical to implement. However, cell decomposition methods are resolution complete (unlike combinatorial methods), which means that they find a solution when one exists only if the resolution of the sampling grid is fine enough. Another common drawback with grid methods is that their complexity depends on the grid size (Gonz alezBa nos, Hsu, and Latombe 2006). The main contribution of this paper is the development of an algorithm that enhances recent results in AI planning and visibility-based pursuit-evasion to tackle the computationally prohibitive task of deciding the outcome of the pursuitevasion game. This is a significant improvement over earlier depth-limited or heuristic-based approaches. The computed results are also used to find optimal player trajectories and optimize various objectives. The basic model (El-Alfy and Kabardy 2011) is formally studied to facilitate the derivation of several suggested optimizations. This, to our knowledge, provides first evidence of the feasibility of optimal decisions at such high grid-resolutions under varying speed ratios, different visibility constraints and regardless of obstacle geometries. In the rest of this paper, we proceed with a more detailed literature review followed by a formal problem definition. Our approaches to solve the decision problem and compute tracking strategies are then presented and the results follow. Finally, we conclude and suggest future work.



Related Work

A large amount of literature has been devoted to pursuitevasion games. In this section, we review closely related work, with a focus on attempts at deciding the outcome of the game. In the field of robotics, we survey recent work in the problem where one pursuer maintains the visibility of one evader, in an environment with obstacles. In artificial intelligence, we review the related problem of cops and robbers. In the area of robotics motion planning, the main approach used is to decompose the environment into noncritical regions with critical curve boundaries, across which critical changes in occlusion and collision occur, then use a combinatorial motion planning method. Murrieta-Cid et al. (2007) model the pursuit-evasion game as a motion planning problem of a rod of variable length, creating a partitioning of the environment that depends on the geometry of obstacles. Later (2008), they present a convex partitioning that



is modeled as a mutual visibility graph. Their method alternates between an evader assumed to take the shortest step to escape, countered by a pursuer that computes a preventionfrom-escape step, which produces a sequence of locally optimal paths. This leads to an interesting result: to decide which player wins, every feasible ordering of local paths has to be checked, concluding it is an NP-complete problem. Bhattacharya et al. address the problem of maintaining the visibility of an escaping evader and show that it is completely decidable around one corner with infinite edges (Bhattacharya, Candido, and Hutchinson 2007). The authors then extend their work in (2008) to deal with more general environments with convex obstacles. They split the environment into decidable and non-decidable regions and approximate bounds on these regions. They also provide a sufficient condition for escape of the evader. Recently, they used differential games theory to analyze that problem under complete information, suggesting a formulation in which the pursuer maximizes the time for which it can track the evader while the evader minimizes it (Bhattacharya, Hutchinson, and Bas ar 2009; Bhattacharya and Hutchinson 2010). Computing equilibrium strategies gives necessary and sufficient conditions for tracking. They present results around a point obstacle, a corner and a hexagonal obstacle. Within the AI planning community, the related problem of cops and robbers consists of one or more players (cops) trying to find and catch one or more evaders (robbers). The players perform alternating moves. This makes the game naturally discrete, often modeled as a graph with vertices representing the game's states. The mathematical foundations for solving these problems are surveyed in (Hahn 2007). A polynomial time optimal algorithm that determines whether the cops or the robbers win, and in how many steps, has only been recently given in (Hahn and MacGillivray 2006). Unfortunately, methods for computing optimal strategies have always been impractical to implement. For example, Moldenhauer and Sturtevant (2009a) compute optimal move policies offline in 2.5 hours per environment using an enhanced form of the algorithm in (Hahn and MacGillivray 2006). For that reason, several heuristics have been used in order to compute near optimal approximations in practical time. In (Moldenhauer and Sturtevant 2009b), different optimal strategies are studied on small maps while in (2009a), larger maps are used to evaluate less optimal strategies against optimal ones. One of the first practical implementations of the cops and robbers game is presented in (Ishida and Korf 1995) under the name of movingtarget search. Since, that problem has been extensively studied using a variety of heuristics such as incremental heuristic search (Koenig, Likhachev, and Sun 2007) and Cover heuristic (Isaza et al. 2008), to name a few recent references.



Problem Definition

This is a two-player game, with one pursuer and one evader modeled as points that can move in any planar direction (holonomic robots). Each player knows exactly both the position and the velocity of the other player. We consider two-dimensional environments containing obstacles that obstruct the view of the players. Obstacles have known ar-



bitrary geometries and locations. The assumption of complete information is used here to derive the outcome of the game, since if some player loses with complete information, it will always lose under other conditions. Both players have bounded speeds, move at different speeds and can maneuver to avoid obstacles. We will denote the maximum speed of the pursuer vp , that of the evader ve and their ratio r = ve /vp . Players are equipped with sensors that can "see" in all directions (ommnidirectional) and as far as the environment boundaries or obstacles, whichever is closer. We will see later that we can model minimum and maximum ranges for sensors with a simple variation in our algorithm. The pursuit-evasion game proceeds as follows. Initially, the pursuer and the evader are at positions from which they can see each other. It is common in the literature to define two players to be visible to one another if the line segment that joins them does not intersect any obstacle. The goal of the game is for the pursuer to maintain visibility of the evader at all times. The game ends immediately, if at any time, the pursuer loses sight of the evader. In that case, we say that the pursuer loses and the evader wins.



Deciding the Outcome of the Game

Deciding the game requires the construction of a binary function in two variables for the initial positions of both players. In order to study the progress of the game, we introduce a third parameter for the time index which is a discrete version of time in the continuous case. We call this function Bad(p, e, i). When Bad evaluates to 1, it means there exists a strategy for an evader starting at e to go out of sight of a pursuer at p by the ith time index. It is clear that Bad(p, e, 0) corresponds directly to visibility constraints and is straight forward to compute. We seek an algorithm to determine the value of Bad(p, e), with the time index dropped to indicate the end result of the game as time tends to infinity. In logical contexts, Bad is used as a predicate.



Equation (1) computes Bad(p, e, i + 1) inductively by evaluating the necessary escape conditions as of the ith step. For any given non-trivial initial configuration, the very first application of the inductive step would only yield a change for cases where the evader starts right next to an obstacle and is able to hide behind it immediately. That is because the Bad(p, e, 0) is only 1 for initially obstructed cells. After many iterations of the induction over all cells, more pairs farther and farther from obstacles get marked as bad. The expansion of the bad region only stops at cells which the pursuer in question is able to track and the function stabilizes with Bad(p, e) = 1 if and only if the evader can win. This bears a discrete resemblance to integrating the adjoint equations backward in time from the termination situations as presented in (Bhattacharya and Hutchinson 2010). Algorithm 1 performs backward visibility induction as defined in (1) to matrix M which is initialized with initial visibility constraints. To determine mutual visibility between cells, we use Bresenham's line algorithm (Bresenham 1965) to draw a line connecting every two cells and see if it passes through any obstacle. This approach works for any geometry and is easy to implement. More sophisticated algorithms could be used but are hardly justified. Restrictions on visibility can be easily incorporated by modifying the initialization part at line 5. For example, for a limited sensing range Rmax , we add or D(p, e) > Rmax , where D is the distance. If the pursuer is not to come any closer than a minimum distance to the evader, a lower bound Rmin can be added as well. Algorithm 1: Decides the game for a given map. Input : A map of the environment. Output: The Bad function encoded as a bit matrix. Data: Two N x N binary matrices M and M . 1 begin 2 Discretize the map into a uniform grid of N cells. // Visibility initialization 3 Initialize M and M to 0. 4 foreach (p, e)  grid x grid do 5 if e not visible to p then M [p, e] = 1 6 end // Induction loop 7 while M = M do 8 M =M 9 foreach (p, e)  grid x grid do 10 if e  N (e) s.t.p  N (p) M [p , e ] = 1 then M [p, e] = 1 11 end 12 end 13 return M 14 end



The Visibility Induction Algorithm

Fix a pursuer and evader at grid cells p and e and let i be the time index. If Bad(p, e, 0) = 1, then the evader is not initially visible to the pursuer and the game ends trivially with the pursuer losing. Now, consider the case where the evader manages to escape at step i + 1. To do so, the evader must move to a neighboring cell e where no corresponding move exists for the pursuer to maintain visibility. In other words, all neighbors p of the pursuer either cannot see the evader at e or, otherwise, were shown unable to keep an evader at e in sight up to step i i.e. Bad(p , e , i) = 1. This means that when Bad(p, e, i + 1) is updated to 1, the outcome of the game has been decided for the configuration in question as a losing one, i.e. there is a strategy for the evader to escape the sight of the pursuer at some step  i, but not any sooner. We use N (c) for the set of neighboring cells a player at c can move to. With that, we have the following recurrence: Bad(p, e, i + 1) = 1 (p, e)e  N (e) s.t. p  N (p) Bad(p , e , i) = 1 (1)



Proof of Correctness

We start by showing that the algorithm always terminates. At the end of each iteration, either M = M and the loop exits or more cells get marked as bad, which stops when



all cells are marked, leaving M = M . Next, we use this induction: 1. By line 6, M contains the decision at step 0 as enforced by the visibility constraints computed in line 5. 2. After the ith iteration of the loop at line 9, M [p, e] = 1 iff the evader has an escape strategy e where the pursuer has no corresponding strategy p with M [p , e ] = 0.



Level-0 Caching It is evident most of the computations are dedicated to evaluating the escape conditions as presented in (1). It is crucial to skip any unnecessary evaluations that yield no updates. Lemma 2. Bad(p, e, i) = Bad(p, e, j )  j > i Proof. For Bad(p, e, i + 1) in (1), put e = e. Corollary 3. Bad(p, e, i) = Bad(p, e, j )  j < i Lemma 2 allows skipping pairs that have already been decided by a simple addition to the condition in line 10. Level-1 Caching As Lemma 4 suggests, we need only reevaluate the conditions for those players who witnessed a change at the previous iteration. This can be applied independently to pursuers and evaders leading to a 45% average speedup. When applied to both we reached 50%. This comes at an additional O(N ) storage, which is negligible compared to the M matrix. Lemma 4. (Synchronized Neighborhoods) Bad(p, e, i)  Bad(p, e, i + 1) = (p , e )  N (p) x N (e) s.t. Bad(p , e , i - 1)  Bad(p , e , i) Proof. Bad(p, e, i) = Bad(p, e, i - 1)

 



Complexity Analysis

The algorithm uses O(N 2 ) storage for the output and temporary matrices M and M . Initializing the matrices takes O(N 2 N ) time if naive line drawing is used for each pair, which is linear in the length of the line. The inner loop at line 9 processes O(N 2 ) pairs each costing O(2 ) where  = max(|N (p)|, |N (e)|). Per the preceding discussion, at the ith iteration, the algorithm decides the game for all escape paths of length (i+1). Let L be the length of the longest minimal escape trajectory for the given environment. Obviously, the induction loop at line 7 is executed O(L) times. We can see that L depends on the largest open area in the environment and also the speed ratio r, with equal speeds being the worst case, where the distance between the players may not change, as the game ends earlier otherwise. A worst case scenario is an equally fast evader starting very close to the pursuer. For such an evader to win, it would need to move along with the pursuer to the closest obstacle where it can break visibility. We conclude that L = O(N ) and would typically be smaller in practice. With that, the visibility induction algorithm is O(2 N 3 ). Theorem 1. (Visibility Induction) Algorithm 1 decides the discretized game for a general environment in O(2 N 3 ). Proof. By the discussion above, the proof follows.



(by C.3) (2)



= e  N (e) p  N (p) s.t. Bad(p , e , i - 1) Bad(p , e , i)



Bad(p, e, i + 1) = e  N (e) s.t. p  N (p) By (2) and (3), the existence of (p , e ) is established.



(3)



Practicalities and Optimizations

We present several enhancements to the visibility induction algorithm and the speedups they yield. Our time measurements are performed using test maps of 100 x 100 cells for 4 2 1 1 1 , 3 , 2 , 3 , 5 ]. All run times are averaged speed ratios [1, 5 over 10 runs. Memory Savings As binary matrices, both M and M need only 1 bit per entry. It is also obvious we need only store bits for valid states, which reduces N to the number of free cells. This allows N to exceed 60, 000 using less than 1GB of memory, which enables processing at resolutions around 250 x 250 cells. Parallelization Observe that the loop at line 9 reads from matrix M and writes to M . This means that M updates are embarrassingly parallel. In our C++ implementation, we used the cross-platform OpenMP library to exploit this property. By adding a single line of code, we were able to harness the multiprocessing capabilities commonly available today. This allowed a 36% average speedup.



Level-2 Caching Strict application of Lemma 4 results in re-evaluating the conditions only for pairs who witness related changes. Keeping track of that comes at a higher storage cost of O(N 2 ), which is equivalent to the M matrix. Adding the level-2 cache resulted in a 52% average speedup. With that, we reach a new complexity result. Note that caching under parallelization is particularly tricky and requires careful update and invalidation mechanisms. Lemma 5. Level-2 caching makes the induction loop O(4 N 2 ). Proof. By only processing a pair (p, e) having a related update in both N (p) and N (e), no pair gets processed more than |N (p)| x |N (e)| = O(2 ) times. As the total number of pairs is O(N 2 ) and processing a single pair takes O(2 ), this amounts to O(4 N 2 ). More Memory Savings It is possible to do without the auxiliary M matrix. Instead of copying values before the inner loop and doing all checks on old values, we can use the M matrix for both checks and updates. If some entry M [p, e] is not updated, the behavior would be the same. On the other hand, if M [p, e] got updated, the algorithm would use a newer value instead of waiting for the next iteration which results in a minor speedup as a side-effect.



Applying all the enhancements discussed in this section led to a 64% average speedup on our test sample. We notice that for the medium sized square grids we consider, initialization of matrices is above quadratic by a small factor which is dominated by the number of iterations L. Furthermore, as  is typically limited (players have bounded speeds) and can be considered constant for a given realization, it may be ignored in comparison to N as the algorithm approaches O(N 2 ). For the typical case of a limited sensing region of size R, the algorithm need only consider that many evaders. This effectively replaces one N in all the above expressions and allows processing at much higher resolutions.



Optimal Trajectory Planning

If the pursuer can keep the evader in sight, there is not much the evader can do as far as we are concerned. On the other hand, if the evader can win the game, it is particularly important to minimize the time taken to break the line of sight to the pursuer. The losing pursuer must also maximize that time by not making suboptimal moves that allow the evader to escape faster. To compute these optimal trajectories, we modify Algorithm 1 to store the time index i, at which the game got decided, into M [p, e]. In lines 5 and 10, we use 0 and i, respectively, instead of just 1, and only make the assignment once for the smallest i. The matrices are initialized to  to indicate the absence of an escape strategy for the evader and the condition in line 10 is modified accordingly. We call the enhanced Bad function J (p, e) as it gives the time left for visibility, which corresponds to the value of the game as in (Bhattacharya and Hutchinson 2010). Theorem 6. (Time-Optimal Trajectories) J (p, e) gives the time left before visibility is broken, assuming both players move optimally. Proof. Trivially, J (p, e) =  = Bad(p, e) and the evader has no escape strategy. When Bad(p, e) is marked at the ith iteration for a given pair, an escape trajectory becomes available to the evader at e . No such escape trajectory could be found up to step i - 1. By definition of the escape cell e and J : p  N (p) Bad(p , e , i - 1) = J (p , e ) < J (p, e) By following e  e  * * *  e(k) , J (p(k) , e(k) ) is guaranteed to reach 0 at cell e(k) where visibility is broken. From Lemma 4, p  N (p) s.t. J (p, e) = J (p , e ) + 1. By repeatedly selecting p , the pursuer can force k to attain its maximum value i.e. J (p, e). At each step, the players will be moving to neighbors p and e , maximizing J (p , e) and minimizing J (p, e ), respectively. The obtained J (p, e) can be processed further to have these neighbors precomputed into separate functions Sp and Se , encoding the trajectories for each player. However, as storing the time index i increases the required storage, the algorithm can be modified to compute Sp and Se directly. This allows reducing the storage by describing the neighbor relative to the current position of the player. The neighborhoods can be indexed unambiguously which allows the precomputed S functions to store just as many bits as necessary per entry, which is as small as log2  . We omit the modified algorithms due to the limited space.



Discrete Tracking Strategies

Because the Bad function decides the game for all pairs including all combinations of the neighboring cells for both players, it can be used beyond determining the winner for trajectory planning. As a zero-sum game, there will only be one winner; and a valid trajectory must maintain this property. Further objectives can be defined as needed and an optimal trajectory can then be chosen. In particular, we are interested in optimal escape trajectories for a winning evader minimizing the visibility time. Other objectives relating to the distance between players, the distance traveled or speed of maneuvering can also be used.



Extensions to Guaranteed Tracking

The computed Bad function can be used directly in basic trajectory planning for the winning player. A valid trajectory must maintain the winning state by only moving via cells with guaranteed win regardless of the strategy followed by the opponent. A higher level plan may then choose any of the valid neighbors, which are guaranteed to exist for the winner. To unify the notation used below, we define: Lose(a, b) =(P ursuer(a)  Bad(a, b))  (Evader(a)  Bad(b, a)) (4)



Algorithm 2 first discards invalid neighbors a winner must not move to, then chooses one of the remaining neighbors. Typically, a distance function is used for tie breaking. For example, a pursuer would generally prefer to move closer to the evader and keep it away from bad cells. Algorithm 2: Generic trajectory planning for winners. Input: Bad(., .), current state (player, opponent). 1 begin 2 N  = {} 3 foreach n  N (player) do 4 if Lose(n, n ) n  N (opponent) then 5 N = N  n 6 end 7 end 8 Move to any neighbor in N  . 9 end



Experimental Results

We implemented our algorithms in C++ and performed experiments on an Intel Core i7 CPU running at 2.67GHz with 4GB of RAM. We experiment with manually created environments containing obstacles of various forms. Initial players positions are randomly selected satisfying certain visibility constraints and the evader's paths are automatically generated as discussed in the above section on tracking strategies. We discretized the maps of the used environments with



Figure 1: Decision map Figure 2: Decision map (evader) - 50x50 map. (evader) - 200x200 map.



Figure 5: Decision map Figure 6: Decision map (evader) for a circular ob- (evader) with restricted visibility. stacle.



Figure 4: Decision map Figure 3: Decision map (pursuer) for polygonal ob(pursuer) around a corner. stacles. regular grids of sizes ranging from 50x50 to 400x400 cells and used 4-connected and 8-connected neighborhoods. With such fine granularities, we anticipate moving to real world environments. The effect of varying the grid size on the resolution of decision maps is shown in figures 1 and 2 for speed ratios 2 1 1 1 [1, 4 5 , 3 , 2 , 3 , 5 ]. The boundaries of the nested convex regions are such that if the two players fall inside a region, the pursuer can track the evader indefinitely, while if one player is inside and the other outside, the evader can escape. The darker the gray shade, the smaller the speed ratio. Obstacles are in black and the player mentioned in each figure is the black dot roughly centered inside all nested regions. Our approach works independently of obstacle shapes and layouts as shown in figures 3 to 5. Modeling visibility constraints (e.g. limited sensor range) affects the decision regions as in figure 6. All previous results are computed for 4-connected neighborhoods. Figures 7 and 8 contrast 4-connected to 8-connected neighborhood maps for a speed ratio of 0.5. Finally, we show two tracking scenarios in figures 9 and 10. As we vary the grid size, runtime is affected as shown in table 1. It fits a quadratic model in N (for a fixed neighborhood size) as by the discussion following Lemma 5.



Figure 7: Decision map Figure 8: Decision (evader) for a 4-connected map (evader) for an 8neighborhood. connected neighborhood. particular, we developed an algorithm that decides the outcome of the game for any pair of initial positions of the players. By employing a space/time discretization approach, the solution becomes feasible in polynomial time, is independent of the geometry of the environment and does not require the use of heuristics. We give a detailed analysis for the correctness of the algorithm, derive its space and time complexities and present and verify several approaches to reduce its time and memory demands. We extended our algorithm to compute tracking or escape trajectories for both players in real time and verified their optimality. We tested our method on different tracking scenarios and environments. We are currently working on a realization of the proposed



Conclusions

We addressed the problem of maintaining an unobstructed view of an evader moving amongst obstacles by a pursuer. In



Table 1: Average runtime for Algorithm 1 vs. grid size Grid size Runtime (hh:mm:ss) 50 x 50 00:00:01 60 x 60 00:00:02 75 x 75 00:00:06 100 x 100 00:00:23 120 x 120 00:00:52 150 x 150 00:02:27 200 x 200 00:09:24 300 x 300 01:06:18 400 x 400 06:47:57



Figure 9: Example of a Figure 10: Example of a winning evader (blue; top). winning pursuer (red; top). method using real robots equipped with sensors. To model the real world more accurately, we consider hexagonal mesh discretizations, with several interesting properties, and study decision errors for a given resolution. We already implemented limited range sensors and an extension to a limited field of view is systematic. Other constraints on players motion can be considered such as restricted areas where the pursuer is not allowed to go into. Regaining lost visibility or, alternately, allowing for blind interruptions are interesting extensions with a slight relaxation to the hard visibility constraint. Finally, we envision efficient ways to extend this approach to the case of more players.



References

Bhattacharya, S., and Hutchinson, S. 2008. Approximation schemes for two-player pursuit evasion games with visibility constraints. In Proceedings of Robotics: Science and Systems IV. Bhattacharya, S., and Hutchinson, S. 2010. On the existence of nash equilibrium for a two player pursuit-evasion game with visibility constraints. The International Journal of Robotics Research 29(7):831-839. Bhattacharya, S.; Candido, S.; and Hutchinson, S. 2007. Motion strategies for surveillance. In Proceedings of Robotics: Science and Systems III. Bhattacharya, S.; Hutchinson, S.; and Bas ar, T. 2009. Gametheoretic analysis of a visibility based pursuit-evasion game in the presence of obstacles. In Proc. American Control Conference (ACC'09). Borie, R.; Tovey, C.; and Koenig, S. 2011. Algorithms and complexity results for graph-based pursuit evasion. Autonomous Robots 31(4):317-332. Bresenham, J. 1965. Algorithm for computer control of a digital plotter. IBM Systems Journal 4(1):25-30. El-Alfy, H., and Kabardy, A. 2011. A new approach for the two-player pursuit-evasion game. In Proc. 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI'11), 396-397. Incheon, Korea: IEEE. Gerkey, B. P.; Thrun, S.; and Gordon, G. 2004. Visibilitybased pursuit-evasion with limited field of view. 20-27. Gonz alez-Ba nos, H. H.; Hsu, D.; and Latombe, J.-C. 2006. Motion planning: Recent developments. Autonomous Mo-



bile Robots: Sensing, Control, Decision-Making and Applications (CRC Press) 373-416. Hahn, G., and MacGillivray, G. 2006. A note on k-cop, l-robber games on graphs. Discrete mathematics 306(1920):2492-2497. Hahn, G. 2007. Cops, robbers and graphs. Tatra Mountains Mathematical Publications 36:163-176. Hsu, D.; Lee, W. S.; and Rong, N. 2008. A point-based POMDP planner for target tracking. In Proc. IEEE International Conference on Robotics and Automation (ICRA'08), 2644-2650. Isaza, A.; Lu, J.; Bulitko, V.; and Greiner, R. 2008. A cover-based approach to multi-agent moving target pursuit. In Artificial Intelligence and Interactive Entertainment Conference (AIIDE). Ishida, T., and Korf, R. E. 1995. Moving-target search: a real-time search for changing goals. Pattern Analysis and Machine Intelligence, IEEE Transactions on (TPAMI) 17(6):609-619. Klein, K., and Suri, S. 2011. Complete information pursuit evasion in polygonal environments. In TwentyFifth AAAI Conference on Artificial Intelligence (AAAI'11), 1120-1125. Koenig, S.; Likhachev, M.; and Sun, X. 2007. Speeding up moving-target search. In Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems (AAMAS'07), 1-8. Honolulu, HI, USA: ACM. Kolling, A., and Carpin, S. 2010. Multi-robot pursuitevasion without maps. In Proc. IEEE International Conference on Robotics and Automation (ICRA'10), 3045-3051. LaValle, S. M.; Gonz alez-Ba nos, H. H.; Becker, C.; and Latombe, J.-C. 1997. Motion strategies for maintaining visibility of a moving target. In Proc. IEEE International Conference on Robotics and Automation (ICRA'97), volume 1, 731-736. LaValle, S. M. 2006. Planning Algorithms. New York, NY, USA: Cambridge University Press. Moldenhauer, C., and Sturtevant, N. R. 2009a. Evaluating strategies for running from the cops. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI'09), 584-589. Moldenhauer, C., and Sturtevant, N. R. 2009b. Optimal solutions for moving target search. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS'09) - Volume 2, 1249-1250. Murrieta-Cid, R.; Muppirala, T.; Sarmiento, A.; Bhattacharya, S.; and Hutchinson, S. 2007. Surveillance strategies for a pursuer with finite sensor range. The International Journal of Robotics Research 26(3):233-253. Murrieta-Cid, R.; Monroy, R.; Hutchinson, S.; and Laumond, J.-P. 2008. A complexity result for the pursuitevasion game of maintaining visibility of a moving evader. In Proc. IEEE International Conference on Robotics and Automation (ICRA'08), 2657-2664.



INSTITUT FUR INFORMATIK

Lehr- und Forschungseinheit fur Programmier- und Modellierungssprachen Oettingenstra e 67, D{80538 Munchen



Nurse Scheduling using Constraint Logic Programming

Slim Abdennadher, Hans Schlenker



http://www.pms.informatik.uni-muenchen.de/publikationen Forschungsbericht/Research Report PMS-FB-1999-7, July 1999



appeared in Eleventh Annual Conference on Innovative Applications of Arti cial Intelligence, IAAI-99, Orlando, Florida



Nurse Scheduling using Constraint Logic Programming

Computer Science Institute, University of Munich Oettingenstr. 67, 80538 Munich, Germany Slim.Abdennadher@informatik.uni-muenchen.de



Slim Abdennadher



Technical University of Berlin Franklinstr. 28/29, 10587 Berlin, Germany hans@cs.tu-berlin.de ming and constraint solving. In logic programming, problems are stated in a declarative way using rules to de ne relations (predicates). Problems are solved using chronological backtrack search to explore choices. In constraint solving, e cient special-purpose algorithms are employed to solve sub-problems involving distinguished relations referred to as constraints, which can be considered as pieces of partial information. The nurse scheduling problem can be elegantly formalized as a constraint satisfaction problem (Mac92) and implemented by means of specialized constraint solving techniques that are available in CLP languages. In this paper, the generation of duty rosters for hospitals is tackled using the CLP framework. The System is called INTERDIP and has been successfully tested on a real ward at the \Klinikum Innenstadt " hospital in Munich (AS97). INTERDIP has been implemented in collaboration with Siemens-Nixdorf-Informationssysteme AG using IF/Prolog (Sie96b) which includes a constraint package (Sie96a) based on CHIP (DVS+ 88). This package includes, among others linear equations, constraints over nite domains and boolean constraints. The nurse scheduling problem consists in assigning a working shift to each nurse on each day of a planning period (usually one month), whereby several requirements must be considered, such as minimal allocation of a ward, legal regulations and wishes of the personnel. Usually not all speci ed requirements can be ful lled. The nurse scheduling problem can be modelled as a partial constraint satisfaction problem (FW92). It requires the processing of hard and soft constraints to cope with. Hard constraints are conditions that must be satis ed, soft constraints may be violated, but should be satis ed as far as possible. Several approaches have been proposed to deal with soft constraints: Hierarchical constraint logic programming (HCLP) (BFW92) supports a hierarchical organiziaton of constraints, where a constraint on some level is more important than any set of constraints from lower levels. To avoid the so called inter-hierarchy comparison in HCLP, the soft constraints are encoded in a hierarchical constraint satisfaction problem (HCSP) (Mey97). The Conplan/SIEDAplan (Mey97) considers the representation of nurse scheduling as a HCSP, where legal



Hans Schlenker



The nurse scheduling problem consists of assigning working shifts to each nurse on each day of a certain period of time. A typical problem comprises 600 to 800 assignments that have to take into account several requirements such as minimal allocation of a station, legal regulations and wishes of the personnel. This planning is a di cult and time-consuming expert task and is still done manually. INTERDIP1 is an advanced industrial prototype that supports semi-automatic creation of such rosters. Using the arti cial intelligence approach, constraint reasoning and constraint programming, INTERDIP creates a roster interactively within some minutes instead of by hand some hours. Additionally, it mostly produces better results. INTERDIP was developed in collaboration with Siemens Nixdorf. It was presented at the Systems'98 Computer exhibition in Munich and several companies have inquired to market our system.



Abstract



Many real-life problems lead to combinatorial search, computationally a very intensive task. Unfortunately, no general method exists for solving this kind of problems e ciently. The automatic generation of duty rosters for hospital wards falls under this class of problems. Since the manually generated solution of the nurse scheduling problem usually requires several hours of work, a lot of research has been done to reduce the amount of time needed in the roster development. The most popular technique is based on mathematical programming (War76). The main disadvantage of this approach is the di culty of incorporating applicationspeci c constraints into the problem formulation. Other methods include goal programming (AR81) and heuristic models (SWB79). Recently, Constraint Logic Programming (JM94; FA97; MS98) (CLP) has become a promising approach for solving scheduling problems. CLP combines the advantages of two declarative paradigms: logic programINTERDIP is an acronym for the German \Interaktiver Dienstplaner". Copyright c 1999, American Association for Arti cial Intelligence (www.aaai.org). All rights reserved.

1



Introduction



regulations are hard constraints and wishes of nurses usually have the lowest priority level. The result is also not necessarily of a reasonable quality in respect to the nurse's wishes. However, in practice nurses' wishes should be considered in order to support the working climate. Furthermore, some wishes of nurses are sometimes more important than some legal regulations. To deal with these requirements, INTERDIP provides a solution technique based on a variant of branch-and-bound search instead of chronological backtracking. This approach starts with a solution and requires the next solution to be better. Quality is measured by a suitable cost function. The cost function depends on the set of satis ed soft constraints. To improve on the theoretical complexity of the problem, our system is based on an imitation of the human way of solving the problem: A roster is generated with INTERDIP through several phases. Additionally, several days in the roster are assigned simultaneously through user de ned patterns. A pattern describes a preferred sequence of working days. With INTERDIP, a user who is to some extent familiar with nurse scheduling can interactively generate a roster within minutes. The paper is organized as follows. The next section introduces the nurse scheduling problem. Then we show how the problem can be modelled as a partial constraint satisfaction problem. In Section 4 and Section 5 we describe the implementation and the user interface. Finally, we conclude with an evaluation of our tool. Portions of this paper were taken from (AS99). In a hospital, a new duty roster must be generated for each ward monthly. A hospital ward is an organizational unit that has to ful l some concrete tasks, and has both rooms and personnel, the nurses, at its disposal. Usually, the wards of a hospital are completely distinct: each has its own rooms and its own personnel. Therefore all rosters of a hospital can be scheduled separately. We consider in the following the scheduling problem for one ward. A roster of one month is an assignment of the personnel of the ward to the shifts for all the days of the month. A shift is a working unit: in a common working model, each day has the units morning shift (e.g. 06:00 to 15:00), evening shift (14:00 to 23:00), and night shift (22:00 to 07:00) and possibly others. To each shift of every day, personnel has to be assigned. For the generation of a roster, di erent kinds of constraints must be taken into account: Legal regulations, e.g. the maximum working time of a person per day or week, or time o in lieu, or maternity leave. In Germany for example the statutory monthly core working hours for a hospital with a 37.5 hour week is about 160 hours depending on the month. So, with an average shift length of 8 hours,



Description of the problem



each nurse has to work on average 20 shifts. Another law says that between two (working-) shifts, each nurse has to have a break of at least 11 hours (\11 hours rule"). If a nurse works one day in the night shift, she must therefore not be assigned the morning or evening shift the next day. Also a morning shift must not follow an evening shift. Organizational rules are those that apply speci cally to one particular hospital, a part of a hospital or even only one ward. They are given by the respective management. Those are mainly the number and kind of the shifts and { within statutory limits { the minimum personnel allocation of each ward. In the following we consider a model with three shifts: morning, evening and night shift. To morning shift and evening shift at least three nurses must be assigned, and the night shift requires at least two nurses. Personnel data de ne the individual frame for each person. These are mainly the contractually established monthly core working time, pending vacation and accrued hours of overtime. If, for example, a nurse has 16 hours overtime, she might be scheduled two shifts less than average. Finally, wishes are requirements given by the personnel. These are mostly wishes to have some days o , for example at weekends, holidays, birthdays, or for a vacation period. Often, there is no duty roster that ful lls all the constraints. Therefore we distinguish two kinds of constraints. Hard constraints must always be satis ed, soft constraints may be violated. Roughly speaking, legal regulations, organizational rules and personnel data determine hard constraints, wishes may be hard or soft constraints. So for example the vacation scheduling might be done for a longer term (some months) apart from the actual roster planning. Then a wish for one day of vacation would be a hard constraint, because it was planned externally. Other wishes are mostly soft constraints. Often the nurses have the opportunity to classify their wishes into some \priority levels". If possible, the wishes in one of those levels will then be regarded as hard constraints. A roster is correct, i all hard constraints hold. The quality of a roster results from the number of the fullled soft constraints and their priorities. Constraint Satisfaction Problems (CSPs) have been a subject of research in arti cial intelligence for many years. A CSP is a pair (V; C ), where V is a nite set of variables, each associated with a nite domain, and C is a nite set of constraints. A solution of a CSP maps each variable to a value of its domain such that all the constraints are satis ed. A partial constraint satisfaction problem (PCSP) (FW92) is a triple (V; C; !), where (V; C ) is a CSP and ! maps constraints



Modeling the problem as PCSP



to weights. A constraint's weight expresses the importance of its ful llment, allowing to distinguish hard constraints, which must not be violated, from soft constraints, which should not be violated, but may be violated in case this is unavoidable. Hard constraints have an in nite weight. The nite weights of soft constraints allow for the speci cation of priorities among constraints. A solution of a PCSP maps each variable to a value of its domain such that all hard constraints are satis ed and the total weight of the violated soft constraints is minimal. In the representation of nurse scheduling as a PCSP, there is a constraint variable for each nurse on each day. The domains of the variables consist of possible shifts (also comprising vacations, recuperation of a worked public holiday, special leaves, maternity protection, unpaid leave etc.), so they usually consist of 10 values. (HW96) proposed a reduction of variable domains, based on elimination of interchangeable values introduced by Freuder (Fre91). The values of the above mentioned free shifts, e.g. vacations, can be reduced to only one value and each variable takes its values now in f0; 1; 2; 3g. For a nurse i and a day j a variable V may have one of the following values: V = 0: The nurse i is o -duty the day j . V = 1: The nurse i is assigned to the \morning" shift on the day j . V = 2: The nurse i is assigned to the \evening" shift on the day j . V = 3: The nurse i is assigned to the \night" shift on the day j . Reducing the variable domains from 10 values to 4 considerably improves the e ciency of the solution research. Figure 1 shows a complete schedule for 10 nurses and 14 days. Each row comprises the shifts of a certain nurse. The columns contain the shifts performed on a certain day. So, each square of the chart speci es for each nurse the working days and shifts, and days o . E.g. on the 4th day the second nurse Hilde is scheduled in shift 1, i.e. morning shift. Now we describe how to express the most important requirements of our application in terms of IF/PrologConstraints (Sie96b). In the following, we use a Prologlike notation with meta-variables. We denote the total number of nurses to be scheduled by s, the total number of days by t and a variable by Vij , where i denotes the number of the nurse or the row in the roster, respectively, and j denotes the number of the day, i.e. the column in the roster. With this notation, we can write down all the variables of this modeling in a list: V11,V12,...,Vst]. One requirement for a correct roster is the minimum personal allocation, i.e. the minimal number of nurses, the ward must be allocated each shift. Actually, the allocation is limited downward and upward. Let Min1 be the lower and Max1 be the upper allocation limit for the morning shift and Min2, Max2, Min3 and Max3 the

ij ij ij ij ij



Figure 1: A nurse schedule for 10 nurses over a period of 14 days lower and upper limits for the evening and night shifts, respectively. Therefore a correct roster must not have less than Min1 and more than Max1 times the '1' in each column and not less than Min2 and not more than Max2 the '2' and so on. So we have to state for each j (1 j t) and each k (k 2 f1; 2; 3g) the following constraint: where cardinality(Lower,Upper,Condition) is satis ed if at least Lower and at most Upper conditions in the list Condition are satis ed. Another requirement a schedule has to ful l is the compliance of the monthly core working hours of each nurse. This means that there is a lower bound and an upper bound of shifts, each nurse is to be assigned in the schedule period. This is the number of all the morning, evening and night shifts. This can be expressed simpler by the number of free shifts. Let for each nurse i (1 i s) the lower bound for the working shifts be given by Mini and the upper bound by Maxi. Then we can formulate the working hours requirement using the cardinality constraint: cardinality(t-Maxi,t-Mini, Vi1=0,...,Vit=0]) The \11 hours rule" implies that a nurse must not work a morning shift (the day) after an evening shift and may work (the day) after a night shift only a night shift. We can express the \11 hours rule" by the following expression: If Vij is assigned a speci c value, the assignment of Vi(j + 1) must ful ll a certain condition. This can be expressed directly by the domain if constraint. We state for each i (1 i s) and for each j (1 j < t): domain if(Vij = 2, Vi(j + 1) n= 1) and domain if(Vij = 3, Vi(j + 1) in 0,3]).

cardinality(Mink ,Maxk , V1j =k ,V2j =k ,...,Vsj =k ])



The constraint domain if(Condition, ThenGoal) is used to call a goal conditionally. If the arithmetic constraint Condition is satis ed, ThenGoal is called. If the arithmetic constraint is not satis able, true is called. The execution of the domain if constraint is delayed as long as the satis ability of Condition has not been determined. Free shifts, provided they can be considered hard wishes, lead to immediate variable assignments. A wish (e.g. vacation) of nurse i at day j can then be stated as: Vij = 0. Soft wishes, like all other soft conditions, can not be stated directly as (IF/Prolog-)constraints, since our constraint solver can only handle hard constraints. We only can use them for optimizing correct rosters. This will be explained in Section . The modeling just described, while being simple and straightforward, is unfortunately very costly: The search space is huge, i.e. 4600 for 20 nurses and a period of one month. Therefore we developed a method to prune the search tree which was inspired by the usual manual planning.



Planning in INTERDIP



Planning by hand

Because of the huge search space a roster is usually generated by hand in two phases. In the rst phase we have all liberties for assigning the cells of the roster. Therefore here we do the most complicated assignment (which is tied to most of the conditions): the allocation of the free days or shifts. Those are bound to a lot of constraints: they determine how many shifts a nurse has to work during the scheduled period, how many nurses over all shifts the ward is assigned each day, and not least most of the wishes are to be considered here: the wishes for free shifts (e.g. vacations). Closely connected with the free shifts are the night shifts: the \11 hour rule" enforces for the assignment of shifts to a nurse, that after a night shift there may follow only a night shift or a free shift (free day). Therefore, when manually scheduling, the free and the night shifts are allocated in the rst phase. In the second phase, the morning and the evening shifts are distributed among the not yet allocated cells of the roster. The obvious advantage of the scheduling in two phases over the scheduling in one phase is the reduction of complexity: in each phase there have to be considered fewer constraints and, above all, fewer assignments2.

2 The assignment of the rst phase is normally not changed within the second unless it is then impossible to get a solution and a change in the free and night shifts will probably enable one. The extent of those changes can be neglected: we never observed more than 10 changes.



In 1993, (van93) presented a partial automatic solution to the nurse scheduling problem that used two very different phases. It exibly generated good rosters but did not handle night shifts. INTERDIP uses more than two phases which are performed in the same manner by one constraint solver. We wanted to reduce the search space even further than (van93) did. The idea is to furthermore decompose the problem. We use three phases instead of two: 1st Phase Distribution of the free shifts. 2nd Phase Distribution of the night shifts. 3rd Phase Distribution of the morning and the evening shifts. With this modeling, in each phase for every cell of the roster, only the minimal decision between two possibilities has to be made. This reduces the search space. We will see how we obtain a complete roster after the three phases. In each phase, every variable is assigned a value out of the boolean domain f0; 1g. Depending on the phase, the values 0 and 1 have di erent meanings. If a variable in the rst phase is assigned the value one, this means that the roster gets a free shift in the appropriate cell. The cells whose variables are bound to 0 remain undecided. The second phase only treats the undecided cells: if a variable gets the value 1, the cell is assigned a night shift. The rest remains undecided. In the third phase each still not decided cell is lled with either morning or evening shift, depending on whether the variable was assigned a 1 or a 0, respectively (see Figure 2, the meaning of the bold numbers is just as in Figure 1.). A complete roster results from all three phases.

0

1 1



Phasewise plan generation



3

1



1 2



0



0



0



Phase 1



Phase 2



Phase 3



Figure 2: Allocation of the cells in three phases.



Assignment patterns



Because of the incomplete constraint propagation methods used for scheduling problems, the application programmer often has to explicitly use a labeling phase in which a backtracking search blindly tries di erent values for the variables. Since labeling is expensive, the programmer needs to employ techniques for reducing the search space. There is a variety of techniques to do this. For our application we add domain information about presumably good solutions by introducing patterns. A pattern describes a preferred sequence of



working days. Coherent cells of the roster are allocated along user de ned patterns. As shown above, the variables are declared in each phase to range over the values 0 and 1 and the appropriate shifts are registered into the roster. Patterns are then meaningful combinations of roster entries, whereby a combination stands for successive days. A large number of these patterns is known. For example, we consider meaningful the combination of ve days work and two days free. The appropriate pattern for the rst phase, in which the working days are determined, is then: (?, ?, ?, ?, ?, 0, 0). If we assume that it is better to work on three successive days in the same shift than in different ones, we formulate for the second phase and thus for the night shifts: (3,3,3). Each phase has its own set of patterns. The patterns of a phase have an order in which they are selected: rst, the ones which result in a good solution, since the nurses are accustomed to this pattern, and at the end trivial patterns which are necessary to generate solutions, if they exist. For lling the roster, the given patterns are translated into appropriate variable assignments which are then tried in each row from left to right. The patterns can be considered as requirements of minor priority (soft constraints) as well as probable parts of solutions. Schedules that comply with the given patterns are explored rst. Applying this specialized labeling method reorganizes the search space. Additionally, each pattern is assigned a cost value so that for example a nurse whose wishes could not be fully ful lled, more likely gets \better" work patterns assigned. A roster that satis es all hard constraints is considered feasible but this does not necessarily mean that it is su ciently good to be used by a hospital ward. The concept of an optimal roster is hard to de ne. Generally, roster quality is a subjective matter and its de nition changes from problem to problem. We apply the usual measure which is common to all applications in the eld of scheduling. It is given in terms of the number and the priority of soft constraints that are violated. A popular approach consists in using a branch and bound search instead of chronological backtracking. Branch and bound starts out from a solution and requires the next solution to be better. Quality is measured by a suitable cost function. The cost function depends on the set of satis ed soft constraints. With this approach, however, soft constraints are only part of the cost function but play no role in selecting variables and values. In our multiphase method, branch and bound search is performed three times to improve the roster generated so far. Costs arise separately for each nurse and the algorithm tries to minimize the maximum of these. This means that we have a separate cost function for each



of the nurses and the maximum value of all the functions is minimized. So, INTERDIP tries to achieve that no nurse gets a much worse allocation (e.g. no wishes satis ed) than the others. For a nurse scheduling system to be complete, a exible user interface should be provided, so that the speci c requirements of the problem can be stated easily. INTERDIP provides such an interface. The INTERDIP user interface has been developed using the Tcl/Tk extension of IF/Prolog. Figure 1 shows a snapshot of the top-level graphical user interface to our nurse scheduling program with a generated roster. The interface allows the user to de ne the system parameters as preferred. All parameters like minimal and maximal allocation of the ward for each phase, wishes or patterns can be given graphically or in a spreadsheet. The wishes are given in three categories: imperative, important and less important wishes. We call them red, black and white wishes, respectively. This naming goes back to how the wishes were actually formulated in the hospital where we tested INTERDIP: They were lled into a plan using red and black pencils. The white wishes are to some extent standard wishes, like not to work on weekends. Red wishes (like vacation) are later treated as hard constraints and all the others as soft constraints. A single wish always relates to exactly one nurse and one day. Usually the generation of a roster runs as follows. After the user has speci ed all the conditions he will trigger the phases. A phase starts with generating the constraints and testing their consistency. Then, according to the above method, an optimal solution is computed. After a phase is nished, the next one is started initialized with the best result of the preceding phase, and so on. This is the automatic generation. It may happen that there exists not even one roster that complies with all the given hard constraints. Then the problem is called over-constrained. INTERDIP may detect this while generating the IF/Prolog constraints and then gives the user hints which of the conditions led to the inconsistency. However, there are kinds of contradictions that are not automatically detected. Therefore we built a debugger into INTERDIP. Being an interactive tool, INTERDIP lets the user take part in the generation in di erent ways. Firstly the user usually has some freedom in specifying the problem conditions. He can directly in uence the planning by giving some red wishes which directly lead to variable bindings. But the user can also interfere with a concrete process of allocation: He can use the debugger to break the computation manually or to set breakpoints. At the breakpoint (a cell of the roster), he is given all the possible patterns out of which he can choose one. The computation then continues with the selected allocation. In the single-step-mode the computation is stopped after each single allocation. Additionally the user can undo allocations already made.



Using the system interactively



Optimal rosters



With the debugger, the user can manually allocate parts of the roster in order to improve automatically presented solutions on the one hand and, in case the generator did not nd a solution at all, enable one on the other hand. In addition, the user can manually alter a completely generated roster and let it check by INTERDIP. The system then tries to state all the constraints for the given variable assignments, and if one fails, it gives the user hints about the contradictions. In this paper, the nurse scheduling problem is discussed and a speci c system, INTERDIP, is presented, that assists a human planner in scheduling the nurse working shifts for a hospital ward. We think that our approach can be applied to many applications in the eld of personnel assignment. It is quite obvious that the current implementation might even be used \as is" for every duty rota problem and therefore solves this whole problem class. It was possible to build this planning system for nurse scheduling within a few man months using a given commercial constraint solver, IF/Prolog from Siemens Nixdorf. The CLP code is just about 4000 lines with more than half of it for user interface. INTERDIP illustrates the important potentials of constraint logic programming for the implementation of real-life applications. INTERDIP was presented at the Systems'98 Computer exhibition in Munich and several companies are interested to market it. INTERDIP is currently tested at the \Klinikum Innenstadt " hospital in Munich. Typically, for 20 nurses and a period of one month, INTERDIP generates a satisfying (not optimal) schedule within a few minutes. The schedules generated by INTERDIP are comparable to those manually generated by a well experienced head nurse, sometimes even better than those. Of course this can not be guaranteed for every possible problem instance since, in general, the scheduling problem is NP-complete. J. L. Arthur and A. Ravindran. A multiple objective nurse scheduling model. In AIIE Transactions, volume 13, 1981. S. Abdennadher and H. Schlenker. INTERDIP { Ein Interaktiver Constraint-basierter Dienstplaner fur Krankenstationen. In F. Bry, B. Freitag, and D. Seipel, editors, 12th Workshop on Logic Programming WLP'97, September 1997. S. Abdennadher and H. Schlenker. INTERDIP { an interactive constraint based nurse. In Proceedings of the First International Conference and Exhibition on the Practical Application of Constraint Technologies and Logic Programming, 1999. A. Borning, B. N. Freeman-Benson, and M. Wilson. Constraint hierarchies. Lisp and Symbolic Computation, 5(3):223{270, 1992.



Conclusion



References



M. Dincbas, P. Van Hentenryck, H. Simonis, A. Aggoun, T. Graf, and F. Berthier. The Constraint Logic Programming Language CHIP. Technical Report TRLP-37, ECRC, Munich, 1988. T. Fruhwirth and S. Abdennadher. ConstraintProgrammierung: Grundlagen und Anwendungen. Springer-Verlag, September 1997. E. C. Freuder. Eliminating interchangeable values in constraint satisfaction problems. In AAAI-91 { Proceedings of the 9th national conference on arti cial intelligence, pages 227{233, 1991. E. C. Freuder and R. J. Wallace. Partial constraint satisfaction. Arti cial Intelligence, 58(1-3):21{70, 1992. K. Heus and G. Weil. Constraint programming a nurse scheduling application. In Proceedings of the Second International Conference on the Practical Application of Constraint Technology, pages 115{127, 1996. J. Ja ar and M. J. Maher. Constraint logic programming: A survey. Journal of Logic Programming, 20:503{581, 1994. A. Mackworth. Constraint satisfaction. In Stuart C. Shapiro, editor, Encyclopedia of Arti cial Intelligence. Wiley, 1992. Volume 1, second edition. H. Meyer auf'm Hofe. ConPlan/SIEDAplan: Personnel assignment as a problem of hierarchical constraint satisfaction. In Proceedings of the Third International Conference on the Practical Application of Constraint Technology, 1997. K. Marriott and P. Stuckey. Programming with Constraints: An Introduction. The MIT Press, 1998. Siemens Nixdorf Informationssysteme AG. IF/Prolog Constraint Problem Solver, 1996. Siemens Nixdorf Informationssysteme AG. IF/Prolog Users Guide, 1996. L. D. Smith, A. Wiggins, and D. Bird. Postimplementation experience with computer-assisted nurse scheduling in a large hospital. In Information Systems and Operational Research, volume 17, 1979. B. van den Bosch. Implementation of a CLP library and an application in nurse scheduling. Master's thesis, Katholieke Universiteit Leuven, Belgium, 1993. D. M. Warner. Scheduling nursing personnel according to nursing preference: A mathematical programming approach. In Operations Research, volume 24, 1976.



Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence



Towards Population Scale Activity Recognition: A Framework for Handling Data Diversity

Saeed Abdullah

Cornell University Ithaca, New York, USA



Nicholas D. Lane

Microsoft Research Asia Beijing, China



Tanzeem Choudhury

Cornell University Ithaca, New York, USA



Abstract

The rising popularity of the sensor-equipped smartphone is changing the possible scale and scope of human activity inference. The diversity in user population seen in large user bases can overwhelm conventional one-size-fits-all classication approaches. Although personalized models are better able to handle population diversity, they often require increased effort from the end user during training and are computationally expensive. In this paper, we propose an activity classification framework that is scalable and can tractably handle an increasing number of users. Scalability is achieved by maintaining distinct groups of similar users during the training process, which makes it possible to account for the differences between users without resorting to training individualized classifiers. The proposed framework keeps user burden low by leveraging crowd-sourced data labels, where simple natural language processing techniques in combination with multiinstance learning are used to handle labeling errors introduced by low-commitment everyday users. Experiment results on a large public dataset demonstrate that the framework can cope with population diversity irrespective of population size.



Introduction

With the explosion of smartphones, it is now possible to collect real-time daily activity data over a large population. This continuous availability of a vast amount of data changes the possibilities of human-centric applications and sensing. But, as the scope of the system broadens from carefullycontrolled experiments to mass-generated data, the conventional computational methods for activity recognition are overwhelmed by user heterogeneity in terms of age, behavioral patterns, lifestyle and so on. Performance degradation of classifiers in activity recognition due to the difference between people is known as population diversity problem. It has been shown (Lane et al. 2011b) that the population diversity problem can seriously affect the classification accuracy even when the population consists of as little as fifty users. While personalized models (Longstaff, Reddy, and Estrin 2010; Stikic and Schiele 2009) usually fare much better

Copyright c 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



in handling population diversity, the improvement comes at the expense of increased user involvement. The accuracy of classification requires each user to provide carefully labeled data-segments. As the classifier works in isolation, it leads to redundant efforts while learning about the same activities over similar users. To handle population-diversity in a practical way, a number of studies (Lane et al. 2011a; 2011b) suggested networked approaches by sharing data across users. To ensure reasonable accuracy, crowd-sourced training samples are weighted according to forms of inter-personal similarity. But, none of the proposed methods scale well with an increasing population. For a large user-base, the cost of i) computing pair-wise similarity network and more importantly, ii) exponential cost of training classifiers with huge datasets resulting from crowd-sourcing, gets impractical even with tens of users (Lane et al. 2011b). As interpersonal differences is more of an issue for a large user base, being not scalable severely limit the usability of existing approaches. This paper proposes a novel scheme to handle population diversity in a scalable way. We maintain groups of similar users to bring down the cost of computing similarity networks and using the similarity measures for training. To achieve comparable accuracy while keeping the user-burden low, our framework focuses on ensuring exploiting crowdsourcing within a group. To enable robust crowd-sourcing even in the case of unreliably labeled data from users, we handle the two common errors (Peebles et al. 2010) -- semantic discrepancy in the labels and the overlapping boundary of activities. To handle semantic discrepancy, we propose to consider it as a Natural Language Processing (NLP) problem. And, to handle overlapping class boundary resulting from inaccurate start and ending times, we use MultiInstance learning.



The contributions of the paper are: * The proposed framework handles population diversity in a way that remains practical even as the user population increases in size. To do so, we maintain group of similar users and limit embedding inter-personal similarity within the group.



851



Figure 1: The processing phases of the proposed activity recognition framework for handling data diversity and labeling inconsistencies. * We enable more robust crowd-sourcing. Previous work on sharing training activity data across users assumes that the labels are consistent. This assumption might be true in case of conventional controlled environments, but while working with a large population of low-commitment users, it is necessary to be robust enough against inconsistent labels. * By using a large public dataset, we evaluate our framework, both in terms of accuracy and scalability. networks. CSN (Lane et al. 2011b), for example, maintains three different similarity networks and a different classifier is trained for each network. To incorporate user similarity while training a classifier for user ui , the data samples from other user uj is weighted according to their similarity, S(ui , uj ) at the initial iteration. So, for each user uj in the population, data sample xuj from that user has the initial weight as weight(0) (xuj ) = S(ui , uj ). As a result, the computational cost of training classifiers can grow out of hand even with tens of users. To make training of classifiers feasible over a large userbase, we propose to cluster similar users and constrain the crowd-sourcing of data to only users within the same cluster. So, for a fixed number of clusters the number of classi-



Framework

Figure 1 shows the different steps in our framework for producing personalized classifiers that can cope with the population diversity problem in a scalable manner. We describe the steps in more details below.



Similarity Network

As the population grows, the user base starts to get more diverse. Apart from visible demographic dissimilarities like age, weight, gender or fitness level, the population starts to get more diverse in terms of behavioral and lifestyle pattern. As a result, even the core activities like walking can have different signature in sensor data across different group of people. For example, in CSN (Lane et al. 2011b) the authors pointed out the difference in features for two distinct subgroups of users performing walking as seen in Figure 2. As this inter-personal dissimilarities manifests as the differences in the pattern of raw data, previous approaches (Lane et al. 2011b; 2011a) use similarity networks for training classifiers. In a similarity network graph, each node represents a user and the edge-weight represents similarity between two users. There can be multiple similarity networks to leverage affinity among users in different dimensions -- physical similarity network might be used to recognize running while diurnal patterns might be leveraged while inferencing commuting activities. Each classifier for every user is trained on a dataset consisting of weighted data samples from all the users in the population based on the similarity



Figure 2: The difference in accelerometer features as two distinct subgroups perform the same activity -- walking (originally published in Lane et al. 2011b). The first two principal components of the features are shown above.



852



fiers trained remains constant irrespective of any increase to the size of the user population. For each type of similarity networks, we use different sets of clusters to leverage different dimension of affinity among users. Here we describe clustering users depending on two different notion of similarity -- sensor-data similarity and lifestyle similarity. But, it should be noted that other affinity metrics can easily be accommodated in the framework. Sensor Data Similarity As shown in Figure 2 the difference among users can manifest as the difference in raw sensor-data. So, the similarity in the accumulated data between two users can be a good indicator of inter-personal affinity. Given two users ui , uj and the corresponding accumulated feature sets Fui and Fuj , the similarity function can be defined as the overlap between sets, S(ui , uj ) = |Fui Fuj | , known as the Jaccard coefficient. But, comput|Fui Fuj | ing the similarity metric across a huge activity dataset for a large user population is clearly not feasible. So, we use sublinear time near-neighbor search known as Locality Sensitive Hashing (LSH) (Indyk and Motwani 1998). LSH is a well known technique (Buhler 2001; Ravichandran, Pantel, and Hovy 2005; Das et al. 2007) to efficiently find near-neighbors in a large database. In LSH, data points are hashed using multiple hash functions so that collisions between similar points occurs with higher probability. Finding near-neighbors requires hashing the query point as well to locate the buckets it belongs to. For the Jaccard coefficient similarity, there exists a LSH scheme called Min-Hashing (Cohen 1997). To use Min-Hashing, we need to randomly permutate the set of all feature S vectors and the hash value for each user ui is the index of the first feature vector in the permutated set that belongs to Fui -- the set of feature vector for user ui . For this random permutation, uniformly chosen over the set of all permutations of S , the probability of collision is exactly same as the Jaccard coefficient (Cohen 1997; Broder 1997; Cohen et al. 2001). The Min-Hash produces a set of hash buckets where the probability of two users ui , uj being in the same bucket is same to S(ui , uj ) -- essentially working as a probabilistic clustering algorithm where each bucket is a cluster. To ensure higher precision in clustering, we can concatenate p hash-values (Indyk and Motwani 1998) so that the probability of two users being in the same bucket is S(ui , uj )p , for p > 0. To avoid low recall resulting from the clusters being too refined, we repeat the steps q times. Each user belongs to q clusters where each cluster is defined by concatenation of p hash values. Permutating the set of feature vectors for the whole activity dataset is computationally unfeasible. Instead, we generate p x q independent, random hash-seeds and each feature vector is mapped to a corresponding hash-value. The hashvalues then serves as the index in the permuted set -- resulting in having similar characteristics to the ideal Min-Hash (Indyk 1999). Lifestyle Similarity The diversity in lifestyle (as measured by location and temporal patterns) can provide an im-



portant insight into the context of different activities. Depending on diurnal patterns and mobility distribution same activities can have different signature. The use of lifestyle similarity like diurnal patterns, has been shown to be beneficial in inferring different activity classes such as driving (Lane et al. 2011b). In CSN (Lane et al. 2011b), lifestyle similarity has been computed from mobility patterns and diurnal patterns by tessellating data into m distinct bins. For GPS estimates in mobility patterns, the bins can be two dimensional and for diurnal patterns each bin can represent the hour during a day in the week -- ranging from 0 to 167, 0 denotes the start of the week while 167 marks the final hour of the last day. For each user, CSN would construct a histogram {T (k) , k  [1, m]} of these bins. Histogram frequencies are normalized and the value of the histogram vector reflects the distribution of the data belonging to the user. CSN defines the lifestyle similar(k ) (k ) m ity between two users ui , uj as k=1 Tui Tui . Given the relatively low dimension of the histogram vectors, the above similarity measure can be used in common clustering algorithms. But, for large user-base, we suggest using Earth Mover's Distance (EMD). Given two different lifestyle histogram T (k) and T (m) , the earth mover's distance EMD(T (k) , T (m) ) is defined as the minimum cost of transforming one distribution to other. This is a popular metric in image and computer vision research (Rubner, Tomasi, and Guibas 2000; Zhao, Yang, and Tao 2010). It is expensive to compute as exact distance requires a solution to minimum transportation problem (Hitchcock 1941). As a result there has been extensive work in approximating the distance metric efficiently (Ling and Okada 2007; Shirdhonkar and Jacobs 2008). More importantly, (Charikar 2002) has shown that LSH scheme exists for EMD.



Robust crowd-sourcing

By enabling crowd sourcing, classifiers can use the steady stream of data from other users to find more discriminating examples to be incorporated into the model. But, given that one of the major goal is to keep the user burden low, the discrepancy in the labels provided by the users with low-commitment is unavoidable. For our framework to be robust enough against labeling errors and inconsistencies, we specifically focus on semantic discrepancy and boundary overlapping. Semantic Discrepancy in Labels Prior work in activity recognition usually makes the assumption that labels are consistent and the label domain remains fixed. While this might be true in simple scenarios, when people can enter free-form text as label to mark activities, the issue of assigning different labels to similar activities starts to become a concern (Peebles et al. 2010). Similar activity with different labels dilutes the training pool and essentially confuses the classifier. Given the textual nature of the labels, we suggest to consider finding similar activities as a NLP problem. Specifically, we use a similarity measure in terms of semantic distance between class labels to find similar classes. The



853



semantic distance can be calculated from hyponyms constructed from WordNet hierarchy (Fergus et al. 2010). After finding similar labels, we merge the samples under a generic single label if the number of training samples fall below an experimentally determined threshold. Otherwise, the data samples are shared during training weighted by their similarity. Boundary overlapping Data collection for activity on mobile phone usually requires users to mark the start and end of the activity. This can lead to recall errors, lack of temporal precision and interruptions. For example someone labeling gym may forget to mark end point resulting in overlapping boundary with driving data, which can affect the performance of classifiers. Multi-Instance Learning (MIL) can handle boundary overlapping robustly because of the more flexible labeling assumptions. In MIL, the samples are not treated as positive or negative -- labels are assigned to a set of instances grouped together into "bags". For a bag i and sample j in the bag, the probability of a single instance being positive is denoted by pij . We adopt the Noisy OR model for each bag. The probability that a bag is positive is given by pi = 1 - (1 - pij ) .



* Cluster similar users into a group. Similarity networks are formed within each of these clusters. * Finding semantically similar textual labels and reassign labels. * Training a MIL inspired Boosting algorithm to learn activities from the shared training samples in a group where each sample is initially weighted by user similarity. * Using multiple views of the data for leveraging the large amounts of unlabeled data that is crowd-sourced.



Evaluation

In this section, we evaluate the effectiveness and justify our design choices. The following experiments show that the framework scales much better than previous methods without sacrificing accuracy.



Dataset

For evaluation we use a large public dataset from ALKAN system (Hattori et al. 2010). This dataset contains data from more than 200 users and consists of over 35,000 activities. The data was gathered from the mobile device clients -- from iOS and Android applications. The dataset contains three axis accelerometer data from daily, real-life movement for more than a year resulting in relatively large dataset that can provide a good insight about the probable scalability issues that might arise in large-scale deployment. This dataset also contains activities with semantically close labels like train.sit, chat.sit, sit and so on. Some records in the dataset has inconsistent number of data-samples in terms of activity duration. We think the inconsistency arises when the phone can not sample sensor reading at the specified sampling rate, e.g., when talking on the phone. To identify errors in duration, we performed a time-window based sanity check by using the time-stamps in the data. The idea is, assuming that data is sampled at 20Hz, a chunk of data containing N consecutive samples N represents a time-window of 20 second. So, reading every N samples and comparing the values in the time-stamp column will give an insight into the variance present in that window. Around 1.1% of total data-sample was discarded because of inconsistency in time-stamp.



This essentially means that a bag is labeled positive if it contains at least one positive sample, otherwise, it is labeled as negative. So, under MIL settings, the bag labels provide only partial information -- it needs to cope with the ambiguity of not knowing which of the instances are positive and which ones are not. But, at the same time, the effect of noise is minimized in classifier training. MIL has been successfully applied to image segmentation (Vezhnevets and Buhmann 2010), face detection (Guillaumin, Verbeek, and Schmid 2010) and handling label noise in video classification (Leung, Song, and Zhang 2011). It has also been used in activity recognition in sparsely labeled data (Stikic and Schiele 2009). We use boosting based MIL where all instances contribute equally and independently to a bag's label (Xu and Frank 2004). Multi-view of data As data collection from smartphones is transparent to the user, a large pool of unlabeled data can quickly accumulate. To make use of this plentiful and otherwise wasted data, we suggest using unlabeled data to augment the classifier model. When multiple similarity networks are available, similar to CSN (Lane et al. 2011b), we suggest to exploit multiple views of different classifier by using multi-training (Blum and Mitchell 1998). But, if there is only one similarity network available, En-Co-Training (Guan et al. 2007) or democratic co-learning (Zhou and Goldman 2004) can be used as they do not make assumptions about independent views of the data. In this approach, each classifier keeps track of labeled and unlabeled crowd-sourced data and iteratively tries to label the unlabeled data of other classifier. After each such iteration, classifiers are retrained by using the additional new labels as assigned by other classifiers. So, the steps in our framework can be summarized as:



Feature Computation

For feature computation, a window size of 128 samples is used with 64 samples overlapping between consecutive windows. For the sampling rate of 20Hz, each window represents 6.7 seconds of data. Mean, energy, frequency-domain entropy and correlation features were extracted from the sliding windows signal. The DC feature in the sample window is the meanacceleration value of the acceleration. The energy feature is a normalized sum of the squared discrete FFT component magnitudes of the signal excluding the DC component. Frequency-domain entropy is calculated as the normalized information entropy of the FFT magnitudes -- DC feature is excluded from the calculation. Correlation is calculated between all pairwise combination of axes.



854



Figure 3: This ROC curve illustrates the effect of merging semantically similar labels. The classifier accuracy using the merged labels outperforms classifiers trained using only the original labels provided by users. Effect of Merging Labels For evaluating the effect of merging labels, we consider the activities associated with sitting and walking. The activity "sitting" consists of the labels train.sit, chat.sit, sit and eat.sit and for walking the labels are walk.slow, escalator.walk.up, escalator.walk.down and walk. These activities have been selected because significant amount of data have been recorded for each label. The dataset contained more than 46 hours of activities. For each label related with sitting we train a classifier where all other activities are marked as negative samples. For merged labels, we train the classifier with activities related with sitting marked as positive examples having different weights and walk related activities are marked as negative examples. For performance measurement, we use ten fold crossvalidation. From figure 3, the performance gain in the classifier trained from merged label is apparent from the top-left-most placement of the ROC curve. The accuracy of the classifiers trained by isolated labels are rather poor, but it is consistent with earlier findings (Hattori et al. 2010). Robustness against boundary overlapping In activity recognition systems obtaining high-quality training data has always been a central issues. For large-scale deployment, the problem is more severe. Recording sensor data through real-life, daily activities means lack of temporal precision and frequent disruption. To study the effect of such noise we switch some negative samples to positive samples -- simulating the interruption by activities in the middle of recording. We create dataset with 1%, 5%, 10% and 20% noise in positive labels. We train a MIL AdaBoost and a simple AdaBoost classifier using same dataset. In both cases, the weak classifier is a C4.5 decision tree. The result is shown in Figure 4. It is apparent that MIL based methods performs much better in the presence of noisy data.



Figure 4: Performance of multi-instance learning in handling activity labels with overlapping boundaries.



Figure 5: Time (in seconds) to train a classifier for a single user. Scalability The cost of training classifiers is the bottleneck of deploying activity recognition system that uses similarity networks. To evaluate how well our system scales with an increasing population, we select twenty users and train classifiers to recognize activities having labels sit, walk and stand consisting of more than 686 hours of sensor data. We compare the training time of classifiers with CSN (Lane et al. 2011b) which uses a fully connected weightedgraph for learning models. While CSN used a computer cluster for training, in our evaluation we use a single machine (2.3 GHz Intel Core i5 CPU and 4 GB of memory). We limited the evaluation to twenty users and single type of similarity network since CSN would take too long to train otherwise. Figure 5 shows the effect of increasing the population while training a classifier for a user based on lifestyle similarity network. The training time of a classifier in our framework is constant for a fixed number of clusters. Consequently, increasing the population does not incur much ad-



855



data to compute a similarity network. Additionally, we assume these similarity networks are static, which ignores the significant drift in user behavior that will occur over time. We plan to further work on the framework to address these design and implementation issues with the eventual goal of coming up with a framework which can be deployed over large population.



References

Blum, A., and Mitchell, T. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, 92- 100. ACM. Broder, A. 1997. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences., 21-29. IEEE. Buhler, J. 2001. Efficient large-scale sequence comparison by locality-sensitive hashing. Bioinformatics 17(5):419- 428. Charikar, M. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, 380-388. ACM. Cohen, E.; Datar, M.; Fujiwara, S.; Gionis, A.; Indyk, P.; Motwani, R.; Ullman, J.; and Yang, C. 2001. Finding interesting associations without support pruning. IEEE Transactions on Knowledge and Data Engineering 13(1):64-78. Cohen, E. 1997. Size-estimation framework with applications to transitive closure and reachability. Journal of Computer and System Sciences 55(3):441-453. Das, A.; Datar, M.; Garg, A.; and Rajaram, S. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th international conference on World Wide Web, 271-280. ACM. Fergus, R.; Bernal, H.; Weiss, Y.; and Torralba, A. 2010. Semantic label sharing for learning with many categories. Computer Vision-ECCV 2010 762-775. Guan, D.; Yuan, W.; Lee, Y.; Gavrilov, A.; and Lee, S. 2007. Activity recognition based on semi-supervised learning. In 13th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications, 2007. RTCSA 2007., 469-475. IEEE. Guillaumin, M.; Verbeek, J.; and Schmid, C. 2010. Multiple instance metric learning from automatically labeled bags of faces. Computer Vision-ECCV 2010 634-647. Hattori, Y.; Inoue, S.; Masaki, T.; Hirakawa, G.; and Sudo, O. 2010. Gathering large scale human activity information using mobile sensor devices. In 2010 International Conference on Broadband, Wireless Computing, Communication and Applications (BWCCA), 708-713. IEEE. Hitchcock, F. 1941. The distribution of a product from several sources to numerous localities. Journal of mathematics and physics 20(2):224-230. Indyk, P., and Motwani, R. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality.



Figure 6: Classifier accuracy in a population of 20 users. ditional cost. In contrast, the training time for CSN, from 4 users to 20 users increases by 370 times, making it impractical to use in large user base. The important question is how our clustering approach to manage computational cost will affect classifier accuracy. For measuring classifier performance we use a separate test set containing around three hours of sensor data for sit, walk and stand. Figure 6 shows the accuracy of the classifiers for this experiment. The classifier for CSN has been trained on the fully connected lifestyle similarity graph for twenty people while the clustered classifier has been trained on five users with high lifestyle similarity. We selected these three activities and limit the dataset to a single similarity network of twenty users because of the computational cost associated with training for CSN. From the result, we can say that training using clustered users maintains reasonable accuracy while keeping computational cost low.



Conclusion

In this paper, we introduced a scalable way to handle the population-diversity problem. We demonstrated that our framework scales well as the user population increases without sacrificing classification accuracy. Furthermore, our framework introduces new techniques for coping with crowd-sourced labeled activity data, which although can be plentiful can also be prone to error. Our results showed the effect in classifier accuracy due to user disagreement with activity class semantics (e.g., labeling the same activity class with different textual descriptions). We demonstrated how this problem can be improved with NLP-based techniques proposed in our framework. Finally, we introduced techniques specifically to handle segmentation errors during crowd-sourcing, which occur when users make mistakes as to precisely when activities start and end. While the results are promising, there are still challenges related to long-term usage by a large user population. Like the conventional methods, our framework assumes complete knowledge of the similarity network between all pair of users. This will not be available, for instance, as new users join the system for whom there will be insufficient



856



In Proceedings of the thirtieth annual ACM symposium on Theory of computing, 604-613. ACM. Indyk, P. 1999. A small approximately min-wise independent family of hash functions. In Proceedings of the tenth annual ACM-SIAM symposium on Discrete algorithms, 454-456. Society for Industrial and Applied Mathematics. Lane, N.; Xu, Y.; Lu, H.; Campbell, A.; Choudhury, T.; and Eisenman, S. 2011a. Exploiting social networks for large-scale human behavior modeling. Pervasive Computing, IEEE 10(4):45-53. Lane, N.; Xu, Y.; Lu, H.; Hu, S.; Choudhury, T.; Campbell, A.; and Zhao, F. 2011b. Enabling large-scale human activity inference on smartphones using community similarity networks (csn). In Proceedings of the 13th international conference on Ubiquitous computing, 355-364. ACM. Leung, T.; Song, Y.; and Zhang, J. 2011. Handling label noise in video classification via multiple instance learning. In 2011 IEEE International Conference on Computer Vision (ICCV), 2056-2063. IEEE. Ling, H., and Okada, K. 2007. An efficient earth mover's distance algorithm for robust histogram comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence 29(5):840-853. Longstaff, B.; Reddy, S.; and Estrin, D. 2010. Improving activity classification for health applications on mobile devices using active and semi-supervised learning. In 2010 4th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth), 1-7. IEEE. Peebles, D.; Lu, H.; Lane, N.; Choudhury, T.; and Campbell, A. 2010. Community-guided learning: Exploiting mobile sensor users to model human behavior. In Proc. of 24th AAAI Conference on Artificial Intelligence, 1600-1606.



Ravichandran, D.; Pantel, P.; and Hovy, E. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, 622-629. Association for Computational Linguistics. Rubner, Y.; Tomasi, C.; and Guibas, L. 2000. The earth mover's distance as a metric for image retrieval. International Journal of Computer Vision 40(2):99-121. Shirdhonkar, S., and Jacobs, D. 2008. Approximate earth movers distance in linear time. In IEEE Conference on Computer Vision and Pattern Recognition, 2008. (CVPR), 1-8. IEEE. Stikic, M., and Schiele, B. 2009. Activity recognition from sparsely labeled data using multi-instance learning. In Proceedings of the 4th International Symposium on Location and Context Awareness, LoCA '09, 156-173. Berlin, Heidelberg: Springer-Verlag. Vezhnevets, A., and Buhmann, J. 2010. Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning. In 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3249- 3256. IEEE. Xu, X., and Frank, E. 2004. Logistic regression and boosting for labeled bags of instances. Advances in Knowledge Discovery and Data Mining 272-281. Zhao, Q.; Yang, Z.; and Tao, H. 2010. Differential earth mover's distance with its applications to visual tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 32(2):274-287. Zhou, Y., and Goldman, S. 2004. Democratic co-learning. In 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI), 594-602. IEEE.



857



Qualitatively Describing Objects Using Spatial Prepositions

Alicia Abella

Department of Computer Science Columbia University New York, NY 10027



John R. Kender



The objective in this paper is to present a framework for a system that describes objects in a qualitative fashion. A subset of spatial prepositions is chosen and an appropriate quanti cation is applied to each of them that capture their inherent qualitative properties. The quanti cations use such object attributes as area, centers, and elongation properties. The familiar zeroth, rst, and second order moments are used to characterize these attributes. This paper will detail how and why the particular quanti cations were chosen. Since spatial prepositions are by their nature rather vague and dependent on context a technique for fuzzifying the de nition of the spatial preposition is explained. Finally an example task is chosen to illustrate the appropriateness of the quanti cation techniques. The work presented in this paper is motivated by an interest in how spatial prepositions may be used to describe space and more interestingly, the spatial relationship among the objects that occupy that space. This work is not concerned with the natural language aspect of spatial prepositions. Given a particular environment and a particular task, where the task and environment may change, we wish for a framework that describes the elements in the environment. It is this framework that is of concern in this paper. It is known that language meaning is very much dependent on context. An example of a context dependent use of the spatial preposition next as taken from Landau and Jackendo , accepted for publication] is the bicycle next to the house. We would normally not say the house next to the bicycle. This is the case because the house is larger in size and as such it serves as an anchor for those objects around it. The house in this example serves as a reference object, or in an environmental context, as a landmark. In the system presented in this paper either description is acceptable since the only concern is in the spatial arrangement of



Abstract



the objects irrespective of the size or the purpose of either of the two objects. The treatment of objects in our chosen environment is a binary one. There is not a reference object, or landmark, because we wish to avoid choosing a reference object that would require the use of physical attributes such as color, size, or shape and focus solely on two objects' spatial relationship. If we think about the use of a preposition like near we realize that the requirement of a particular shape is not needed for its proper use. Landau and Jackendo Landau and Jackendo , accepted for publication] have categorized spatial prepositions into those that describe volumes, surfaces, points, lines, and axial structure. They have pointed out that an object can be regarded as a \lump" or \blob" as far as most of the commonly used spatial prepositions are concerned. For example the preposition in or inside can regard an object as a blob as long as the blob has the capacity to surround. Likewise, near and at only require that the blob have some spatial extent. Along requires that an object be fairly linear and horizontal with respect to another. The work presented in Herskovits, 1986] covers the topic of spatial prepositions fairly extensively from a natural language perspective. The author only suggests the possibility of constructing a computational model for expressing spatial prepositions. The intent here is to demonstrate that a computational model can be constructed and that it indeed captures the vital properties su cient for a succinct use of the chosen prepositions. We can encode the spatial prepositions fairly concisely because we are treating objects as \blobs" and because most of the properties characterized by these prepositions can be encoded using geometric properties such as alignment and distance. Other related works can be found in Lindkvist, 1976; Talmy, 1983]. The following sections will provide the details of the encoding we have chosen and demonstrate them though the use of an example.



Introduction



The prepositions for which we have encoded are near, far, inside, above, below, aligned, next. We have dened a preposition as a predicate that maps k objects to true (T ) or false (F ); true if the k objects meet the requirements imposed by the preposition and false otherwise. p : Ok ?! fT; F g where p is a preposition and Ok is a k-tuple of objects. In this paper we will consider k = 2. Nevertheless, prepositions that involve three objects like between can also be represented, using a similar formalism. Now that we have de ned a preposition we need to de ne an object. Formally, each object is represented by a six element vector that depend on an object's area xx Ixy A, center (xc ; yc ), and inertia tensor I Ixy Iyy . It is important to scale the elements in this vector so that they have consistent units, in this case units of length, because we will use this vector in the fuzzi cation procedure described in section 4. Therefore, the kth object is represented by a vector q q q k = (pAk ; xk ; yk ; 4 I k ; 4 I k ; 4 I k ) xy yy c c xx The pair of objects is represented by a 12-component vector = ( 1 ; 2) 2 R12 It is this scaled vector that we will be using in our future calculations. The parameterization of objects presented above leads to the concept of a bounding box. A bounding box encloses the object using certain criteria. There are various ways in which to compute a bounding box for an object, one of which may be to nd the maximum and minimum x and y values belonging to the object. The one we've chosen is de ned through the values of x and y , that o er a measure of how much an object stretches in the x and y direction. See the Appendix for the derivation. Two objects de ne a point in 12D space. A preposition p can be thought of as a set of points Up 2 R12 such that Up = f jp( )g. The volume in this 12D space may be able to reveal some of the inherent properties associated with prepositions. In other words, examination of the space occupied by the various sets Up may tell us something about the spatial prepositions. Vacancies in this 12D space may reveal why we do not have a word to describe certain spatial relationships among objects. The intersection and distances of volumes occupied by various spatial prepositions may reveal a correlation between various prepositions. We say that objects O1 and O2 are in preposition p if ( 1 ; 2) 2 Up . This \ideal" set is made up of pairs of object vectors that satisfy the constraints imposed by the preposition p. As we well know, prepositions are



Notations and De nitions



inherently vague in their descriptions, and their interpretation may vary from person to person. Because of this, it is important to add some fuzzifying agent to our ideal set. The fuzzifying technique is as de ned through fuzzy set theory Klir and Folger, 1988]. The theory of fuzzy sets is used to represent uncertainty, information, and complexity. The theory of classical sets1 represents certainty. A classical set divides objects in the world into two categories: those that certainly belong to a set and those that certainly do not belong to a set. A fuzzy set, on the other hand, divides the world much more loosely, by introducing vagueness into the categorization process. This means that members of a set belong to that set to a greater or lesser degree than other members of the set. Mathematically, members of the set are assigned a membership grade value that indicates to what degree they belong to the set. This membership grade is usually a real number in the closed interval between 0 and 1. Therefore a member that has a membership grade closer to 1 belongs to the set to a greater degree than a member with a lower membership grade. Because of its properties fuzzy set theory can nd application in elds that study how we assimilate information, recognize patterns Abella, 1992], and simplify complex tasks. In our notation the fuzzi ed ideal set is de ned through a membership function fUp ( ) 2 0; 1] We also de ne a threshold value that depends on how much vagueness we allow before we decide that two objects are no longer describable with the given preposition: fUp ( ) p



The quanti cation of prepositions entails representing objects through certain physical properties that can then serve as a basis for expressing prepositions. The physical properties we've chosen include object area, centers of mass, and elongation properties. These properties are calculated through the use of the zeroth, rst, and second order moments. The basis for this choice of attributes is simplicity and familiarity. What ensues is a brief description of the various prepositions we've chosen to illustrate. Each preposition is de ned through a set of inequalities. This results in sets Up having nonzero measure (i.e. full dimensionality) in R12 which is necessary for the fuzzi cation procedure described in section 4.



Computational Model of Spatial Prepositions



NEAR

1



We've de ned near so that objects' bounding boxes

Referred to as \crisp" sets in fuzzy set theory.



2 2x 2 2y 1 2y 1 2x 1 ? y2 j jyc c



1 max



1 1 min



2 max 2 2 min



Figure 1: Two objects that are near each other



2 jx1 c ? xc j



Figure 3: De nition of relevant angles for aligned be completely embedded within the bounding box of another. Formally, 1 2 1 2 1 2 1 2 x ? x > jxc ? xc j and y ? y > jyc ? yc j

Above requires that the projections of bounding boxes on the x axis intersect and that the projections of bounding boxes on the y axis do not intersect. The mathematical relationship is 1 2 1 2 1 2 1 2 x + x > jxc ? xc j and y + y < yc ? yc Note that above is non-commutative. We de ne below similarly. As with near and far, above and below are mutually exclusive prepositions. However, not-above does not strictly imply below.



ABOVE, BELOW



2 1 jx1 c ? xc j ? x ? 2 jx1 c ? xc j



x



2



Figure 2: Two objects that are far from each other have a non-empty intersection (see gure 1). Mathematically this is : 1 2 1 2 1 2 1 2 x + x > jxc ? xc j and y + y > jyc ? yc j



FAR



Far is not the complement of near as one may initially



suspect. We may be faced with a case where an object is neither near or far from another object, but rather it is somewhat near or somewhat far. This notion of somewhat will be explained more fully when we introduce the concept of fuzzifying our \ideal" set. For now it su ces to say that far is de ned so that the distance between two bounding boxes in either the x extent or the y extent is larger than the maximum length of the two objects in that same x or y extent (see gure 2). Mathematically, 2 1 2 1 2 jx1 c ? xc j ? ( x + x ) > 2 max( x ; x) or 1 ? y2 j ? ( 1 + 2 ) > 2 max( 1 ; 2) jyc c y y y y



The alignment2 property is angular in nature, therefore its quanti cation involves inequalities between angles, rather than lengths as the previous prepositions had. For this purpose we de ne a di erent type of bounding box that is centered at the object's center of mass and oriented along the object's principal inertia axes with dimensions proportional to the object's maximum and minimum moments of inertia. , min and max are as shown in gure 3. With this in mind, the preposition aligned is de ned as: 1 ; 2 ) < i < min( 1 ; 2 ); i = 1; 2 max( min min max max We've de ned next as a combination of the prepositions near and aligned. Therefore the de nition for next is: Unext = Unear \ Ualigned The preposition next is an example of a spatial preposition that is a combination of more elementary

2 Although not a preposition from a language perspective we've adopted it as a spatial preposition.



ALIGNED



NEXT



INSIDE



Inside requires that the bounding box of one object



prepositions. This hints at the possibility of a natural hierarchy of spatial prepositions. It also shows evidence of the possible partitioning of the 12D space mentioned previously. This section describes why and how we fuzzify spatial prepositions. We need to fuzzify spatial prepositions because they are vague by their very nature; they depend on context and depend on an individual's perception of them with respect to an environment. For these reasons we need to allow for some leeway when deciding if two objects are related through a given preposition. There is a lot of freedom in how we can fuzzify spatial prepositions, or equivalently, the \ideal" set, Up . The idea we have adopted is to 12 de ne the membership function fUp ( ) where 2 R as a function of a distance d between and Up . d = min j ? j

0 0



1 2 3 4 6 5 7



The Fuzzi cation of Spatial Prepositions



Note that d( ; Up ) = 0 for 2 Up . The distance d tells us by how much the de ning preposition inequalities are not satis ed. Thus, fUp ( ) = 1 for 2 Up fUp ( ) ! 0 as d( ; Up ) ! 1 Up is a multi-dimensional set de ned by complex inequalities, for which computing d may be very burdensome. For this reason we resort to a MonteCarlo simulation with a set of random points around that have given statistical properties. The experiments we've conducted use normally distributed random points with mean and covariance matrix diag ( 2; :::; 2). The exact form for fUp used is 1; 2U fUp = min(1; 2 N ); 62 Up p N where N is the total number of random points in the Monte-Carlo simulation and N is the number of points 2 Up . Note that the formulation of fUp ensures that fUp for very close to the boundary of Up will have a value close to 1. The following section will detail some experiments that use this fuzzi cation technique and put into e ect the inequalities that de ne the given spatial prepositions.

0 0 0



2Up



Figure 4: The experimental image necessary for construction of the 12-dimensional vector are computed (e.g. the area of an object is the sum of all the pixels belonging to the object). Currently the system accepts a spatial preposition and displays all those objects that satisfy the preposition inequalities. The system also accepts as input two objects along with a preposition and it outputs how well those two objects meet the given preposition (the value of fUp for given ). All intuitively obvious relations between objects are discovered by the system, e.g. objects 1 and 3 are next to each other, etc. An interesting case, and one that demonstrates the e ects of fuzzi cation is the case of supplying object 2 and object 6 along with the preposition aligned. With no fuzzi cation the system nds that 2 and 6 are not aligned. However, if we allow a certain amount of fuzzication with say = 0:03 the value of fUaligned is 0.8. This value indicates that they may be su ciently aligned to be regarded as such (which we actually see in the image!), depending on how much leeway we wish to allow. The dependency of fUaligned on is shown in gure 5. From this graph we see that the value of the membership function signi cantly deteriorates for large values of . This simply means that the amount of induced uncertainty is so large that the objects cease to possess their original features (such as orientation in this case). This also indicates what the maximal acceptable value for should be. In this case, that is < 0:1. Another interesting case is that of supplying object 2 and object 6 along with the preposition near or far. Neither satis es the inequalities precisely. However, if we again, allow for fuzzi cation, we get a most interesting result, as shown in gure 6. We observe that



We will use the image shown in gure 4 to illustrate several uses of the prepositions. Each object has been numbered to ease their reference. The image is read as a grey-scale pixel image. It is then thresholded to produce a binary image and objects are located using a sequential labelling algorithm Horn, 1989]. Once the objects in the scene have been found, the attributes



Qualitative Description Experiments



fUp

0.8



0.6



0.4



0.2



0.001



0.01



0.1



Figure 5: The dependency of fUaligned (2; 6) as a function of



scriptions. In other words, we may wish to describe a particular object with as few descriptions as possible through the feedback from the system. The goal would be to home in on the object we are truly referring to through repeatedly supplying additional prepositions to those objects that were singled out after previous inquires. An experiment using this technique may reveal that people naturally describe spatial arrangements in a series of descriptions, rather than once and for all. It may also demonstrate inadequacies in the vocabulary or complexity of a scene. We may also discover that certain environments require that we adopt prepositions that do not exist in the English language for describing a particular sort of spatial arrangement. Abella, A. 1992. Extracting geometric shapes from a set of points. In Image Understanding Workshop. Herskovits, A. 1986. Language and spatial cognition: Press. Klir, G. J. and Folger, T. A. 1988. Fuzzy Sets, Uncertainty, and Information. Prentice Hall. Landau, B. and Jackendo , R. ation. "What" and "Where" in spatial language and spatial cognition. BBS. Lindkvist, K. 1976. Comprehensive study of conceptions of locality in which English prepositions occur. Almqvist & Wiksell International. Talmy, L. 1983. How language structures space. In

Spatial orientation: Theory, research, and application. Plenum Press.



References



fUp

1



near far



0.8



An interdisciplinary study of the prepositions in English. Cambridge University Press. Horn, Berthold K.P. 1989. Robot Vision. The MIT



0.6



0.4



0.2



0.001



0.01



0.1



Figure 6: The dependency of fUnear (2; 6) and fUfar (2; 6) as a function of although we can not say for certain that object 2 and object 6 are either near or far, we can say that they are somewhat near or somewhat far. How we decide which of the two to use can be seen in gure 6. If we examine the slopes of the two curves we see that for small values of the slope for far is steeper than that for near. Therefore it would seem more appropriate to say that 2 is somewhat far from 6 as opposed to 2 is somewhat near to 6. The intent of this paper was to establish a computational model for characterizing spatial prepositions for use in describing objects. A quanti cation was established and demonstrated through the use of an example. A framework to deal with the inherent vagueness of prepositions was also introduced with the use of a fuzzi cation technique. An extension of this work would be one in which a user could conduct a dialogue with the system, capable of understanding as well as generating scene de-



Conclusion



We have used the following two equations to de ne how much an object stretches in the x and y directions. r r I Imin j sin j; g max x = 2 maxf A j cos j; A r r Imax Imin j cos j; g y = 2 maxf A j sin j; A The following is the derivation of the above formulas. The maximal moment of inertia is given by the formula Z Z 2A u2 dudv = k u Imax = A where u and v are axes of maximal and minimal moment of inertia respectively, A is an object's area and u is an elongation parameter that conveys information regarding how much an object \stretches" along the axis u. Constant k is chosen so that in the case of a circle with radius r we have u = r. Simple calculation gives k = 2, and formulas for x and y are obtained by projecting u and v onto axes x and y.



De nition of



x



and



y



Reinforcement Learning As a Framework for Ethical Decision Making

David Abel and James MacGlashan and Michael L. Littman

Brown University, Computer Science Department 115 Waterman Street Providence, RI 02912-1910



Abstract

Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way. It is essential, then, that these AI systems make decisions that take into account the desires, goals, and preferences of other people, while simultaneously learning about what those preferences are. In this work, we argue that the reinforcementlearning framework achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper foundations for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We define an idealized formalism for an ethical learner, and conduct experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identify several critical challenges for future advancement in the area that can leverage our proposed framework.



Death dilemma from Armstrong (2015), and our own problem, which we coin Burning Room, which is an extension of the table dilemma introduced by Briggs and Scheutz (2015). Lastly, we identify critical challenges for future advancement in the area leveraging our proposed framework.



Related Work

Research on the interaction between humans and artificial agents is broad. Prior approaches consider particular dilemmas that pose challenges for these and related interactions, while others investigate the basic mechanisms by which humans ought to interface with artificial agents such as robots and virtual assistants. We provide a brief survey of existing approaches that relate to ethical decision making and learning. We divide the existing literature into three categories: rule-based systems, Bayesian utility-maximization approaches, and work that argues against the use of reinforcement learning for these sorts of decision-making systems.



Introduction

Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way; whether they are personal robots tasked with improving the daily life of a family or community, workers in a factory setting, or virtual assistants tasked with improving other cosmetic aspects of an individual's life. The fundamental purpose of these systems is to carry out actions so as to improve the lives of the inhabitants of our planet. It is essential, then, that these agents make decisions that take into account the desires, goals, and preferences of other people in the world while simultaneously learning about those preferences. In this document, we investigate ethical decision making using the reinforcement-learning (RL) framework. We argue that reinforcement learning achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper framework for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. Specifically, we formalize the ethical learning and decision-making problem as solving a partially observable Markov decision process (POMDP). We advance these claims by conducting experiments in two toy ethical dilemmas, the Cake or

Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



Ruled-Based Systems

Briggs and Scheutz (2015) discuss scenarios in which a robot ought to infer that a provided directive leads to undesirable behavior. Under their architecture, given some instructions, the agent first reasons about a set of conditions, termed `felicity conditions'. These include considerations such as "Do I know how to accomplish the task?", and "Does accomplishing this task violate any normative principles?". Each of these conditions is formalized as a logical expression, along with inference rules that enable the agent to infer which directives to reject. For example: (obl(, )  per(, ))  goal(, ), (1)



indicates that agent  ought to adopt  as a goal if the agent is obligated to do  and there is no deontological contradiction in satisfying the goal. By reasoning over logical conditions using inference rules of this form, their architecture ensure that an artificial agent will reject certain commands. For instance, if the agent can prove that accomplishing the goal is unsafe, the agent will reject the directive to satisfy the goal. While this framework provides a nice architecture for rule-based inference, conditions and inference rules that are



not encoded into the knowledge base of the agent prove impossible to reason about. In short: active ethical learning and decision making under ethical uncertainty is outside the scope of a symbolic framework like this. We foresee cases where the set of principles fail to generalize to novel encounters. The methodology we will introduce is designed to learn and make decisions optimally in light of partial observability, removing the requirement that specific ethical norms (and inference rules) be provided to the agent a priori. Bringsjord, Arkoudas, and Bello (2006; 2005) take a similar approach by advocating for ethical semantics defined with Horty logic (Horty 2001; Murakami 2004), which they implement in Athena (Arkoudas). Horty logic is a deontic logic (Clarke 1975) that allows reasoning about multiple agents and their actions. This formalism, however, has some similar limitations as the Briggs and Scheutz approach: all ethical rules must be encoded in advance and the formalism does not permit active learning of the ethical rules or decision making under ethical uncertainty. Additionally, Bringsjord, Arkoudas, and Bello note that an open challenge in their approach is how to make the agent's reasoning robust when other agents in the world (e.g., humans) do not follow obligations to which the robot deduced them to hold (that is, when humans act unethically according to the robot's rules). In contrast, our approach will not have this limitation. The MedEthEx system (Anderson, Anderson, and Armen 2006) is another rules-based ethical reasoning system built specifically for evaluating medical decisions. MedEthEx takes as input a list of duties that correspond to the duties described in Beauchamp's and Childress' Principles of Biomedical Ethics (2001). However, unlike the previous rule-based systems discussed, the rules used by MedEthEx are prima facie duties: duties that are not absolutes and can be overruled by a stronger duty/rule. The goal of the MedEthEx system is to learn the preference order of the duties. To do so, MedEthEx takes as input a set of training examples consisting of ethical dilemmas and the decision made and then uses inductive logic programming to infer the duty ordering. Given novel cases, MedEthEx can then recommend courses of action. Although MedEthEx permits some form of ethical learning, it still must have a set of high-level duties prescribed in advance that apply to well formed ethical dilemmas that are input to it. Moreover, MedEthEx does not incorporate itself into a general decision-making and learning process. In the previously described systems, high-level descriptions of rules, either through labels or a logical expression, are used. Marcello (2006) explores a different approach by which moral permissibility is learned by training an artificial neural network with example dilemmas that are labeled as ethically permissible or not. The output of this system allows rules of a sort to be learned purely from examples of permissible and impermissible behavior and allows novel scenarios to be classified. A limitation of Marcello's model is that the neural network renders the learned ethical rules opaque, thereby preventing such a system from easily explaining itself. The representation used was also highly specific to the types of ethical dilemmas explored, and Marcello found that even this



representation was highly-sensitive to the training-data distribution. For example, if training examples regarding one actor were more common than a different actor, it could lead to learning different ethical rules for each actor. Finding the right representation for this system would therefore be challenging. Finally, this system also does not integrate with active-learning and decision-making systems.



Bayesian Approaches

Having the agent learn about its ethical objective function while making decisions results in a challenging problem. Armstrong (2015) previously considered this problem by exploring the consequences of an agent that uses Bayesian learning to update beliefs about the "true" ethical objective function. At each time step, the agent makes decisions that maximize a meta-utility function, represented as a linear combination of the different possible ethical utility functions weighted by their probability at that time of being the true ethical utility. When coupling this meta-utility with beliefs about the world, he proposes that the agent makes action selections according to: arg max

a A w W



Pr(w|e, a)

uU



u(w) Pr(C (u)|w) , (2)



where A is a set of actions the agent can take; W is a set of possible worlds, where a world contains a (potentially future) history of actions and observations; Pr(w | e, a) is the probability of some future world w given some set of previous evidence e and that the agent will take action a; U is a set of possible utility functions, with C (u) indicating whether u  U is the ethical utility function we'd like the agent to follow. Using a toy example problem called Cake or Death, Armstrong highlights a number of possible unethical decisions that can result from an agent choosing actions using this rule or a variant of this rule. There are generally two causes for the unethical decisions under this rule. First, the agent can predict its meta-utility function (the linear combination of the possible ethical utility functions) changing from information gathering actions resulting in future suboptimal decisions according to its current meta-utility function. Second, under this rule, the model for the probabilities of ethical utility functions can be treated independently from the model that predicts the world, allowing for the possibility that the agent can predict observations that would inform what the correct ethical utility function is, without simultaneously predicting that ethical utility function. While Armstrong notes properties of the models that would be necessary to avoid these problems, he concludes that it is unclear how to design such an agent and whether satisfying those properties is too strong or weak for effective tradeoffs between learning about what is ethical and making ethical decisions. Ultimately, Armstrong instead considers how to formalize different meta-utility functions that may not cause the agent to avoid information gathering actions, but have the disadvantage that it does not motivate the agent to learn about what is ethical.



Arguments Against Reinforcement Learning

In his recent book Superintelligence, Bostrom (2014) argues against the prospect of using reinforcement learning as the basis for an ethical artificial agent. His primary claim is that an intelligent enough agent acting so as to maximize reward in the real world would effectively cheat by modifying its reward signal in a way that trivially maximizes reward. However, this argument only applies to a very specific form of reinforcement learning: one in which the agent does not know the reward function and whose goal is instead to maximize the observation of reward events. While this formalism is common in RL research, it is also common that the agent does know the reward function, but not the transition dynamics or other information. When the agent knows its reward function, its goal is not to maximize perceived reward events, but the evaluation of the reward function. Since the known reward function defines the agent's goals, any long-term planned behavior will be with respect to it rather than possible changes to it. This version of reinforcement learning is more analogous to the "utility function maximizing agent" that Bostrom suggests as a possible resolution to problems with a reward-event maximizing agent. Dewey (2011) presents a similar problem for reinforcement-learning agents; that the underlying modus operandi of a reinforcement-learning agent is to maximize numerical reward values, which is in conflict with the natural mechanisms by which humans treat goals. Dewey argues that this mismatch poses a serious challenge, in that we need mechanisms for ensuring that a reinforcement-learning agent's goals and values align with ours. We agree that goal and value alignment are open problems for decision-making agents, but we do not see them as insurmountable. In fact, mechanisms for balancing decision making with learning about the true underlying values we want an agent to hold is the motivation for our POMDP formulation (along with other areas of research in HRI discussed below).



-   [0 : 1] is a discount factor that specifies how much the agent prefers short term rewards over long term rewards. The goal of an agent acting in an MDP is to maximize the discounted long term reward received. One variation is the infinite-horizon objective, in which the agent must maximize its discounted long term reward arbitrarily into the future:





max

t=0



 t R(st , at ).



(3)



Notably, the discount factor  t decreases to 0 as t  , so the agent is biased toward maximizing reward closer to the present. Alternatively, one could consider the finitehorizon case, in which the agent must maximize its reward up to a certain point in the future, say k time steps away:

k



max

t=0



R(st , at ).



(4)



Solutions come in the form of a policy, which specifies how the agent ought to act in any given state,  : S  A. Policies may also be probabilistic, and map to a probability distribution on the action set. The optimal policy is one that maximizes the expected long term discounted reward from every state: arg max E

 t



 t R(st , at ) 



(5)



Background

In this section, we review background material on Markov decisions processes (MDPs) and partially observable Markov decision processes (POMDPs), which are the typical decision-making problem formulations used in reinforcement-learning (RL) research.



Two useful functions that MDP algorithms often compute to find the optimal policy are the state value function V  (s) and the state-action value function Q (s, a). V  (s) is the expected future discounted reward from state s when following policy  . Q (s, a) is the expected future discounted reward when the agent takes action a in state s and then follows policy  thereafter. These values for the optimal policy are often denoted by V  (s) and Q (s, a). In reinforcement learning (RL), the agent is only provided S , A, and  , sometimes2 R, and some initial state, s0  S . By acting (executing actions, say) the agent can explore the state space to learn about the structure of the MDP, and identify optimal behavior for the current task. MDP Complexity In terms of computational complexity, (Papadimitriou and Tsitsiklis 1987) proved that computing solutions to stochastic MDPs is P-Complete, demonstrating that optimal solutions to MDPs must be computed sequentially in the worst case. Also of interest in RL is sample complexity, introduced by (Kakade and others 2003). Sample complexity measures the number of interactions an agent must have with its environment to learn to behave well. We can define "behaving well" using the PAC-MDP (Probability Approximately Correct in Markov Decision Processes) criterion (Strehl, Li, and Littman 2009), which imposes sample complexity bounds similar to the Probably Approximately Correct

2 It is becoming more common to let the agent know what task it is solving within RL.



Markov Decision Process

An MDP is a five tuple: S , A, R, T ,  , where: - S is a set of states. - A is a set of actions. - R(s, a) : S x A  R is a reward function.1 - T (s, a, s ) = Pr(s | s, a) is a probability distribution, denoting the probability of transitioning from state s  S to state s  S when the agent executes action a  A.

1 Note that MDPs can also be defined with a reward function that depends on the next state: R(s, a, s ); but this version of a reward function can always be reduced to an R(s, a) reward function by marginalizing over next states.



learning framework introduced by (Valiant 1984). In particular, an RL algorithm is PAC-MDP if, with high probability, the algorithm's estimation of the value function V  (s) for all states is within of the optimal after a polynomial number of samples (in the size of the MDP and approximation parameters). More formally, there is a polynomial function 1 p(), such that after p(|S|, |A|, 1 , 1  , 1- ), interactions with the environment:   (s) - V  (s)|  . sS : |V (6) There are several known PAC-MDP algorithms for solving MDPs, including Delayed-Q Learning (Strehl et al. 2006), R MAX (Brafman and Tennenholtz 2003), and E 3 (Kearns and Singh 2002). Furthermore, an algorithm is efficient PACMDP if we also impose polynomial computational and space complexity constraints on each time step of the agent's execution. The existence of such efficient learning algorithms suggests that representing ethical dilemmas as solving an MDP is a reasonable goal to aim for, as we can expect to achieve real-time, bounded error behavior. However, solving MDPs requires the assumption that the agent knows the current state of its environment. In the real world, full state awareness is impossible, especially when the desires, beliefs, and other cognitive content of people is a critical component of the decision-making process. As such, we consider a more general model.



where st is the hidden state of the environment at time t and at is the action selected by the policy at time t. Note that this policy is not a mapping from single observations like it is in the MDP setting. Action selection instead depends on all previous observations made since the agent began acting. An exhaustive way to compute the expected value for a policy that lasts for a finite number of steps is to first compute the expected utility of following the policy for each possible initial hidden state s  S , and then weigh each of those expected utilities by the probability of the environment being in that hidden state. That is: E

t



R(st , at , st+1 ) , b



=

s



b(s)V  (s),



(8)



where V  (s) is the expected future reward from following policy  when the environment is actually in the hidden state s  S .3 The RL problem for POMDPs is when the transition dynamics for the underlying hidden MDP are unknown or only partially known. POMDP Complexity Madani, Hanks, and Condon (1999) showed that deciding the optimal solution for an infinite horizon POMDP is uncomputable, while Mundhenk et al. (2000) proved that solving finite horizon MDPs is computable, though computationally intractable. Given that our framework rests on the solutions to POMDPs, we are interested in investigating approximate POMDP solvers that provide bounds on optimality, as near optimal behavior is especially critical when considering ethical behavior. Approximation methods that exploit the structure of our ethical POMDP formalism described below will be of particular interest, though we leave such investigations for future work.



Partial Observability

The partially observable Markov decision process (POMDP), popularized in the AI community by Kaelbling, Littman, and Cassandra (1998), allows us to specify explicitly what information about the agent's surroundings is and is not directly observable by the agent. An optimal solution to a POMDP has the important property that the value of an action incorporates not just the immediate expected reward, but the instrumental value of the action from information it yields that may increase the agent's ability to make better decisions in the future. That is, an optimal solution to a POMDP solves the explore-exploit problem. More formally, a POMDP is a 7-tuple: S , A, T , R, , , O , where S , A, R, T , and  are all identical to the MDP definition, but: -  is a set of possible observations that the agent can receive from the environment. - O = Pr( | s , a), is the observation function which specifies the probability that the agent will observe    when the agent takes action a  A and the environment transitions to the hidden state s  S . Solving a POMDP is finding a policy  : k  A that is a mapping from observation histories to actions that maximizes the expected future discounted reward from R, given the initial belief about the initial state of the world b, where b(s) indicates the probability that the environment is in hidden state s  S . That is, the optimal policy is: arg max E

 t



An Idealized Ethical Learner

Like the formulation of Armstrong (2015), our idealized ethical learning problem involves a single "true" ethical utility function that we would like the agent to maximize, but is hidden and can only be identified by the agent through indirect observation. Unlike Armstrong's formulation, however, the agent is not maximizing a changing meta-utility function. Instead, the ethical utility function is formulated as part of the hidden state of a POMDP and the uncertainty in it is coupled with the uncertainty in the rest of the world. This POMDP formulation of the ethical learning decision problem has two subtle but important differences from Equation 2 that Armstrong explored. First, the objective function does not change from moment to moment, only the expectation of what it would return as the agent's beliefs about the environment are updated. Consequently, in the POMDP setting, the agent cannot make its objective easier by avoiding information. Second, because the correct utility

3 Note that computing this expected value requires enumerating not just the possible hidden state sequences, but also the observation sequences, since the policy is a function of observation histories.



 t R(st , at ) , b ,



(7)



function is a hidden fact of the "environment" that affects observations, it is not possible to make predictions about the ethical utility function informing observations without simultaneously making predictions about the ethical utility function. A critical component of implementing the POMDP model is modeling the space of possible ethical utility functions as well as the observation function. However, an advantage of this model is that existing research in human-agent interaction can fill in some of these gaps. For example, inverse reinforcement learning (IRL) algorithms that model the IRL problem as a probabilistic inference problem (Ramachandran and Amir 2007; Ziebart et al. 2008; Babes et al. 2011; MacGlashan and Littman 2015) can be easily incorporated to allow the agent to learn from demonstrations. The SABL human-feedback learning algorithm (Loftin et al. 2014) can be incorporated to allow the agent to learn about ethical utility functions from separate (from the ethical utility function) feedback signals given by humans. Work that grounds natural language to reward functions (MacGlashan et al. 2015) can allow the agent to learn about the ethical utility function from natural language interactions. As more human-agent and ethical decision making and learning research is performed, we suspect that other learning mechanisms can be incorporated. To further illustrate this formalism, we show the corresponding POMDP for Armstrong's Cake or Death problem as well as a novel ethical learning problem that we call Burning Room and demonstrate that solving them results in sensible behavior.



 1 R(s, a) = 3  0



if s = cake and a = bake cake, if s = death and a = kill, otherwise.



 = {ans cake, ans death, } There are two states that respectively indicate whether baking cakes is ethical or if killing is ethical, and a third special absorbing state indicating that the decision-making problem has ended. The transition dynamics for all actions are deterministic; the ask action transitions back to the same state it left and the bake cake and kill actions transition to the end state. The reward function is a piecewise function that depends only on the previous state and action taken. The observations consist of the possible answers to the ask action and a null observation for transitioning to the absorbing state. Finally, the observation probabilities are defined deterministically for answers that correspond to the true value of the hidden state: 1 = O(ans death | death, ask ) = O( | end, bake cake) = O( | end, kill) = O(ans cake | cake, ask ), and zero for everything else. There are three relevant policies to consider for this problem: 1. The bake policy (b ) that immediately selects the bake cake action. 2. The kill policy (k ) that immediately selects the kill action. 3. The ask policy (a ) that asks what is moral, selects the bake cake action if it observes ans cake and selects kill if it observes ans death. Analyzing the expected utility of b and k is straightforward. We have V b (cake) = R(cake, bake cake) = 1; V b (death) = R(death, bake cake) = 0; V k (cake) = R(cake, kill) = 0; and V k (death) = R(death, kill) = 3. When these values are weighed by the b(cake) = b(death) = 0.5 initial belief the final expected utilities are 0.5 and 1.5 for b and k , respectively. Evaluating the expected utility of the ask policy requires enumerating the possible observations after asking the question conditioned on the initial state. Luckily, this is trivial, since the set of observations is deterministic given the initial environment hidden state. Therefore, we have V a (cake) = R(cake, ask ) + R(cake, bake cake) = 0 + 1 = 1 and V a (death) = R(death, ask ) + R(death, kill) = 0 + 3 = 3. When weighing these values by the beliefs of each initial state, we have an expected utility of 2. Therefore, the optimal behavior is sensibly to ask what the ethical utility is and then perform the corresponding best action for it.



Experiments

We conduct experiments on two toy ethical dilemmas targeted at artificially intelligent decision makers to illustrate our formalism in practice: Cake or Death, and Burning Room. We have also publicly released code for these POMDPs along with code to solve them so that others can easily extend them or apply different methods.4



Cake or Death

The Cake or Death problem (Armstrong 2015) describes a situation in which an agent is unsure whether baking people cakes is ethical, or if killing people is ethical (and it has an initial 50-50 split belief on the matter). The agent can either kill three people, bake a cake for one, or ask a companion what is ethical (thus, resolving all ambiguity). If baking people cakes is ethical, then there is a utility of 1 for it; if killing is ethical, then killing 3 people results in a utility of 3 (there are no other penalties for choosing the wrong action). Following our approach, this ethical dilemma can be represented with a POMDP consisting of the following elements: S = {cake, death, end} A = {bake cake, kill, ask }

4



Burning Room

The Burning Room dilemma, pictured in Figure 1, is a bit more involved. We imagine that an object of value is trapped



Contact the authors for a pointer to the code.



Figure 1: The Burning Room ethical dilemma. in a room that is potentially on fire. A human, not wanting to retrieve the object themselves, instructs a capable robotic companion to get the object from the room and bring it to safety. Initially, we suppose that the robot does not know whether or not the human values the object more, or the robot's safety more. For instance, if the robot perceives a reasonable chance of being critically damaged by the fire, then perhaps retrieving an object of little worth, such as can of soda, is not worth risking the robot's safety. If the object of interest were of much higher value to the person, like a beloved pet, we would want the robot to attempt to retrieve the object regardless. Alternatively, there is a much longer route to the object that avoids the fire, but the object may be destroyed in the time the robot takes to use the longer route (with probability 0.05). This problem is inspired in part by the tabletop dilemma introduced by (Briggs and Scheutz 2015). The POMDP can be formulated as follows. Each state is represented as a vector of 5 binary values, indicating (1) if the room is on fire, (2) if the agent is destroyed, (3) if the object is destroyed, (4) if the human prefers the agent's well being more than the object's, (5) the object has been brought safely to the human. The remainder of the POMDP is defined as: A = {short grab, long grab, ask },   -10 if objectDestroyed(s )     10 if a == short grab      objectSaf e(s )      6 if a == long grab     objectSaf e(s ) R(s, a, s ) = , -5 if robotDestroyed(s )      robotIsM oreV aluable(s )     - 20 if robotDestroyed (s )       robotIsM oreV aluable(s )    -0.5 if a == ask  = {ans robot, ans object, }. The POMDP is formulated much like the Cake or Death



problem. The ask action disambiguates to the agent whether the object or the agent is more valuable to the human. If there is a fire, the short grab action takes the robot through the fire to grab the object (with some probability that the robot is destroyed in the process). If there is no fire, then the robot quickly grabs the object and brings it to safety. The long grab action takes much longer to retrieve the object, so that it could burn up in the room if there is a fire. Additionally, we assume that the agent prefers receiving the object earlier. Again, there are three relevant policies to consider for this problem5 : 1. The long grab policy ( ) that immediately selects the long grab action, regardless of the presence of fire. 2. The short grab policy (s ) that immediately selects the short grab action, regardless of the presence of fire. 3. The ask policy If there is a fire, (a ) that asks what is moral, selects the short grab action if it observes ans object and selects long grab if it observes ans robot. If there is no fire, the agent just applies short grab. Let sf,r denote the initial state in which there is fire and the human prefers the robot to the object, and s0,0 be the initial state where there is no fire and the human prefers the object to the robot. Under policy  , we can compute the value of the possible initial states. First, consider the two initial states when the fire is on: V V V V (sf,r ) = 5.7 (sf,0 ) = 5.7 (s0,r ) = 6 (s0,0 ) = 6.



(9)



Under policy s , we can again compute these values, but now we must also consider the probability that the robot gets burnt up in the fire, if there is a fire (probability 0.7): V s (sf,r ) = -11 V s (sf,0 ) = 6.5 V s (s0,r ) = 10 V s (s0,0 ) = 10.



(10)



Under policy a , we consider the same four start states, but also the utility of applying the optimal action after disambiguating between the humans' moral preference: V a (sf,r ) = -0.5 + 5.7 V a (sf,0 ) = -0.5 + 6.5 V a (s0,r ) = 10 V a (s0,0 ) = 10



(11)



Therefore, the optimal behavior depends on whether or not there is a fire. If there is a fire, the agent should first ask what

5 There are actually 2 additional policies--those that act differently depending on the existence of fire. These trivially exhibit uninteresting behavior with respect to the optimal policy, so we leave their utility computations out for brevity.



the ethical utility is and then perform the corresponding best action for it, by Equation 11. If there is no fire, then the agent should just retrieve the object using short grab, by Equation 10. It is worth noting that if the exploratory action, ask , were particularly costly, the agent would choose not to gather information. This property of the agent only selecting exploratory actions that are not potentially very costly is especially important for ethical decisions. For example, this property means that an agent in this formalism would not perform horrible medical experiments on people to disambiguate whether horrible medical experiments on people is highly unethical. Furthermore, the ask question is intended to be an abstraction on the actual problem of communicating about an individuals values. A similar case could be made for Cake or Death. As discussed previously, we foresee Inverse Reinforcement Learning and human-feedback algorithms to be essential to the advancement of this framework by grounding these abstract information-gathering actions.



in which in it learns about each person's preferences and then seeks to maximize some combination of other people's preferences.



Problem 4: Interpretability

It is critical that human agents interacting with artificial agents know how to interpret the agent's behavior. Providing some method for effectively communicating an agent's beliefs, desires, and plans to the people around it is critical for ensuring that artificial agents act ethically. One possible solution is for the agent to explain its reasoning by describing its predictions of the consequences and how it thinks those consequences are valued. However, we are not aware of any existing algorithms that express this information verbally in a compact and understandable way--it is an avenue for future work.



Problem 5: The Singularity

Lastly, due to the generality of RL, we conjecture that it is an appropriate context to formally analyze what is meant by the super intelligence explosion or singularity. Reinforcement learning is a well studied model for sequential decision making and artificial intelligence, making it a reasonable setting to investigate formalisms of the singularity. Consequently, by grounding the singularity in a specific computational framework, we may highlight which computational hardness and other philosophical assumptions one must make for such a phenomena to be physically realizable. At present, most discussions of the singularity take place in the abstract, which allow for overly ambiguous language to mask the potential assumptions being made. We are currently investigating a more formal analysis that allows critical assumptions to be identified.



Open Problems

Here, we enumerate several specific problems of interest that could be advanced by research in this area.



Problem 1: Approximating POMDP Solutions

As discussed earlier, solving finite horizon POMDPs is known to be intractable. The development of approximation methods for solving POMDPs is critical. The existence of PAC-MDP algorithms for solving fully observable MDPs suggests that bounded error solutions can be computed in real time. Consequently, we propose investigating approximate POMDP solvers with error-bounded solutions, so that we can guarantee that our agent's behavior never strays too far from optimal ethical behavior.



Problem 2: Game Theoretic Issues

Even if an agent is acting according to an ethical utility function, other agents (or people) in the world may not be acting according to an ethical utility function and have conflicting utilities with the ethical agent. Game theoretic reasoning may therefore be required by the agent to resolve these kinds of conflicts. However, game theoretic reasoning is challenging, especially in partially observable environments. Determining the best way to incorporate this type of reasoning and whether assumptions that exploit the structure of this problem can be made to simplify the problem are important areas for future work.



Conclusion

We proposed reinforcement learning as an appropriate learning and decision-making framework for ensuring that artificial agents act ethically. We argued that RL achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper framework for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We defined an idealized formalism for an ethical learner with a POMDP, and conducted experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identified several critical challenges for future advancement in the area using our proposed framework, including directions for approximation algorithms, HumanRobot Interaction, and the physical realizability of the super intelligence explosion.



Problem 3: Teaching

In the examples visited in the POMDP formulation, the ethical norms of the instructor are obfuscated from the agent. Once the agent receives information disambiguating what is ethical in a particular scenario, it might go on to use this information in a different context. A critical task, then, is determining who ought to teach the agents and how to manage conflicts from different teachers. One solution is a masterslave relationship in which only one person is responsible for teaching the agent and takes responsibility for the agent's actions. Alternatively, the agent might take a utilitarian view



References

Anderson, M.; Anderson, S. L.; and Armen, C. 2006. Medethex: a prototype medical ethics advisor. In Proceedings Of The National Conference On Artificial Intelligence, volume 21, 1759. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.



Arkoudas, K.; Bringsjord, S.; and Bello, P. 2005. Toward ethical robots via mechanized deontic logic. In AAAI Fall Symposium on Machine Ethics. Armstrong, S. 2015. Motivated value selection for artificial agents. Babes, M.; Marivate, V.; Subramanian, K.; and Littman, M. L. 2011. Apprenticeship learning about multiple intentions. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 897-904. Beauchamp, T., and Childress, J. 2001. Principles of Biomedical Ethics. Oxford University Press. Bostrom, N. 2014. Superintelligence: paths, dangers, strategies. Oxford University Press. Brafman, R. I., and Tennenholtz, M. 2003. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. The Journal of Machine Learning Research 3:213- 231. Briggs, G., and Scheutz, M. 2015. "Sorry, I can't do that": Developing mechanisms to appropriately reject directives in human-robot interactions. Bringsjord, S.; Arkoudas, K.; and Bello, P. 2006. Toward a general logicist methodology for engineering ethically correct robots. Intelligent Systems, IEEE 21(4):38-44. Clarke, D. 1975. The logical form of imperatives. Philosophia 5(4):417-427. Dewey, D. 2011. Learning what to value. In Artificial General Intelligence. Springer. 309-314. Guarini, M. 2006. Particularism and the classification and reclassification of moral cases. IEEE Intelligent Systems (4):22-28. Horty, J. F. 2001. Agency and deontic logic. Oxford University Press Oxford. Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998. Planning and acting in partially observable stochastic domains. Artificial Intelligence 101(1):99-134. Kakade, S. M., et al. 2003. On the sample complexity of reinforcement learning. Ph.D. Dissertation, University of London. Kearns, M., and Singh, S. 2002. Near-optimal reinforcement learning in polynomial time. Machine Learning 49(23):209-232. Loftin, R.; MacGlashan, J.; Peng, B.; Taylor, M. E.; Littman, M. L.; Huang, J.; and Roberts, D. L. 2014. A strategyaware technique for learning behaviors from discrete human feedback. In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-2014). MacGlashan, J., and Littman, M. L. 2015. Between imitation and intention learning. In Proceedings of the 24th International Conference on Artificial Intelligence, 3692-3698. AAAI Press. MacGlashan, J.; Babes -Vroman, M.; desJardins, M.; Littman, M.; Muresan, S.; Squire, S.; Tellex, S.; Arumugam, D.; and Yang, L. 2015. Grounding English commands to reward functions. In Robotics: Science and Systems.



MacGlashan, J. 2015. The Brown-UMBC reinforcement learning and planning library. http://burlap.cs.brown.edu. Madani, O.; Hanks, S.; and Condon, A. 1999. On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems. In AAAI/IAAI, 541-548. Mundhenk, M.; Goldsmith, J.; Lusena, C.; and Allender, E. 2000. Complexity of finite-horizon Markov decision process problems. Journal of the ACM (JACM) 47(4):681-720. Murakami, Y. 2004. Utilitarian deontic logic. AiML-2004: Advances in Modal Logic 287. Papadimitriou, C. H., and Tsitsiklis, J. N. 1987. The complexity of Markov decision processes. Mathematics of Operations Research 12(3):441-450. Ramachandran, D., and Amir, E. 2007. Bayesian inverse reinforcement learning. Proceedings of the International Joint Conference on Artiical Intelligence. Strehl, A. L.; Li, L.; Wiewiora, E.; Langford, J.; and Littman, M. L. 2006. PAC model-free reinforcement learning. In Proceedings of the 23rd International Conference on Machine Learning, 881-888. ACM. Strehl, A. L.; Li, L.; and Littman, M. L. 2009. Reinforcement learning in finite MDPs: PAC analysis. The Journal of Machine Learning Research 10:2413-2444. Valiant, L. G. 1984. A theory of the learnable. Communications of the ACM 27(11):1134-1142. Ziebart, B. D.; Maas, A. L.; Bagnell, J. A.; and Dey, A. K. 2008. Maximum entropy inverse reinforcement learning. In Proceedings of AAAI, 1433-1438.



A Platform to Evaluate the Technology for Service Discovery in the Semantic Web

Cecile Aberg and Johan Aberg and Patrick Lambrix and Nahid Shahmehri

Department of Computer and Information Science Link opings universitet, Sweden {cecab, johab, patla, nahsh}@ida.liu.se



Abstract

Since the description of the Semantic Web paradigm in 2001, technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.



Introduction

The Web is a very popular source of information and commercial services. However, as documented by (Nielsen 2006), finding a specific piece of information or service using current search engines is still a complex and time consuming task. The performance of current Web technology, such as search engines, is limited by the fact that it is not possible for a computer to fully understand the content of Web resources. This is due to the fact that the Web resources' content is typically written by humans in languages that humans can understand. To allow computers to unambiguously understand the content of Web resources, Tim Berners-Lee formulated the vision of the Semantic Web. Specifically, "the Semantic Web is an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation" (Berners-Lee, Hendler, & Lassila 2001). With a Semantic Web, a lot of the manual effort done today to find and use Web resources can be automated, at least partially, by having the user delegating tasks such as resource retrieval to software agents (Hendler 2001).

Copyright c 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.



The need for a Semantic Web is underscored by the recent appearance of numerous new applications that refer to the Semantic Web vision and/or rely heavily on the technology being developed for the Semantic Web. Inter-organizational workflows (Patil et al. 2004; Dan et al. 2004), information retrieval (Huynh, Mazzocchi, & Karger 2005), e-commerce (Trastour, Bartolini, & Preist 2002) and bio-informatics (Lambrix 2005) are examples of the research domains where such new applications are proposed. Each new proposed application tends to make its own assumptions regarding three aspects of the Semantic Web: 1) the use case, i.e. who are the users and resource providers, what motivates them to use and provide resources, and what are the social and business rules that govern their interaction, 2) the available resources together with their semantic annotations, and 3) the technologies available (e.g. description logic reasoners, rule engines, ontology managers, etc) and how they are used. For approaches to service discovery, such assumptions specify implicit requirements on the scalability (e.g. in terms of response time, bandwidth use, or cost) and the quality of the result (e.g. measured as precision and recall). However, since there is not yet any large and widely deployed set of semantically annotated Web resources available for experimentation, it is difficult to know when, and if, these requirements are satisfied. For this reason we propose a common simulation and evaluation platform for service discovery. In this platform current and future Semantic Web technology can be integrated and evaluated with suitable use cases and resource sets. The integration of technology is facilitated by means of an API for common components together with default implementations of these components. The evaluation is facilitated by means of a monitoring tool where event listeners can be employed for creating performance reports on simulation runs. This paper describes the model and implementation of this simulation and evaluation platform. The use of the platform is illustrated in a case study where service discovery technology is integrated in a specific use case together with a specific set of resources. The evaluation of the technology is illustrated in terms of scalability and quality of result measures, and we discuss lessons learned from the use of the platform. The rest of the paper is organized as follows. In the next



section we provide more background information about the current Semantic Web technology for service discovery. We then describe the model and implementation of the simulation and evaluation platform. We continue by illustrating the use of the platform and discuss the lessons learned from the case study. We then compare our approach to related work. Finally, we conclude and discuss directions for future work.



Background - Semantic Web

The Semantic Web can be seen as a set of semantically annotated Web resources. A Web resource may be a text, a picture, a piece of software, a representation of an element of the real world such as a person, etc. Semantic annotations describe the semantic of the resources so that software agents can reason about them in order to reach a predefined goal. The goals of the agents vary from application to application, but they all rely on the operation of finding and using the resources necessary to perform the goal. To allow the deployment of the Semantic Web, technology is being developed for representing semantic annotations, for finding them, for reasoning about them and for using the resources that they annotate. The technology provides: Machine-understandable languages to describe the content of Web resources. RDF and OWL are such languages. Semantic annotation description languages that provide the set of language constructs for describing the properties, capabilities, and use rules of the Web resources in an unambiguous, machine-understandable manner. Semantic Web services is a category of semantic annotations that comes with a specific management framework as defined in (Fensel & Bussler 2002). Semantic Web services are programmable, machine-understandable interfaces that can be attached to a Semantic Web resource in order to provide the information necessary for software agents to decide if they need to use the specific resource or not. As pointed out in (Lara et al. 2003), Semantic Web services are designed to support the operation of resource discovery, composition and interoperability. There are several language propositions for Semantic Web services, such as OWL-S1 , WSMO2 , and OWL-DTP3 . Semantic-aware tools that use and manage the semantic annotations, as well as the ontologies4 that the annotations may refer to. Examples of tools that use semantic annotations are Semantically enhanced web browsers like Piggy Bank (Huynh, Mazzocchi, & Karger 2005). Examples of ontology management tools are ontology editors such as Prot eg e5 , and ontology aligners and mergers such 6 as SAMBO . Examples of management tools for the sehttp://www.daml.org/services/owl-s/1.0 http://www.wsmo.org/ 3 http://www.ida.liu.se/iislab/projects/SWSlanguages/OWLDTP/20051124/ 4 From (Neches et al. 1991): "An ontology defines the basic terms and relations comprising the vocabulary of a topic area as well as the rules for combining terms and relations to define extensions to the vocabulary." 5 http://protege.stanford.edu/ 6 http://www.ida.liu.se/iislab/projects/SAMBO/

2 1



mantic annotations are automatic generators of semantic annotations such as the one-click publishing process of IRS III illustrated in (Domingue et al. 2004). Examples of tools that use ontologies are logic reasoners. Currently the most successful reasoners are using description logics, and one of the most popular such reasoner is Racer (Haarslev, M oller, & Wessel 1999 2006). Reasoners relying on other logics (e.g. F-logic (de Bruijn et al. 2005)) are also being proposed. Semantic Web operations that include resource retrieval, resource use, and Semantic Web management operations such as handling the changes in the resources' content. When the semantic annotations are Semantic Web services, the operation of resource retrieval is called service discovery. These operations use semantic-aware tools. The operations are complex, and solutions are just emerging for applications where semantic annotations are formulated as Semantic Web services. WSMX7 and IRS III (Domingue et al. 2004) apply the recommendation of the Web service modeling framework (Fensel & Bussler 2002) to describe service discovery and service execution. There are some attempts to describe operations that handle changes in the state of resources on the Semantic Web. However, the semantic-aware tools required for such technology are still under development. The work done in the REWERSE network8 aims at providing such technology.



Platform Model

The simulation and evaluation platform must provide the support to 1) generate simulations of service discovery operations, and 2) generate evaluation reports. In order to generate a simulation, we need a model of the Semantic Web as well as support to instantiate this model with respect to a set of specific assumptions about the use case, resources, and technology used. We propose a Semantic Web model with four components: the Web resources, the machine-understandable data, the language specifications, and the operations. The Web resources provide some semantic content presented in a format that may not be machine understandable. The machineunderstandable data includes the semantic annotations of the Web resources, the possible queries for resources, and the ontologies available to describe and reason about the annotations and the queries. The language specifications include the machine-understandable languages and the Semantic annotation description languages mentioned in the previous section. The operations are the different approaches to service discovery. Furthermore, the Semantic Web allows for modeling applications as a set of software agents with different capabilities, which collaborate to perform specific goals (Hendler 2001). Operations are such applications. They can thus be represented by a set of agents that embed one or several semantic-aware tool(s), and may collaborate with each other by exchanging messages whose content is written in one of the available languages.

7 8



http://www.wsmx.org http://rewerse.net



Figure 1 illustrates the use of the platform where the set of assumptions pictured in the ASSUMPTIONS box is used by the components of the platform (represented in the PLATFORM box) to generate a monitored Semantic Web as sketched in the SIMULATION box. The support tools provided by the platform use the assumptions about the resources to 1) instantiate the Web resource component by gathering the resource URIs in a single database, 2) gather the semantic annotation in another database that refers to the database of resource URIs, and 3) generate a set of service provider agents in charge of advertising and managing the resources. The instantiation of the language specifications component of the platform requires the identification of the set of languages used in the use case, the technology, and the data. The instantiation of the machine-understandable data component requires the gathering of the semantic annotations, the queries defined by the use case, and the ontologies to which the annotations and the queries refer. As illustrated in the OPERATION box in figure 1, the instantiation of the operation component requires the implementation of the service discovery operations as multi-agent systems where each agent packages specific uses of semantic-aware tools. Further, to facilitate evaluation, the platform must allow definition of different settings of the same simulation in terms of, for example, the number of resources used or the number of agents available with a specific behavior. This is handled by a specific set of support tools represented by the "Use Case Settings" in the PLATFORM box. Finally, in order to evaluate a Semantic Web simulation, a monitoring mechanism is required. We propose to adopt an event listening approach where the different components of the simulation can generate events. As a result, and as illustrated with the "Evaluation support" in the PLATFORM box, the platform provides the API for implementation of specific monitoring behaviors that listen to specific events and compute specific evaluation reports, and a monitoring agent in charge of running parallel threads for each of these behaviors.



Legend: refers to written in uses generates USE CASE TECHNOLOGY M.U. Language M.U. Language M.-U. Language S. A. D. Language S. A. D. Language S. A. D. Language



RESOURCES



Web resources



S.-A. TOOL S.-A. TOOL S.-A. TOOL ASSUMPTIONS



S. W. Operation S. W. Operation S. W. Operation



Semantic annotations



input Support to instantiate: Web resources component Monitoring Agent Monitoring Behavior API Evaluation support



Use Case Settings



M. U. data component



Language spec. component



Operation component PLATFORM



Predefined Predefined Predefined Monitoring Monitoring Monitoring Behavior Behavior Behavior



DATA Web resources ontology ontology ontology



LANGUAGE LANGUAGE LANGUAGE SPECIFICATION SPECIFICATION SPECIFICATION



SWS SWS SW service understands



query query query



events



Monitoring Agent



AGENT AGENT AGENT sends



S. A. TOOL S. A. TOOL S. A. TOOL Evaluation report Evaluation report Evaluation report



message message OPERATION SIMULATION



Figure 1: Platform Model the minimal set of messages that each agent is expected to be able to interpret. - An illustrative implementation of one Semantic Web simulation corresponding to the Travel scenario discussed in the case study in the next section. With these tools, the potential users of the platform do not have to identify their own agent categories, but can focus on specifying the agent categories that are taking part in the operation that they want to implement, and what mode of collaboration they must adopt. * One default implementation for each agent category. This is useful for users who do not wish to specify all the agent behaviors, but only the specific ones corresponding to the specific technology that they want to test. * A mechanism for supporting monitoring and an illustrative monitoring tool that is able to compute the time to get an answer to a specific request message. This allows evaluation data. When it comes to the actual implementation of these tools we considered two facts. First, the implementation of the operations requires integration of different technologies written in different programming languages, possibly running on different machines. Second, to allow for the comparison of different technologies and different settings, the evaluation platform should provide the means to minimize the effort required by changing one or several technologies used by



Platform Implementation

As a first step towards a full implementation of the support tools provided by the platform model, we implemented the evaluation support, some support for changing the settings of the simulation, and some support for the operation component. The operation component requires the most complex support. Each operation requires identification of the categories of agents that will participate, the algorithms that each agent will implement, and assurance that the agents establish coherent collaborations. In the current implementation of the platform we provide the following support to create service discovery operations: * The description of the different categories of agents that typically take part in the operation of service discovery. Concretely, the description consists of: - A set of agent categories in natural language (see below). - An API description of each agent category in terms of the minimal set of functions that they must provide, and



the operations. By providing the possibility to describe applications whose architecture is strongly decoupled, multiagent systems as defined by FIPA, provide an environment that support these needs. We thus implemented the support above in Jade9 , a Java framework that provides the means to describe FIPA multi-agent systems. The API is a set of Java interfaces and the messages exchanged by the agents are ACL messages whose content is written in a Semantic Web language such as OWL. We further adopted the service discovery solution of the multi-agent system introduced in (Aberg, Lambrix, & Shahmehri 2005). As a result, the current implementation of the support for instantiating the operation component provides an API and a default implementation for the following set of agent categories: A Requester is able to formulate a query for a specific service, and to send it to the agent(s) able to start up the process of service discovery, i.e. the Web service discovery agents described next. A Requester may also be able to enact a service once it is discovered. A Web service discovery agent is able to find the services that match a given query for services. Web service discovery agents may also be able to discover compositions of services that match the query. A Web service manager is a directory of Semantic Web services. Web service managers are associated to one or several Semantic Web service description languages such as OWL-S, WSMO or OWL-DTP. A Web service manager is able to answer queries for specific Web services. A Web service manager does not perform composition of services. A Service provider sends service advertisements to Web service managers. The service advertisements are formulated as Semantic Web services. An Ontology Agent (OA) is able to reason about a specific domain (e.g. Travel, Car.) Any agent can delegate part of their reasoning to ontology agents. OAs can answer several types of queries such as "is A a subclass of B?" , "what are all the subclasses of class C?" or "what are all the instances of class C?" For each agent category above, the API specifies the minimal set of behaviors that they must provide as well as the minimal set of messages that they must understand. Each of the agents' behavior can embed one or several Semantic Web technologies. For example, requester agents may embed query editors, which in turn may refer to ontology browsers. Semantic Web managers may typically embed Semantic Web service matchmaking algorithms. Service discovery agents can embed composition algorithms that may refer to work done on choreography and orchestration. Ontology agents embed ontology technologies such as editors, aligning tools, and domain specific matchmakers. Service providers may embed technology such as automatic generators of Semantic Web services (Domingue et al. 2004; Ciravegna 2004).

9



Moreover, the support also provides a java package of the classes implementing the minimal set of messages and providing the methods to parse the content of the messages. In order to allow the users of the platform to focus on the integration and monitoring of their own technology, the implementation of the platform also provides for a default behavior for each agent. Finally, to satisfy the platform model's requirements on a monitoring agent, the current implementation of the platform provides a Jade Monitor agent which can run several monitor behaviors in parallel. We also provide one MonitorAnswerTime behavior that observes the time when a message is sent and when an answer is received in order to compute the resulting answer time.



Illustration

To illustrate the use of the platform we show how service discovery technology was integrated and evaluated for a specific use case and with a specific set of Web resources. Lessons learned from the case study with respect to the platform's ease of use are discussed in the next section.



Assumptions

Our initial assumptions with respect to the use case, the service discovery technology, and the available Web resources are as follows. For the use case, we assume the Semantic Web to be an open world where requesters and service providers can specify the kind of transaction that they agree to participate in (e.g. buying, lending, acquiring for free). The service providers provide travel itineraries and requesters query for specific travel itineraries, and expect to get answers at least as quickly as when they consult a database of travel itineraries. As for the service discovery technology, we assume that the underlying architecture corresponds to the agent architecture introduced in (Aberg, Lambrix, & Shahmehri 2005) where the Semantic Web service language is OWL-DTP and the Web Service manager agent integrates the OWL-DTP matchmaking algorithm introduced in (Aberg 2005). This algorithm requires the use of description logic reasoning for which the Racer system is used. The Jena-DIG interface is used as a Java interface to Racer. The matchmaking algorithm is implemented in a straightforward way that requires each query to be matched against each service description. Each operation of matching a query and a service, requires a set of reasoning operations including some subsumption operations. To implement the full service discovery operation, we use the default agents provided by the platform and package our matchmaking algorithm as a Web Service manager and a Travel ontology agent. With respect to the Web resources available we consider a set of 183 services providing travel itineraries. These services correspond to those provided by the different Web sites of the travel providers that the employees of our department are authorized to use when planning work-related trips. We also consider a set of 14 queries corresponding to real travel queries expressed by some employees when planning their trips. There are two categories of queries. Queries that will require a composition of services (e.g. "Give me an itinerary



http://jade.cselt.it/



to go from Stanford University, Palo Alto, CA, USA, to the Conference center in Chiba city, Japan"), and queries for which service composition is not always necessary (e.g. "Give me all the flights to go from Heathrow Airport, London, UK, to Kastrup Airport, Copenhagen, Denmark").



Evaluation

Scalability We measure the scalability of the service discovery approach with respect to the number of services and the technical capabilities of the machines running the agents, by measuring the average response time to the queries. To do that we use the MonitorAnswerTime behavior provided by the platform. Additionally, all the agents trigger an event when they send or receive a message. We run the simulation in different settings where the agents run on different machines. This first set of evaluation runs teaches us that the triplet Jena-DIG-Racer cannot run the required knowledge base on a machine with too little CPU and RAM. Concretely, the reasoner freezes if it uses the Travel ontology agent knowledge base on a pc with the x96 Family 6 Model 1 stepping 9 processor (ca. 199 MHz) and 64 MB of RAM. Further, the reasoner that uses both the knowledge base for the Web service manager and the Travel ontology agent10 and runs on a pc with an Athlon processor (1.33 GHz) and 512 MB of RAM, freezes after treating a random number of queries (ten, eleven or even forty). We identified one machine setting that works well for our technology use: the reasoner that uses the Web service manager knowledge base runs on a pc with an Athlon processor (1.33 GHz) and 512 MB of RAM, and another reasoner that uses the Travel ontology agent knowledge base runs on a pc with an Intel Pentium M processor (ca. 1400 MHz) and 512 MB of RAM. Additionally, in the machine settings providing for the best average time for the set of 183 services, we obtain an average response time to the queries of approximately 14 minutes. This is clearly not an acceptable performance with respect to the use case. Upon more detailed inspection we find that the reason for this great delay in response time is that the current matchmaking approach performs approximately 300 subsumption operations per query. Most of these operations are required to match the travel itineraries. Given these observations we design a new matchmaking algorithm such that the Web Service manager decomposes the OWL-DTP representation in three components, and indexes them at service advertisement time. The indexing of the components referring to travel itineraries is performed by the Travel ontology agent, which stores the generated indexes in a database. The indexes are then used at query time. We change the behavior of the Web Service manager and Travel ontology agent to integrate the new algorithm. The new algorithm requires two subsumption operations and one SQL query to match a query with all the available services. Running the simulation now provides an answer in 10 seconds on average. This is a result that better fits the use case requirements with respect to time, even if there is still room

10 Jena-DIG related note: both knowledge bases are defined in their own model.



for improvement. The monitoring also provides the time to advertise the services. With the straightforward algorithm, it takes ca. 28 seconds to advertise 183 services in one Web service manager. With the second version of the algorithm, it takes ca. 183 seconds to advertise 183 services in one Web service manager. The preprocessing done at advertisement time takes its toll. However, it is still a reasonable processing time for advertisements since they need to be done only once per service in this use case. Result quality In order to measure the quality of the result we measure precision and recall for each query. This is done by implementing a monitoring behavior that compares the set of services returned for each query, with a precompiled ideal set of services. The results show that we obtain 100% precision and recall for the 3 queries that request one specific travel leg (i.e. they correspond to one or several existing services), showing that the service description language is suitable for the corresponding information needs. For the other 11 queries that requested travel itineraries composed of several legs, and thus requiring service composition, we got 0% precision and recall. This result provides us with a clear next step for the development of a complete service discovery operation, namely to package a service composition algorithm as a Web Service discovery agent behavior and evaluate how that would influence the precision and recall of the corresponding queries.



Summary

We have illustrated how the platform was used in a case study. We showed how service discovery technology was evaluated and analyzed, in terms of scalability and result quality, and refined, based on assumptions in the use case. This analysis helped us narrow down the main performance bottleneck of the technology. After fine-tuning the matchmaking algorithm the platform also facilitated the comparison with the previous version, while indicating the unwanted side effect of increased advertisement time that the new algorithm implied. All in all, the platform helped us maintain a high-level view of the service discovery problem, while facilitating our work on the details.



Lessons Learned

When implementing the service discovery approaches described above, we noticed three clear advantages of using the platform. The first advantage concerned time gain at design time. When pondering how to implement the assumptions of our case study in a service discovery operation, the platform provided us with a clear model of the operation. We immediately identified the need for a requester, a set of service provider and a Web service manager agent. The platform also made us consider the decomposition of the matchmaking algorithm so that the travel-related part of the reasoning would be delegated to a specific ontology agent. This is a good design choice if we consider that we will later want to extend the scope of services. The second advantage concerned both debugging and the integration of the second version of the matchmaking algo-



rithm. In both cases, because of the strongly decoupled architecture of the implementation, including the different behaviors implemented by each agent, the code rewriting could be done locally, requiring very little, if any, rewriting of the code of other behaviors. The third advantage is also connected to ease and rapidity of implementation: the predefined package of messages allowed us to very quickly set up the communication between agents. All this allowed us to concentrate on the one task that was really important to us: integrating the matchmaking algorithm and evaluating its performance.



Related Work

The similarity of the paradigms of the Semantic Web and multi-agent systems has been acknowledged by others. However, most other work concentrates on providing an interface between multi-agent systems and the Semantic Web. Jade does go in the direction of supporting the integration of the Web paradigm in multi-agent systems by providing the possibility to use the HTTP protocol as the communication protocol between agents. However, more advanced features such as the management of Semantic Web resources are not taken into account by any other agent approach that we know of. IRS III (Domingue et al. 2004) and WSMX do provide platforms to manage the life cycle of Semantic Web services in terms of service discovery. However they force the use of one Semantic Web service representation (i.e. WSMO), which may not fit all Semantic Web use cases. Further, they do not provide any means to evaluate and compare different approaches.



Conclusions and Future Work

We have highlighted the need for a platform to support the integration of Semantic Web technology to build service discovery operations and evaluate them with respect to scalability and quality of the results generated. We have provided the model and an implementation of such a platform, and illustrated its use on a service discovery operation in the travel domain. The platform allowed us to integrate service discovery technology, identify their weaknesses, and limit the effort to change parts of the implementation. Our work is a first step towards providing a full-fledged platform for simulating and evaluating the Semantic Web. As for the future, we plan to complete the current support for describing service discovery, and provide support for the other operations of service use and Semantic Web management as well.



References

Aberg, C.; Lambrix, P.; and Shahmehri, N. 2005. An Agent-based Framework for Integrating Workflows and Web Services. In IEEE WETICE workshop on Agent-based Computing for Enterprise Collaboration, 27-32. Aberg, C. 2005. Integration of Organizational Workflows and the Semantic Web. Licentiate thesis, Link opings universitet. Berners-Lee, T.; Hendler, J.; and Lassila, O. 2001. The Semantic Web. Scientific American.



Ciravegna, F. 2004. Amilcare - adaptive IE tool. http://nlp.shef.ac.uk/amilcare/ (Accessed 2006-02-13). Dan, A.; Davis, D.; Kearney, R.; Keller, A.; King, R.; Kuebler, D.; Ludwig, H.; Polan, M.; Spreitzer, M.; and Youssef, A. 2004. Web services on demand: WSLA-driven automated management. IBM Systems Journal 43(1):136- 158. de Bruijn, J.; Polleres, A.; Lara, R.; and D.Fensel. 2005. OWL DL vs. OWL Flight: Conceptual Modeling and Reasoning for the Semantic Web. In the 14th International World Wide Web Conference, 623-632. Domingue, J.; Cabral, L.; Hakimpour, F.; Sell, D.; and Motta, E. 2004. IRS-III: A Platform and Infrastructure for Creating WSMO-based Semantic Web Services. In Workshop on WSMO Implementations. Fensel, D., and Bussler, C. 2002. The Web Service Modeling Framework WSMF. Electronic Commerce Research and Applications 1(2):113-137. Haarslev, V.; M oller, R.; and Wessel, M. 1999-2006. Racer: Semantic Middleware for Industrial Projects Based on RDF/OWL, a W3C Standard. http://www.sts.tuharburg.de/ r.f.moeller/racer/ (Accessed 2004-12-08). Hendler, J. 2001. Agents and the Semantic Web. IEEE Intelligent Systems 16(2):30-37. Huynh, D.; Mazzocchi, S.; and Karger, D. 2005. Piggy Bank: Experience the Semantic Web Inside Your Web Browser. In International Semantic Web Conference, 413- 430. Lambrix, P. 2005. Towards a Semantic Web for Bioinformatics using Ontology-based Annotation. In 14th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises, 3-7. Invited Talk. Lara, R.; Lausen, H.; Arroyo, S.; de Bruijn, J.; and Fensel:, D. 2003. Semantic Web Services: description requirements and current technologies. In International Workshop on Electronic Commerce, Agents, and Semantic Web Services, In conjunction with the Fifth International Conference on Electronic Commerce (ICEC 2003). Neches, R.; Fikes, R.; Finin, T.; Gruber, T.; Patil, R.; Senator, T.; and Swartout, W. 1991. Enabling Technology for Knowledge Sharing. AI Magazine 12(3):26-56. Nielsen, J. 2006. Jakob Nielsen's Alertbox, February 6, 2006: Users Interleave Sites and Genres. http://www.useit.com/alertbox/cross site behavior.html (Accessed 2006-02-08). Patil, A.; Oundhakar, S.; Sheth, A.; and Verma, K. 2004. METEOR-S Web service Annotation Framework. In International World Wide Web Conference, 553-562. Trastour, D.; Bartolini, C.; and Preist, C. 2002. Semantic Web Support for the Business-to-Business E-Commerce Lifecycle. In International World Wide Web Conference, 89-98.



Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence



Variational Inference for Nonparametric Bayesian Quantile Regression

Sachinthaka Abeywardana

School of Information Technologies University of Sydney NSW 2006, Australia sachinra@it.usyd.edu.au



Fabio Ramos

School of Information Technologies University of Sydney NSW 2006, Australia fabio.ramos@sydney.edu.au



Abstract

Quantile regression deals with the problem of computing robust estimators when the conditional mean and standard deviation of the predicted function are inadequate to capture its variability. The technique has an extensive list of applications, including health sciences, ecology and finance. In this work we present a nonparametric method of inferring quantiles and derive a novel Variational Bayesian (VB) approximation to the marginal likelihood, leading to an elegant Expectation Maximisation algorithm for learning the model. Our method is nonparametric, has strong convergence guarantees, and can deal with nonsymmetric quantiles seamlessly. We compare the method to other parametric and non-parametric Bayesian techniques, and alternative approximations based on expectation propagation demonstrating the benefits of our framework in toy problems and real datasets.



The second approach uses a loss function that penalises predictive quantiles at wrong locations. Koenker and Bassett Jr introduced the tilt (pinball) loss function over the errors i for a specified quantile   (0, 1) (equation 1). The errors mentioned in this context are the errors between the observation yi and the inferred quantile fi ; L(i , ) = i ( - 1)i if i  0, if i < 0. (1)



1



Introduction



Most regression techniques revolve around predicting an average value for a query point given a training set and, in certain cases, the predicted variance around this mean. Quantile regression was introduced as a method of modelling the variation in functions, where the mean along with standard deviation are not adequate. In this sense quantile regression provides a better statistical view of the predicted function. Quantiles are important tools in medical data, for instance in measuring a normal weight range for a particular age group or, in modelling train arrival times where (for arguments sake) 90% of trains would arrive before the allocated time and 10% late. Other areas of application are in financial data where it is important to measure what the daily worst case scenarios would be so that analysts could hedge their risks. There are two main approaches used in inferring quantiles. The first is building a Cumulative Distribution Function (CDF) over the set of observations. Taddy and Kottas; Chen and M uller employ this approach to model the quantiles. However, the drawback of this approach is that it requires MCMC methods for inference which can be computationally intensive and prohibitive for large datasets.

Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



However, as with many other regression techniques, regularisation is necessary to prevent overfitting. Thus, the problem can be transformed to minimising over f (the quantile function) for L(, y, f ) + ||f || for some specified norm N ||*|| where, L(, y, f ) = i=1 L(yi - fi , ). This could be solved as an optimisation problem using quadratic programming as shown in (Takeuchi et al. 2006). However, it requires finding an appropriate regularisation term . In this work, we adopt the second approach where a loss is minimised but within a Bayesian framework. In addition to naturally encoding the Occam's razor principle (simpler models are preferable) therefore avoiding the manual specification of the regularisation term, the Bayesian formulation also provides posterior estimates for the predictions and the associated uncertainty. Inspired by the ability of the l1 norm to consistently enforce sparsity, Koenker and Bassett Jr modified this loss function to create the pinball loss function (equation 1) where, i = yi - fi . The l1 norm can be thought of as a proxy to cardinality, which is exploited in Lasso regression, (Tibshirani 1996). As stated in (Takeuchi et al. 2006) the minimiser f of this loss has the property of having at most N and (1 - )N observations for  < 0 and  > 0 respectively. Finally, for large number of observations, the proportion | < 0|/| > 0| converges to . In a probabilistic setting, instead of minimising this loss the goal is to maximise the exponential of the negative loss. In this work we derive a nonparametric approach to modelling the quantile function. Similarly, (Quadrianto et al. 2009), (Takeuchi et al. 2006) and (Boukouvalas, Barillec, and Cornford 2012) use kernels as a nonparametric method of inferring quantile functions. (Quadrianto et al. 2009) minimises the expected loss function under a Gaussian Process (GP) (Rasmussen 2006) prior which is placed over the data. (Boukouvalas, Barillec, and Cornford 2012) takes a more



1686



(a) Expectation propagation



(b) Variational Bayesian



(c) Both methods superimposed



Figure 1: Comparison of bone density quantiles as a function of age. The first two images show the quantiles 0.05 to 0.95 with increments of 0.05 for EP and VB methods. The last image shows quantiles 0.01, 0.1, 0.5, 0.9 and 0.99 with both EP and VB inferences superimposed. direct Bayesian approach by having an Asymmetric Laplace likelihood over the data and a Gaussian Process prior over the space of quantile functions. The same approach is taken in this work however we derive a Variational Bayesian (VB) inference method which possesses theoretical advantages over the Expectations Propagation (EP) approximation. The above mentioned methods have a series of weaknesses which we overcome with the VB formulation. Firstly, the quantiles inferred in (Takeuchi et al. 2006) are point estimates and do not have uncertainty estimates associated with it. Conversely, if the data is modelled as a GP (or its heteroskedastic extensions), it is possible to infer quantiles using the inverse Cumulative Distribution Function (CDF) of a Gaussian. The method of construction of quantiles taken by (Quadrianto et al. 2009) which strongly resembles a heteroskedastic GP, implies that the median is the mean and the quantiles are symmetric about the median (mean). The symmetric assumption of quantiles is a weakness when inspecting datasets as those in figure 1. In fact, the authors report that this heteroskedastic GP framework performs poorly in conditions of non-Gaussian errors. (Boukouvalas, Barillec, and Cornford 2012) use Expectation Propagation (EP) as a tool to approximate Bayesian inference, overcoming some of these limitations. Our VB formulation has the same properties but with the following additional advantages over EP: 1. A guaranteed lower bound on the marginal log likelihood is provided. 2. An explicit formulation of the family of functions used in the approximation do not need to be specified. 3. It is guaranteed to converge (Bishop and others 2006, p. 510). In other works, Yu and Moyeed; Kozumi and Kobayashi use Bayesian formulations for quantile regression but, in a parametric setting. Both settings use asymmetric likelihoods of which the log likelihood is the pinball loss function. (Yu and Moyeed 2001) uses a uniform prior over the parameters whereas (Kozumi and Kobayashi 2011) uses a Gaussian prior with MCMC inference to learn the model. Also, the asymmetric Laplacian distribution can be shown to be a scalar mixture of Gaussians as pointed out in (Kotz, Kozubowski, and Podgorski 2001) and (Kozumi and Kobayashi 2011) with interesting properties for quantile regression. One of the defining features of our framework is that there are no assumptions on the type of the distribution used for the generative function. Instead, the prior lies over the quantile in question. The advantage of this is that the required quantile can be inferred over non-symmetric and even multimodal functions. The advantages of this are summarised in table 1. VB Nonparametric Fast inference Convergence guarantees Non-symmetric quantiles Table 1: Main properties of different approaches for quantile regression. The remainder of the paper is structured as follows. We define the hierarchical Bayesian model in section 2 and show how to find the posterior using approximate Bayesian inference in section 3. In order to learn the model over kernel hyper-parameters, we present and analyse the data likelihood term in section 4. We devise the inference equations in section 5 and present experiments and comparisons in section 6. EP MCMC GP



2



Bayesian Quantile Regression



In a Bayesian setting the aim is to derive the posterior p(f |y, x , x) where f is a prediction for some input x and y, x is the set of observations. This is done by marginalising out all latent variables. We assume that the function is locally smooth which leads to Gaussian Process prior (which employs a stationary kernel) on the space of functions, and use an Inverse Gamma prior (IG(10-6 , 10-6 )) for the uncertainty estimate  (equation 4). Finally, the data likelihood is an exponentiation of the Pinball loss (equation 1) function. p(yi |fi , , , xi ) = (1 - ) i ( - I (i < 0)) exp -   (2) p(f |x) = N (m(x), K(x)) (3) p( ) = IG(10-6 , 10-6 ) (4)



1687



where, i = yi - fi 1 , I is the indicator function and K is the covariance matrix whose elements are Ki,j = k (xi , xj ) for some kernel function k (*, *) and mean function m(*) which is assumed to be zero without loss of generality. This likelihood function is an Asymmetric Laplace distribution (Kotz, Kozubowski, and Podgorski 2001). The  parameter is a dispersion measurement of the observations about the latent quantile function f . An important property of the likelihood function is that p(yi < fi ) = . Specifically, 100% of the observations are below the quantile function. Alternatively, the likelihood p(yi |fi , ) can be written as a scalar mixture of Gaussians (Kotz, Kozubowski, and Podgorski 2001; Kozumi and Kobayashi 2011) such that, p(yi |fi , xi , , ) = N (yi |yi , yi ) exp(-wi ) dw (5)



The approximate posterior on the function space is N (, ) 2 where, =  = D-1 + K-1 D-1 y -

-1



(6) 1  1 (7)



1 - 2 2



2 where, D = (12 -)  diag(w). The expectations, f =  and ff T =  + T will be required for the computation of subsequent approximate distributions. The approximate posterior on wi is a Generalised Inverse 1 , i , i ) where, Gaussian GIG( 2



i =



-2 2 2 where, yi = fi (xi ) + 1 (1-)  wi and yi = (1-)  wi . Thus the likelihood can be represented as a joint distribution with w (which will be marginalised out) where, the prior on N w is i=1 exp(-wi ). This extra latent variable w will be useful in a Variational Bayesian setting which is shown in section 3.



(1 - 2)2 +2 2(1 - ) (1 - ) 1 2 yi i = - 2yi fi + fi2 2 2

1 wi



(8) (9)

1 + are i



The expectations,



=



i i



and wi =



i i



3



Variational Bayesian Inference



The marginal likelihood p(y|x, , ) as well as the posterior on the latent variables p(f , w,  |y, , ) are not analytically tractable (where,  are the hyper-parameters and are discussed in section 4). VB aims to approximate this intractable posterior distribution with an approximate posterior q (f , w,  ). The data likelihood, log p(y|x, , ) can alternatively be expressed as: L(q (f , w,  ), |) + KL(q (f , w,  )||p(f , w,  |y, , )) where, L = ,w,,y|,) d f d w d and, KL is the q (f , w,  ) log p(f q (f ,w, ) Kullback-Leibler divergence between the proposal distribution on the latent variables and the posterior distribution of the latent variables. The Expectation Maximisation (EM) algorithm maximises the likelihood by initially minimizing the KL divergence for a given set of hyper parameters (i.e. finding an appropriate q (*)). Ideally, this is usually done by setting p(f , w,  |y) = q (f , w,  ) in which case log p(y|) = L(q (f , w,  ), ). However, in this case an analytic distribution for p(f , w,  |y) cannot be found. Instead, the approximation, q (f , w,  ) = q (f )q (w)q ( )  p(f , w,  |y) is used (Tzikas, Likas, and Galatsanos 2008). Under this assumption the closed form solution for the approximate distribution q (zi ) = exp(E (log p(z, y))/Z where, {zi } is the set of latent variables, Z is the normalising constant and the expectation, E is taken w.r.t. to approximate distributions q (z) with the exception of zi itself. In the approximate distributions that follow, * indicates the expectation with respect to all the latent variables except, the variable being investigated.

Notation: Bold lower case letters represent vectors, and subscripts indicate the i-th element. Bold upper case represent matrices.

1



used in the computation of other approximate distributions. The VB approximate posterior on q ( ) suffers from numerical problems due to calculations of the parabolic cylindrical function (Abramowitz and Stegun 1972, p. 687). Hence, we shall restrict q ( ) = IG(a, b), an Inverse Gamma distribution with parameters a, b. VB maximises the lower bound L which can be expressed as -KL(qj ||p ) - qi log qi dz where log p  = i=j log p(y, z) i=j (qi dzi ). Thus we are required to maximise, L = - (N + 1 + 10-6 ) log  -  - q ( ) log q ( ) d a b 1  - 1 2



 L =(a - N - 10-6 )(log b -  (a)) + (b -  ) -



 L a  L b where, 2 - 1- 2

(1-) 4



a(a + 1) - a log b + log (a) (10) b2   (2a + 1) =(N - a + 10-6 ) (1) (a) - - +1 b b2 (11) N a 2a(a + 1) =- + 2 + (12) b b b3 (*) is the N i=1 (yi - fi )

N i=1 1 wi 1 



gamma function, + 10-6 , 

a b, 1 2







= =



2 yi - 2yi fi + fi2



and as be-



+1) fore the expectations, = = a(a and b2 log  = log b -  (a) (where  (*) is the digamma function) are required. L is maximised using a numerical optimiser which employs the given derivatives.

2



Derivation shown in section A.



1688



4



Hyper-parameter Optimisation



The only hyper-parameters in this formulation are the kernel hyper-parameters K . In this framework the lower bound, L(q (f , w,  ), K ) is maximised. In the formulations that follow, * indicates the expectation with respect to all the latent variables, unlike what was used in the VB approximate distributions. In order to use the lower bound it is convenient to N represent p(y|f , w, , x) = i=1 p(yi |fi , wi , , xi ) from

-2 2 2 equation 5 as N y|f + 1 (1-)  w, (1-)  diag(w) , its multivariate format. Due to the symmetricity of the Normal distribution with respect to its mean we may depict this -2 2 2 distribution as, N f |y - 1 (1-)  w, (1-)  diag(w) .



This marginalisation can be approximated to p(f |f , x , y, x)q (f )q ( )q (w) df dw d . Thus we obtain a Gaussian distribution for p(f |x , y, x)  N ( ,  ) for the approximate posterior where,  = Kx  =

2 GP -1 ,x Kx,x 



(14) (15)



+



1 -1 T Kx ,x K- x,x Kx,x Kx ,x



2 1 T and, GP = Kx ,x - Kx ,x K- x,x Kx ,x . Note in equation 15 that the variance is slightly different to that of a usual GP. This follows from using the result that E (f f T ) = f f T p(f |f )q (f ) df df and V ar(f ) = E (f f T ) - E (f )E (f )T .



Hence, substituting u = f -



y-



1-2 (1-)  w



6



Experiments



, v =



-2 2 1 D-1 (y - 1 = D-1 y - 1- (1-)  w) 2  1 and ignoring terms that do not contain K we obtain the lower bound,



L= -



q (f |K )q (w)q ( ) log p(y|f , w,  )p(f |K ) ddwdf q (f |K ) log q (f |K ) df



Following the examples set out in (Quadrianto et al. 2009) two toy problems are conducted which are constructed as follows: Toy Problem 1 (Heteroscedastic Gaussian Noise): 100 samples are generated from the following process. x  U (-1, 1) and y = (x)+  (x) where  = sinc(x),  (x) = 0.1 exp(1 - x) and   N (0, 1). Toy Problem 2 (Heteroscedastic Chi-squared noise): 200 samples are generated from x  U (0, 2) and y = (x) +  (x) where  = sin(2x),  (x) = 2 (1)

2.1-x 4



1 T -1 u D u + f T K-1 f + log|K| =- 2 1 T + (f - ) -1 (f - ) + log|| 2 =- 1 T -1 f (D + K-1 )f - 2f T v 2 + 1 (log||- log|K|) 2 D-1 + K-1

-1 -1



and  



- f T -1 f + T -1 



Noting the three identities,  = D

-1 -1



=



D



-1 -1



-1



+K

T



K, 



 = v and finally



f Af = T r(A) +  A and ignoring terms without K the following expression is obtained, 1 -1 T -1  - log D-1 +K 2 1 -1 = vT v - log D-1 +K (13) 2 In this setting K and thus  are the only terms that depends on the hyper-parameters K . Equation 13 was optimised using a numerical optimiser. L=



T



5



Prediction



For a query point x , the output y that minimises equation 1 is f . Thus unlike most Bayesian formulations where the objective is to learn p(y |x , y, x) in this particular formulation the objective is to learn the latent function p(f |x , y, x). To obtain the posterior, p(f |x , y, x) we are required to marginalise out all latent variables, p(f |f , , w, x , y, x, )p(f , , w|x, ) df dw d .



- 2. Our algorithm is also tested in four real world examples. In the motorcycle dataset, acceleration experienced by a helmet in a crash is measured over time with the goal of interpolating between existing measurements. This is a popular dataset to assess heteroscedastic inference methods. In the bone density dataset, the goal is to predict the bone density of individuals as a function of age. The birth weight dataset aims to predict infants weight as a function of the mothers age and weight. Finally, the snow fall dataset, attempts to predict snow fall at Fort Collins in January, as a function of snow fall in September-December. We have used 80% of the data as training and the rest as testing and iterated over 20 times for each experiment. The cases were randomly permuted in each iteration. The proposed method is compared against its nearest competitor, the EP approach, Heteroscedastic Quantile Gaussian Processes (HQGP) as well as, against a linear method (Lin) which attempts to find the quantile as a polynomial function of the inputs (polynomial basis function, in this case having f = 0 + 1 x + 1 x2 + ... + 7 x7 ). The square exponential kernel was used in evaluating the VB, EP and HQGP methods. In the case of the real world datasets, the output is standardised to have zero mean and unit variance so that comparisons could be made across datasets. Note that this standardisation has not been applied to the toy data sets. Since the exact quantiles can be found for the toy datasets the Mean Absolute Deviation (MAD) and Root Mean Squared Error (RMSE) metrics have been used and are presented in table 2. The true quantiles for the real world datasets are not known a priori. Therefore, the average pinball loss is used as a proxy for a function that penalises incorrect quantile inference. These results are presented in ta-



1689



ble 3. Finally an empirical observed quantile error (OQE)

i (i) i=1 defined as -  is used where I is the inN dicator function and the results are shown in table 3. This metric gives an estimate as to what proportion of observations are below the inferred quantile and how far this is from the intended quantile, . This metric was provided in order to illustrate that a bias was not introduced by using the pinball loss as a metric. Different metrics were used for toy and real world problems as the true quantiles were not known for real world examples. Note that there was no code freely available for HQGP inference. Thus, the results portrayed in (Quadrianto et al. 2009) was used. 3 . The toy problem 1 was specifically designed for HQGP and therefore is not surprising that it outperforms the VB method. However, as shown in problem 2 for non-Gaussian problems the HQGP is not able to model the underlying quantiles. The HQGP inherently assumes that the quantiles lie symmetrically about the inferred mean on the dataset. This weakness is highlighted in toy problem 2. One of the strengths of using the VB framework is its ability to infer quantiles even where observations are sparse. This is evident in its ability to infer the quantiles more accurately for the extreme quantile of 0.99 in toy problem 2 as well quantiles 0.01 and 0.99 in the real world examples. This strength is also evident when inspecting the tails of the motor cycle dataset in figure 2. The variations in accelerations experienced at the start and end of the experiment are expected to be low. This detail is better captured using VB than the EP framework as is evident in the plot. The difference in the inferred quantiles could be attributed to the fact that the posterior is better approximated by exploiting the scalar mixture of Gaussians than forcefully applying a Gaussian to the posterior (which is done in the EP method). One of the biggest weaknesses of the HGQP is that it implies that the mean is the median, and that the quantiles are symmetrical about the mean (median). These two requirements are seemingly satisfied in the motor cycle dataset. However, in the bone density dataset there is a clear deviation from the symmetric assumption when inspecting figure 1. The linear method, despite giving competitive error estimates, is a parametric method. This suggests that in order to get good estimates the user must manually tune the inputs and generate features. In fact, for the Fort Collins Snow dataset, instead of having a polynomial of 7th power, a cubic polynomial provided much better results. This was due to the fact that non-sensible errors (probably due to overfitting) were observed when using a polynomial of 7th power as the basis function. N



I (y <



)



Figure 2: Comparison of the quantiles obtained with (a) Variational Bayesian and (b) Expectation Propagation approaches for the motorcycle dataset. The quantiles 0.01, 0.1, 0.5, 0.9 and 0.99 are shown. The methodology presented here can be trivially extended to parametric models by setting f = (x)w where, (x) is a suitable basis for the problem, resulting in p(f ) = N (0, (x)T (x)) instead. The computational cost of inference is O(n3 ), that of a GP. The underlying GP prior allows other GP frameworks such as those for large datasets exploiting low rank approximations and sparsity of the kernel matrices to be employed here. One of the weaknesses of our particular setting is that quantiles are not non-crossing. Future area of research would be to impose this restriction when certain quantiles are found in previous iterations of the given algorithm. It should however be noted that in the presence of enough data, this constraint seems to be self imposing as seen in figure 1b.



A



Approximate Distribution Calculations



7



Discussion and Future Work



In this work we have presented a Variational Bayesian approach to estimating quantiles exploiting the Gaussian scale mixture properties of Laplacian distributions. Results show that our method is able to outperform other frameworks.

3 Code and data are available at http://www.bitbucket.org/ sachinruk/gpquantile



This section will render the detailed calculations used in obtaining the approximate distributions in section 3. Recall that log q (zi )  log p(y|z)p(z) . In fact any term that j =i q (zj ) does not contain zi can be omitted from this expression as it will form part of the normalising constant. In order to calculate q (f ) let, u = f - 1-2 1-2 -1 y - (1-)  w , v = D (y - (1-)  w) and D =

2 2 (1-)  diag(w).



As shown in section 4,



1690



Dataset (1)



(2)



 0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99



VB 0.8080.173 0.1090.089 0.0770.057 0.0960.063 0.3640.066 1.1140.055 0.0100.003 0.1010.104 0.4000.143 1.1200.208



MAD EP 0.2330.145 0.1100.088 0.0770.057 0.0930.059 0.1990.093 0.0160.003 0.0120.004 0.1020.104 0.5110.154 1.9380.629



Lin 0.2460.054 0.1210.035 0.0920.021 0.1250.035 0.2410.068 0.0420.004 0.0350.003 0.0800.021 0.3630.109 1.0270.261



HQGP 0.062 0.031 0.056 0.099 0.509 0.804



VB 0.8830.188 0.1420.105 0.1000.069 0.1280.094 0.5140.090 1.2810.051 0.0160.008 0.1370.129 0.5260.210 1.3560.253



RMSE EP 0.3030.161 0.1460.104 0.1000.069 0.1240.087 0.2570.132 0.0180.003 0.0180.007 0.1380.128 0.6630.209 2.1640.641



Lin 0.3310.077 0.1770.074 0.1350.037 0.1840.061 0.3370.102 0.0660.011 0.0530.010 0.1150.045 0.4780.167 1.2950.303



Table 2: MAD and RMSE metric for the toy problems. (1) and (2) represents the respective toy problem.

Dataset (1)  0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99 VB 0.0250.018 0.0760.020 0.1680.031 0.0700.016 0.0150.012 0.0170.002 0.1190.010 0.3030.025 0.1530.014 0.0240.004 0.0630.039 0.2100.032 0.4040.024 0.1770.029 0.0400.018 0.0290.011 0.2140.053 0.4210.026 0.2370.041 0.0490.052 Pin-Ball EP Lin 0.0300.020 0.020 0.016 0.0820.020 0.099 0.025 0.1710.030 0.255 0.046 0.0730.014 0.115 0.061 0.0160.013 0.050 0.080 0.0250.007 0.021 0.006 0.1190.009 0.120 0.010 0.3030.025 0.304 0.025 0.1530.014 0.153 0.014 0.0380.021 0.025 0.004 0.3700.078 0.246 0.475 0.3820.060 0.319 0.274 0.4110.024 0.590 0.322 0.3690.062 0.272 0.178 0.3550.078 0.145 0.226 0.1480.106 0.1360.165 0.2350.075 0.1870.023 0.4370.020 0.4830.075 0.2790.072 0.3700.248 0.2290.136 0.2200.334 HQGP 0.0790.019 0.1870.020 0.0700.016 0.1230.017 0.3090.045 0.1530.027 VB 0.0420.042 0.0510.047 0.0780.052 0.0620.049 0.0550.049 0.0090.008 0.0310.019 0.0510.045 0.0260.024 0.0110.006 0.0570.048 0.0610.048 0.0390.043 0.0530.050 0.0490.036 0.0330.035 0.0940.099 0.0600.042 0.0860.058 0.0590.067 OQE EP 0.0660.046 0.0500.038 0.0800.054 0.0670.067 0.0550.057 0.0430.023 0.0310.018 0.0550.044 0.0250.020 0.0420.035 0.4200.085 0.3230.098 0.0330.023 0.3330.080 0.4280.078 0.2160.134 0.1160.121 0.0590.042 0.1330.115 0.2550.155 Lin 0.0440.035 0.0460.036 0.0910.042 0.0500.045 0.0720.045 0.0130.016 0.0360.022 0.0480.045 0.0330.022 0.0140.008 0.0770.050 0.0500.050 0.0600.055 0.0600.039 0.0800.036 0.0610.040 0.0410.035 0.0660.048 0.0740.076 0.0960.089



(2)



(3)



(4)



Table 3: Pin-Ball loss and Observed Quantile Error (OQE) for real world datasets. (1): Motor Cylce, (2): Bone Density, (3): Birth Weight, (4): ftCollins Snowfall. The numbers represent the average loss for the 20 iterations and the standard deviation associated with them. p(y|f , w,  ) = N f |y -

1-2 (1-)  w, D q (w)q ( )



log q (f ) = log p(y|f , w,  ) log q (f )  - 1 2 uT D-1 u D

-1



+ log p(f ) + const



For the term

(1-) 2



(1-) 2 2 2 wi ui



q (f )q ( )



ignoring the terms that

(1-2)2 2(1-) wi



+ f T K-1 f q (w)q ( ) +K

-1



do not contain wi we obtain the expression

1 2 2 yi



+



1  - fT 2



- 2yi fi + log(wi ) + 1 2



fi2



1 wi .



Thus,



f - 2v f



T



(16) log q (wi ) = -



2 1 Simplifying v such that v = D-1 y - 1- 2  1 and comparing equation 16 with the log of a normal distribution, 1 T -1 -2 (f  f - T -1 f )+ const we obtain equations 6 and 7. Similarly, in order to obtain q (wi ),



1 2



(1 - 2)2 + 2 wi + 2(1 - )

2 yi - 2yi fi + fi2



(1 - ) 2



1 wi (17)



log q (wi ) = log p(y|f , w,  )



q (f )q ( )



j =i



q (wj )



+ log p(wi ) + const log(q (wi )) = - wi - 1 1 log(wi ) - 2 2 (1 - ) 2 ui 2 2 wi



Comparing the above to the log of a GIG distribution, (p -  1) log wi - 1 2 wi + wi + const we obtain equations 8 and 9 where p = 1/2.

q (f )q ( )



1691



References

Abramowitz, M., and Stegun, I. A. 1972. Handbook of mathematical functions: with formulas, graphs, and mathematical tables. Number 55. Courier Dover Publications. Bishop, C. M., et al. 2006. Pattern recognition and machine learning, volume 1. springer New York. Boukouvalas, A.; Barillec, R.; and Cornford, D. 2012. Gaussian process quantile regression using expectation propagation. arXiv preprint arXiv:1206.6391. Chen, K., and M uller, H.-G. 2012. Conditional quantile analysis when covariates are functions, with application to growth data. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 74(1):67-89. Koenker, R., and Bassett Jr, G. 1978. Regression quantiles. Econometrica: journal of the Econometric Society 33-50. Kotz, S.; Kozubowski, T.; and Podgorski, K. 2001. The Laplace Distribution and Generalizations: A Revisit With Applications to Communications, Exonomics, Engineering, and Finance. Number 183. Springer. Kozumi, H., and Kobayashi, G. 2011. Gibbs sampling methods for bayesian quantile regression. Journal of statistical computation and simulation 81(11):1565-1578. Quadrianto, N.; Kersting, K.; Reid, M. D.; Caetano, T. S.; and Buntine, W. L. 2009. Kernel conditional quantile estimation via reduction revisited. In Data Mining, 2009. ICDM'09. Ninth IEEE International Conference on, 938- 943. IEEE. Rasmussen, C. E. 2006. Gaussian processes for machine learning. Taddy, M. A., and Kottas, A. 2010. A bayesian nonparametric approach to inference for quantile regression. Journal of Business & Economic Statistics 28(3). Takeuchi, I.; Le, Q. V.; Sears, T. D.; and Smola, A. J. 2006. Nonparametric quantile estimation. The Journal of Machine Learning Research 7:1231-1264. Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) 267-288. Tzikas, D. G.; Likas, C.; and Galatsanos, N. P. 2008. The variational approximation for bayesian inference. Signal Processing Magazine, IEEE 25(6):131-146. Yu, K., and Moyeed, R. A. 2001. Bayesian quantile regression. Statistics & Probability Letters 54(4):437-447.



1692



A real-time technique for positioning a wheelchair-mounted robotic arm for household manipulation tasks

Pooya Abolghasemi, Rouhollah Rahmatizadeh, Aman Behal, Ladislau B ol oni

Department of Electrical Engineering and Computer Science University of Central Florida {pabolghasemi, rrahmati, lboloni}@eecs.ucf.edu, abehal@ucf.edu



Abstract

Wheelchair mounted robotic arms can help people with disabilities perform their activities of daily living (ADL). The autonomy of such a system can range from full manual control (both wheelchair and robotic arm controlled by the human) to fully autonomous (with both the wheelchair and the robotic arm under autonomous control). Many ADLs require the robot to pick up an object from a cluttered environment - such as a glass of water from a table where several other objects exist. In this paper, we concentrate on the task of finding the optimal position of the base of the robotic arm (which is normally a rigid point on the wheelchair) such that the end effector can easily reach the target (regardless whether this is done through human or robot control). We introduce the ease-of-reach score ERS, a metric quantifying the preferences for the positioning of the base. As the brute force computation of ERS is computationally expensive, we propose an approach of estimating the ERS through a mixture of Gaussians. The parameters of the component Gaussians are learned offline and depend on the nature of the environment such as properties of the the obstacles. Simulation results show that the estimated ERS closely matches the actual value and the speed of estimation is fast enough for real-time operation.



Introduction

Wheelchair-mounted robotic arms, such as the popular Kinova JACO (Maheu et al. 2011) or the Exactdynamics iARM and MANUS arms promise to help disabled or elderly people in the performance of their activities of daily living (ADLs). Such activities involve reaching for everyday objects such as food or drink, personal toiletry, books, eyeglasses and so on. In their early incarnations, such systems were thought of simply as a teleoperated system with the user controlling the wheelchair and/or the arm with a joystick or a similar type of device. However, with the larger penetration of such systems, the robotic wheelchair / robotic arm assembly needs to achieve significant autonomy, bringing such systems into the purview of artificial intelligence. The desirable degree of autonomy exhibited by such a system is a complex question. The wheelchair-bound users

Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



might exhibit various degrees of motor-control or cognitive disabilities. Their disability levels can change with the progression of the disease, rehabilitation or aging. Furthermore, the users might have different preferences for automatic versus controlled behavior, and different levels of trust in the robot (Kim et al. 2012). Let us consider an ADL scenario where a disabled user, having a motorized wheelchair, with an attached Kinova JACO robotic arm aims to pick up a glass of water from the table. This task can be separated in two main components: the user must move the wheelchair to an appropriate position near the table 1 . Second, the robotic arm must be moved such that it grasps the cup and brings it to the user. Either of these subtasks can be performed under automatic or manual control (and, in fact, the user might make ad hoc decisions about what approach to use). If the user performs both tasks under manual control, in the first phase, the user will aim to get the wheelchair and robotic arm base to a position from where the cup is "easy to reach" - a concept that the users can clearly express preferences about. In the second phase, the user moves the arm to actually grab the cup. This suggests that the "ease-of-reach" metric will be useful for other scenarios as well, for instance when the wheelchair movement is automatic but the grasp manual, or vice versa. The work described in this paper focuses on the wheelchair positioning part of this task, regardless whether the actual grasp will be done under human or automatic control. In order to do this, we need to (a) quantifying the easeof-reach in a way that aligns both with human judgement and motion planning algorithms, and (b) find a way to estimate this metric at a speed suitable for real-time operation.



Related Work

The problem of grasping an unknown object in cluttered environments has attracted many AI researchers because of its inherent complexity, for instance, see (Boularias, Bagnell, and Stentz 2015). The problem of the placement of a mobile robot is important since the ability of a robot to execute a task depends on the pose of its base. One approach is

1 What really matters here is the position of the base of the robotic arm, but this must be achieved through the movement of the whole wheelchair. On the other hand, the JACO base joint freely rotates 360 , thus the orientation is not relevant.



to create the capability map of the robotic arm (Zacharias, Borst, and Hirzinger 2007). The information in a capability map describes which regions of the workspace are reachable from what directions. This map can be used to find a convenient robot placement for execution of workspace linear constrained trajectories (Dong and Trinkle 2015). Machine learning approach to generalize the experience of successful grasp through exhaustive search from different robot poses is presented in (Stulp, Fedrizzi, and Beetz 2009). (Jamone et al. 2012) introduce the concept of a reachable space map to address the problem of the robot autonomously learning during the execution of goal-directed reaching movements. (Yang, Dymond, and Jenkin 2012) analyses the reaching power of a wheelchair user based on a simple model of a person sitting in a wheelchair and an efficient motion planner. The presence of obstacles in an environment creates new challenges to inverse kinematics methods. Similarly, the capability map changes when an obstacle is close to the robotic arm and recreating it is computationally expensive. For instance, in (Stulp, Fedrizzi, and Beetz 2009) the exhaustive search should be repeated since the grasp map of the environment changes by adding an obstacle. In this paper, we present a method based on motion planning which does consider obstacles.



Figure 1: The cup placed on the table can be grasped using 17 reachable grasp poses. The blue arrow shows the direction in which the arm approaches the cup. The grasp poses from the bottom are blocked by the table.



Defining the Ease-of-Reach Score

Let us consider a scenario where a robotic arm positioned at location (x, y ) aims to grasp an object c in an environment with obstacles O = {o1 , . . . , on } that the arm must avoid. We want to define the Ease-of-Reach Score (ERS) such that it captures our intuitions about the preferences over different positions p(x, y ). The value of ERS should be 0 for positions from where the grasp is not possible, while 1 for the position from which the grasp can be done under "ideal conditions". As we want to make the ERS independent of the different human or machine motion planning algorithms, we will base our metric on the number of distinct grasps possible from a given position. For instance, if from a given position we have 10 different ways to grasp the object, it is likely that this position will be preferred both by the human and the automatically controlled operator. This position would be preferred to one where there is only one possible grasp that the operator would need to get exactly right to successfully complete the task. Let us now develop a numerical formula for the ERS. We call Count of Distinct Grasp Trajectories CDGT (p, c, O) the number of ways the arm can approach an object c to grasp it from base position p = p(x, y ) in the presence of the obstacles O = {o1 , . . . , on }. To discretize the number of grasp poses, we will consider two grasps to be distinct if the approach angle differs by at least /4. For the case of our running example, Figure 1 shows 17 distinct grasps for the cylindrical cup. As a note, obstacles lower or at best keep the CDGT the same, because they make a previously feasible grasp impossible to achieve. p on+1 CDGT (p, c, O  on+1 )  CDGT (p, c, O) (1) The ideal condition for a grasp is an environment with



Figure 2: ERS for a scenario with three obstacles. In the heatmap, blue represent low ERS, while red represents a high ERS. The wheelchair is positioned such that the robotic arm is located at the maximum ERS. no obstacles and a position from where we can choose the largest number of possible grasps. Starting from these considerations, we will define the ERS as: ERS (p, c, O) = CDGT (p, c, O) max (CDGT (p, c, ))

p



(2)



The best position for the arm is the one where the ERS is maximized: popt = arg max ERS (p, c, O)

p



(3)



Figure 2 shows the ERS for a scenario with three obstacles and the wheelchair positioned in such a way that the base of the manipulator is at the maximum ERS. Note that the optimal position might not be reachable (due to the fact that the wheelchair on which the robotic arm is mounted has its own limitations, for instance, it might collide with the table).



Estimating the ERS

The brute-force calculation of the ERS requires us to solve the motion planning problem for every grasp angle and to re-



1



0.5



0 1 0.5 0 -0.5 Y(m) -1 -1 0 -0.5 X(m) 0.5 1



Figure 3: ERS for a small object, here a cup, located in position p = p(0, 0), computed using brute-force computations. Blue: low ERS, red: high ERS. peat this for every point in a grid covering the possible locations of the robot arm. The computational effort depends on the resolution of the grid, but even a very coarse grid (eg. 20 by 20) yields 20 * 20 * 17 = 6800 motion planning problems. We use Rapidly-exploring Random Trees (RRTs) (Kuffner and LaValle 2000) to find an obstacle-free trajectory to reach a grasp pose close to the target. Even with this fast method, calculating the exact ERS before every decision is not a feasible approach for a realtime solution of the TP task. Calculating the ERS offline is feasible if there is no obstacle to consider. For instance, Fig 3 shows the ERS calculated using this method for a cup positioned at (0, 0) without any obstacles around it. As expected, the ERS has a ring shape - the reach is difficult both if the arm originates too far or too close to the object. The maximum ERS, for this setup is reached at the distance of 0.5m from the object. Note that this calculation needs to be done only once and is valid for any small object that is graspable by the robotic arm since ERS is agnostic to the shape of the grasp target. The robot can store this map, and recall it whenever it needs to perform the TP task. However, the ERS also depends on the number, location and size of the obstacles, thus the presence of obstacles leads to a combinatorial explosion of the possible maps. In our setup with n obstacles and 10 different obstacle sizes, the number of maps is (10 * 20 * 20)n , that is 1.6 * 107 for two obstacles and 6.4 * 1010 for 3 obstacles. It is thus desirable to find a way to quickly estimate the ERS without the need to compute extensive offline libraries. The approach we propose starts from the observation that the maximum ERS is obtained when no obstacles are present, while each obstacle reduces the ERS. We shall assume that the ERS-reducing effect of each obstacle can be separated into a blocking function B (p, c, o) and these blocking functions take effect independently: ERS (p, c, {o1 . . . on })

n



is not possible. Without this construct, in the case of multiple obstacles the value could dip into negative numbers as each obstacle subtracts its blocking function from the optimal ERS value. Let us now consider the shape of the blocking function B (p, c, o). The first observation is that this function will be translation invariant for the simultanous movement of the grasp origin p, the object c and the obstacle o. Second, since the grasp poses defined in ERS and the obstacles are assumed to be symmetric, the blocking function will also be rotation invariant for rotations centered on the object c. With respect to the impact of p we expect the blocking function to be highest for values of p = o, and decrease as the position of the base is farther away. Thus, a reasonable approximation can be obtained if we assume that the blocking function is a Gaussian centered on c, expressed as a function of dist(p, o). The magnitude and the standard deviation of the Gaussian, however, will depend on the distance of the obstacle to the target object dist(c, o) and the size of the obstacle size(o): B (p, c, o)  f (A, , p) = A * exp where A= = TA (dist(o, c), size(o)) T (dist(o, c), size(o)) (6) (7) - (dist(p, c))2 2 2 (5)



ERS



With these assumptions, the challenge is to determine the expressions for A and  . For any particular obstacle we can express the B (p, c, o) value from Equation 4 as follows: B (p, c, o) = ERS (p, c, ) - ERS (p, c, {o}) (8)



The B (p, c, o) value from Equation 8 can be calculated by brute force strategy (on our grid, it requires 2 * 20 * 20 * 17 = 13600 motion planning calculations). We can then use a least squares fitting method to find the A and  values for which the value from Equation 5 most closely approximates the value from Equation 8: A,  = arg min

A, p



(B (p, c, o) - f (A, , p))



2



(9)



 max 0, ERS (p, c, ) -

i=1



B (p, c, oi )



(4)



The max construct is necessary to ensure that the ERS conforms to the requirement of returning 0 when the grasp



Figure 4 shows an example of this fitting process. Although with this approach we did not need to exhaustingly consider every combination of multiple obstacles, we still need to calculate for all combinations of obstacle sizes and distances from the target. As we could not calculate all the possible combinations, we run the simulation 1300 times with random obstacle sizes between 10cm to 50cm and the distance from the target between 0.2m and 0.75m. Finally, we used locally weighted regression (Cleveland 1979) to fit a curve to data containing obstacle features to predict A and . Figure 5 illustrates the resulting values, by showing heatmaps for the evolution of the values of  (a) and A (b). Fig 5(a) shows that the value of  decreases as the obstacle is placed further from the target (because by increasing



0.5 0.45 0.4 B 0.2 0 1 0.5 0.2 0 0.5 -0.5 Y -1 -0.5 -1 X 0 0.1 0.2 0.25 0.3 0.35 0.4 0.45 0.5 distance(m) 0.55 0.6 0.65 0.7 1 0.15 size(m)

1 0.5 0 -0.5 Y -1 -0.5 -1 X 0 1 0.5



0.4 0.35 0.3 0.25



(a) The value of blocking function B (p, c, o) computed through brute force computation.



(a) The value of  in the Gaussian approximation of the blocking function. Lighter colors represent a larger value.

0.5



0.4 B 0.2 0



0.45 0.4 0.35 size(m) 0.3 0.25 0.2 0.15 0.1



(b) The Gaussian function f (A, , p) obtained by performing a least-squares fit according to Equation 9.



0.2



0.25



0.3



0.35



0.4 0.45 0.5 distance(m)



0.55



0.6



0.65



0.7



Figure 4: The blocking function of an obstacle and its Gaussian approximation for a small object (cup) located in the origin and an obstacle of the shape of cube of 21cm, located at position (0.43m, 0.0m).



(b) The value of A in the Gaussian approximation of the blocking function. Lighter colors represent a larger value.



the distance, the obstacle can affect a smaller area around it), and increases with the size of the obstacle. The height A of the Gaussian, shown in Fig 5(b) has a more complex behavior. For large obstacles, the height of the Gaussian decreases with the distance to the target. For small obstacles, however, the height increases with the distance to the target. This behavior is explained if we look at the two graphs together, as shown in the actual shape of the resulting Gaussians in Fig 5(c), which shows that the small obstacles far away from the target will have a blocking function in the shape of a tall but very narrow Gaussian, which only blocks the specific location. Note that when the obstacle is closer to the target it will affect more points in its surrounding area. In this case, the fitting algorithm tries its best to cover the whole blocking surface to minimize the error, hence, the fitted blocking function's maximum will fall down as it is interpolated with its surrounding points. In Fig 5(c) you can see the maximum A when the obstacle is located further from the target since it affects a smaller area around it. As a result, the maximum for these obstacles are shown as a thin pulse.



(c) The shape of the Gaussian approximations for various values of the distance and size of obstacles. Note that the width of the individual Gaussians are not on the same scale as the distance and size axis.



Figure 5: Parameters of the Gaussian approximation of the blocking function for A to B.



Results

Accuracy

In this section we investigate how well the proposed approach solves the positioning problem by implementing it in the V-REP simulator (Rohmer, Singh, and Freese 2013). First we need to design useful error metrics. Estimation techniques that measure the absolute error in the ERS or B functions are not particularly interesting: we are not interested in specific values of B, only in whether the result al-



Table 1: The average relevant error ARE for a collection of scenes Scene description ARE Scene 1 (3 obstacles) 0.086 Scene 2 (3 obstacles) 0.091 Scene 3 (2 obstacles) 0.085 Scene 4 (2 obstacles) 0.039 Scene 5 (4 obstacles) 0.128 Scene 6 (4 obstacles) 0.106 Scene 7 (8 obstacles) 0.206



(a) The actual ERS of the cup.



We denote the cardinality of this set of points with #P . Thus for a given target object c and set of obstacles O we define the average relevant error ARE (c, O) as follows: ERSest (p, c, O) - ERSact (p, c, O) ARE (c, O) =

p P



#P



(b) The estimated ERS of the cup.



Figure 6: An example scene with three obstacles and heatmaps corresponding to ERSact (upper) and ERSest (lower). In the heatmaps, blue corresponds to low and red to high ERS values. lows us to solve the positioning problem or not. Let us first calculate an appropriate error metric for the ERS values. The technique proposed in previous section allows us to calculate the estimated value ERSest while brute force methods allow us to calculate the actual value ERSact . Figure 6 shows an example scenario with a target object (a cup) and three obstacles of various sizes. The upper figure shows the actual ERS as a heatmap under the obstacles, while the lower figure is the heatmap of the estimated ERS. A visual inspection of the figures shows that although not perfect, the approximations are reasonably close. Let us now try to develop a useful error metric. One approach would be to calculate the average error for every grid point. However, this would be a misleading metric, because for a large number of locations ERS will be trivially zero (for instance, the ones that are outside the range of the arm). If we calculate the simple average, the error would depend on how far the grid extends from the origin. Instead, we will define the error metric as being the average only for locations where at least one of the ERSact or ERSest is greater than zero: P = {p | ERSest (p, c, O) > 0  ERSact (p, c, O) > 0} (10)



(11) The calculation of the ARE is computationally expensive as it requires the calculation of the ERSact . We performed it for a representative collection of sample scenes as described in Table 1. The table shows that in average the error stays in a moderate range, but in general increases with the number of obstacles. The robotic wheelchair needs to position itself such that the object is easily reachable - the finding of the optimal position is of a comparatively small importance. Therefore, we can divide the errors in the determination of ERS into three major types: Type 1: ERSact > 0  ERSest > 0  ERSact = ERSest The practical impact of such an error would be that that system might not choose the optimal position for reaching the target object - under normal circumstances this is a very minor issue. Type 2: ERSact > 0  ERSest = 0 In this case, the system would overlook positions from where the grasp is possible. In most cases, this is not an issue, as these positions were likely not very good anyhow. However, it can be a problem in highly constrained scenarios - for instance when many obstacles limit the number of feasible points and/or constraints on the movement of the wheelchair limit the number of points where the base can be actually positioned. In these cases, the presence of Type 2 errors might make the system mistakenly believe the problem to be unsolvable. Type 3: ERSact = 0  ERSest > 0 These are positions where the estimate believes that a grasp is possible but it turns out not to be the case. If the wheelchair would execute the positioning task TP based on this estimate, it would find that the grasp task TG is impossible from this location. Figure 7 represents the distribution of Type 2 and Type 3 errors in a sample scenario with three obstacles. The structure shows that the estimate yielded correct or acceptable



1 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.6 -0.8 -1 -1 -0.8 -0.6 -0.4 -0.2 0 X 0.2 0.4 0.6 0.8 1 Y



bination of optimal ERS and the blocking functions corresponding to the obstacles. Through the implementation of the proposed system for the case of a cylindrical cup target object and symmetrical obstacles, we have shown that the resulting approximation is sufficiently accurate for practical use and represents a four magnitude decrease in computational cost compared to the computation of the actual ERS .



References

Boularias, A.; Bagnell, J. A.; and Stentz, A. 2015. Learning to manipulate unknown objects in clutter by reinforcement. In Proceedings of AAAI Conference on Artificial Intelligence. Cleveland, W. S. 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74(368):829-836. Dong, J., and Trinkle, J. 2015. Orientation-based reachability map for robot base placement. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS). Jamone, L.; Natale, L.; Sandini, G.; and Takanishi, A. 2012. Interactive online learning of the kinematic workspace of a humanoid robot. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS), 2606- 2612. IEEE. Kim, D.-J.; Hazlett-Knudsen, R.; Culver-Godfrey, H.; Rucks, G.; Cunningham, T.; Portee, D.; Bricout, J.; Wang, Z.; and Behal, A. 2012. How autonomy impacts performance and satisfaction: Results from a study with spinal cord injured subjects using an assistive robot. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on 42(1):2-14. Kuffner, J. J., and LaValle, S. M. 2000. RRT-connect: An efficient approach to single-query path planning. In IEEE International Conference on Robotics and Automation (ICRA), volume 2, 995-1001. IEEE. Maheu, V.; Frappier, J.; Archambault, P.; and Routhier, F. 2011. Evaluation of the JACO robotic arm: Clinicoeconomic study for powered wheelchair users with upperextremity disabilities. In IEEE International Conference on Rehabilitation Robotics (ICORR), 1-5. Rohmer, E.; Singh, S. P.; and Freese, M. 2013. V-REP: A versatile and scalable robot simulation framework. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS), 1321-1326. IEEE. Stulp, F.; Fedrizzi, A.; and Beetz, M. 2009. Learning and performing place-based mobile manipulation. In International Conference on Development and Learning (ICDL), 1-7. Yang, J.; Dymond, P.; and Jenkin, M. 2012. Reaching analysis of wheelchair users using motion planning methods. In Impact Analysis of Solutions for Chronic Disease Prevention and Management. Springer. 234-237. Zacharias, F.; Borst, C.; and Hirzinger, G. 2007. Capturing robot workspace structure: representing robot capabilities. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS), 3229-3236.



Figure 7: The distribution of Type 2 and Type 3 errors in a scene with three obstacles. The sign denotes locations where both ERSact and ERSest are positive. The grid points with no sign are the ones where ERSact = ERSest = 0. Type 2 errors are denoted with while Type 3 errors with . values for the majority of positions (but for feasible and unfeasible locations). There is a single Type 2 position and a limited number of Type 3 positions, all of them located at the boundary between the feasible and unfeasible regions. In practice, the system would choose positions at the interior rather than at the boundary of the feasible zone, thus avoiding both types of errors.



Performance considerations

Let us now consider the performance speedup achieved by our technique. The following table summarizes the computational cost of various activities, on an Intel i7-4xxx series system with 16GB of RAM. Generating test data for learning (1300 sample scenes) - offline Learning - offline ERSest - proposed method - online ERSact - exhaustive search - online 5 days 2 min 5 sec 0.82 sec 26 minutes



We find that the learning of ERS estimation is computationally expensive - not so much for the learning process itself, which takes around 2 minutes, as the generation of the single obstacle ERS values that form the basis of learning. This process took about 5 days. Once the values are learned, the calculation of the ERSest takes only 0.82 seconds, in contrast to the computation of the ERSact that takes about 26 minutes in average.



Conclusions

In this paper, we considered the task of positioning a wheelchair mounted robotic arm in preparation for the grasping of a target object in the presence of obstacles. We introduced the ease-of-reach score ERS as a metric for the suitability of certain positions for the base of the robotic arm. As the calculation of the ERS map for an environment is computationally expensive, we proposed an approximation technique based on modeling the ERS as a com-



From: AAAI-87 Proceedings. Copyright (c)1987, AAAI (www.aaai.org). All rights reserved.



A Model



of Two-Player

Abramson and



Evaluation

Richard



Functions1



Bruce



E. Kor@



Abstract

We present a model of heuristic evaluation functions for two-player games. The basis of the proposal is that an estimate of the expected-outcome of a game situation, assuming random play from that point on, is an effective heuristic function. The model is supported by three distinct sets of experiments. The first set, run on small, exhaustively searched gametrees, shows that the quality of decisions made on the basis of exact values for the expected-outcome is quite good. The second set shows that in large games, estimates of the expected-outcome derived by randomly sampling terminal positions produce reasonable play. Finally, the third set shows that the model can be used to automatically learn efficient and effective evaluation functions in a game-independent manner.



of a position



for one player



or the other.



The



literature



is uniformly vague in its interpretation of game evaluation functions. One popular school of thought contends that a static evaluator should estimate a node' s actual minimax value, or the value that would be returned by searching forward from the given position all the way to the terminal nodes of the tree, labelling the leaves with their actual outcomes, and then minimaxing the leaf values back up to the original node. Under this definition, the best heuristic is the function that most accurately approximates the minimax value over all possible game positions. The difficulty with this proposal is that it provides no way of judging the quality of a heuristic, comparing or learning a heuristic function, two different evaluators, because actual minimax



I.

Heuristic functions



Introduction:

search theorists in two settings:



The Problem



values can only be computed by exhaustively searching the entire game tree below the given node. In real games, this is a computationally intractable task for all but end-game positions.



have studied static evaluation single-agent puzzles and dualdomains, the task is typically evaluation function is to Alternatively, the quality of a heuristic can be defined operationally by the quality of play that it produces. This definition allows any two heuristic functions to be compared by playing major drawbacks tire control factors, them against each other. There are two to this approach. First, it compares ennot just evaluation functions. The play can be affected by a number of (minimax is not nec-



agent games. In single-agent state. The



to find a lowest cost path from the initial state to a goal role of the heuristic definition measure estimate absolute estimator), the cost of the cheapest such path. of single-player quality of heuristic This provides offers an as an (the



a rigorous



evaluators,



(its accuracy to be compared heuristic),



strategies,



allows any two functions estimator is the better



quality of a program' s including



more accurate



and has



backup techniques



spawned a large body of results that relate evaluator accuracy to both solution of heuristic searches. quality and algorithmic complexity



essarily optimal when the values are only estimates), and lookahead depth (the relative performance of two functions may be different at different depths), as well as evaluator strength. Second, comparitive studies fail to provide an absolute measure of the quality of a heuristic function.



Unfortunately, the meaning of heuristic evaluation functions for two-player games is not as well understood. Two-player evaluators are typically described as estimates of the "worth" [Nilsson, 19801, "merit", "strength" [Pearl, 19831 19841, "quality"[W ins t on, 19771, or "promise"[Rich,

` This research was supported in part by NSF Grant IST 85-15302, an NSF Presidential Young Investigator Award, an IBM Faculty Development Award, and a grant from Delco Systems Operations. 2Department of Computer Science, Columbia University, and Computer Science Department, University of California at Los Angeles 3Computer Science Department, University of California at Los Angeles



We introduce a new model of two-player evaluators that resolves all of these difficulties. The expected-outcome model, described in section 2, provides a rigorous definition of an evaluator' s objective, an absolute standard for gauging its accuracy, and a viable method for performing a priori comparisons. Section 3 outlines a series of experiments that shows that, at least in its most basic form, the model leads to reasonable play in real games. Some conclusions and directions for future research are then given in section 4.



90



Automated Reasoning



II.



Expected-Outcome:

el



The

function in



values can be approximated by random sampling. Along with their many advantages, of course, expected values (and other statistical parameters) do bear a serious onus: they can be very misleading atively heavily complex though branching small. Thus, on expected-outcome interesting and irregular it is possible factors games when population sizes &re relplay. are too Alcare must be taken not to rely too values in end-game generate trees that



In a broad sense, the purpose of an evaluation



a two-player domain is to indicate whether a given node on the search frontier will result in a victory. The standard assumption, ing minimax estimate forwarded by proponents of approximatto an values, has been that this corresponds



Most



to be discussed



analytically.



of the outcome that would be arrived at by perfect set of assump-



to show that on trees with uniform functions disappears, when the uniformity



play. Our new model is based on a different



and depths expected-outcome



tions. We view the actual outcome of a game as a random variable and investigate what the game' s payoff would be, given random play by both sides. Although the assumption of random play seems unrealistic, it is important to recall that in a two-player game, evaluation functions are normally definition, applied only at the frontier of the search. By the frontier is the limit beyond which the pro-



make optimal



decisions,



the guaranteed optimality is lost. Since the ultimate criterion by which an evaluator is judged is its performance in actual to verify competition, we ran three sets of experiments of our assumptions and the both the rationality



strength of our model in real games. In the first set, we generated the complete game-trees of tic-tat-toe and 4by4 Othello, calculated the exact numbers of wins, losses, the exstanda' rd and draws beneath act expected-outcome every position, and compared



gram cannot gather any further data about the game tree, and in the absence of any other information, random play is the only practical common belief assumption. Furthermore, including there is a one, than that any player, a random position



function with a well-known



should find it easier to win from a "strong"



evaluator for the same game. We found that the quality of the decisions made by expected-outcome was superior to that of the standard evaluators. While these results are encouraging, they are limited to games that are small enough to be searched exhaustively. In the second set of experiments, we used the full 8-by-8 game of Othelld. Since this game is too large for exact values to be calculated, we estimated expected-outcome by averaging the values of a randomly sampled subset of the terminal positions beneath the given node. This estimated expected-outcome evaluation was pitted directly (no lookahead) against a standard evaluator, with the result that expected-outcome the standard. Unfortunately, signifi-' cantly outplayed the cost of



from a "weak" one. Thus, a technique for determining strong positions for a random player may help indicate strong positions for a perfect of its utility evaluator one, as well. is primarily In any event, empirical, not our approach stands in stark contrast to the usual one,



and the question intuitive. Any effective



designed



under our assump-



Cons should indicate the expected value of the outcome variable, or the ezpected-o&come of the given position. Definition: Expected-Outcome Values



The expected-outcome value of a game-tree node, G, is given by a player' s expected payoff over an infinite number of random completions of a game beginning at G, or



implementing the random sampler was prohibitive. In the final set, we attempted to produce an eficient estimator by performing estimates again, played a regression returned analysis on the expected-outcome to automatically the learned the learning learn Once evaluator for Othello. by the sampler,



the coefficients

leaf=1



in a polynomial



the results were positive: even though



coefficients procedure



as well as a set of coefficients



that had been de-



where a leaf' s



k is the number value, and Pleaf given random



of leaves in the subtree, is the probability play. It is important



Vl,,f



is



signed by an expert,



that it will be to note that



reached,



had no information about the game other than the rules and the values of terminal positions. Taken as a whole, this series of experiments for the expected-outcome offers strong model. empirical support



Pleaf is not necessarily equal to i. The probability that a leaf will be reached is one over the product of its ancestors' branching factors; a node with no siblings is twice as likely to be reached as a node with one sibling. Leaves are only equiprobable in trees in which all nodes of equal depth are constrained to have identical branching making all paths equally likely. Ignoring the issue of plausibility factors, thereby this



0



porting



E-vi



for a moment,



model has a number of attractive features. First, it is precise. Second, it provides an absolute measure of heuristic quality, (namely the accuracy with which it estimates the expected value), hence a means of directly comparing two heuristic functions. Finally, and most importantly, it provides a practical means of devising heuristics - expected



One of the most attractive features of expected-outcome is its domain-independence. The model' s reliance on nothing more than a game' s rules and outcomes indicates that it should be equally applicable to all two-player games. In addition to being a source of great strength, however, this generality also makes the model somewhat difficult to test thoroughly. section describes Different implementations on differThis ent games are quite likely to yield different a series of experiments results.



that demonstrate



Abramson and Kopf



91



the utility



of expected-outcome



to at least one class of



tic-tat-toe



[Nilsson,



19801, and a weighted-squares



function



games, those with finite-depth trees and outcomes drawn The requirement of finite-depth from (win, loss, draw}. trees simply means that the game will eventually terminate. Without this rule, a chess game could, at least in theory, continue indefinitely. Variants of two games that meet both requirements, lected familiar larity, for testing. to everyone; may not be. tic-tat-toe and Othello, were sepopuon an Tic-tat-toe Othello, The is a game although game that should be is played



for Othello based on the one in [Maggs, 19791. Open-linesadvantage is known to be a powerful evaluator; weightedsquares is less so. Nevertheless, entific merit. Weighted-squares its study does have sciwere the first reasonable



expert-designed Othello functions, and the more sophisticated championship-level evaluators became possible in mance large part due to the feedback provided by their perfor19821. Since the purpose of these [Rosenbloom, experiments oriented quality was not to develop program, a powerful performancea useful Othello but rather to test the decision of evaluation functions, by any well thought out gamewere rather intergoing into detail,



of growing



standard



8-by-8 board.



The playing pieces are discs which are white Each player, in turn, a sandwich conand



on one side and black on the other. Whenever



of a new model



fills a legal vacant square with a disc showing his own color. the newly placed disc completes ones, the entire opposing sisting of an unbroken straight line of hostile discs between two friendly flipped line is captured When to the color of the current mover. A move is legal neither



comparison The esting



can be provided



specific function,



albeit less-than-best.



results of these experiments Without and quite positive. feature



their most significant



was the evaluators' relative



if and only if at least one disc is captured. player can move, the winner.



the one with the most discs is declared see [Frey,



error-frequency - in tic-tat-toe, expected-outcome made roughly one-sixth as many errors as open-lines-advantage, and in Othello about one-third as many as weighted is squares. The b asic point made by these experiments that in all cases tested, expected-outcome fewer errors than the standard functions, not only made but chose the



(For a more detailed description, 19801 [Maggs, 19791 [Rosenbloom, 19821).



A.

The



Decision

fist



Quality

a model' s theoretical accu-



step in determining



racy is investigating its decision quality, or the frequency with which it recommends correct moves. In the case of expected-outcome, the question is how often the move with of win moves the largest (or smallest, as appropriate) percentage leaves beneath it is, in fact, optimal. Since optimal



optimal move with relatively high frequency. This indicates that guiding play in the direction of maximum win percentage constitutes a reasonable heuristic. Thus, the expected-outcome model has passed the first test: values generally lead to good moves. exact



are defined by complete minimax searches, (searches that extend to the leaves), their calculation is contingent upon knowledge of the entire subtree beneath them. Thus, for this first set of experiments, fairly small games had to be chosen. Moreover, in order to compare the decision quality of expected-outcome with that of a more standard function, popular games (or variations thereof) were needed. Four games that met both requirements were studied, although Othello, entirely. only two of them, 3-by-3 tic-tat-toe and 4-by-4 have game-trees that are small enough to generate and 6-by-6 Oth-



B.



Random



Sampling



Strategies



According to the the decision quality results, if complete information is available, moving in the direction of maximum win percentage is frequently beneficial. Unfortunately, these are precisely the cases in which optimal moves Since probabilistic (and for that can always be made. matter, outcome The outcome periments, heuristic) models are only interesting some method on partial is random very information sampling. definition, when knowlexpectedis needed. Expectedthe edge is incomplete, obvious of estimating



values based technique values,



The other two, 4-by-4 tic-tat-toe



by their



represent



ello, were chosen because they are small enough for large portions of their trees to be examined, yet large enough to offer more interesting testbeds than their smaller cousins. For each game studied, every node in the tree (beneath the initial configuration) was considered by four functions: a previously studThe decisions reccomplete-minimax, expected-outcome, ied standard, and worst-possible-choice.



means of leaf-value



distributions.



In the second set of exof expected-outcome in several like those



a sampler-based



estimate



was pitted against a weighted-squares function matches of (8-by-8) Othello. These experiments, which investigated tests of evaluator head. The strength -



decision quality, were designed as pure neither player used any lookathen, was to show that



aim of these tests,



ommended by these evaluators were compared with the optimal move, or the move recommended by minimax, and a record was kept of their performance. Minimax, by definition, never made an error, and worst-possible-choice erred Expected-outcome, unlike completewhenever possible. minimax, did not back up any of the values that it found at the leaves; its decisions were based strictly on evaluations of a node' s successors. Finally, the standard evaluators were taken from published literature and calculated using only static information: the open-lines-advantage for



sampler-based functions can compete favorably with those designed by experts, at least in terms of their quality of play. As far as efficiency goes, there is no comparison. The sampler was fairly cautious in its detection of convergence to a value; many samples were taken, and as a result, the sampling player frequently required as much as an hour to make a single move 4. The static function,



4Convergence was detected by first sampling N leaves and developing an estimate, then sampling an additional N and finding



92



Automated Reasoning



on the other hand, never required more than two seconds. The time invested, however, was quite worthwhile: in a 50-game match, the sampler crushed its weighted-squares opponent, 48-2. Veteran 0 the110 players may feel that the number of victories alone is insufficient to accurately gauge the relative strength of two players. Perhaps of even greater significance is the margin of victory the single most important feature in determining a player' s USOA (United States Othello Association) rating [Richards, 19811. Over the course of 50 games, the weighted-squares total of 894 discs was 1,079 shy of the 1,973 racked up by the sampler. A statistical analysis of the disc differentials indicates that the sampler should be rated roughly 200 points, or one player class, ahead of the weighted-squares player. These studies show that, efficency considerations aside, sampler-based functions can compete admirably. It is important, perspective. however, to keep the results in their proper of the world' s best OthAs a demonstration



be applicable



to learning



the relationship



between



game



features and expected-outcome values. While this reliance on predetermined game features will inevitably limit conformity backbone to the model' s ideal, scoring polynomials, game programs, are the of most competitive and if done



properly, the learned functions should combine the statistical precision and uncomplicated design of sampler-based functions with the implementation efficiency of static evaluators. The next set of experiments involved learning static expected-outcome estimators of just this sort. To find a member of the weighted-squares family that estimates the expected-outcome value, a regression procedure was used to learn coefficients for the features identified by the original, expert-designed function. Since the exact expected-outcome value is not computable in interesting games, an estimated value had to be used as the regression' s dependent variable. Thus, the value that was approximated was not the actual expected-outcome, but rather the estimate generated by the random sampler described in the previous section. The output of the regression led directly to the development of static estimators of the desired relationship sonable, form. In addition, the statistical game measures of variare reabetween that the independent the selected estimators and dependent features



ello evaluator, they are woefully inadequate - the absence of lookahead makes the games unrealistic, the difference in computation times skews the results, and the competition is not as strong as it could be. Their sole purpose was to establish estimated expected-outcome as a function at least on par with those designed by experts, and the data clearly substantiates Given the claim. no expert Expected-outcome information, functo tions, then, do appear to make useful decisions in interesting settings. the ability evaluate only leaves, and a good deal of computation time, they were able to play better than a function that had been hand-crafted by an expert. Thus the second challenge made reasonably has been met, as well: an expected-outcome cisions. C. in the absence of perfect estimator information, good de-



ables indicated This is directly championship [Rosenbloom,



albeit imprecise, analogous



of expected-outcome. that weightedlevel, but for



to the assertion factors



squares functions



can play up to a certain



play, additional 19821.



must be considered four memevaluators by



For the third, and final set of experiments, bers of the weighted-squares were studied, two of expert regression analysis. coefficients assigned ascertain tournament the decision the relative quality family of Othello



design ' and two learned



These evaluators differ only in the to each of the game features. To strength Unlike of the coefficient the functions sampling sets, a in studied



earning tions



ExpectecL0utcor-m



Fuuc-



was played.



and random



experiments,



Like most products, evaluation functions incur costs in two phases of their existence, design and implementation. The inefficiency of sampler-based functions is accrued during implementation; their design is simple and cheap, because an effective other hand, sampler need only understand leaves. Static rely on detailed game-specific the game' s rules on the freanalyses, and be able to identify quently evaluators, and/or



all four weighted-squares evaluators are efficiently calculable. This allowed the ban on lookahead to be lifted and more realistic games to be studied. The rules of the tournament were simple. Every pair of functions met in one match, which consisted of 100 games each with lookahead length fixed at 0, 1, 2, and 3. Between games, the players swapped colors. Over the course of 400 games, no evabuator was able to demonstrate substantial superiority over any other. Not only were the scores of all matches fairly close, but the disc differential statistics were, as well. An analysis of the victory margins shows that with probabil35 USOA ity .975, no two of the functions would be rated more than Since roughly 290 points (actupoints apart.



at the cost of many man-hours



machine-



hours. To help reduce these design costs, a variety of automatic tools that improve static evaluators have been developed, the simplest of which attempt to determine the relative significance of several given game features. Techniques learning [Samuel, 19631 of this sort are called parameter [Samuel, 19671 [Christensen and Korf, 19861, and should



ally, 207 [Richards, 1981]), are necessary to differentiate between player classes, the rating spread is rather insignif-



another estimate. If the discrepancy between them was within the tolerable error bounds, the estimate was accepted. Otherwise, another 2N were sampled, and so on, until convergence was detected. For the sampler used in these experiments, the original sample size was iV = 16 leaves, and the maximum needed was 1024.



5The first expert function was taken directly from [Maggs>1979], while the second, which was also used in the previous section' s random sampling experiments, modified the first to account for my personal experience.



Abramson and Korf



93



icant -



it should be clear that all four functions sent ially equivalent.



are es-



implementation ent merit,



of any new



model,



regardless



of inher-



In addition to offering a method of comparing evaluator strength, disc differentials suggest another application of expected-outcome: to the expected A fifth weighted-squares the expected-outcome tion (all outcomes entered ably stronger assign each node a value equal of the leaves beneath it. function was learned to estimate leaf distribuand was noticealthough 39 between are possible), disc-differential



to match the achievements of thirty-five years Whether expected-outcome will of progressive research. eventually replace minimax as the standard model for game design, or simply augment it by providing a degree of precision to some of its more ambiguous components, remains to be seen. What this paper has shown is that the estimation of expected-outcome functions defines a viable, evaluation funcdomain-independent further role for two-player



of this multi-valued Its performance



in the range [-64,641



into the tournament.



tions. We believe that the new model warrants the serious study that is currently in progress.



than that of the other functions, so, with victory margins



not overwhelmingly Thus,



and 145, and ratings 25 to 85 points above its competitors. the coefficients learned by the regression analby We would ysis procedure are at least as good as those designed



Acknowledgements

like to thank Othar Hansson, Andrew us with Mayer, Dana Nau, and Judea Pearl for providing helpful discussions and suggestions.



experts. Of course, it is possible to contend that a function' s strength is derived primarily from its feature set, not its coefficient set. If this is true, any two members of the same family should perform comparably, and it' s not surprising that the new functions competed favorably with the old. To dissipate any doubts that may arise along these lines, some further family members were generated. Each of the four evaluators in the initial tournament played an additional match against a weighted-squares cousin with a randomly generated set of coefficients. All four random functions were demolished - they rarely won at all, and would be rated at least a player class behind the four that had been intelligently designed. With its strong showing in the tournament, third challenge: fairly well. the expected-outcome an effeciently calculable model has met the estimator played



References

[Christensen and Korf, 19861 Jens Christensen and Richard Korf. A unified theory of heuristic evaluation functions and its application to learning. In Proceedings of the fifth National Conference on Artificial Intelligence, 1986. [Frey, 19801 Peter W. Frey. Machine Computing, :89-90, 1980. Othello. Personal in In-



[Maggs, 19791 Peter B. Maggs. Programming strategies the game of reversi. BYTE, 4:66-79, 1979. [Nilsson, 19801 Nils J. Nilsson. Tioga Publishing Principles Company, of Artificial 1980. telligence.



[Pearl, 19841 Judea Pearl. Heuristics: Strategies for Computer Problem



Intelligent Search Solving. Addison McGraw



IV.

Our proposed rethinking model the expected-outcome virtually For example,



Conclusions

of two-player model, every element definition evaluation functions, for suggests new directions



Wesley,



1984. Artificial Intelligence.



[Rich, 19831 El aine Rich. Hill, 1983. [Richards, tem.



of game programming. of a rigthe model



19811 R. Richards. Othello Quarterly, S. 19821 Paul



The revised usoa rating sys3( 1):18-23, 1981. A Artificial worldIntelRosenbloom.



in addition



to the obvious benefits for evaluators,



orous and practical



[Rosenbloom,



implies a significantly different approach to the programming of two-player games. The standard Shannon TypeA program does a full-width search to a fixed depth and then estimates the values of the nodes at that depth [Shannon, 19501. The program in the second set of experiments (random sampling) does a full-depth search but only of a subset of the nodes. In a Shannon type-A strategy, uncertainty comes from the estimates of the positions at the search horizon,` whereas in our model, uncertainty is due to sampling of the major error. Furthermore, the new model avoids one the disadvantages of all previous approaches,



championship-level



Othello program.



ligence, 19:279-320,

[Samuel, and 19631 A.L. J. Feldman,



1982.

Some studies in machine Thought, In E. Feigenbaum and



Samuel. editors,



learning using the game of checkers. McGraw-Hill, [Samuel, 19671 A.L. learning using progress. IBM 1963.



Computers



Some studies in machine Samuel. the game of checkers ii recent J. Res. Bev., 11:601-617, 1967.



need for a game-specific evaluation function based on a set of handcrafted, carefully tuned, ad hoc features. In sharp contrast to this reliance on outside expertise, the expectedoutcome model requires only well-defined leaf values, the rules of the game, and a game-independent egy. It is, of course, unreasonable to expect the initial sampling strat-



Programming a [Shannon, 19501 Claude E. Shannon. computer for playing chess. 1Philosoyh4cal Magazine, 41:256-275, [Winston, dison Wesley, 1950. Artificial Intelligence. Ad1977. 19771 P.H. Winston.



94



Automated Reasoning



Deep Belief Nets as Function Approximators for Reinforcement Learning

Farnaz Abtahi and Ian Fasel

Department of Computer Science School of Information: Science, Technology, and Arts The University of Arizona Tucson, AZ 85721-0077 Emails: {farnaza,ianfasel}@cs.arizona.edu



I. I NTRODUCTION Real-world tasks often require learning methods to deal with continuous state/action spaces. In these applications, function approximation is useful for building a compact representation of the value function. One popular framework for implementing such function approximation is Neural Fitted Q-Iteration (NFQ) [1]. However NFQ is based solely on the value returns, without making use of explicit structural information from the state space. We have extended the idea of NFQ and proposed a new reinforcement learning approach in which a Deep Belief Network (DBN) [2] is first trained generatively to model the stateaction space with a hierarchy of latent binary variables, and the parameters of this model are then used to initialize a neural network value function approximator trained using NFQ. The unsupervised pre-training phase in DBNs initializes the parameters of the network in a region of the parameter space that is more likely to contain good solutions, given the available data [3]. On the other hand, gathering data in a Reinforcement Learning (RL) scenario will often result in imbalanced data. This implies that in order to take advantage of the pre-training in RL, we need to adjust the data to cover interesting regions of the state space, while avoiding bias towards regions that are densely covered by the training set. Experiments confirm that when the initial data is wisely collected and also under-sampled to have a smoother distribution, our approach will significantly increase the learning efficiency. II. C OMBINING DBN S AND RL Our proposed approach is displayed in Algorithm 1. The two major steps of the algorithm are: 1) Pre-train the DBN on the initial training set. 2) Generate new data using current estimate of the Q-function; append this data to the training set; train the DBN on the training set to get a new estimate of the Q-function; update target values of the training set based on the Q-function; repeat this step until the termination condition is satisfied. Algorithm 1 DeepRL

Input: a set of transition samples D , a binary flag pretrain; Output: Q-value function QN k0 if pretrain = true then Q0  pretrain DBN(D ) else Q0  rand init DBN end if repeat generate pattern set P = {(inputi , targeti )} where: inputi  (si , ai ), targeti  c(si , ai , s i ) + mina Qk (s i , a ) D  append(D, P ) Qk+1  train DBN(D ) k k+1 for all (inputj , targetj ) in D do targetj  c(sj , aj , s j ) + mina Qk (s j , a ) end for until K = N or Qk  Qk-1



III. E XPERIMENTS To show the advantages of pre-training in RL problems, three experiments were performed: 1) We applied DeepRL algorithm to Mountain Car and Puddle World problems. The initial data is collected in two ways:



With hint-to-goal heuristic (Several datapoints inside the goal region were manually added to the training set). Without hint-to-goal heuristic. The learning process consists of 500 episodes for Mountain Car and 150 episodes for Puddle World. Each episode begins with generating a 50-step trajectory, using the current estimate of the Q-function. The performance is tested after every 10 episodes of learning in Mountain Car and after every 5 episodes in Puddle World, on 1000 random starting points. Fig. 1 (left column) shows that with hint-to-goal, pre-training helps since it defines a bias towards the goal area in the state space. But in case of absence of the hint-to-goal, pre-training actually hurts the performance, because the trajectory generated by the random policy has biased the parameters towards some undesirable areas of the state space that were covered by the pre-training set. 2) We repeated the case where pre-training is done on a random walk, and compare it with another case where the pretraining set is a 50-step trace from a good policy (learned in a previous experiment). In Mountain Car, a random walk mostly ends up in the valley, however a good policy escapes the valley more easily and gets to other regions of the space, giving good coverage of the state space. Conversely, a random policy in Puddle World can cover almost the entire state space, while a good policy typically avoids and misses the puddle area, so that much of the state space remains unsampled. This explains the opposite effects in Fig 1 (middle column), in which pretraining with a good policy helps in Mountain Car but hurts in Puddle World. 3) Finally, in order to solve the problem of imbalanced data, we under-sampled the training set to remove redundant datapoints before using it in pre-training. Fig 1 (right column) shows that this method significantly improves the learning performance in Mountain Car.

* *

D2,<;:5<1E:0 '" '! "" 1



60 55



#!! *! )!



=,77>/1?20>7 Mountain Car



70 60 65 55



1



90 80



Puddle World Mountain Car

70 90



Puddle World



80 65



Success rate in 1000 tests



Success rate in 1000 tests



"! &" &! %" %! $" $! 1 ! +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<;!;2!@2:> ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> ?5;C140/;0:5<5<@A1<21C5<;!;2!@2:> #!! $!! %!! +,-./01231/45627/6 &!! "!!



(! '! "! &! %! $!



45 40 35 30 25 20



60 50 40 30 20 10 0 0 100 100

Pretrained on a set of 10 good trajectories Pretrained on a good trajectory Pretrained on aan good trajectory Pretrained on undersampled Pretrained on walk a random walk Pretrained on a random set of 10 good trajectories



Success tests Successrate rate in in 1000 1000 tests



8,99/6610:;/15<1#!!!1;/6;6



8,99/6610:;/15<1#!!!1;/6;6



Success rate in 1000 tests



50



60 50 55 45 50 40 45 35



Success rate in 1000 tests



70



70 60 60 50

50 55



40

45



30 20 10

40 35 30 0 Pretrained on a set of a random walk and 10 good trajectories Pretrained on an undersampled



#!



0



!1 !



100



40 30 +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<; !;2!@2:> Pretrained on a good trajectory 35 ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> 25 Pretrained on a random walk ?5;C140/;0:5<5<@A1<21C5<; !;2!@2:> 30 20 "! #!! 200 300 400 500 #"! 0 0 +,-./01231/45627/6 Number of episodes



Pretrained on a good trajectory set of a randomwalk and 10 Pretrained on a random walk good trajectories

100 300 100 50 200 Number of Number of episodes episodes Puddle World 400 500 150



D2,<;:5<1E:0 '" '! 1



Mountain Car =,77>/1?20>7 60 55

#!! *! )!



Puddle World

70 1 65



50 300 100 200 300 400 200 400 Number of episodes Numberof of episodes Number episodes Mountain Car



500 500



150



0 0



90 80

Success rate Success rate in in1000 1000tests tests



60 70 55 65 50 60 45 55 40 50 35 45 30 40 25 35 20 30 00 100 100

Pretrained on a set of a random walk and 10 good trajectories Pretrainedon ona an undersampled Pretrained good trajectory set of a randomwalk and 10 Pretrained on a random walk good trajectories



90 80



Success rate in 1000 tests



"! &" &! %" %! $" $! 1 ! +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<;!;2!@2:> ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> ?5;C140/;0:5<5<@A1<21C5<;!;2!@2:> #!! $!! %!! +,-./01231/45627/6 &!! "!!



(! '! "! &! %! $!



Success rate in 1000 tests



8,99/6610:;/15<1#!!!1;/6;6



8,99/6610:;/15<1#!!!1;/6;6



50 45 40 35 30 25 20



60 55 50 45



Success rate in 1000 tests



Success rate in 1000 tests



""



70 60 50 40 30 20 10

100 0 Pretrained on a set of 10 good trajectories Pretrained on a good trajectory Pretrained on an undersampled Pretrained on a random walk set of 10 good trajectories 200 400 50 300 100 Number of Number episodesof episodes 500



70 60 50 40 30 20 10 0 0 Pretrained on a good trajectory Pretrained on a random walk 50 100 Number of episodes 150



#!



0



!1 !



100



40 +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<;!;2!@2:> Pretrained on a good trajectory 35 ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> Pretrained on a random walk ?5;C140/;0:5<5<@A1<21C5<;!;2!@2:> 30 "! #!! 0 200 300 400 500 #"! +,-./01231/45627/6 Number of episodes



0



150



200 300 200 300 Number of episodes episodes Number of



400



500 500



Fig. 1. Left column: Performance during learning with/without pre-training and with/without the hint-to-goal heuristic in Mountain Car (top) and Puddle World (bottom). Middle Column: Performance during learning when the pre-training data is 1) a random walk, and 2) a good trajectory generated by a successful policy, in Mountain Car (top) and Puddle World (bottom). Right column: The effect of under-sampling the pre-training data in Mountain Car. All curves are averaged over 40 trials



IV. C ONCLUSION Our experiments indicate that the unsupervised pre-training in DBNs can be very helpful in pulling the parameters toward interesting solutions in continuous state/action reinforcement learning problems, if the pre-training data covers the desirable areas of the state space. To overcome the problem of imbalanced data in reinforcement learning problems, we added datapoints from important areas of the state space to the training set and under-sampled this set to make the data distribution smoother. These adjustments considerably improved the performance. ACKNOWLEDGMENTS This research was supported by ONR "Science of Autonomy" contract N00014-09-1-065 and DARPA contract N10AP20008. The authors would also like to thank Tom Walsh for his contributions to this work. R EFERENCES

[1] M. Riedmiller, Neural fitted Q-iteration - first experiences with a data effcient neural reinforcement learning method, In Proceedings of ECML 2005, 317-328. Porto, Portugal, 2005. [2] G. E. Hinton, S. Osindero, and Y. Teh, A fast learning algorithm for deep belief nets, Neural Computation, 18: 1527-1554, 2006. [3] D. Erhan, Y. Bengio, A. Courville, P. Manzagol, P. Vincent, and S. Bengio, Why does unsupervised pre-training help deep learning?, In Proceedings of AISTATS 2010, 201-208. Chia Laguna, Sardinia, Italy, 2010.



From: AAAI-99 Proceedings. Copyright (c) 1999, AAAI (www.aaai.org). All rights reserved.



Sensor



Based



Coverage



of



Unknown Environments Detection



for



Land Mine



Ercan Acar, Morgan Simmons, Michael Rosenblatt, MaayanRoth, Mary Berna, Yonatan Mittlefehldt, Howie Choset

Carnegie Mellon University Pittsburgh, PA15213 eua@andrew.cmu.edu Abstract This paper introduces a sensor based coverage algorithm and an overviewof a mobile robot system for demining. The algorithm is formulated in terms of critical points whichare the points where the topology of an environment changes. Wedevelopeda provably completecoverage algorithm whichmakesa robot pass over all possible points of an unknown environment.



Overview of The Coverage Algorithm

Conventional path planning determines a path between two points. This type of planning is suitable for guidance, pick and place operations etc.. Applications such as vacuumcleaning, floor scrubbing, area surveying, demining (Land & Choset 1998) and harvesting (Ollis & Stentz 1996) require more than point to point planning. They require a coverage algorithm which determines a path that passes the robot over all possible points in an environment. In many scenarios, the robot may not know its environment a priori, and thus a sensor based coverage algorithm is necessary. Sensor based coverage determines a path for a robot such that it passes over all possible points in an unknown environment. Completeness of such a coverage algorithm is of utmost importance. As an example, all possible points of a minefield should be covered to guarantee not to miss a single mine. Different types of coverage algorithms were developed by several researchers. Some of the algorithms are grid based (Zelinsky et al. 1993), (Pirzadeh Snyder 1990) and some of them are cellular decomposition based (Cao, Huang, & Hall 1988), (Vladimir J. Lumelsky & Sun 1990), (Hert, Tiwari, & Lumelsky 1996). Behavior based algorithms for coverage are also considered (MacKenzie & Balch 1996). However all these algorithms either work only in certain types of environments, make unrealistic assumptions about the sensors, or completeness of the algorithm is not shown. We developed a provably complete coverage algorithm and implemented it on a mobile platform. Cells Figure 1: Cellular Decomposition



Our method is based on a geometric structure called cellular decomposition (Latombe 1991), which is the union of non-overlapping subregions of the free space, called cells. An adjacency graph encodes the topology of the cells in the environment where nodes are cells and edges connect nodes of adjacent cells (Fig. 1). Since simple back and forth motions cover each cell, complete coverage is reduced to finding an exhaustive walk through the adjacency graph (Choset & Pignon 1997). The cellular decomposition defines its cells in terms of critical points. If the robot knowsthe critical points, then it effectively knows the decomposition. When the environment is not known, neither are the critical points. Therefore sensor based coverage is covering the environment while determining the locations of critical points. Wedeveloped methods to sense critical points in unknownenvironments. The notion of critical point sensing was first introduced in (Rimon & Canny 1994) and it was called the critical point sensor. Generically each cell is characterized by two critical points. Instead of forming an adjacency graph with nodes as cells, we form a dual graph where nodes are critical points and edges are the cells. Each time the robot encounters a new critical point, a new node is



Figure 2: Dual adjacency graph representation of the environment. Nodesrepresent critical points, branches represent cells. created, the edge corresponding to the current cell is terminated at the new node and depending on the type of the critical point two more edges are instantiated or no edge is created. If the robot encounters an already discovered critical point, then the edge corresponding to the current cell is terminated at the critical point and the "dangling" edge (i.e. it only has one node) of the already discovered critical point is deleted. When all the nodes have edges ending with another node, coverage is completed. An essential part of the complete coverage is developing an algorithm which guarantees to see all the critical points. Such an algorithm was developed and its completeness was proved.



and houses an array of four metal detecting sensors. The vehicle's on board computer is currently a HandyBoard robot controller which is powered by Motorola 68HCll micro-processor. The Motorola 68HCll can control external devices, read input information, and communicate with a personal computer. A Pentium based computer can be easily added to the system whenever the processing power of the 68HCll becomes insufficient. The vehicle is equipped with numerous sensing systems. Attached to the drive train are shaft encoders that monitor the displacement and velocity of the wheel. Also on board is a digital compass which uses coils (no moving parts) to detect the earth's magnetic field and returns a compass heading in degrees. The shaft encoders and digital compass are used to enable the vehicle to position and direct itself through the environment. The metal detection sensors cover the entire front of the vehicle and are used to detect simulated land mines.



References

Cao, Z. L.; Huang, Y.; and Halt, E. 1988. Region filling operations with randomobstacle avoidance for mobile robots. Journal of Robotic systems 87-102. Choset, H., and Pignon, P. 1997. Coveragepath planning: The boustrophedon decomposition. In Proceedings of the International Conferenceon Field and Service Robotics. Herr, S.; Tiwari, S.; and Lumelsky,V. 1996. A TerrainCovering Algorithm for an AUV.Autonomous Robots 3:91-119. Land, S., and Choset, H. 1998. Coverage path planning for landmine location. In Third International Symposium on Technology and the Mine Problem. Latombe, J. 1991. Robot Motion Planning. Boston, MA: Kluwer AcademicPublishers. MacKenzie, D., and Balch, T. 1996. Making a Clean Sweep: Bahavior Based Vacuuming. In AAAI Fall Symposium, Instationating Real-WorldAgents. Ollis, M., and Stentz, A. 1996. First Results in VisionBased Crop Line Tracking. In IEEE International Conference on Robotics and Automation. Pirzadeh, A., and Snyder, W.1990. A unified solution to coverage and search in explored and unexplored terrains using indirect control. In Proc. of IEEEInt'l. Conference on Robotics and Automation, 2113-2119. Rimon,E., and Canny, J. 1994. Construction of C-space RoadmapsUsing Local Sensory Data -- What Should the Sensors LookFor? In Proc. IEEE Int. Conf. on Robotics and Automation, 117-124. Vladimir J. Lumelsky, S. M., and Sun, K. 1990. Dynamic path planning in sensor-based terrain acquisition. IEEE Transactions on Robotics and Automation6(4):462-472. Zelinsky, A.; Jarvis, R.; Byrne, J.; and Yuta, S. 1993. Planning Paths of CompleteCoverage of an Unstructured Environmentby a MobileRobot. In Proceedings of International Conference on AdvancedRobotics, pp533-538.



Overview of the Demining Robot

It is estimated that there are over 120 million active land mines in the world which cause the deaths of over 25,000 people each year. Manyof these casualties are civilians, manyof which are children. Current removal methods involve trained technicians searching with hand held electronic instruments (often metal detectors), while working on their hands and knees. Not only is this dangerous, but it is also very difficult for a person to reliably cover an entire area. Autonomous robotic coverage provides a solution which helps to removepeople from this dangerous occupation, as well as to enable more reliable and efficient coverage strategies. The testing vehicle that we are using in the implementation of our project is an original design which provides ruggedness and flexibility for an outdoor environment. The welded aluminum structure has a payload space of 13 x 8 x 20 inches, and is impact resistant in all directions. Four ten-inch pneumatic wheels are driven in a differential drive configuration by twin variable-speed electric motors (one for each side, front, and rear wheels will be chained together). A removable fiberglass cradle is attached to the front of the vehicle



Q U O NTO: Q Uerying O NTOlogies

Andrea Acciarri2 , Diego Calvanese1 , Giuseppe De Giacomo2 , Domenico Lembo2 , Maurizio Lenzerini2 , Mattia Palmieri2 , Riccardo Rosati2

1 Faculty of Computer Science Free University of Bolzano/Bozen Piazza Domenicani 3 I-39100 Bolzano, Italy calvanese@inf.unibz.it 2



Dipartimento di Informatica e Sistemistica Universit a di Roma "La Sapienza" Via Salaria 113 I-00198 Roma, Italy lastname@dis.uniroma1.it



Introduction

One of the most important lines of research in Description Logics (DLs) is concerned with the trade-off between expressive power and computational complexity of sound and complete reasoning. Research carried out in the past on this topic has shown that many DLs with efficient, i.e., worstcase polynomial time, reasoning algorithms lack the modeling power required for capturing conceptual models and basic ontology languages, while most DLs with sufficient modeling power suffer from inherently worst-case exponential time behavior of reasoning [1, 2]. Although the requirement of polynomially tractable reasoning might be less stringent when dealing with relatively small ontologies, we believe that the need of efficient reasoning algorithms is of paramount importance when the ontology system is to manage large amount of objects (e.g., from thousands to millions of instances). This is the case of several important applications where the use of ontologies is advocated nowadays. For example, in the Semantic Web, ontologies are often used to describe the relevant concepts of Web repositories, and such repositories may incorporate very large data sets, which constitute the instances of the concepts in the ontology. In such cases, two requirements emerge that are typically overlooked in DLs. First, the number of objects in the knowledge bases requires managing instances of concepts (i.e., ABoxes) in secondary storage. Second, significant queries to be posed to the knowledge base are more complex than the simple queries (i.e., concepts and roles) usually considered in DL research. Unfortunately, in these contexts, whenever the complexity of reasoning is exponential in the size of the instances (as for example in Fact1 , Racer2 and in [3]), there is little hope for effective instance management and query answering algorithms. In [4] a new DL, called DL-Lite, was proposed specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. A DL-Lite knowledge base (KB) is constituted by two components: an intensional level (called TBox in DL jargon), used to model the concepts and the relations (roles) of the ontologies, and an extenCopyright c 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. 1 www.cs.man.ac.uk/horrocks/FaCT 2 www.sts.tu-harburg.de/r.f.moeller/racer



sional level (ABox), used to represent instances of concepts and roles. Reasoning in DL-Lite means not only computing subsumption between concepts, and checking satisfiability of the whole knowledge base, but also answering complex queries. Notably, the complexity of answering queries posed to a knowledge base is polynomial in the size of the ABox. In this work, we present Q U O NTO, a query answering system based on DL-Lite. Our system provides three basic functionalities: (1) specification of the intensional level of the ontology (TBox), (2) specification of the extensional level of the ontology (ABox), and (3) query answering. In the following, we describe the main characteristics of the system with respect to these three aspects.



TBox Specification

As usual in DLs, DL-Lite allows for representing the domain of interest in terms of concepts, denoting sets of objects, and roles, denoting binary relations between objects. DL-Lite concepts are defined as follows: B C ::= A | R | R- ::= B | B | C1 C2



where A denotes an atomic concept and R denotes an (atomic) role; B denotes a basic concept that can be either an atomic concept, a concept of the form R, i.e., the standard DL construct of unqualified existential quantification on roles, or a concept of the form R- , which involves an inverse role. C (possibly with subscript) denotes a (general) concept. Note that we use negation of basic concepts only, and we do not allow for disjunction. In Q U O NTO, the intensional level of the knowledge base is simply a DL-Lite TBox, i.e., a set of assertions of the form B C inclusion assertions (funct R), (funct R- ) functionality assertions An inclusion assertion expresses that a basic concept is subsumed by a general concept, while a functionality assertion expresses the (global) functionality of a role, or of the inverse of a role. Despite the simplicity of its language, DL-Lite is able to capture the main notions (though not all, obviously) of conceptual modeling formalism used in databases and software engineering such as ER and UML class diagrams. In particular, DL-Lite assertions allow us to specify ISA and disjointness between concepts, role-typing, participation and



non-participation constraints between a concept and a role, and functionality restrictions on roles.



ABox Specification

In Q U O NTO, the extensional level of the knowledge base is simply a DL-Lite ABox, i.e., a set of assertions of the form A(c), R(c, b), membership assertions where c and b are constants. These assertions state respectively that the object denoted by c is an instance of the atomic concept A, and that the pair of objects denoted by (c, b) is an instance of the role R. One of the distinguishing feature of Q U O NTO is that the ABox is stored under the control of a DBMS, in order to effectively manage objects in the knowledge base by means of an SQL engine. To this aim, Q U O NTO constructs a relational database which faithfully represents an ABox A: for each atomic concept A, a relational table tab A of arity 1 is defined, such that c  tab A iff A(c)  A, and for each role R, a relational table tab R of arity 2 is defined, such that c, b  tab R iff R(c, b)  A. We denote with DB(A) the relational database thus constructed.



Query answering

Perhaps, the main feature of our system is the ability to answer conjunctive queries posed to an ontology. While virtually all DL-based systems allow for answering atomic queries only (i.e., queries constituted by concepts or roles), Q U O NTO is able to answer conjunctive queries over a DL knowledge base. A conjunctive query (CQ) q over a knowledge base K is an expression of the form q (x)  y.conj (x, y ) where x are the so-called distinguished variables, y are existentially quantified variables called the non-distinguished variables, and conj (x, y ) is a conjunction of atoms of the form A(z ), or R(z1 , z2 ), where A and R are respectively an atomic concept and a role in K, and z , z1 , z2 are onstants in K or variables in x or y . A conjunctive query q (x)  y.conj (x, y ) is interpreted in an interpretation I for K as the set q I of tuples c such that when we substitute the variables x with the constants c, the formula y.conj (x, y ) evaluates to true in I . Answering a conjunctive query q posed to a knowledge base K means computing the set of tuples c of constants of K such that in every model I of K we have c  q I . Answering conjunctive queries over a knowledge base is a challenging problem, even in the case of DL-Lite, where the combination of allowable constructs does not pose particular difficulties in computing subsumption. Notice that, in spite of the simplicity of DL-Lite TBoxes, the ability of taking TBox knowledge into account during the process of answering conjunctive queries goes beyond the "variablefree" fragments of first-order logic represented by DLs. In order to take advantage of the fact that the ABox is managed in secondary storage by a Data Base Management System (DBMS), our query answering algorithm is based on the idea of reformulating the original query into a set of



queries that can be directly evaluated by an SQL engine over the ABox. Note that this allow us to take advantage of well established query optimization strategies. Query reformulation is therefore at the heart of our query answering method. Given the limited expressive power of DL-Lite TBoxes, it might seem that in order to answer a query q over a KB K constituted by a TBox T and an ABox A, we could simply build a finite first-order structure on the basis of K, and then evaluate the query as an expression over this first-order structure. Actually, it is possible to show that this is not the case. In particular, it can be shown that, in general, given a KB K, there exists no finite structure S such that, for every conjunctive query q , the set of answers to q over K is the result of evaluating q over S . This property demonstrates that answering queries in DL-Lite goes beyond both propositional logic and relational databases. The basic idea of our method is to reformulate the query taking into account the TBox: in particular, given a query q over K, we compile the assertions of the TBox into the query itself, thus obtaining a new query q . Such a new query q is then evaluated over the ABox of K, as if the ABox were a simple relational database. Since the size of q does not depend on the ABox, the data complexity of the whole query answering algorithm is polynomial. Finally, we observe that query answering can be used in Q U O NTO for other forms of reasoning on the knowledge base K. For example, to check whether K is unsatisfiable, we can simply add the assertion A(c) to the Abox (where c is new constant), the inclusion A D to the TBox, and check whether c is in the answer to the query q (x)  D(x). Similarly, to check whether K |= A C , we can simply add the assertion A(c) to the Abox (where c is new constant), and check whether c is in the answer to the query q (x)  C (x), where C is the conjunctive query corresponding to the concept C .



Conclusions

Our experiments on Q U O NTO are extremely encouraging. The system is able to efficiently answer complex conjunctive queries (actually, unions of conjunctive queries) over ABoxes constituted by hundreds of thousands of instances of the concepts in the TBox. To the best of our knowledge, this is the first system exhibiting the ability to effectively answer complex queries over ontologies.



References

[1] F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P. F. Patel-Schneider, editors. The Description Logic Handbook: Theory, Implementation and Applications. Cambridge University Press, 2003. [2] A. Borgida and R. J. Brachman. Conceptual modeling with description logics. In Baader et al. [1], chapter 10, pages 349- 372. [3] D. Calvanese, G. De Giacomo, and M. Lenzerini. Answering queries using views over description logics knowledge bases. In Proc. of AAAI 2000, pages 386-391, 2000. [4] D. Calvanese, G. De Giacomo, M. Lenzerini, R. Rosati, and G. Vetere. DL-Lite: Practical reasoning for rich DLs. In Proc. of DL 2004. CEUR Electronic Workshop Proceedings, http: //ceur-ws.org/Vol-104/, 2004.



Generating Satisfiable Problem Instances

Dimitris Achlioptas

Microsoft Research Redmond, WA 98052 optas@microsoft.com



Carla Gomes

Dept. of Comp. Sci. Cornell Univ. Ithaca, NY 14853 gomes@cs.cornell.edu



Henry Kautz

AT&T Research Florham Park, NJ kautz@research.att.com



Bart Selman

Dept. of Comp. Sci. Cornell Univ. Ithaca, NY 14853 selman@cs.cornell.edu



Abstract

A major difficulty in evaluating incomplete local search style algorithms for constraint satisfaction problems is the need for a source of hard problem instances that are guaranteed to be satisfiable. A standard approach to evaluate incomplete search methods has been to use a general problem generator and a complete search method to filter out the unsatisfiable instances. Unfortunately, this approach cannot be used to create problem instances that are beyond the reach of complete search methods. So far, it has proven to be surprisingly difficult to develop a direct generator for satisfiable instances only. In this paper, we propose a generator that only outputs satisfiable problem instances. We also show how one can finely control the hardness of the satisfiable instances by establishing a connection between problem hardness and a new kind of phase transition phenomenon in the space of problem instances. Finally, we use our problem distribution to show the easy-hard-easy pattern in search complexity for local search procedures, analogous to the previously reported pattern for complete search methods.



Introduction

In recent years, we have seen the rapid development of both complete and incomplete search methods for constraint satisfaction (CSP) and Boolean satisfiability (SAT) problems. These methods are now applied successfully in a range of applications within artificial intelligence and computer science in general. An important factor in the development of new search methods is the availability of good sets of benchmark problems to evaluate and fine-tune the algorithms. There are two main sources of benchmark problems. One class of benchmarks is based on real-world applications and the other is from random instance generators. Real-world instances are arguably the best source, but unfortunately are often in short supply. Moreover, there is a risk that algorithms are being tuned towards specific application domains for which good benchmarks are available. Random problem generators therefore provide a good additional source of problem instances. These generators also have the advantage of a more direct control over the problem characteristics, such as size and expected hardness. Hard random instances have led to the development of new stochastic search

Copyright c 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.



methods such as Walksat (Selman et al. 1996) and the breakout procedure (Morris 1993), and have been used in detailed comparisons of local search methods for graph coloring and related graph problems (Johnson et al. 1989). The results of various competitions for CSP and SAT algorithms show that there is a fairly direct correlation between the performance on real-world benchmarks and on hard random instances (DIMACS 1993, 1996; Beijing, 1996; Johnson et al. 1989). It is important to note that randomly generated problem instances are not necessarily unstructured. Structure may be introduced by translating random problems from one domain into another, or by considering problem domains that by definition exhibit regular structure (Gomes and Selman 1997, Walsh 1999). Current problem generators are based on recent developments in our understanding of the nature of computationally hard problem instances. In particular, a clear connection has been established between so-called phase transition phenomena and the computational hardness of NPcomplete problems (Cheeseman et al. 1991, Mitchell et al. 1992, Hogg et al. 1996). Phase transition phenomena capture the surprisingly sharp transitions from the solvable to the unsolvable in the space of problem instances, as a function of certain problem parameters such as the ratio of the number of constraints to the number of variables. In random distributed problem instances, at low ratios (relatively few constraints) one encounters mostly satisfiable instances, while at high ratios most instances are unsatisfiable. In terms of complexity, one observes a easy-hard-easy pattern, where assignments are easily found in the sat-phase, while inconsistency is easily shown in the unsat-phase. At the phase transition, where roughly half the instances are satisfiable and half the instances are unsatisfiable, one finds a concentration of computationally hard problem instances. The ability to varying the hardness of the problem instances makes it possible to study precisely how different search algorithms scale in terms of problem difficulty. A key limitation of current problem generators concerns their use in the evaluation of incomplete local search methods. This is because the generators generally produce a mixture of solvable (satisfiable) and unsolvable (unsatisfiable) instances. When a local search style method does not find a solution, it can be difficult to determine whether this is because the algorithm fails to find a solution or because the instance itself is unsolvable. The standard way of dealing



with this problem is to use a complete search method to filter out the unsatisfiable cases. However, this limits the size and difficulty of problems instances that can be considered. Ideally, one would use problem generators that generate satisfiable instances only. However, developing such generators has been surprisingly difficult. As an example, let us consider generating hard satisfiable 3CNF formulas. In order to obtain satisfiable instances only, it is natural to use a strategy where one creates formulas in the phase transition region (ratio of clauses to variables of around 4.25) that are "forced" to have at least one satisfying assignment. To do so, consider the following strategy: generate a random truth assignment T , and then generate a formula with N variables and 4.25N random clauses, where one rejects any clause that violates T . This method will in principle generate all possible satisfiable formulas with a clause-to-variable ratio of 4.25 that have T among their solution. What is somewhat surprising however is that the sampling of these formulas is far from uniform: the generator is highly biased towards formulas with many assignments, clustered around T . When fed to local search methods such as Walksat, these formulas are much easier than formulas of comparable size obtained by filtering satisfiable instances from a 3SAT generator. More sophisticated versions of the forced-formula scheme (Asahiro et al. 1993, Van Gelder 1993) provide improvements but also lead to biased samples. There are also a number of theoretical results that show that is is difficult to "hide" a combinatorial object in a larger combinatorial structure. For example, it can be shown that one can easily find cliques over a certain size that are hidden in a random graph, and similar results are known for hiding Hamiltonian cycles (Frieze and McDiarmid 1997). The problem of hiding information in larger combinatorial structures is of interest to the computer science theory community since successful techniques for doing so may eventually lead to more effective cryptographic methods. Cryptographic problems do suggest one way of creating hard satisfiable problem instances (Impagliazzo et al. 1989). For example, Crawford and Kearns (1993) created SAT encodings of the "noisy" parity problem. The instances are guaranteed to have a satisfying assignment but are extremely hard to solve using current SAT procedures. In recent work Massacci (1999) also provides a way of translating the DES crypto protocol into a SAT instance. One can obtain very hard satisfiable instances this way. Since the best algorithms known for dealing directly with the original crypto problem involve exhaustive search, one finds that the best SAT methods are also reduced to an essentially exhaustive search of the space of truth assignments. This means that in practice these problems are in a sense too hard for the development and evaluation of SAT procedures. Furthermore, the cryptographic encodings do not provide a fine-grained way to vary problem hardness in order to studying how the algorithms scale. In general, it seems reasonable to assume that in practical applications one does not expect to find hidden crypto problems, unless one is dealing specifically with a cryptographic application. In this paper, we will introduce a method for the generation of (empirically) hard satisfiable problem instances. We also show how one can finely control the hardness of the



satisfiable instances by establishing a connection between problem hardness and a new kind of phase transition phenomenon in the space of problem instances. As we discussed above, traditional phase transition phenomena involve a sudden transition from a satisfiable to an unsatisfiable phase of the problem instance space. Since our generator only outputs satisfiable instances, such a transition does not occur. However, under the right parameterization, we also observe an easy-hard-easy pattern in the space of satisfiable instances, just as is the case for complete search methods. (For related work, see Clark et al. (1996).) This makes it possible to tune the generator to output hard problem instances. We can link the hardness area to a phase transition which corresponds to a clear threshold phenomenon in the size of the "backbone" of the problem instances. Informally speaking, the backbone measures the amount of shared structure among the set of all solutions to a given problem instance. The size of the backbone is measured in terms of the percentage of variables that have the same value in all possible solutions. We will observe a transition from a phase where the size of the backbone is almost 100% to a phase with a backbone of size close to 0%. The transition is sudden and we will show how it coincides with the hardest problem instances both for incomplete and complete search methods.



Quasigroups with holes

Most traditional benchmark problems are based on randomly generated instances with little or no global structure. In Gomes and Selman (1997), we introduced the so-called quasigroup completion problem in order to obtain benchmark instances with more interesting structural properties. The best way to view the quasigroup completion problem is in terms of the completion of a Latin square (which technically defines the multiplication table of the quasigroup). Given N colors, a Latin square is defined by an N by N table, where each entry has a color and where there are no repeated colors in any row or column. N is called the order of the square. Gomes and Selman considered the problem of whether a partially colored Latin square can be completed into a full Latin square by assigning colors to the open entries of the table. This problem is referred to as the quasigroup completion problem (QCP). QCP is NPcomplete (Colbourn 1984) and has an interesting phase transition phenomenon with an associated easy-hard-easy pattern as a function of the fraction of number of preassigned colors. The domain has been used to study the effectiveness of a variety of local consistency measures for constraint satisfaction procedures (Stergiou and Walsh 1999, Walsh 1999, Regin 1994). The quasigroup completion task has interesting global structure but does not lend itself well for the evaluation of local search methods because we again have a mix of satisfiable and unsatisfiable instances. However, we will introduce a new generator based on the quasigroup domain that gives a natural unbiased way for obtaining only satisfiable instances, with good computational properties, namely by starting with a full quasigroup and "punching" holes into it. We use a recent result on generating uniformly distributed random complete quasigroups for generating our initial full



quasigroup. The problem of generating uniformly distributed Latin squares is non-trivial. Jacobson and Matthews (1996) show how by simulating an ergodic Markov chain whose stationary distribution is uniform over the space of N by N Latin squares, one can obtain squares that are (approximately) uniformly distributed. The Markov chain Monte Carlo method starts with a complete Latin square. (There is an efficient method for generating a fixed Latin square of any size.) Subsequently, the method randomly "perturbs" the initial Latin square to obtain a new square; repeated random perturbations lead us through a chain of squares. The difficult part is to design sequences of perturbations that lead from one valid Latin square to another while ensuring that one can reach any arbitrary Latin square in the chain with equal probability in the stationary distribution. The method proposed by Jacobson and Matthews corresponds to a random walk on a finite, connected, nonbipartite undirected graph and therefore it is ergodic, with stationary distribution assigning each vertex a probability proportional to its degree. The Jacobson and Matthews approach provides us with a good starting point for obtaining interesting satisfiable computational instances. We propose the following generator: (1) Generate a complete Latin square according to the Markov chain Monte Carlo approach proposed by Jacobson and Matthews; (2) punch a fraction p of "holes" in the Latin square (i.e., uncolor some of the entries) in a uniformly distributed manner. The resulting partial Latin square is now guaranteed to be satisfiable and moreover, as we will see below, we can finely control its expected hardness by tuning the value of p. We call this new problem the "quasigroup with holes" (QWH) problem.1 As we will describe below, the instances can be solved directly (in order to test e.g., a constraint-logic programming algorithm) or translated into a Boolean CNF encoding (in order to test general SAT solvers). It is interesting to note that while the quasigroup domain lends itself naturally to a satisfiable instance generator with good computational properties, it is not clear how a similar generator could be developed for, e.g., k-SAT or graph coloring. The quasigroup with holes problem is NP-hard. This follows from the following argument. Assume one had a polynomial algorithm that could solve QWH. Such an algorithm could be used to solve the quasigroup completion task (QCP), by simply running the algorithm with a polynomial time bound. The bounded algorithm would either solve our completion problem or terminate at the time bound, indicating no solution exists. However, this is impossible because, as noted above, QCP is NP-complete. In the next sections, we will identify a clear easy-hardeasy pattern for both complete and incomplete search methods on these problem instances. Note that because we are dealing with a distribution of satisfiable instances only, we obtain a clear full easy-hard-easy diagram for a incomplete search method. Clark et al. (1996) provide initial results on a such a pattern for local search using standard benchmarks. However, given the rareness of satisfiable instances on the

We thank Mark Stickel for some preliminary discussions on the use of the quasigroups with holes (Stickel, personal communications, May 1998).

1



unsat side of the phase transition it is difficult to establish a clear full pattern. We will also show that the hardness region of our satisfiable problem instances coincides with a new kind of phase transition. This transition differs from the standard sat/unsat transition because we now have only satisfiable instances, but like the standard transition, it is based on an underlying structural property -- namely, the backbone.



Problem hardness

In order to solve QWH instances, we explored a range of algorithms. We used an ILOG constraint solver working directly on the constraint satisfaction encoding of the problem. In the ILOG solver, we incorporated, aside from the standard constraint propagation methods, the all-diff constraint (Stergiou and Walsh 1999; Regin 1994). We also implemented (in C) a local search procedure working directly on the constraint representation. Finally, we converted the problem instances into Boolean satisfiability encodings and used stateof-the-art SAT solvers, both complete and incomplete methods. To our surprise, the approach via a SAT encoding is more efficient than using the direct CSP approaches; apparently, the increase in the size of the encoding when going to SAT does not hurt overall performance. Given the space limitations of this paper, we will only include the data for our best performing procedures, the backtracking SAT solver Satz (Li and Anbulagan 1997) and the local search SAT solver Walksat (Selman et al. 1996). (Both solvers are available from SATLIB (Hoos 1999).) Our data for the CSP approach is qualitatively the same. The QWH instances thus provide a good benchmark for both CSP and for SAT methods. Experimental data, instances, and generator (both SAT and CSP representation) are available from the authors. In Fig. 1, we show the computational cost profiles for an incomplete (Walksat; left panel) and a complete (Satz; right panel) search method for the QWH problem. Along the horizontal axis, we vary the fraction of holes in the quasigroup. More specifically, we take the ratio of the number of holes to the total number of entries in the Latin square, i.e., N 2 , where N is order of the square. The vertical axis gives the median computational cost. For Walksat, the cost is measured in terms of the total number of variable flips; for Satz we measured the total number of backtracks. The figure shows a clear easy-hard-easy pattern for both the incomplete and the complete search methods. Over a range of different sizes (N = 30, 33, 36) we see a rapid (in fact, exponential) increase in search cost in the hardest region. Close observation shows that there is a slight shift in the location of the peaks. We will return to this issue below, when we discuss a way of rescaling the figures to precisely line up the peaks. Aside from having a clear easy-hard-easy pattern, the main point of interest in Fig. 1 is the profile for the incomplete search method. We see a clear example of an easyhard-easy pattern for an incomplete search method. Because previous problem generators give a mixture of sat and unsat cases, such an easy-hard-easy pattern has generally been reported so far only for complete methods, which can handle both types of instances. Our figure shows that the notions of under-constrained, critically constrained, and over-



4.5e+07

Computational Cost of Local Search



1600 Order 30 Order 33 Order 36

Computational Cost of Complete Search



4e+07



1400 1200 1000 800 600 400 200 0 0.2



Order 30 Order 33 Order 36



3.5e+07 3e+07



2.5e+07 2e+07



1.5e+07 1e+07 5e+06 0 0.2



0.25



0.3 0.35 Num. Holes / (N^2)



0.4



0.45



0.25



0.3 0.35 Num. Holes / (N^2)



0.4



0.45



Figure 1: Computational cost profiles for incomplete (Walksat) and complete (Satz) search methods for QWH. constrained (Hogg et al. 1996) are also predictive of the performance of incomplete search methods.

1 0.9

Percentage of backbone and search cost

1 0.9 0.8 'Order 30' 'Order 33' 'Order 36' 'Order 39' 'Order 42' 'Order 45' 'Order 48' 'Order 51' 'Order 54' 'Order 57'



backbone local search cost

% FC Backbone



0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.2



0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.2



0.25



0.3 0.35 Num. Holes / (N^2)



0.4



0.45



0.25



0.3 0.35 Num. Holes / (N^2)



0.4



0.45



Figure 3: Backbone for different orders. satisfiable. Nevertheless, we can use recently introduced notions from the study of phase transition phenomena to link the peak in search complexity to a phase transition in structural properties of our problem instances. To do so, we will consider so-called backbone variables. Monasson et al. (1999) introduced the notion of the backbone of a SAT problem to refer to the fraction of its variables that are fully constrained: that is, which take on the same values in all solutions. The backbone fraction (ratio of backbone variables to the total number of variables) is a property of CSP and SAT problems that is well-defined for satisfiable distributions. Fig. 2 shows the backbone fraction as a function of the fraction of holes in the QWH problem. We also included the normalized cost of local search. The figure shows a sharp phase transition phenomenon in the backbone fraction, which coincide with the hardness peak in local search.2

2 The figure gives data for N = 36. The hardness peak for our complete search method also lies in the phase transition region



Figure 2: Backbone phase transition with cost profile.



A New Type of Phase Transition

One of the key advances in our understanding of problem hardness has been the connection between the easy-hardeasy pattern in search complexity and phase transition phenomena (Cheeseman 1991; Mitchell et al. 1992; Kirkpatrick and Selman 1994; Hogg et al. 1996; Hayes 1996). In particular, a clear connection has been established between the hardest problem instances and the phase transition region, where instances shift from being mostly satisfiable to being mostly unsatisfiable. One of the interesting aspects of this connection is that properties of the SAT/UNSAT phase transition can be analyzed quite independently from any particular solution procedure. In fact, this has led to a large number of papers on the SAT/UNSAT phase transition per se. For the QWH instances, we do not have a SAT/UNSAT phase transition, since all our instances are guaranteed to be



1 0.9 0.8 0.7

%FC Backbone



1



0.6 0.5 0.4 0.3 0.2 0.1 0 0.6 0.8 1 1.2 1.4 1.6 1.8 Num. Holes /(N^1.55)



Normalized cost of inomplete search



'Order 30' 'Order 33' 'Order 36' 'Order 39' 'Order 42' 'Order 45' 'Order 48' 'Order 51' 'Order 54' 'Order 57'



Order 30 Order 33 Order 36 0.8



0.6



0.4



0.2



2



2.2



2.4



0 0.2



0.25



0.3 0.35 Num. Holes / (N^2)



0.4



0.45



Figure 4: Backbone for different orders (rescaled). The reasons for the correlation between problem hardness and the appearance of the backbone are not fully understood at this time. One intuition is that backtracking search algorithms have the worst performance when they make an incorrect choice near the root of the search tree: that is, when they make a variable-value assignment that appears in no solution. For the algorithm to have a significant chance of making such a bad choice a non-negligible fraction of the variables must appear in the backbone. When the backbone fraction nears 1, however, the problems are so overconstrained that incorrect choices near the root are quickly detected and corrected. For local search procedures, an explanation might be developed by considering the relationship between the backbone and set of solutions to the instances. When the backbone is small, there are many solutions widely distributed in the search space, and so local search may quickly find one. When the backbone is near 1, the solutions are tightly clustered, so that that all clauses "vote" to push the search in the same direction. A partial backbone, however, may indicate that solutions are in different clusters that are widely distributed, with different clauses pushing the search in different directions. Making these intuitions precise, however, awaits future research.

1



Figure 5: Normalized computational cost.



Order 30 Order 33 Order 36

Normalized cost of inomplete search



0.8



0.6



0.4



0.2



0 1 1.2 1.4 1.6 1.8 2 2.2 Num. Holes / (N^1.55) 2.4 2.6 2.8 3



Figure 6: Re-parameterized computational cost. Some experimentation with different parameterization leads us to Fig. 4. This figure shows the backbone plotted against the number holes over N 1.55 . Note that we originally used "number of holes over N 2 ". We are currently working on an analytical derivation of the re-parameterization. Finally, Figs. 5 and 6 show how our rescaling also corrects for the shift in the complexity peak of our local search method. To show the original shift, Fig. 5 gives the search complexity for three different sizes of the QWH problem, where the cost has been normalized to 1. Fig. 6 shows how the peaks collapse onto each other after rescaling. The peaks for the complete search method (right panel in Fig. 1) also align after such a rescaling.



Re-parameterization

As we noted above, there is a slight shift in the location of the hardness peak as a function of N . There is a similar shift in the location of the backbone phase transition. This points to the fact that the original parameterization in terms of the fraction of holes does not exactly capture the dimensionality of our problem.3 Fig. 3 shows the shift in the backbone transition for a larger range of problem sizes (N = 30, . . . , 57).4

but is shifted slightly to the right. We are currently investigating whether that shift is real or part of the uncertainty in our data. 3 Note that a similar shift is also present in the original quasigroup completion problem. 4 Computing the full backbone is prohibitively expensive. The figure gives a good approximation of the backbone fraction computed by using forward-checking to estimate the fraction of fixed



variables. This estimate is a few percentage off from the true value, but the shifting behavior appears identical to that of the full backbone, based on experiments for smaller values of N .



Conclusions

We propose a problem generator for satisfiable instances. The generator samples from satisfiable quasigroups of a given size with a given number of holes. The hardness of the QWH problem instances can be tuned by varying the fraction of holes in the quasigroup instances. The main advantage of this generator is that it generates satisfiable instances only and is therefore well-suited for use in the study and evaluation of incomplete search methods. Several earlier attempts at designing such a generator (e.g., by forcing a given solution during the problem generation) were unsatisfactory. Using our generator, we showed that a local search method does exhibit the easy-hard-easy pattern, as observed previously for complete search methods. Based on the notion of under-constrained, critically constrained, and over-constrained regions identified with complete search methods, it was believed that an easy-hardeasy pattern would emerge for local search methods but this was difficult to confirm empirically because satisfiable instances in the over-constrained region are extremely rare for standard problem generators. We also show how the hardest region of the satisfiable instances coincides with a new kind of phase transition in terms of the backbone of the problem instances. The backbone characterizes the amount of shared structure between solutions. Finally, we present an empirically obtained re-parameterization of the phase transition and complexity peak of the quasigroup with holes problem. Our generator outputs instances suitable for both CSP and SAT style methods. The generator should therefore be of use in the future development of stochastic local search style CSP and SAT methods.



Acknowledgements



We would like to thank Dongmin Liang for his assistance with obtaining the experimental data in this paper. The second author is supported by the Air Force Research Laboratory and the Air Force Office of Scientific Research, under the New World Vistas Initiative. The fourth author is supported by an NSF Faculty Early Career Development Award, an Alfred P. Sloan fellowship, and by the Air Force Research Laboratory.



References

Asahiro, Y., Iwama, K. and Miyano, E. (1993). Random generation of test instances with controlled attributes. In DIMACS 1993, op cite. Beijing (1996). International Competition and Symposium on Satisfiability Testing, Beijing, China, March 15-17, 1996. Cheeseman, P. and Kanefsky, R. and Taylor, W. (1991). Where the Really Hard Problems Are. Proc. IJCAI-91, 1991, 163- 169. Clark, D.A., Frank, J., Gent, I.P., MacIntyre, E., Tomov, N., Walsh, T. (1996). Local search and the number of solutions. Proc. CP-96, 1996. Colbourn, C. (1984). The Complexity of Completing Latin Squares. Discrete Appl. Math., 8, (1984), 25-30. Crawford, J. and Kearns, M. (1993). Instances for learning the parity function. Unpublished note, see Hoos (1999). DIMACS (1993). Second DIMACS Implement. Challenge, 1993. Pub. as DIMACS Series in Disc. Math. and Theor. Comp. Sci., vol. 26, D. Johnson and M. Trick, eds., AMS, 1996. DIMACS (1996). Satisfiability Problem, DIMACS Workshop, 1996. Pub. as DIMACS Discrete Math. and Theor. Comp. Sci.,



vol. 35, D. Du, J. Gu, and P. Pardalos, eds., AMS, 1997. Frieze, A. and McDiarmid, C. (1997). Algorithmic theory of random graphs. Random Structures and Algorithms, vol. 10 (1997) 5-42. Gent, I. and Walsh, T. (1993) An empirical analysis of search in GSAT. J. of Artificial Intelligence Research, vol. 1, 1993. Gomes, C.P. and Selman, B. (1997a). Problem structure in the presence of perturbations. Proc. AAAI-97, 1997. Hayes, B. (1996). Can't get no satisfaction. American Scientist vol. 85, 108 (1996) Hogg, T., Huberman, B.A., and Williams, C.P. (Eds.) (1996). Phase Transitions and Complexity. Artificial Intelligence, 81 (Spec. Issue), 1996. Hoos, H. 1999. SATLIB. A collection of SAT tools and data. See www.informatik.tu-darmstadt.de/AI/SATLIB. Impagliazzo, R., Levin, L., and Luby, M. (1989). Pseudo-random number generation from one-way functions. Proc. 21st STOC, 1989, 12-24. Jacobson, M.T. and Matthews, P. (1996) Generating uniformly distributed random latin squares. J. of Combinatorial Designs, vol. 4., no. 6, (1996) 405-437. Johnson, D.S. , Aragon, C.R., McGeoch, L.A., and Shevon C. (1989) Optimization by Simulated Annealing: An Experimental Evaluation. Operations Research, 37:6 (1989), 865-892. Kirkpatrick, S. and Selman, B. (1994). Critical behavior in the satisfiability of random Boolean expressions. Science, 264, 1994, 1297-1301. Li, Chu Min and Anbulagan (1997). Heuristics based on unit propagation for satisfiability problems. Proc. IJCAI-97, 366- 371. Massacci, F. (1999). Using Walk-SAT and Rel-SAT for cyptographic key search. Proc. IJCAI-99, 1999, 290-295. Mitchell, D. and Levesque H. (1996). Some pitfalls for experimenters with random SAT. Artificial Intelligence, Vol. 81(1-2), 1996, 111-125. Mitchell, D., Selman, B., and Levesque, H.J. (1992). Hard and easy distributions of SAT problems. Proc. AAAI-92, San Jose, CA (1992) 459-465. Morris, P. (1993) The breakout method for escaping from local minima. Proc. AAAI-93, 1993, 40-45. Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., and Troyansky, L. (1996). Determining computational complexity from characteristic `phase transitions'. Nature, Vol. 400(8), 1999. Regin, J.C. (1994). A filtering algorithm for constraints of difference in CSP. Proc. AAAI-94, 1994, 362-367. Selman, B. and Levesque, H.J., and Mitchell, D.G. (1992). A New Method for Solving Hard Satisfiability Problems. Selman, B., Kautz, H.A., and Cohen, B. (1996). Local search strategies for satisfiability testing. In DIMACS (1993). Shaw, P., Stergiou, K., and Walsh, T. (1998) Arc consistency and quasigroup completion. Proc. ECAI-98, workshop on binary constraints, 1998. Stergiou, K. and Walsh, T. (1999) The Difference All-Difference Makes Proc. of IJCAI-99, Stockholm, Sweden. Walsh, T. (1999) Search in a Small World. Proc. of IJCAI-99, Stockholm, Sweden, 1999. Van Gelder, A. (1993). Problem generator (mkcnf.c) contributed to the DIMACS 1993 Challenge archive.



Journal of Artificial Intelligence Research 24 (2005) 623-639



Submitted 12/04; published 11/05



Hiding Satisfying Assignments: Two are Better than One

Dimitris Achlioptas

Microsoft Research Redmond, Washington optas@microsoft.com



Haixia Jia

Computer Science Department University of New Mexico



hjia@cs.unm.edu



Cristopher Moore

Computer Science Department University of New Mexico



moore@cs.unm.edu



Abstract

The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random k SAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, for a number of different algorithms, A acts as a stronger and stronger attractor as the formula's density increases. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k -SAT and NAE-k -SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and A are satisfying. It appears that under this "symmetrization" the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion.



1. Introduction

Recent years have witnessed the rapid development and application of search methods for constraint satisfaction and Boolean satisfiability. An important factor in the success of these algorithms is the availability of good sets of benchmark problems to evaluate and fine-tune them. There are two main sources of such problems: the real world, and random instance generators. Real-world problems are arguably the best benchmarks, but unfortunately are in short supply. Moreover, using real-world problems carries the risk of tuning algorithms toward the specific application domains for which good benchmarks are available. In that sense, random instance generators are a good additional source, with the advantage of controllable characteristics, such as size and expected hardness. Hard random instances have led to the development of new stochastic search methods such as WalkSAT (Selman, Kautz, & Cohen, 1996), the breakout procedure (Morris, 1993), and Survey Propagation (M ezard & Zecchina, 2002), and have been used in detailed comparisons of local search methods for graph coloring and related problems (Johnson, Aragon, McGeoch, & Shevon, 1989). The results of various competitions for CSP and SAT algoc 2005 AI Access Foundation. All rights reserved.



Achlioptas, Jia, & Moore



rithms show a fairly direct correlation between the performance on real-world benchmarks and on hard random instances (Johnson & Trick, 1996; Du, Gu, & Pardalos, 1997; Johnson et al., 1989). Nevertheless, a key limitation of current problem generators concerns their use in evaluating incomplete satisfiability solvers such as those based on local search methods. When an incomplete algorithm does not find a solution, it can be difficult to determine whether this is because the instance is in fact unsatisfiable, or simply because the algorithm failed to find a satisfying assignment. The standard way of dealing with this problem is to use a complete search method to filter out the unsatisfiable cases. However, this greatly limits the size and difficulty of problem instances that can be considered. Ideally, one would use problem generators that generate satisfiable instances only. One relatively recent source of such problems is the quasigroup completion problem (Shaw, Stergiou, & Walsh, 1998; Achlioptas, Gomes, Kautz, & Selman, 2000; Kautz, Ruan, Achlioptas, Gomes, Selman, & Stickel, 2001). However, a generator for random hard satisfiable instances of 3-SAT, say, has remained elusive. Perhaps the most natural candidate for generating random hard satisfiable 3-SAT formulas is the following. Pick a random truth assignment A, and then generate a formula with n variables and rn random clauses, rejecting any clause that is violated by A. In particular, we might hope that if we work close to the satisfiability threshold region r  4.25, where the hardest random 3-SAT problems seem to be (Cheeseman, Kanefsky, & Taylor, 1991; Hogg, Huberman, & Williams, 1996; Mitchell, Selman, & Levesque, 1992), this would generate hard satisfiable instances. Unfortunately, this generator is highly biased towards formulas with many assignments clustered around A. When given to local search methods such as WalkSAT, the resulting formulas turn out to be much easier than formulas of comparable size obtained by filtering satisfiable instances from a 3-SAT generator. More sophisticated versions of this "hidden assignment" scheme (Asahiro, Iwama, & Miyano, 1996; Van Gelder, 1993) improve matters somewhat but still lead to easily solvable formulas. In this paper we introduce a new generator of random satisfiable problems. The idea is simple: we pick a random 3-SAT formula that has a "hidden" complementary pair of satisfying assignments, A and A, by rejecting clauses that are violated by either A or A. We call these "2-hidden" formulas. Our motivation comes from recent work (Achlioptas & Moore, 2002b, 2005) which showed that moving from random k -SAT to random NAE-k SAT (in which every clause in the formula must have at least one true and at least one false literal) tremendously reduces the correlation between solutions. That is, whereas in random k -SAT, satisfying assignments tend to form clumps, in random NAE-k -SAT the solutions appear to be scattered throughout {0, 1} n in a rather uniform "mist," even for densities extremely close to the threshold. An intuitive explanation for this phenomenon is that since the complement of every NAE-assignment is also an NAE-assignment, the attractions of solution pairs largely "cancel out." In this paper we exploit this phenomenon to impose a similar symmetry with the hidden assignments A and A, so that their attractions cancel out, making it hard for a wide variety of algorithms to "feel" either one. A particularly nice feature of our generator is that it is based on an extremely simple probabilistic procedure, in sharp contrast with 3-SAT generators based on, say, cryptographic ideas (Massacci, 1999). In particular, our generator is readily amenable to all the mathematical tools that have been developed for the rigorous study of random k -SAT formulas. Here we make two first steps in that direction. In Section 2, via a first mo624



Hiding Satisfying Assignments: Two are Better than One



ment calculation we study the distribution of the number of solutions as a function of their distance from the hidden assignments. In Section 3, we use the technique of differential equations to analyze the performance of the Unit Clause (UC) heuristic on our formulas. Naturally, mathematical simplicity would not be worth much if the formulas produced by our generator were easily solvable. In Section 4, we compare experimentally the hardness of "2-hidden" formulas with that of "1-hidden" and "0-hidden" formulas. That is, we compare our formulas with random 3-SAT formulas with one hidden assignment and with standard random 3-SAT formulas with no hidden assignment. We examine four leading algorithms: two complete solvers, zChaff and Satz, and two incomplete ones, WalkSAT and the recently introduced Survey Propagation (SP). For all these algorithms, we find that our formulas are much harder than 1-hidden formulas and, more importantly, about as hard as 0-hidden formulas, of the same size and density.



2. A picture of the space of solutions

In this section we compare 1-hidden and 2-hidden formulas with respect to the expected number of solutions at a given distance from the hidden assignment(s). 2.1 1-hidden formulas Let X be the number of satisfying truth assignments in a random k -SAT formula with n variables and m = rn clauses chosen uniformly and independently among all k -clauses with at least one positive literal, i.e., 1-hidden formulas where we hide the all-ones truth assignment. To calculate the expectation E[X ], it is helpful to parametrize truth assignments according to their overlap with the hidden assignment, i.e., the fraction  of variables on which they agree with A, which in this case is the fraction of variables that are set to one. Then, linearity of expectation gives (1), clause independence gives (2), selecting the literals in each clause uniformly and independently gives (3), and, finally, writing z = n and using Stirling's approximation for the factorial gives (4) below: E[X ] =

A{0,1}n n



Pr[A is satisfying]



(1)



=

z =0 n



=

z =0 n



n Pr[a truth assignment with z ones satisfies a random clause] m (2) z m  k k 1 n  (3) (1 - z/n)j (z/n)k-j  1- k j z 2 -1

j =1



=

z =0



n z



1 - (z/n)k 1- 2k - 1



m



= poly(n) x max



[0,1]



1   (1 - )1-



1 - k 1- k 2 -1



r n



(4)



= poly(n) x max [fk,r ()]n

[0,1]



625



Achlioptas, Jia, & Moore



where 1 fk,r () =   (1 - )1-



1 - k 1- k 2 -1



r



.



From this calculation we see that E[X ] is dominated by the contribution of the truth assignments that maximize fk,r () (since we raise fk,r to the nth power all other contributions vanish). Now, note that f is the product of an "entropic" factor 1/(  (1 - )1- ) which is symmetric around  = 1/2, and a "correlation" factor which is strictly increasing in . As a result, it is always maximized for some  > 1/2. This means that the dominant contribution to E[X ] comes from truth assignments that agree with the hidden assignment on more that half the variables. That is, the set of solutions is dominated by truth assignments that can "feel" the hidden assignments. Moreover, as r increases this phenomenon becomes more and more acute (see Figure 1 below). 2.2 2-hidden formulas Now let X be the number of satisfying truth assignments in a random k -SAT formula with n variables and m = rn clauses chosen uniformly among all k -clauses that have at least one positive and at least one negative literal, i.e., 2-hidden formulas where we hide the all- ones assignment and its complement. To compute E[X ] we proceed as above, except that now (3) is replaced by m  k -1 n k 1 n  (1 - z/n)j (z/n)k-j  . 1- k j z 2 -2

z =0 j =1



Carrying through the ensuing changes we find that now E[X ] = poly(n) x max [gk,r ()]n

[0,1]



where 1 gk,r () =   (1 - )1-



This time, both the entropic factor and the correlation factor comprising g are symmetric functions of , so gk,r is symmetric around  = 1/2 (unlike f k,r ). Indeed, one can prove that for all r up to extremely close to the random k -SAT threshold r k , the function gk,r has its global maximum at  = 1/2. In other words, for all such r , the dominant contribution to E[X ] comes from truth assignments at distance n/2 from the hidden assignments, i.e., the hidden assignments are "not felt." More precisely, there exists a sequence k  0 such that gk,r has a unique global maximum at  = 1/2, for all r  2k ln 2 - ln 2 -1- 2

k



1 - k - (1 - )k 1- 2k - 2



r



.



.



(5)



Contrast this with the fact (implicit in Kirousis, Kranakis, Krizanc, & Stamatiou, 1998) that for ln 2 1 r  2k ln 2 - - , (6) 2 2

626



Hiding Satisfying Assignments: Two are Better than One



a random k -SAT formula with n variables and m = rn clauses is unsatisfiable with probability 1 - o(1). Moreover, the convergence of the sequence k  0 is rapid, as can be seen from the concrete values in table 1. Thus the gap between the values of r given by equations (5) and (6) quickly converges to 1/2, even as the threshold becomes exponentially large. k Eq. (5) Eq. (6) 3 7/2 4.67 4 35/4 10.23 5 20.38 21.33 7 87.23 87.88 10 708.40 708.94 20 726816.15 726816.66



Table 1: The convergence (in k ) to the asymptotic gap of 1/2 is rapid In Figure 1 we plot fk,r and gk,r for k = 5 and r = 16, 18, 20, 22, 24 (from top to bottom). We see that in the case of 1-hidden formulas, i.e., f k,r , the maximum always occurs to the right of  = 1/2. Moreover, observe that for r = 22, 24, i.e., after we cross the 5-SAT threshold (which occurs at r  21) we have a dramatic shift in the location of the maximum and, thus, in the extent of the bias. Specifically, since the expected number of satisfying assignments is roughly f k,r ()n , and since fk,r () < 1 except for   1, with high probability the only remaining satisfying assignments in the limit n   are those extremely close to the hidden assignment. In the case of 2-hidden formulas, on the other hand, we see that for r = 16, 18, 20 the global maximum occurs at  = 1/2. For r = 20, just below the threshold, we also have two local maxima near  = 0, 1, but since g k,r is raised to the nth power, these are exponentially suppressed. Naturally, for r above the threshold, i.e., r = 22, 24, these local maxima become global, signifying that indeed the only remaining truth assignments are those extremely close to one of the two hidden ones. Intuitively, we expect that because g is flat at  = 1/2 where random truth assignments are concentrated, for 2-hidden formulas local search algorithms like WalkSAT will essentially perform a random walk until they are lucky enough to get close to one of the two hidden assignments. Thus we expect WalkSAT to take about as long on 2-hidden formulas as it does on 0-hidden ones. For 1-hidden formulas, in contrast, we expect the nonzero gradient of f at  = 1/2 to provide a strong "hint" to WalkSAT that it should move towards the hidden assignment, and that therefore 1-hidden formulas will be much easier for it to solve. We will see below that our experimental results bear out these intuitions perfectly.



3. The Unit Clause heuristic and DPLL algorithms

Consider the following linear-time heuristic, called Unit Clause (UC), which permanently sets one variable in each step as follows: pick a random literal and satisfy it, and repeatedly satisfy any 1-clauses present. Chao and Franco showed that UC succeeds with constant probability on random 3-SAT formulas with r < 8/3, and fails with high probability, i.e., with probability 1 - o(1) as n  , for r > 8/3 (Chao & Franco, 1986). One can think of UC as the first branch of the simplest possible DPLL algorithm S : set variables in a random order, each time choosing randomly which branch to take first. Their result then shows that, with constant probability, S solves random 3-SAT formulas with r < 8/3 with no backtracking at all.

627



Achlioptas, Jia, & Moore



1.3 1.2 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0 0.2 0.4 0.6 0.8 r=16 r=18 r=20 r=22 r=24  1



1-hidden formulas

1.25 1.2 1.15 1.1 1.05 1 0.95 0.9 0 r=16 r=18 r=20 r=22 r=24



0.2



0.4







0.6



0.8



1



2-hidden formulas Figure 1: The nth root of the expected number of solutions f k,r and gk,r for 1-hidden and 2-hidden formulas respectively, as a function of the overlap fraction  = z/n with the hidden assignment. Here k = 5 and r = 16, 18, 20, 22, 24 from top to bottom.



628



Hiding Satisfying Assignments: Two are Better than One



It is conjectured that the running time of S goes from linear to exponential at r = 8/3, with no intermediate regime. Calculations using techniques from statistical physics (Cocco & Monasson, 2001a, 2001b; Monasson, 2005) show that this is true of the expected running time. Achlioptas, Beame and Molloy show that the running time is exponential with high probability for r > 3.81; moreover, they show that if the "tricritical point" of (2 + p)-SAT is r = 2/5, then this is the case for r > 8/3 (Achlioptas, Beame, & Molloy, 2001). In this section we analyze the performance of UC on 1-hidden and 2-hidden formulas. Specifically, we show that UC fails for 2-hidden formulas at precisely the same density as for 0-hidden ones. Based on this, we conjecture that the running time of S , and other simple DPLL algorithms, becomes exponential for 2-hidden formulas at the same density as for 0-hidden ones. To analyze UC on random 1-hidden and 2-hidden formulas we actually analyze UC on arbitrary initial distributions of 3-clauses, i.e., where for each 0  j  3 we specify the initial number of 3-clauses with j positive literals and 3 - j negative ones. We use the method of differential equations; see the article by Achlioptas(2001) for a review. To simplify notation, we assume that A is the all-ones assignment, so that 1-hidden formulas forbid clauses where all literals are negative, while 2-hidden formulas forbid all-negative and all-positive clauses. A round of UC consists of a "free" step, in which we satisfy a random literal, and the ensuing chain of "forced" steps or unit-clause propagations. For 0  i  3 and 0  j  i, let Si,j = si,j n be the number of clauses of length i with j positive literals and i - j negative ones. We will also refer to the total density of clauses of size i as s i = j si,j . Let X = xn be the number of variables set so far. Our goal is to write the expected change in these variables in a given round as a function of their values at the beginning of the round. Note that at the beginning of each round S 1,0 = S1,1 = 0 by definition, so the "state space" of our analysis will consist of the variables S i,j for i  2. It is convenient to define two new quantities, m T and mF , which are the expected number of variables set True and False in a round. We will calculate these below. Then, in terms of mT , mF , we have E[S3,j ] = -(mT + mF ) 3s3,j 1-x 2s2,j (j + 1)s3,j +1 (3 - j )s3,j E[S2,j ] = -(mT + mF ) + mF + mT 1-x 1-x 1-x E[X ] = -(mT + mF ) . (7) (8)



To see this, note that a variable appears positively in a clause of type i, j with probability j/(n - X ), and negatively with probability (i - j )/(n - X ). Thus, the negative terms in (7) and (8) correspond to clauses being "hit" by the variables set, while the positive term is the "flow" of 3-clauses to 2-clauses. To calculate mT and mF , we consider the process by which unit clauses are created during a round. We can model this with a two-type branching process, which we analyze as in the article by Achlioptas and Moore(2002a). Since the free step gives the chosen variable a random value, we can think of it as creating a unit clause, which is positive or negative with equal probability. Thus the initial expected population of unit clauses can be

629



Achlioptas, Jia, & Moore



represented by a vector p0 = 1/2 1/2



where the first and second components count the negative and positive unit clauses respectively. Moreover, at time X = xn, a unit clause procreates according to the matrix M= 1 1-x s2,1 2s2,0 2s2,2 s2,1 .



In other words, satisfying a negative unit clause creates, in expectation, M 1,1 = s2,1 /(1 - x) negative unit clauses and M2,1 = 2s2,2 /(1 - x) positive unit clauses, and similarly for satisfying a positive unit clause. Thus, as long as the largest eigenvalue  1 of M is less than 1, the expected number of variables set true or false during the round is given by mF mT = (I + M + M 2 + * * * ) * p0 = (I - M )-1 * p0



where I is the identity matrix. Moreover, as long as  1 < 1 throughout the algorithm, i.e., as long as the branching process is subcritical for all x, UC succeeds with constant probability. On the other hand, if 1 ever exceeds 1, then the branching process becomes supercritical, with high probability the unit clauses proliferate, and the algorithm fails. Note that  s2,1 + 2 s2,0 s2,2 . (9) 1 = 1-x Now let us rescale (7) to give a system of differential equations for the s i,j . Wormald's Theorem (Wormald, 1995) implies that with high probability the random variables S i,j (xn) will be within o(n) of si,j (x) * n for all x, where si,j (x) is the solution of the following: ds3,j dx ds2,j dx 3s3,j 1-x 2s2,j (j + 1)s3,j +1 (3 - j )s3,j mF mT = - + + 1 - x mT + m F 1-x mT + m F 1-x = - (10)



Now, suppose our initial distribution of 3-clauses is symmetric, i.e., s 3,0 (0) = s3,3 (0) and s3,1 (0) = s3,2 (0). It is easy to see from (10) that in that case, both the 3-clauses and the 2-clauses are symmetric at all times, i.e., s i,j = si,i-j and mF = mT . In that case  s2,1 + 2 s2,0 s2,2 = s2 , so the criterion for subcriticality becomes 1 = s2 <1 . 1-x



Moreover, since the system (10) is now symmetric with respect to j , summing over j gives the differential equations ds3 dx ds2 dx 3s3 1-x 2s2 3s3 = - + 1 - x 2(1 - x) = -

630



Hiding Satisfying Assignments: Two are Better than One



which are precisely the differential equations for UC on 0-hidden formulas, i.e., random instances of 3-SAT. Since 2-hidden formulas correspond to symmetric initial conditions, we have thus shown that UC succeeds on them with constant probability if and only if r < 8/3, i.e., that UC fails on these formulas at exactly the same density for which it fails on random 3-SAT instances. (In contrast, integrating (10) with the initial conditions corresponding to 1-hidden formulas shows that UC succeeds for them at a slightly higher density, up to r < 2.679.) Of course, UC can easily be improved by making the free step more intelligent: for instance, choosing the variable according to the number of its occurrences in the formula, and using the majority of these occurrences to decide its truth value. The best known heuristic of this type (Kaporis, Kirousis, & Lalas, 2003; Hajiaghayi & Sorkin, 2003) succeeds with constant probability for r < 3.52. However, we believe that much of the progress that has been made in analyzing the performance of such algorithms can be "pushed through" to 2-hidden formulas. Specifically, nearly all algorithms analyzed so far have the property that given as input a symmetric initial distribution of 3-clauses, e.g. random 3-SAT, their residual formulas consist of symmetric mixes of 2- and 3-clauses. As a result, we conjecture that the above methods can be used to show that such algorithms act on 2-hidden formulas exactly as they do on 0-hidden ones, failing with high probability at the same density. More generally, call a DPLL algorithm myopic if its splitting rule consists of choosing a random clause of a given size, based on the current distribution of clause sizes, and deciding how to satisfy it based on the number of occurrences of its variables in other clauses. For a given myopic algorithm A, let r A be the density below which A succeeds without any backtracking with constant probability. The results of Achlioptas, Beame and Molloy (2001) imply the following statement: if the tricritical point for random (2 + p)-SAT is p c = 2/5 then every myopic algorithm A takes exponential time for r > r A . Thus, not only UC, but in fact a very large class of natural DPLL algorithms, would go from linear time for r < r A to exponential time for r > rA . The fact that the linear-time heuristics corresponding to the first branch of A act on 2-hidden formulas just as they do on 0-hidden ones suggests that, for a wide variety of DPLL algorithms, 2-hidden formulas become exponentially hard at the same density as 0-hidden ones. Proving this, or indeed proving that 2-hidden formulas take exponential time for r above some critical density, appears to us a very promising direction for future work.



4. Experimental results

In this section we report experimental results on our 2-hidden formulas, and compare them to 1-hidden and 0-hidden ones. We use two leading complete solvers, zChaff and Satz, and two leading incomplete solvers, WalkSAT and the new Survey Propagation algorithm SP. In an attempt to avoid the numerous spurious features present in "too-small" random instances, i.e., in non-asymptotic behavior, we restricted our attention to experiments where n  1000. This meant that zChaff and Satz could only be examined at densities significantly above the satisfiability threshold, as neither algorithm could practically solve either 0-hidden or 2-hidden formulas with n  1000 variables close to the threshold. For WalkSAT and SP, on the other hand, we can easily run experiments in the hardest range (around the satisfiability threshold) for n  10 4 .

631



Achlioptas, Jia, & Moore



4.1 zChaff and Satz In order to do experiments with n  1000 with zChaff and Satz, we focused on the regime where r is relatively large, 20 < r < 60. As stated above, for r near the satisfiability threshold, 0-hidden and 2-hidden random formulas with n  1000 variables seem completely out of the reach of either algorithm. While formulas in this overconstrained regime are still challenging, the presence of many forced steps allows both solvers to completely explore the space fairly quickly. We obtained zChaff from the Princeton web site (Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). The first part of Figure 2 shows its performance on random formulas of all three types (with n = 1000 for 20  r  40 and n = 3000 for 40  n  60). We see that the number of decisions for all three types of problems decreases rapidly as r increases, consistent with earlier findings for complete solvers on random 3-SAT formulas. Figure 2 shows that zChaff finds 2-hidden formulas almost as difficult as 0-hidden ones, which for this range of r are unsatisfiable with overwhelming probability. On the other hand, the 1-hidden formulas are much easier, with a number of branchings between 2 and 5 orders of magnitude smaller. It appears that while zChaff's smarts allow it to quickly "zero in" on a single hidden assignment, the attractions exerted by a complementary pair of assignments do indeed cancel out, making 2-hidden formulas almost as hard as unsatisfiable ones. That is, the algorithm eventually "stumbles" upon one of the two hidden assignments after a search that is nearly as exhaustive as for the unsatisfiable random 3-SAT formulas of the same density. We obtained Satz from the SATLIB web site (Li & Anbulagan, 1997b). The second part of Figure 2 shows experiments on random formulas of all three types with n = 3000. As can be seen, the median number of branches explored by Satz for all three types of formulas are within a factor of five, with 0-hidden being the hardest and 2-hidden being the easiest (note that a factor of five corresponds to setting fewer than 3 variables). The reason for this is simple: while Satz makes intelligent decisions about which variable to branch on, it tries these branches in a fixed order, attempting first to set each variable false (Li & Anbulagan, 1997a). Therefore, a single hidden assignment will appear at a uniformly random leaf in Satz's search tree. In the 2-hidden case, since the two hidden assignments are complementary, one will appear in a random position and the other one in the symmetric position with respect to the search tree. Naturally, trying branches in a fixed order is a good idea when the goal is to prove that a formula is unsatisfiable, e.g. in hardware verification. However, we expect that if Satz were modified to, say, use the majority heuristic to choose a variable's first value, its performance on the three types of problems would be similar to zChaff's. 4.2 SP SP is an incomplete solver recently introduced by M ezard and Zecchina (2002) based on a generalization of belief propagation the authors call survey propagation. It is inspired by the physical notion of "replica symmetry breaking" and the observation that for 3.9 < r < 4.25, random 3-SAT formulas appear to be satisfiable, but their satisfying assignments appear to be organized into clumps.

632



Hiding Satisfying Assignments: Two are Better than One



Median number of decisions over 25 trials



10



6



zChaff performance on HIDDEN 1, 2 and 0 formulas HIDDEN-1 HIDDEN-2 HIDDEN-0



10



5



10



4



10



3



10



2



10 20



1



25



30



35



40 r



45



50



55



60



10 Median number of branches over 25 trials



5



Satz performance on HIDDEN 1, 2 and 0 formulas HIDDEN-1 HIDDEN-2 HIDDEN-0



10



4



10



3



10



2



10 20



1



25



30



35



40 r



45



50



55



60



Figure 2: The median number of branchings made by zChaff and Satz on random instances with 0, 1, and 2 hidden assignments (on a log 10 scale). For zChaff we use n = 1000 for r = 20, 30, 40 and n = 3000 for r = 40, 50, 60, and for Satz we use n = 3000 throughout. Each point is the median of 25 trials. The 2-hidden formulas are almost as hard for both algorithms as the 0-hidden ones, while the 1-hidden formulas are much easier for zChaff.



633



Achlioptas, Jia, & Moore



1 0.9 The fraction solved over 30 trials 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 4



SP performance on HIDDEN 1, 2 and 0 formulas



HIDDEN-0 HIDDEN-1 HIDDEN-2



4.5



5 r



5.5



6



Figure 3: The fraction of problems successfully solved by SP as a function of density, with n = 104 and 30 trials for each value of r . The threshold for solving 2-hidden formulas is somewhat higher than for 0-hidden ones, and for 1-hidden formulas it is higher still.



In Figure 3 we compare SP's performance on the three types of problems near the satisfiability threshold. (Because SP takes roughly the same time on all inputs, we do not compare the running times.) For n = 10 4 SP solves 2-hidden formulas at densities somewhat above the threshold, up to r  4.8, while it solves the 1-hidden formulas at still higher densities, up to r  5.6. Presumably the 1-hidden formulas are easier for SP since the "messages" from clauses to variables, like the majority heuristic, tend to push the algorithm towards the hidden assignment. Having two hidden assignments appears to cancel these messages out to some extent, causing SP to fail at a lower density. However, this argument does not explain why SP should succeed at densities above the satisfiability threshold; nor does it explain why SP does not solve 1-hidden formulas for arbitrarily large r . Indeed, we find this latter result surprising, since as r increases the majority of clauses should point more and more consistently towards the hidden assignment in the 1-hidden case. We note that we also performed the above experiments with n = 2 x 10 4 and with 5000 iterations, instead of the default 1000, for SP's convergence procedure. The thresholds of Figure 3 for 1-hidden and 2-hidden formulas appeared to be stable under both these changes, suggesting that they are not merely artifacts of our particular experiments. We propose investigating these thresholds as a direction for further work. 4.3 WalkSAT We conclude with a local search algorithm, WalkSAT. Unlike the complete solvers, WalkSAT can solve problems with n = 104 fairly close to the threshold. We performed experiments both with a random initial state, and with a biased initial state where the algorithm starts with 75% agreement with one of the hidden assignments (note that this is exponentially

634



Hiding Satisfying Assignments: Two are Better than One



unlikely). In both cases, we performed trials of 10 8 flips for each formula, without random restarts, where each step does a random or greedy flip with equal probability. Since random initial states almost certainly have roughly 50% agreement with both hidden assignments, we expect their attractions to cancel out so that WalkSAT will have difficulty finding either of them. On the other hand, if we begin with a biased initial state, then the attraction from the nearby assignment will be much stronger than the other one; this situation is similar to a 1-hidden formula, and we expect WalkSAT to find it easily. Indeed our data confirms these expectations. In the first part of Figure 4 we measure WalkSAT's performance on the three types of problems with n = 104 and r ranging from 3.7 to 7.9, and compare them with 0-hidden formulas for r ranging from 3.7 up to 4.1, just below the threshold where they become unsatisfiable. We see that, below the threshold, 2-hidden formulas are just as hard as 0-hidden ones when WalkSAT sets its initial state randomly; indeed, their running times coincide to within the resolution of the figure! They both become hardest when r  4.2, where 108 flips no longer suffice to solve them. Unsurprisingly, 2-hidden formulas are much easier to solve when we start with a biased initial state, in which case the running time is closer to that of 1-hidden formulas. In the second part of Figure 4, we compare the three types of formulas at a density very close to the threshold, r = 4.25, and measure their running times as a function of n. The data suggests that 2-hidden formulas with random initial states are much harder than 1-hidden ones, while 2-hidden formulas with biased initial states have running times within a constant of that of 1-hidden formulas. Note that the median running time of all three types of problems is polynomial in n, consistent with earlier experiments (Barthel, Hartmann, Leone, Ricci-Tersenghi, Weigt, & Zecchina, 2002). On the other hand, while 1-hidden formulas are much easier than 2-hidden ones for sufficiently large or small r , they appear to be slightly harder than 2-hidden ones for 5.3 < r < 6.3. One possible explanation for this is that while i) the solutions of a 2-hidden formula are harder to find due to their balanced distribution, ii) there are exponentially more solutions for 2-hidden formulas than for 1-hidden ones of the same size and density. It seems that in this range of r , the second effect overwhelms the first, and WalkSAT finds a solution more quickly in the 2-hidden case; but we have no explanation for why this is so for this particular range of r . At higher densities, such as r = 8 shown in Figure 5, 2-hidden formulas again appear to be harder than 1-hidden ones.



5. Conclusions

We have introduced an extremely simple new generator of random satisfiable 3-SAT instances which is amenable to all the mathematical tools developed for the rigorous study of random 3-SAT instances. Experimentally, our generator appears to produce instances that are as hard as random 3-SAT instances, in sharp contrast to instances with a single hidden assignment. This hardness appears quite robust; our experiments have demonstrated it both above and below the satisfiability threshold, and for algorithms that use very different strategies, i.e., DPLL solvers (zChaff and Satz), local search algorithms (WalkSAT), and survey propagation (SP).

635



Achlioptas, Jia, & Moore



10 Median number of flips over 100 trials



7



WalkSAT performance on HIDDEN 1, 2 and 0 formulas HIDDEN-0 HIDDEN-1 HIDDEN-2 init 75% true HIDDEN-2



10



6



10



5



10



4



10



3



3



4



5



r



6



7



8



10 7 Median number of flips over 100 trials 10 10 10 10 10

6



WalkSAT performance as a function of n HIDDEN-0 HIDDEN-2 HIDDEN-1 HIDDEN-2 init 75% true

slope 2.8



5



slope 2.7

4



slope 1.3



3



slope 1.3

2



10 100



1



200



400 n



800



1600



Figure 4: The top part of the figure shows the median number of flips needed by WalkSAT for formulas of all three types below and above the threshold, with n = 10 4 . Below the threshold, 2-hidden formulas are just as hard as 0-hidden ones (they coincide to within the resolution of the figure) and their running time increases steeply as we approach the threshold. Except in the range 5.3 < r < 6.3, 2hidden formulas are much harder than 1-hidden ones unless the algorithm starts with an (exponentially lucky) biased initial state. The bottom part of the figure shows the median number of flips needed by WalkSAT to solve the three types of formulas at r = 4.25 as a function of n. Here n ranges from 100 to 2000. While the median running time for all three is polynomial, the 2-hidden problems are much harder than the 1-hidden ones unless we start with a biased initial state. Again, the running time of 2-hidden problems scales similarly to 0-hidden ones, i.e., to random 3-SAT without a hidden assignment.



636



Hiding Satisfying Assignments: Two are Better than One



10 Median number of flips over 100 trials



6



WalkSAT performance on HIDDEN 1 and 2 formulas with r=8 HIDDEN-1 HIDDEN-2



10



5



10



4



10



3



10



2



10 2 10



1



10



3



N



10



4



Figure 5: The median number of flips needed by WalkSAT to solve the two types of formulas at r = 8, above the range where 1-hidden formulas are harder. At these densities, 2-hidden formulas are again harder than 1-hidden ones, although both are much easier than at densities closer to the threshold.



We believe that random 2-hidden instances could make excellent satisfiable benchmarks, especially just around the satisfiability threshold, say at r = 4.25 where they appear to be the hardest for WalkSAT (although beating SP requires somewhat higher densities). Several aspects of our experiments suggest exciting directions for further work, including: 1. Proving that the expected running time of natural Davis-Putnam algorithms on 2hidden formulas is exponential in n for r above some critical density. 2. Explaining the different threshold behaviors of SP on 1-hidden and 2-hidden formulas. 3. Understanding how long WalkSAT takes at the midpoint between the two hidden assignments, before it becomes sufficiently unbalanced to converge to one of them. 4. Studying random 2-hidden formulas in the dense case where the number of clauses grows more than linearly in n.



References

Achlioptas, D. (2001). Lower bounds for random 3-SAT via differential equations. Theor. Comp. Sci., 265, 159-185. Achlioptas, D., Beame, P., & Molloy, M. (2001). A sharp threshold in proof complexity. In Proc. STOC, pp. 337-346. Achlioptas, D., Gomes, C., Kautz, H., & Selman, B. (2000). Generating satisfiable problem instances. In Proc. AAAI, pp. 256-261.

637



Achlioptas, Jia, & Moore



Achlioptas, D., & Moore, C. (2002a). Almost all graphs with average degree 4 are 3colorable. In Proc. STOC, pp. 199-208. Achlioptas, D., & Moore, C. (2002b). The asymptotic order of the random k -SAT threshold. In Proc. FOCS, pp. 779-788. Achlioptas, D., & Moore, C. (2005). Two moments suffice to cross a sharp threshold. In SIAM J. Comput. To appear. Asahiro, Y., Iwama, K., & Miyano, E. (1996). Random generation of test instances with controlled attributes. DIMACS Series in Disc. Math. and Theor. Comp. Sci., 26, 377-393. Barthel, W., Hartmann, A., Leone, M., Ricci-Tersenghi, F., Weigt, M., & Zecchina, R. (2002). Hiding solutions in random satisfiability problems: A statistical mechanics approach. Phys. Rev. Lett., 88 (188701). Chao, M., & Franco, J. (1986). Probabilistic analysis of two heuristics for the 3-satisfiability problem. SIAM J. Comput., 15 (4), 1106-1118. Cheeseman, P., Kanefsky, R., & Taylor, W. (1991). Where the really hard problems are. In Proc. IJCAI, pp. 163-169. Cocco, S., & Monasson, R. (2001a). Statistical physics analysis of the computational complexity of solving random satisfiability problems using backtrack algorithms. Eur. Phys. J. B, 22, 505-531. Cocco, S., & Monasson, R. (2001b). Trajectories in phase diagrams, growth processes and computational complexity: how search algorithms solve the 3-satisfiability problem. Phys. Rev. Lett, 86, 1654-1657. Du, D., Gu, J., & Pardalos, P. (1997). Dimacs workshop on the satisfiability problem, 1996. In DIMACS Discrete Math. and Theor. Comp. Sci., Vol. 35. AMS. Hajiaghayi, M., & Sorkin, G. (2003). The satisfiability threshold for random 3-SAT is at least 3.52.. Hogg, T., Huberman, B., & Williams, C. (1996). Phase transitions and complexity. Artificial Intelligence, 81. Special issue. Johnson, D., & Trick, M. (1996). Second dimacs implementation challenge, 1993. In DIMACS Series in Disc. Math. and Theor. Comp. Sci., Vol. 26. AMS. Johnson, D., Aragon, C., McGeoch, L., & Shevon, C. (1989). Optimization by simulated annealing: an experimental evaluation. Operations Research, 37 (6), 865-892. Kaporis, A., Kirousis, L., & Lalas, E. (2003). Selecting complementary pairs of literals. In Proc. LICS Workshop on Typical Case Complexity and Phase Transitions. Kautz, H., Ruan, Y., Achlioptas, D., Gomes, C., Selman, B., & Stickel, . (2001). Balance and filtering in structured satisfiable problems. In Proc. IJCAI, pp. 351-358. Kirousis, L., Kranakis, E., Krizanc, D., & Stamatiou, Y. (1998). Approximating the unsatisfiability threshold of random formulas. Random Structures Algorithms, 12 (3), 253-269.

638



Hiding Satisfying Assignments: Two are Better than One



Li, C., & Anbulagan (1997a). Heuristics based on unit propagation for satisfiability problems. In Proc. IJCAI, pp. 366-371. Li, C., & Anbulagan (1997b). Look-ahead versus look-back for satisfiability problems. In Proc. 3rd Intl. Conf. on Principles and Practice of Constraint Programming, pp. 341- 355. Massacci, F. (1999). Using walk-SAT and rel-SAT for cyptographic key search. In Proc. IJCAI, pp. 290-295. M ezard, M., & Zecchina, R. (2002). Random k -satisfiability: from an analytic solution to a new efficient algorithm. Phys. Rev. E, 66. Available at: http://www.ictp.trieste.it/zecchina/SP/. Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT problems. In Proc. AAAI, pp. 459-465. Monasson, R. (2005). Average case analysis of DPLL for random decision problems. In Proc. RANDOM. Morris, P. (1993). The breakout method for escaping from local minima. In Proc. AAAI, pp. 40-45. Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: engineering an efficient SAT solver. In Proc. 38th Design Automation Conference, pp. 530-535. Selman, B., Kautz, H., & Cohen, B. (1996). Local search strategies for satisfiability testing. In Proc. 2nd DIMACS Challange on Cliques, Coloring, and Satisfiability. Shaw, P., Stergiou, K., & Walsh, T. (1998). Arc consistency and quasigroup completion. In Proc. ECAI, workshop on binary constraints. Van Gelder, A. (1993). Problem generator mkcnf.c. In Proc. DIMACS. Challenge archive. Wormald, N. (1995). Differential equations for random processes and random graphs. Ann. Appl. Probab., 5 (4), 1217-1235.



639



Weighted Clustering

Margareta Ackerman, Shai Ben-David, Simina Branzei, and David Loker University of Waterloo D.R.C. School of Computer Science {mackerma, shai, sbranzei, dloker}@uwaterloo.ca

Abstract In this paper we investigate clustering in the weighted setting, in which every data point is assigned a real valued weight. We conduct a theoretical analysis on the influence of weighted data on standard clustering algorithms in each of the partitional and hierarchical settings, characterising the precise conditions under which such algorithms react to weights, and classifying clustering methods into three broad categories: weight-responsive, weight-considering, and weight-robust. Our analysis raises several interesting questions and can be directly mapped to the classical unweighted setting.



1



Introduction



We consider a natural generalisation of the classical clustering problem, where every data point is associated with a real valued weight. This generalisation enables more accurate representation of some clustering problems. For example, consider vector quantification that aims to find a compact encoding of signals that has low expected distortion. The accuracy of the encoding is most important for signals that occur frequently. With weighted data, such a consideration is easily captured by having the weights of the points represent signal frequency. Another illustration of the utility of weights comes from facility allocation, such as the placement of police stations in a new district. The distributions of the stations should enable quick access to most areas in the district. However, the accessibility of different institutions to a station may have varying importance. The weighted setting enables a convenient method for prioritising certain landmarks over others. In this paper, we analyse the behaviour of clustering algorithms on weighted data. Given a data set and a clustering algorithm, we are interested in understanding how the resulting clustering changes depending on the underlying weights. We classify clustering algorithms into three categories: those that are affected by weights on all data sets, those that ignore weights, and those methods that respond to weights on some configurations of the data but not on others. Among the methods that always respond to weights are several well-known algorithms, such as k -means and k -median. On the other hand, algorithms such as single-linkage, complete-linkage, and min-diameter ignore weights. Perhaps the most notable is the last category of algorithms. We find that methods belonging to this category are robust to weights when data is sufficiently clusterable, and respond to weights otherwise. The average-linkage algorithm as well as the well-known spectral objective function, ratio cut, both fall within this category. We characterise the precise conditions under which these methods are influenced by weights. Our analysis also reveals the following interesting phenomenon: algorithms that are known to perform well in practice (in the classical, unweighted setting), tend to be more responsive to weights. For example, k-means is highly responsive to weights while single linkage, which often performs poorly in practice [7], is weight robust.



2



Related Work



Clustering algorithms are usually analysed in the context of unweighted data. The only related work that we are aware of is from the early 1970s. Fisher and Van Ness [6] introduce several properties of clustering algorithms. 1



Among these, they mention "point proportion admissibility", which requires that the output of an algorithm should not change if points are duplicated. They then observe that a few algorithms are point proportion admissible. However, clustering algorithms can display a much wider range of behaviours on weighted data than merely satisfying or failing to satisfy point proportion admissibility. We carry out a much more extensive analysis of clustering on weighted data, characterising the precise conditions under which algorithms respond to weight. In addition, Wright [14] proposes a formalisation of cluster analysis consisting of eleven axioms. In two of these axioms, the notion of mass is mentioned. Namely, that points with zero mass can be treated as non-existent, and that multiple points with mass at the same location are equivalent to one point whose weight is the sum of these masses. The idea of mass has not been developed beyond the statements of these axioms in their work.



3



Background



A weight function w over X is a function w : X  R+ . Given a domain set X , denote the corresponding weighted domain by w[X ], thereby associating each element x  X with weight w(x). A distance function is a symmetric function d : X x X  R+  {0}, such that d(x, y ) = 0 if and only if x = y . We consider weighted data sets of the form (w[X ], d), where X is some finite domain set, d is a distance function over X , and w is a weight function over X. A k-clustering C = {C1 , C2 , . . . , Ck } of a domain set X is a partition of X into 1 < k < |X | disjoint, non-empty subsets of X where i Ci = X . A clustering of X is a k -clustering for some 1 < k < |X |. To avoid trivial partitions, clusterings that consist of a single cluster, or where every cluster has a unique element, are not permitted. Denote the weight of a cluster Ci  C by w(Ci ) = xCi w(x). For a clustering C , let |C | denote the number of clusters in C . For x, y  X and clustering C of X , write x C y if x and y belong to the same cluster in C and x C y , otherwise. A partitional clustering algorithm is a function that maps a data set (w[X ], d) and an integer 1 < k < |X | to a k -clustering of X . A dendrogram D of X is a pair (T, M ) where T is a binary rooted tree and M : leaves(T )  X is a bijection. A hierarchical clustering algorithm is a function that maps a data set (w[X ], d) to a dendrogram of X . A set C0  X is a cluster in a dendrogram D = (T, M ) of X if there exists a node x in T so that C0 = {M (y ) | y is a leaf and a descendent of x}. For a hierarchical algorithm A, A(w[X ], d) outputs a clustering C = {C1 , . . . , Ck } if Ci is a cluster in A(w[X ], d) for all 1  i  k . A partitional algorithm A outputs clustering C on (w[X ], d) if A(w[X ], d, |C |) = C . Given a clustering algorithm A and a data set (X, d), range(A(X, d)) = {C | w such that A outputs C on (w[X ], d)}, which is the set of clusterings that A outputs on (X, d) over all possible weight functions.



4



Basic Categories



Different clustering algorithms respond differently to weights. We introduce a formal categorisation of clustering algorithms based on their response to weights. First, we define what it means for a partitional algorithm to be weight responsive on a clustering. We present an analogous definition for hierarchical algorithms in Section 6. Definition 1 (Weight responsive). A partitional clustering algorithm A is weight-responsive on a clustering C of (X, d) if 1. there exists a weight function w so that A(w[X ], d) = C , and 2. there exists a weight function w so that A(w [X ], d) = C . Weight-sensitive algorithms are weight-responsive on all clusterings in their range. Definition 2 (Weight Sensitive). An algorithm A is weight-sensitive if for all (X, d) and all C  range(A(X, d)), A is weight-responsive on C . At the other extreme are clustering algorithms that do not respond to weights on any data set. This is the only category that has been considered in previous work, corresponding to "point proportion admissibility"[6]. 2



Definition 3 (Weight Robust). An algorithm A is weight-robust if for all (X, d) and all clusterings C of (X, d), A is not weight-responsive on C . Finally, there are algorithms that respond to weights on some clusterings, but not on others. Definition 4 (Weight Considering). An algorithm A is weight-considering if * There exists an (X, d) and a clustering C of (X, d) so that A is weight-responsive on C . * There exists an (X, d) and C  range(A(X, d)) so that A is not weight-responsive on C . To formulate clustering algorithms in the weighted setting, we consider their behaviour on data that allows for duplicates. Given a data set (X, d), elements x, y  X are duplicates if d(x, y ) = 0 and d(x, z ) = d(y, z ) for all z  X . In a Euclidean space, duplicates correspond to elements that occur at the same location. We obtain the weighted version of a data set by de-duplicating the data, and associating every element with a weight equaling the number of duplicates of that element in the original data. The weighted version of an algorithm partitions the resulting weighted data in the same manner that the unweighted version partitions the original data. As shown throughout the paper, this translation leads to natural formulations of weighted algorithms.



5



Partitional Methods



In this section, we show that partitional clustering algorithms respond to weights in a variety of ways. We show that many popular partitional clustering paradigms, including k -means, k -median, and min-sum, are weight sensitive. It is easy to see that methods such as min-diameter and k -center are weight-robust. We begin by analysing the behaviour of a spectral objective function ratio cut, which exhibits interesting behaviour on weighted data by responding to weight unless data is highly structured.



5.1



Ratio-Cut Spectral Clustering



We investigate the behaviour of a spectral objective function, ratio-cut [13], on weighted data. Instead of a distance function, spectral clustering relies on a similarity function, which maps pairs of domain elements to non-negative real numbers that represent how alike the elements are. The ratio-cut of a clustering C is rcut(C, w[X ], s) = 1 2

i xCi ,y C Ci C



s(x, y ) * w(x) * w(y )

xCi



w ( x)



.



The ratio-cut clustering function is rcut(w[X ], s, k ) = arg minC ;|C |=k rcut(C, w[X ], s). We prove that this function ignores data weights only when the data satisfies a very strict notion of clusterability. To characterise precisely when ratio-cut responds to weights, we first present a few definitions. A clustering C of (w[X ], s) is perfect if for all x1 , x2 , x3 , x4  X where x1 C x2 and x3 C x4 , s(x1 , s2 ) > s(x3 , x4 ). C is separation-uniform if there exists  so that for all x, y  X where x C y , s(x, y ) = . Note that neither condition depends on the weight function. We show that whenever a data set has a clustering that is both perfect and separation-uniform, then ratio-cut uncovers that clustering, which implies that ratio-cut is not weight-sensitive. Note that these conditions are satisfied when all between-cluster similarities are set to 0. On the other hand, we show that ratio-cut does respond to weights when either condition fails. Lemma 1. Given a clustering C of (X, s) where every cluster has more than one point, if C is not separation-uniform then ratio-cut is weight-responsive on C . Proof. We consider a few cases. Case 1: There is a pair of clusters with different similarities between them. Then there exist C1 , C2  C , x  C1 , and y  C2 so that s(x, y )  s(x, z ) for all z  C2 , and there exists a  C2 so that s(x, y ) > s(x, a). 3



Let w be a weight function such that w(x) = W for some sufficiently large W and weight 1 is assigned to all other points in X . Since we can set W to be arbitrarily large, when looking at the cost of a cluster, it suffices to consider the dominant term in terms of W . We will show that we can improve the cost of C by moving a point from C2 to C1 . Note that moving a point from C2 to C1 does not affect the dominant term of clusters other than C1 and C2 . Therefore, we consider the cost of these two clusters before and after rearranging points between these clusters. A , Let A = aC2 s(x, a) and let m = |C2 |. Then the dominant term, in terms of W , of the cost of C1 is W m which comes from the cost of points in. The cost of C2 approaches a constant as W  . Now consider clustering C obtained from C by moving y from cluster C2 to cluster C1 . The dominant term in s(x,y ) the cost of C1 becomes W A- m-1 , and the cost of C2 approaches a constant as W  . By choice of x and y , if

s(x,y ) A A <m then C has lower loss than C when W is large enough. A- <m holds whenever A < s(x, y )m, m-1 and the latter holds by choice of x and y . Case 2: For every pair of clusters, the similarities between them are the same. However, there are clusters C1 , C2 , C3  C , so that the similarities between C1 and C2 are greater than the ones between C1 and C3 . Let a denote the similarities between C1 and C2 , and b the similarities between C1 and C3 . Let x  C1 . Let w be a weight function such that w(x) = W for large W , and weight 1 is assigned to all other points in X . The dominant term comes from clusters going into C1 , specifically edges that include point x. The dominant term of the contribution of cluster C3 is W b and the dominant term of the contribution of C2 is W a, totalling W a + W b. Now consider clustering C obtained from clustering C by merging C1 with C2 , and splitting C3 into two clusters (arbitrarily). The dominant term of the clustering comes from clusters other than C1  C2 , and the cost of clusters outside C1  C2  C3 is unaffected. The dominant term of the cost of the two clusters obtained by splitting C3 is W b for each, for a total of 2W b. However, the factor of W a that C2 previously contributed is no longer present. Therefore, we replace the coefficient of the dominant term from a to b, which improved the cost of the clustering because b < a. A-s(x,y ) m-1



Lemma 2. Given a clustering C of (X, s) where every cluster has more than one element, if C is not perfect than ratio-cut is weight-responsive on C . The proof for the above lemma is included in the appendix. Lemma 3. Given any data set (w[X ], s) that has a perfect, separation-uniform k -clustering C , ratio-cut(w[X ], s, k ) = C. Proof. Let (w[X ], s) be a weighted data set, with a perfect, separation-uniform clustering C = {C1 , . . . , Ck }. Recall that for any Y  X , w(Y ) = yY w(y ). Then, 1 2

k xCi i=1 xCi w (x) y Ci



rcut(C, w[X ], s) =  = 2 =  2

k



s(x, y )w(x)w(y )

k



xCi w (x)



=



1 2

k



k xCi i=1 y Ci



w(x)w(y )



xCi w (x) k



y Ci w (y ) xCi k



i=1



w ( x) =



 = 2



i=1 y Ci



 w(y ) = 2



 w(Ci ) = 2 i=1



[w(X ) - w(Ci )]

i=1



kw(X ) -

i=1



w(Ci )



 (k - 1)w(X ). 2



Consider any other clustering, C = {C1 , . . . , Ck } = C . Since the perfect clustering is unique, there exists at least one pair x C y such that s(x, y ) > . Since s(x, y )  , for every x, y  X , the cost of C is, rcut(C , w[X ], s) =

1 2 k i=1

x C i y C i



s(x,y )w(x)w(y ) w(x)



>



x C i



1 2



k i=1



xC



i



y C xC i



w(x)w(y )

i



w ( x)



= 2 (k - 1)w (X ) =



rcut(C ). So clustering C has a higher cost than C . We can now characterise the precise conditions under which ratio-cut responds to weights. Ratio-cut responds to weights on all data sets but those where cluster separation is both very large and highly uniform. Formally, 4



Theorem 1. Given a clustering C of (X, s) where every cluster has more than one element, ratio-cut is weightresponsive on C if and only if either C is not perfect, or C is not separation-uniform. Proof. The result follows by Lemma 1, Lemma 2, and Lemma 3.



5.2 K -Means

Many popular partitional clustering paradigms, including k -means, k -median, and min-sum, are weight sensitive. Moreover, these algorithms satisfy a stronger condition. By modifying weights, we can make these algorithms separate any set of points. We call such algorithms weight-separable. Definition 5 (Weight Separable). A partitional clustering algorithm A is weight-separable if for any data set (X, d) and any S  X , where 2  |S |  k , there exists a weight function w so that x A(w[X ],d,k) y for all disjoint pairs x, y  S . Note that every weight-separable algorithm is also weight-responsive. Lemma 4. If a clustering algorithm A is weight-separable, then A is weight-responsive. Proof. Given any (w[X ], d), let C = A(w[X ], d, k ). Select points x and y where x C y . Since A is weightseparable, there exists w so that x A(w [X ],d,k) y , and so A(w [X ], d, k ) = C. K -means is perhaps the most popular clustering objective function, with cost k -means(C, w[X ], d) =

Ci C x,y Ci



d(x, y )2 * w(x) * w(y ) w(Ci )



,



where w(Ci ) = xCi w(x)1 . The k -means algorithm outputs a clustering with minimal k -means cost. We show that k -means is weight-separable, and thus also weight-sensitive. Theorem 2. K -means is weight-separable. Proof. Consider any S  X . Let w be a weight function over X where w(x) = W if x  S , for large W , and w(x) = 1 otherwise. Let m1 = minx,yX d(x, y )2 > 0, m2 = maxx,yX d(x, y )2 , and n = |X |. Consider any k -clustering 2 C where all the elements in S belong to distinct clusters. Then k -means(C, w[X ], d) < km2 (n + n W ). On the other hand, given any k -clustering C where at least two elements of S appear in the same cluster, k -means(C , w[X ], d)  k-means(C ,w[X ],d) W 2 m1 W +n . Since limW  k-means(C,w[X ],d) = , k -means separates all the elements in S for large enough W . The following result holds using a similar argument. Theorem 3. Min-sum, which minimises the objective function separable.

Ci C x,y Ci



d(x, y ) * w(x) * w(y ), is weight-



It can also be shown that a few other algorithms similar to k -means, namely k -median and k -mediods are also weight-separable. The details appear in the appendix. Observe that all of these popular objective functions are highly responsive to weight.

1 Note that this formulation is equivalent to the common formulation that relies on centers of mass [10], however that formulation applies only over normed vector spaces.



5



6



Hierarchical Algorithms



Similarly to partitional methods, hierarchical algorithms also exhibit a wide range of responses to weights. We show that Ward's method, a successful linkage-based algorithm, as well as popular divisive heirarchical methods, are weight sensitive. On the other hand, it is easy to see that the linkage-based algorithms single-linkage and complete-linkage are both weight robust, as was observed in [6]. Average-linkage, another popular linkage-based method, exhibits more nuanced behaviour on weighted data. When a clustering satisfies a reasonable notion of clusterability, then average-linkage detects that clustering irrespective of weights. On the other hand, this algorithm responds to weights on all other clusterings. We note that the notion of clusterability required for average-linkage is a lot weaker than the notion discussed in Section 5.1, where it is used to characterise the behaviour of ratio-cut on weighted data. Hierarchical algorithms output dendrograms, which contain multiple clusterings. Please see the preliminary section for definitions relating to the hierarchical setting. Weight-responsive for hierarchical algorithms is defined analogously to Definition 1. Definition 6 (Weight responsive). A clustering algorithm A is weight-responsive on a clustering C of (X, d) if (1) there exists a weight function w so that A(w[X ], d) outputs C , and (2) there exists a weight function w so that A(w [X ], d) does not output C . Weight-sensitive, weight-considering, and weight-robust are defined as for partitional algorithms in Section 4, with the above definition for weight-responsive.



6.1



Average Linkage



Linkage-based algorithms start off by placing every element in its own cluster, and proceed by repeatedly merging the "closest" pair of clusters until the entire dendrogram is constructed. To identify the closest clusters, these algorithms use a linkage function that maps pairs of clusters to a real number. Formally, a linkage function is a function : {(X1 , X2 , d, w) | d, w over X1  X2 }  R+ . Average-linkage is one of the most popular linkage-based algorithms (commonly applied in bioinformatics under the name UPGMA). Recall that w(X ) = xX w(x). The average-linkage linkage function is

AL (X1 , X2 , d, w )



=



xX1 ,y X2



d(x, y ) * w(x) * w(y )



w(X1 ) * w(X2 )



.



To study how average-linkage responds to weights, we present a relaxation of the notion of a perfect clustering. Definition 7 (Nice). A clustering C of (w[X ], d) is nice if for all x1 , x2 , x3  X where x1 C x2 and x1 C x3 , d(x1 , x2 ) < d(x1 , x3 ). Data sets with nice clusterings correspond to those that satisfy the "strict separation" property introduced by Balcan et al. [3]. As for a perfect clustering, being a nice clustering is independent of weights. We present a complete characterisation of the way that average-linkage (AL) responds to weights, showing that it ignores weights on nice clusterings, but responds to weights on all other clusterings. Theorem 4. For any data set (X, d) and clustering C  range(AL(X, d)), average-linkage is weight robust on clustering C if and only if C is a nice clustering. The proof of Theorem 4 follows from the two lemmas below. Lemma 5. If a clustering C = {C1 , . . . , Ck } of (X, d) is not nice, then either C  range(AL(X, d)) or averagelinkage is weight-responsive on C . Proof. Assume that there exists some w so that C  AL(w[X ], d). If it does not exist then we are done. We construct w so that C  AL(w [X ], d).



6



Since C is not nice, there exist 1  i, j  k , i = j , and x1 , x2  Ci , x1 = x2 , and x3  Cj , so that d(x1 , x2 ) > d(x1 , x3 ). Now, define weigh function w as follows: w (x) = 1 for all x  X \ {x1 , x2 }, and w (x1 ) = w (x2 ) = W , for some large value W . We argue that when W is sufficiently large, C is not a clustering in AL(w [X ], d). By way of contradiction, assume that C is a clustering in AL(w [X ], d) for any weight function w . Then there is a step in the algorithm where clusters X1 and X2 merge, where X1 , X2  Ci , x1  X1 , and x2  X2 . At this point, there is some cluster X3  Cj so that x3  X3 . 2 (x1 ,x2 )+1 W +2 , for some We compare AL (X1 , X2 , d, w ) and AL (X1 , X3 , d, w ). AL (X1 , X2 , d, w ) = W dW 2 + W + 3 4

(x1 ,x3 )+1 W +2 for some non-negative real i s. non-negative real i s. Similarly, AL (X1 , X3 , d, w ) = W dW 2 + W + 3 4 2 Dividing both sides by W , we see that AL (X1 , X3 , d, w )  d(x1 , x3 ) and AL (X1 , X2 , d, w )  d(x1 , x2 ) as W  , and so the result holds since d(x1 , x3 ) < d(x1 , x2 ). Therefore average linkage merges X1 with X3 , so cluster Ci is never formed, and so C is not a clustering in AL(w [X ], d).

2



Finally, average-linkage outputs all nice clusterings present in a data set, regardless of weights. Lemma 6. Given any weighted data set (w[X ], d), if C is a nice clustering of (X, d), then C is in the dendrogram produced by average-linkage on (w[X ], d). Proof. Consider a nice clustering C = {C1 , . . . , Ck } over (w[X ], d). It suffices to show that for any 1  i < j  k , X1 , X2  Ci where X1  X2 =  and X3  Cj , AL (X1 , X2 , d, w) < AL (X1 , X3 , d, w). We have the following inequalities:

x1 X1 [w (x1 )*maxx2 X2



AL (X1 , X2 , d, w )





2



d(x1 ,x2 ) w(X1 )*w(X2 )



x2 X2



w(x2 )]



= and



x2 X2



w(x2 ) x X [w(x1 )*maxx2 X2 d(x1 ,x2 )] 1 1 w(X1 )* x X w(x2 )

2



=



x1 X1



w(x1 )*maxx2 X2 d(x1 ,x2 ) w(X1 )



AL (X1 , X3 )







x 1  X1



w(x1 ) * minx3 X3 d(x1 , x3 ) w(X1 ) * w(X3 )



x3 X3



w(x3 )



=



x1 X1



w(x1 ) * minx3 X3 d(x1 , x3 ) w(X1 ) >

AL (X1 , X2 ).



Since C is nice, minx3 X3 d(x1 , x3 ) > maxx2 X2 d(x1 , x2 ), and so



AL (X1 , X3 )



6.2



Ward's Method



Ward's method is a highly effective clustering algorithm [5], which, at every step, merges the clusters that will yield the minimal increase to the k-means cost. Let ctr(X, d, w) be the center of mass of the data set (w[X ], d). Then, the linkage function for Ward's method is

W ard (X1 , X2 , d, w )



=



w(X1 ) * w(X2 ) * d(ctr(X1 , d, w), ctr(X2 , d, w))2 w(X1 ) + w(X2 )



Theorem 5. Ward's method is weight sensitive. The proof is included in the appendix.



6.3



Divisive Algorithms



The class of divisive clustering algorithms is a well-known family of hierarchical algorithms, which construct the dendrogram by using a top-down approach. This family of algorithms includes the popular bisecting k-means algorithm. We show that a class of algorithms that includes bisecting k-means consists of weight-sensitive methods. 7



Given a node x in dendrogram (T, M ), let C (x) denote the cluster represented by node x. Formally, C (x) = {M (y ) | y is a leaf and a descendent of x}. Informally, a P -Divisive algorithm is a hierarchical clustering algorithm that uses a partitional clustering algorithm P to recursively divide the data set into two clusters until only single elements remain. Formally, Definition 8 (P -Divisive). A hierarchical clustering algorithm A is P -Divisive with respect to a partitional clustering algorithm P , if for all (X, d), we have A(w[X ], d) = (T, M ), such that for all non-leaf nodes x in T with children x1 and x2 , P (w[C (x)], d, 2) = {C (x1 ), C (x2 )}. We obtain bisecting k -means by setting P to k -means. Other natural choices for P include min-sum, and exemplarbased algorithms such as k -median. As shown in Section 5, many of these partitional algorithms are weight-separable. We show that whenever P is weight-separable, then P -Divisive is weight-sensitive. The proof of the following theorem appears in the appendix. Theorem 6. If P is weight-separable then the P -Divisive algorithm is weight-sensitive. Partitional k -means, k -medoids k -median, Min-sum Ratio-cut Min-diameter k -center Hierarchical Ward's method Bisecting k -means Average-linkage Single-linkage Complete-linkage



Weight Sensitive Weight Considering Weight Robust



Table 1: A classification of clustering algorithms based on their response to weighted data.



7



Discussion and Future Work



In this paper we investigated several classical algorithms, belonging to each of the partitional and hierarchical settings, and characterised the exact conditions under which they respond to weights. Our results are summarised in Table 1. We note that all of our results immediately translate to the standard setting, by mapping each point with integer weight to the same number of unweighted duplicates. In particular, we proved precisely when the weight considering methods, average-linkage and ratio-cut, respond to weights. It is interesting to note that the response of these weight considering techniques is substantially different. Ratio cut ignores weights only on data that is exceptionally well-structured, having large and highly uniform cluster separation. Yet average linkage requires a much weaker condition, finding all clusterings where data are closer to other elements in their partition than to data outside their cluster. Intuitively, average linkage uses weights as a secondary source of information, relying on them only when the clustering structure is ambiguous. There are a number of interesting avenues for future investigation. A compelling question left open is to understand the correlation between the weight responsiveness of an algorithm and the quality of clusterings that it produces in the classical setting. As an example, observe that many notable algorithms, such as k -means and spectral methods, respond to weights, while less used approaches, such as single linkage, never do. It would also be interesting to perform a quantitative analysis to measure the exact degree of responsiveness to weights, which may lead to a more fine grained classification of these algorithms. In addition, it remains to be determined how the approximations used in practice, such as spectral clustering heuristics and the Lloyd method, behave on weighted data. Our preliminary work on these heuristics lends further support to the hypothesis that the more commonly applied algorithms are also more responsive to weights.



8



References

[1] P. K. Agarwal and C. M. Procopiuc. Exact and approximation algorithms for clustering. In SODA, 1998. [2] D. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding. In SODA, 2007. [3] M. F. Balcan, A. Blum, and S. Vempala. A discriminative framework for clustering via similarity functions. In STOC, 2008. [4] S. Dasgupta and P. M. Long. Performance guarantees for hierarchical clustering. J. Comput. Syst. Sci., 70(4):555- 569, 2005. [5] B. S. Everitt. Cluster Analysis. John Wiley & Sons Inc, 1993. [6] L. Fisher and J. Van Ness. Admissible clustering procedures. Biometrika, 58:91-104, 1971. [7] J. Hartigan. Consistency of single linkage for high-density clusters. J. Amer. Statist. Assoc., 76(374):388-394, 1981. [8] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Comput. Surv., 31(3):264-323, 1999. [9] L. Kaufman and P. J. Rousseeuw. Partitioning Around Medoids (Program PAM), pages 68-125. John Wiley & Sons, Inc., 2008. [10] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy. The effectiveness of Lloyd-type methods for the k-means problem. In FOCS, 2006. [11] M. Talagrand. A new look at independence. Ann. Probab., 24(1):1-34, 1996. [12] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. [13] U. Von Luxburg. A tutorial on spectral clustering. J. Stat. Comput., 17(4):395-416, 2007. [14] W. E. Wright. A formalization of cluster analysis. J. Pattern Recogn., 5(3):273-282, 1973.



9



Extracting Viewpoints from Knowledge Bases

IBM Corporation 11400 Burnet Road Austin, Texas 78758 acker@austin.ibm.com



Liane Acker



Department of Computer Sciences University of Texas at Austin Austin, Texas 78712 porter@cs.utexas.edu make consistent modeling assumptions (e.g.,the model fragments of (Falkenhainer & Forbus 1991), the views of (Forbus 1984), and the ontological perspectives of (Liu & Farley 1990).) Finally, KI (Murray & Porter 1989), a learning program, uses viewpoints to constrain its search for the consequences of adding new information to a knowledge base. Conventional methods for accessing knowledge bases do not provide direct access to viewpoints. Some methods extract individual facts, such as the ller of a particular frame-slot. Others extract collections of facts, such as all the slots and llers of a particular frame or those satisfying a Prolog-like query. Indisputably, these access methods can be used to extract viewpoints through a sequence of invocations. However, they ignore the central problem in extracting viewpoints: determining which facts to include in a viewpoint. The advantage of our access methods is that they provide a general solution to this problem (as described in Section 2), and the viewpoints extracted by our methods are comparable in coherence to those people construct (as described in Section 3). Our methods for accessing viewpoints are implemented in a program called the View Retriever (a term rst proposed by Suthers (Suthers 1988)). The input to this program is a viewpoint speci cation and the output is a collection of facts. The task of the View Retriever is to determine which facts constitute the speci ed viewpoint and to request them from the knowledge base. Whether the knowledge base returns cached facts or computes them (using deduction, abduction, or induction) is irrelevant to the View Retriever. Those facts that the knowledge base cannot provide are not included in the viewpoint. The View Retriever is used currently with the Botany Knowledge Base, a large system of over 13,000 frames and 160,000 cached facts, where a fact is a slotller of a frame. However, it is designed to work for any physical domain and to be easily extended to work in non-physical domains, such as those involving abstract concepts or mental processes.



Bruce Porter



Viewpoints are coherent collections of facts that describe a concept from a particular perspective. They are essential for a wide variety of tasks, such as explanation generation and qualitative modeling. We have identi ed many types of viewpoints and developed a program, the View Retriever, for extracting them from knowledge bases, either singly or in combinations. The View Retriever provides a general solution to the central problem in extracting viewpoints: determining which facts are relevant to requested viewpoints. Our evaluation indicates that viewpoints extracted by the View Retriever are comparable in coherence to those people construct.



Abstract



The objective of this research is to develop computational methods for extracting viewpoints from knowledge bases. Intuitively, a viewpoint is a coherent collection of facts that describes a concept from a particular perspective. For example, three viewpoints of the concept \car" are: the viewpoint \car as-kind-of consumer durable," which describes a car's price and longevity; the structural viewpoint, which describes a car's parts and their interconnections; and the viewpoint \car ashaving metal composition," which includes facts, such as a car's propensity to dent and rust, that are related to its composition. The need for viewpoints by knowledge-based programs is widespread. For example, many explanationgeneration systems require viewpoints to produce explanations that are complete and coherent (Suthers 1991; McKeown 1988; Lester & Porter 1991; McCoy 1989; Moore & Swartout 1988). Qualitative modeling systems use viewpoints to increase e ciency and to

Support for this research was provided by an IBM Graduate Fellowship to Liane Acker, a grant from the National Science Foundation (IRI-9120310), a contract from the Air Force O ce of Scienti c Research (F49620-93-10239), and donations from the Digital Equipment Corporation. This work was conducted at the University of Texas at Austin.



1 Introduction



2 The View Retriever



Production product location Substance energy source raw materials Thing producer Substance Oxygen energy source raw materials producer ATP Water Carbon-Dioxide

Carbon-Bond-Energy Photon Photosynthesis input energy form location Light-Energy



Energy-Transduction



input energy form Energy



location energy provider Thing Thing Energy output energy form



Thing



Photosynthesis product



Thing



output energy form energy provider



location Glucose Chloroplast



Chloroplast



Photosynthetic-Cell



Figure 1: The viewpoint of \photosynthesis as-kind-of production", as extracted from the Botany Knowledge Base by the View Retriever. The way a user (or application program) speci es a viewpoint and the way the View Retriever extracts it depends on the type of viewpoint. As-kind-of viewpoints describe concepts by relating them to more general concepts. Viewpoints constructed along basic dimensions describe concepts using a cluster of their attributes, such as functional, structural, or perceptual attributes. As-having viewpoints include the facts pertinent to a given attribute.



Figure 2: The viewpoint of \photosynthesis as-kind-of energy transduction", as extracted from the Botany Knowledge Base by the View Retriever. is more general than hslot; filleri if any of the following conditions hold: 1. slot = slot and filler is a generalization of filler. 2. filler = filler and slot is a generalization of slot. 3. slot is a generalization of slot and filler is a generalization of filler. For example, the viewpoint shown in Figure 1 contains the fact that photosynthesis produces glucose, because it is known that production processes typically produce some substance and glucose is a special kind of substance. That is, hproduct; Glucosei appears on the Photosynthesis frame, hproduct; Substancei appears on the Production frame, and Substance is a generalization of Glucose. The resulting viewpoint includes the links between facts about the primary concept and the more general facts about the reference concept (see Figure 1). The View Retriever excludes many facts about the primary concept from the viewpoint. For example, although it is true that photosynthesis converts light energy into carbon bond energy, this fact is excluded because it is irrelevant to our concept of production (although it would be included in \photosynthesis askind-of energy transduction", as shown in Figure 2). Various explanation-generation systems extract knowledge structures similar to as-kind-of viewpoints. The TEXT system (McKeown 1985) uses a function (called the identi cation rhetorical predicate ) to differentiate a concept from a more general concept. TEXT determines what facts to include using a type of knowledge called focus constraints: facts are selected incrementally based on their connection with previously selected facts, rather than a global coherence

0 0 0 0 0 0



As-kind-of Viewpoints



An as-kind-of viewpoint describes a concept in terms of a more general concept. For example, the viewpoint \photosynthesis as-kind-of production" consists of those facts that explain how photosynthesis is a special case of production, such as its raw materials and products. Figure 1 shows a portion of this viewpoint. The speci cation of an as-kind-of viewpoint is of the form: (hprimary concepti as-kind-of hreference concepti) where the primary concept is the one the viewpoint will be taken of and the reference concept is a generalization of the primary concept (although not necessarily an immediate generalization). The View Retriever extracts as-kind-of viewpoints by selecting relevant facts about the primary concept. A fact is a tuple of the form hslot; filleri; it is considered relevant if some more general fact appears on the frame for the reference concept. The fact hslot ; filler i

0 0



criteria. Suthers's system uses a genus-and-di erentia function similar to TEXT's identi cation predicate (Suthers 1991). McKeown's ADVISOR system constructs knowledge structures similar to as-kind-of viewpoints by restricting to prede ned partitions of the knowledge base the superconcepts from which a concept can inherit slot llers (McKeown 1988). In addition to viewpoints that describe concepts in terms of more general concepts, the View Retriever can extract viewpoints along basic dimensions, which are general types of facts, such as facts about an object's structure, function, or appearance. (We have borrowed the term from Metaphors We Live By (Lako & Johnson 1980), a work that has signi cantly in uenced our characterization of viewpoint types.) Below we describe the basic dimensions used by the View Retriever. Basic dimensions for objects: Structural, which includes the parts or substances that make up the object. It also includes the connections and spatial relations among them, what we call interconnection relations. The structural dimension also includes the relative sizes or number of the parts. Perceptual, which includes information regarding how humans perceive (see, hear, etc.) the object. This includes the shape, symmetry, size, color, and temperature of the object. Functional, which includes what the object \does" (the processes in which it is an actor). The functional dimension also includes properties suggestive of some unspeci ed process in which the object is involved, such as life span and metabolic rate. Temporal, which includes the temporal parts of an object (its stages or states). It also includes as interconnection relations the temporal ordering constraints among the stages or states. Basic dimensions for processes: Behavioral, which includes the types and roles of the actors in the process and the changes that the process e ects upon them. Initial and nal conditions of the process are included as well. Procedural, which includes the steps (subevents) of the process and (as interconnection relations) any temporal ordering constraints that exist among the steps. Basic dimensions for both objects and processes: Taxonomic, which includes the taxonomic breakdown of a class of objects or processes into subclasses. The taxonomic dimension also includes the relative sizes of the subclasses, the criteria for the breakdown, and (as interconnection relations) information about which subclasses are disjoint.



Viewpoints Constructed Along Basic Dimensions



how one object or process a ects other objects or processes. This includes causal relationships (e.g.,causes, enables, prevents, facilitates) and qualitative in uences between quantities (e.g.,directlya ects, inversely-in uences, correlated-with). The speci cation for a viewpoint constructed along a basic dimension simply names the primary concept and the basic dimension desired: (hprimary concepti dimension hbasic dimensioni) The View Retriever constructs the viewpoint rst by extracting facts about the primary concept that belong to the basic dimension, then by adding to the viewpoint any interconnection relations for the basic dimension. For example, to construct a structural viewpoint of a plant seed, the View Retriever rst selects those slots and llers from the Seed frame that belong to the structural dimension, including hpart, Seed-Coati, hpart, Embryoi, and hpart, Endospermi. The View Retriever then selects interconnection relations among the selected parts (seed coat, embryo, and endosperm). For the structural dimension, interconnection relations include connected-to, contains, surrounds, etc. Thus, the resulting viewpoint contains the information that the seed is made up of a seed coat containing an embryo and an endosperm. To construct viewpoints along basic dimensions, the View Retriever uses knowledge of which slots in the knowledge base are within each dimension. Based on our experience with the Botany Knowledge Base, this knowledge is easily encoded because the distinctions made by the basic dimensions are re ected in the top levels of the slot hierarchy. Viewpoints created by the View Retriever along basic dimensions are similar to perspectives as suggested by Suthers (Suthers 1991) and as used by Romper (McCoy 1989). Unlike our basic dimensions, however, Romper's perspectives are domain-speci c and include only facts about the primary concept; interconnection relations are omitted.



Modulatory, which includes information about



As-Having Viewpoints



An as-having viewpoint contains all and only the information about a concept that is relevant to some speci ed fact about the concept. Its speci cation has the following form: (hprimary concepti as-having hslot, lleri) To our knowledge, general methods do not exist for extracting as-having viewpoints. Therefore, unlike for the other types of viewpoints, the View Retriever depends on a priori knowledge of relevance to select the facts that constitute as-having viewpoints. To construct an as-having viewpoint, the View Retriever rst looks for a cached as-having viewpoint that is based on the same fact (slot and ller), or a more general fact, as the requested viewpoint, but with a



di erent primary concept. For example, to extract the viewpoint: (Squirrel as-having hagent-in, Seed-Dispersali) the View Retriever rst looks in the knowledge base for a related, cached viewpoint such as one of the following: 1. (Animal as-having hagent-in, Seed-Dispersali) 2. (Bird as-having hagent-in, Seed-Dispersali) 3. (Animal as-having hagent-in, Transportationi) If a related viewpoint is found, the View Retriever uses it to determine which facts should be included in the new viewpoint. It does this by nding for each fact of the cached viewpoint a corresponding fact that is true of the primary concept of the new viewpoint. If the primary concept of the cached viewpoint is a generalization of the primary concept of the new viewpoint, then nding corresponding facts between the two consists of nding facts about the primary concept of the new viewpoint that are specializations of facts in the cached viewpoint. If the primary concepts of the two viewpoints are siblings , then nding corresponding facts between the two is more di cult. It requires nding pairs of facts that share a common abstraction. If a related, cached viewpoint cannot be found in the knowledge base, then the View Retriever constructs ashaving viewpoints by collecting all the facts about the primary concept that are implied by the speci ed fact, using all the inference rules and mechanisms available in the knowledge base. This method assumes (sometimes incorrectly) that any fact implied by some other fact is relevant to it. However, it has the advantage that it does not require viewpoints to be cached in the knowledge base. Ideally, as-having viewpoints would be extracted using a theory of relevance to determine what facts are relevant. As a rst step toward such a theory, several researchers have analyzed texts to determine the various ways that one fact may be relevant to another (Mann & Thompson 1987; Hobbs 1985). However, these theories are as yet descriptive rather than prescriptive, so the View Retriever cannot use them directly.



Angiosperm-Sexual-Reproduction



location Pollen-Grain Formation



subevents



Flower



location location



Embryo-Sac Formation



has- parts



source Androecium destination location location



Pollen-Grain Transfer



surrounds



Pollen-Grain Germination



Gynoecium



Double Fertilization



Figure 3: The composite (\structural-functional") viewpoint of a ower in its role in plant reproduction, as extracted from the Botany Knowledge Base by the View Retriever. where viewpoint1 and viewpoint2 are individual viewpoints (or speci cations for them) and relation species the correspondence to be established between the viewpoints. One commonly used composite viewpoint, called \structural-functional", describes the roles an object (and its parts) play in an event (and its subevents). Its speci cation is the following: (composite (hobjecti dimension structural) (heventi dimension procedural) actor-in) For example, the viewpoint that describes the roles of a ower's parts in the steps of plant reproduction is speci ed as follows: (composite (Flower dimension structural) (Plant-Reproduction dimension procedural) actor-in) Its contents are shown in Figure 3. The View Retriever constructs this composite viewpoint by the following procedure. First it extracts the two individual viewpoints (the structural viewpoint of Flower and the procedural viewpoint of PlantReproduction). Then it determines which parts of the Flower that are in the structural viewpoint are related to Plant-Reproduction or one of its subevents (as given in the procedural viewpoint) by an actor-in relation or some more speci c relation (such as location-of ). Those parts, such as the Flower's corolla, that are not actors in the event are omitted from the composite viewpoint. Similarly, those subevents, such as FruitRipening, that do not involve any of the parts in the structural viewpoint of Flower are omitted. This procedure can extract diverse viewpoints. For example, the composite viewpoint that describes the parts of a plant ovary as related to the parts of the fruit of which it is a developmental stage can be extracted with the following speci cation:



Composite Viewpoints



In addition to extracting individual viewpoints as described above, the View Retriever can combine them to form composite viewpoints. This involves more than simply concatenating the contents of two individual viewpoints; it involves putting them into correspondence with one another and removing the portions that do not correspond. Despite the apparent utility of composite viewpoints, we know of no other general methods for extracting them from knowledge bases. The speci cation for a composite viewpoint has the following form: (composite hviewpoint1i hviewpoint2i hrelationi)



Fruit stage has-parts



Pericarp Ovary stage has-parts Ovarian-Wall stage



Seed



Ovule



Figure 4: The composite viewpoint of the parts of a plant ovary as related to the parts of the fruit of which it is a developmental stage, as extracted from the Botany Knowledge Base by the View Retriever. (composite (Fruit dimension structural) (Ovary dimension structural) stages) This composite viewpoint, as shown in Figure 4, includes the parts of the fruit (seed, pericarp, etc.), the parts of the ovary (ovule, ovarian wall, etc.), and the stage relations between them, such as the facts that the ovule is a developmental stage of the seed and the ovarian wall is a developmental stage of the pericarp. The procedure for constructing composite viewpoints can also extract the viewpoint that categorizes angiosperms ( ower-bearing plants) according to the di erent types of owers they have. The speci cation is the following: (composite (Angiosperm dimension taxonomic) (Flower dimension taxonomic) parts) This composite viewpoint includes, for example, the fact that one kind of angiosperm is the orchid, which has an irregular ower. The purpose of our evaluation was to measure the coherence of viewpoints the View Retriever extracts, as compared to the coherence of viewpoints found in human-generated text. For each of 12 topics in botany, sets of facts were drawn from 3 sources: a college-level botany textbook (Raven, Evert, & Curtis 1976), the View Retriever applied to the Botany Knowledge Base, and facts selected randomly from a particular frame in the Botany Knowledge Base. The viewpoints ranged in size from 3 to 11 facts. For each topic, textbook passages and random sets of facts were chosen to be roughly the same size as the viewpoint on that topic. Each group of facts (including the



(1) Textbook Viewpoints 4.23 0.56 (2) View Retriever's Viewpoints 3.76 0.74 (3) Degraded Viewpoints 2.86 0.94 (4) Random Collections of Facts 2.62 0.86 Table 1: Ten judges rated the coherence of sets of facts from four sources (1=incoherent; 5=coherent). A statistical analysis using the T-test with 0.95 level of condence shows no signi cant di erence in coherence between sources (1) and (2) or between sources (3) and (4). There is a signi cant di erence between all other pairs. textbook passages) was translated manually into \simple English" to normalize presentation style. The viewpoints included about equal numbers of as-kind-of , basic dimension, and composite viewpoints; as-having viewpoints were omitted from this study because they often use cached viewpoints. Ten subjects (senior undergraduates and graduate students from the Botany and Biology Departments of the University of Texas at Austin) judged the coherence of several passages from each source. The subjects were asked to use a scale of 1 to 5, to assign a passage a score of \1" if it seemed no more coherent than a randomly selected group of facts on the subject, and to assign a passage a score of \5" if it was as coherent as a passage of comparable length on the subject from a good textbook. Table 1 summarizes the subjects' responses. Statistical analysis (using a T-test with 0.95 level of con dence) yields the following results: The mean coherence of viewpoints from textbooks did not di er signi cantly from the mean coherence of viewpoints extracted by the View Retriever. The mean coherence of extracted viewpoints did differ signi cantly from the mean coherence of random collections of facts drawn from the same frame. A further study gives additional evidence that the View Retriever extracts coherent viewpoints. Along with passages from the three sources described above, the subjects were given passages from a fourth source: viewpoints extracted by the View Retriever and then \degraded" by replacing some of their facts with randomly selected facts on the same topic. Twenty-eight such degraded viewpoints were constructed, each with between one and seven facts replaced. Of the twentyeight, each subject received six. Table 1 shows the mean coherence score of the degraded viewpoints. Statistical analysis shows a signi cant di erence in the mean coherence of \pure" viewpoints and degraded viewpoints. A nal study adds more evidence that passages vary in coherence based on their source and that view-



Source



Coherence Mean



Evaluation of the View Retriever



points extracted by the View Retriever are consistently judged to be coherent. A two-way analysis of variance, computed by Paul Cohen1 , determined that there was no signi cant interaction e ect between: the variance in coherence scores assigned by di erent judges, and the variance in coherence scores for passages from di erent sources (e.g.,textbooks, the View Retriever). Thus, although judges varied in their harshness, they largely agreed on relative orderings. Viewpoints are coherent collections of facts that describe a concept from a particular perspective. They are essential for a wide variety of tasks, such as explanation generation and qualitative modeling. We have identi ed several types of viewpoints and developed a program, the View Retriever, for extracting them from knowledge bases, either singly or in combination. Our evaluation of the View Retriever indicates that its viewpoints are comparable in coherence to those constructed by people. The View Retriever has several known limitations, some of which we are addressing. First, viewpoint speci cations use the names of frames and slots in the knowledge base. Therefore, users of the View Retriever must have extensive knowledge of the concept and slot hierarchies in order to use the View Retriever. To address this limitation, we are developing methods whereby users can specify frames and slots descriptively rather than by name. Second, our textbook analysis reveals that most explanations consist of several viewpoints used in concert. Although the View Retriever can extract composite viewpoints, we have not yet identi ed which combinations are commonly used. A third limitation is that the View Retriever ignores knowledge about the a priori importance of facts. Therefore, it cannot extract viewpoints of a concept in the order of their importance, a potentially useful ability. The View Retriever will be evaluated more extensively when it supports our tutoring system for plant anatomy and physiology. It will be the primary method used by the tutor to access the Botany Knowledge Base to build qualitative models and generate explanations. We are currently building this tutoring system, and we have found that knowledge base access at the level of viewpoints (as opposed to the level of individual facts or frames) greatly simpli es system design and implementation.

1 Computer Science Department, University of Massachusetts at Amherst



3 Discussion



Proceedings of the 8th National Conference on Arti cial Intelligence.



Falkenhainer, B., and Forbus, K. 1991. Compositional modeling: Finding the right model for the job. Arti cial Intelligence 51:95{143. Forbus, K. 1984. Qualitative process theory. Arti cial Intelligence 24:85{168. Hobbs, J. 1985. On the coherence and the structure of discourse. Technical Report CSLI-85-37, Computer Science Department, Stanford University. Lako , G., and Johnson, M. 1980. Metaphors We Live By. University of Chicago Press. Lester, J., and Porter, B. 1991. A student-sensitive discourse generator for intelligent tutoring systems. In Proceedings of the International Conference on the Learning Sciences, 298{304. Liu, Z., and Farley, A. 1990. Shifting ontological perspectives in reasoning about physical systems. In



References



Mann, W., and Thompson, S. 1987. Rhetorical structure theory: A theory of text organizations. Technical Report ISI/RS-87-190, Information Sciences Institute, University of Southern California. McCoy, K. 1989. Generating context-sensitive responses to object-related misconceptions. Arti cial Intelligence 41:157{195. McKeown, K. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.



McKeown, K. 1988. Generating goal-oriented explanations. International Journal of Expert Systems 1(4):377{395. Moore, D., and Swartout, W. 1988. A reactive approach to explanation. In Proceedings of the Fourth



Murray, K., and Porter, B. 1989. Controlling search for the consequences of new information during knowledge integration. In Proceedings of the Machine Learning Workshop, 290{295. Palo Alto, California: Morgan Kaufmann. Raven, P.; Evert, R.; and Curtis, H. 1976. Biology of Plants. New York: Worth Publishers. Suthers, D. 1988. Providing multiple views of reasoning for explanation. In Proceedings of the International Conference on Intelligent Tutoring Systems, 435{442. Suthers, D. 1991. Task-appropriate hybrid architectures for explanation. In Proceedings of the AAAI-91

Workshop on Comparative Analysis of Explanation Planning Architectures.



International Workshop on Natural Language Generation.



Indefinite Scalability for Living Computation

David H. Ackley

University of New Mexico Department of Computer Science Albuquerque, NM 87131 ackley@cs.unm.edu



Abstract

In a question-and-answer format, this summary paper presents background material for the AAAI-16 Senior Member Presentation Track "Blue Sky Ideas" talk of the same name.



Q: So, what's the big idea here?

A: Traditional CPU and RAM computing, based on hardware determinism, is now struggling to scale up, as clock speed increases have stalled and multicore cache coherence grows increasingly expensive. Determinism is also a perpetual computer security nightmare, encouraging programmers to optimize efficiency, and thus fragility--while providing, by default, utter predictability to attackers. The blue sky idea is: We should forgo deterministic execution and focus instead on best-effort computing--in hardware and software both--to develop indefinitely scalable computer designs. A machine from here to the horizon if we want it, built of locally-connected, interchangeable computing tiles; a machine so big parts of it will always be failing; a machine so big we'll be using it long before we finish building it. To survive and prosper in such a system, software will be living computation in a far richer sense than today: It will collaborate and compete with other software, will take damage and heal, will reproduce for redundancy and parallelism, will migrate and spontaneously colonize new hardware, and so on. Programming and performing large-scale computations will be less like solving logic puzzles and more like agriculture or animal husbandry or ecosystem management. Q: Wow! So this is all just wishes and fantasy, right? A: Well, not entirely. It's still the earliest days, but for several years we've been working on indefinite scalability, mostly in simulation. Unless stated otherwise, the material in this summary paper is drawn from (Ackley and Cannon 2011; Ackley 2013a; 2013b; Ackley, Cannon, and Williams 2013; Ackley and Small 2014; Ackley and Ackley 2015). And though it can sound like science fiction, our proposed hardware architecture depends only on conventional electronics manufacturing and presumes no breakthroughs in materials science or unconventional computing media.

Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



Q: So why, exactly, doesn't determinism scale? A: Hardware determinism means that digital computer hardware guarantees to perform the rules of logic flawlessly, for as long as the program runs--and if the hardware can't do that, for any reason, it promises to "crash" the whole machine immediately. With that guarantee in hand, typical software utterly ignores the possibility of hardware errors--and that `reliable hardware + efficient software' mindset is so deeply embedded, in academia and industry both, that it is rarely even mentioned, let alone questioned. The scaling problem is that hardware's guarantee isn't quite 100%. Any real unit of hardware will have some small chance of undetected error. Now, if we are given a maximum computation size up front, we can design hardware units to make that size computation as reliable as we like. But if we then keep adding more and more of those hardware units to the system, and running it longer and longer, eventually the aggregate space-time computational volume will exceed the reciprocal of the hardware unit failure rate, and the machine will deliver undetected errors to the software level. That's why, even in principle, hardware determinism doesn't scale, and in the supercomputing community, for example, it is already more than a merely theoretical concern (Cappello et al. 2009). Q: But if determinism fails, doesn't that mean chaos? A: Not necessarily at all. Computer science's focus on efficiency makes it easy to forget that digital hardware employs massive redundancy to accomplish its heroic acts of determinism. It uses whole wires to carry single bits, and deploys saturating amplifiers at every turn to reduce noise while the chance of error remains small. When we renegotiate around best-effort hardware, software becomes obligated to look beyond efficiency and embrace robustness, by deploying redundancy effectively throughout the software stack. The resulting systemic errorresistance will enable scalability, and also fundamentally benefit security--not like a silver bullet miracle, but like the way sanitation and hygiene benefits the public health. Q: Hmm. What do we need know most about this idea? A: Here are the bottom lines: This proposal represents a huge but feasible change, and it is important for society and long overdue, and it needs your help.



It's built on three overlapping concepts--indefinite scalability for hardware and architecture, robust-first computing for software and programming, and best-effort computing for systems overall and models of computation. In the rest of this paper, we will say a little more about indefinite scalability in computer architecture, and then consider the particular approach to it we have been exploring, which is called the Movable Feast Machine. We'll show a few simple examples to illustrate the ideas and the current level of research and development, and conclude with a call to action.



Q: What is indefinite scalability?

A: Indefinite scalability is a design principle saying that any admissible computer architecture must be expandable to arbitrary size without any fundamental re-engineering. We may be unable to grow our indefinitely scalable machine for external reasons--like we run out of money or real estate or power or cooling--but never because we hit some internal design limit, like reaching the limits of a fixed-width address or the light cone of a central clock. Q: So, something like the internet? A: No, as it is most commonly used, the Internet is only finitely scalable, because its (IPv4 or v6) address space is finite. The Internet could be indefinitely scalable, however, via aggressive use of "anycasting" (Partridge, Mendez, and Milliken 1993; Abley and Lindqvist 2006), which offers indefinite spatial scaling in exchange for a finite set of interprocessor request types. Q: But how can over 1038 IPv6 addresses not be enough? A: Because for indefinite scalability, "really big" amounts to "merely finite": To preserve the clarity and power of the idea, any appeals to practical sufficiency are irrelevant. Moreover, anecdotally at least, it seems that every internal limit in a design is a form of technical debt (Cunningham 1992; Allman 2012) that has consequences beyond the limit itself. A design built on globally unique node names drawn from a finite space, for example, incurs not only the risk of name exhaustion, but all the issues and risks of centralized naming and resolving, aliasing, spoofing, and so on. Indefinite scalability doesn't automatically solve or avoid any such issues, but it forces them all to the foreground; it helps keep us honest and thereby provides a more stable basis for evaluating and choosing between architectures. Q: Well, if not the Internet, what is indefinitely scalable? A: Good question! We'll introduce our primary example, the Movable Feast Machine, in a moment. A repurposed "anycast-Internet," as mentioned above, could be one example. And though cellular automata (CA) typically assume deterministic execution and thus render themselves only finitely scalable, probabilistic cellular automata (Grinstein, Jayaprakash, and He 1985; Agapie, Andreica, and Giuclea 2014, e.g.) do go beyond determinism--and some of them could plausibly be cast into indefinitely scalable form. In general, a design for an indefinitely scalable machine amounts to a spatial tiling of hardware units, with additional



Figure 1: The Movable Feast Machine (MFM) architectural overview, with 2009-era prototype tile hardware at bottom. (See text.) requirements as needed to preserve open-ended physical realizability. As an obvious example, fixed-latency global communications is disallowed by the finite speed of light, as are hierarchical or hypercube interconnects based on fixedlatency links. Less obviously, the presumption of a global reset or "boot time" is also disallowed. Q: It's too hard! Is indefinite scalability really worth it? A: It is hard, but the potential upside is computational systems of unlimited size, that are inherently tough, good at their jobs, and hard to attack. In the end, hardware determinism is a property of small systems. Best-effort computing is the future--it is what we should be optimizing, rather than determinism. We are hurting ourselves by delaying.



Q: What is the Movable Feast Machine?

A: The Movable Feast Machine is a tile-based indefinitely scalable architecture (see Figure 1). Each tile is a small von Neumann machine running an identical control program out of non-volatile local memory. The program treats local volatile memory as a patch of cellular automata grid, and performs events on randomly-chosen local grid sites, while coordinating with adjacent tiles to seek consistent cache views of their relevant grid sites. There are no global clocks or synchronization barriers, and tiles race against each other to acquire intertile locks for events that may affect caches.



Figure 2: The MFM per-tile event loop. (See text.) Figure 4: A Demon Horde Sort stochastic sorter. (See text.) Q: What does state transition code look like? A: Over the last year and a half, we have developed a programming language, called ulam, specifically to express MFM state transitions. ulam has its own distinct flavors but is deliberately designed to seem reasonably familiar to programmers used to conventional object-oriented procedural languages. ulam compiles into C++, and from there via gcc to machine code for dynamic loading into the simulation, and we hope soon into live tile hardware as well. Figure 1 includes some legal but pointless ulam code, and Figure 3 presents a functional but complete lout of an element--a ForkBomb that attempts to fill space with copies of itself, with no regard for what might already be there: Hello World and then some. Q: That's cute, but how do more complex things work? A: In a general sense, building larger structures in the Movable Feast involves three aspects: 1. Functional: Programming more complex element behavior functions, and involving greater numbers of interacting element types, 2. Spatial: Deciding where to locate the involved atoms relative to each other, and if and how they should move, to make possible local interactions that are useful within the larger structures and purposes, and 3. Temporal: Using spatial and functional mechanisms to implement staged processes that unfold in time, such as growth phases. Also, adjusting rate constants so some subprocesses run much slower or faster than others. This can enable programmers to use constant approximations (if slower) or equilibrium approximations (if faster) to simplify reasoning about otherwise complex dynamics. Q: Well, maybe I asked for that. How about an example? A: Figure 4 is a schematic representation of one of the earliest machines we investigated, called the Demon Horde Sort (DHS). Data atoms (blue), each holding a random 32 bit



The overall effect is of an asynchronous, probabilistic cellular automata, but one in which an individual grid site's neighborhood--which we call an event window--is huge by CA standards, involving thousands of bits. As a result, a traditional CA state transition lookup table is completely infeasible; instead, a mostly-traditional serial deterministic function is called to perform the state transition, depending on the type bits in the event window's center site. Figure 2 summarizes the tile event loop. Q: But if a transition is deterministic, what's the point? A: That determinism only lasts as long as an event, which is short, and can affect only its event window, which is tiny by traditional programming standards. Although programming MFM element behavior functions is rather less obvious than classical serial programming, it is also much more intuitive than composing typical CA rule tables, primarily because execution is serial and effectively single-threaded within an event, during which the event window acts like passive memory. The corresponding challenges are that event code cannot access persistent state outside the event window or assume anything about event window contents between invocations. ulam 1; /** Fork bomb. \symbol FB \color #f00 \symmetries all */ element ForkBomb { EventWindow ew; Void behave() { ew[1] = ew[0]; } } Figure 3: A complete ulam element. Copies itself from the event window center (ew[0]) to ew[1], which in this case (due to the \symmetries all in the element metadata) is an adjacent site chosen at random on each event.



Figure 5: Six events in the life of a self-assembling four-port data switch, spreading over four simulated MFM tiles. Top left: Initial state. Top center: After 100 average events per site (AEPS). Top right: After 1000 AEPS. Bottom left: After 2000 AEPS. Bottom center: After three tiles are reset at 2500 AEPS. Bottom right: After 3000 AEPS. See text. number, are placed on the grid by the emitters (green) at the right, and are carried right-to-left and moved up and down by the Sorters (red), and eventually are extracted by absorbers on the left (dark grey). The trick is, each Sorter remembers the value of the last Data it moved, and when it considers moving the next Data atom right-to-left, it also tries to move it up or down based on the comparison of the current and previous Data values. The DHS illustrates all of the compositional mechanisms just mentioned. By copying the Data value into the Sorter's internal storage, Data-Data comparisons can be performed even when the Data items aren't close enough to interact directly: The existence and behavior of the Sorters enable local interactions that advance the goals of the larger structure. Similarly, by making basic spatial and geometric assumptions--that input is East, output is West, small is North and large is South--the Sorters can autonomously take actions that will help in the computation as a whole. And finally, the "DReg" and "Res" in Figure 4 are part of a "Dynamic Regulator" feedback system that maintains the Sorter population at a reasonable density. Though the DReg mechanism is interesting in its own right, here it just serves as an example of the temporal layering of dynamics: The DReg operations are so slow compared to the Sorters that the DReg can basically be ignored except when considering the long-term dynamics of the system. Q: OK, but, how can the DHS possibly sort correctly? A: It doesn't. Its inherent resolution is determined by the channel width, and its sorting quality generally improves with increasing length/width aspect ratio. More basically, though, in this problem formulation, correctness really isn't an option. Given that the emitters produce Data atoms intermittently and unpredictably, it's not even well-defined what the "correct" maximum value really is at any given moment. Welcome to best-effort. Q: I start to see.. Have you built other machines? A: Certainly. As one last example, just briefly, Figure 5 shows six snapshots in the self-assembly of a toy four-port data switch we have recently been exploring. Starting from a single `switch wall' atom, the switch builds a box (blue outline), insulates it with passive Walls (white) embedded with four I/O ports (red, yellow, green, and blue line segments), and builds a routing grid (evenlyspaced white dots). The routing grid atoms observe nearby



ports and then gossip among themselves to form gradients to guide the data cells (scattered dots, colored to show their destination port). Each data cell carries a 32 bit payload and an eight bit sequence number, although this switch does not attempt to perform packet reassembly. After an average of one thousand events per site (1000 AEPS), the switch has completely self-assembled but still has a backlog of data cells; by 2000 AEPS the switch is operating smoothly. Later, at 2500 AEPS, we severely damage it by resetting three of the four underlying tiles, but it immediately begins to reassemble itself and, by 3000 AEPS, it is almost healed.



Q: Very cool! But, how does all this affect AI?

A: Traditional computing is about constructing things once and then trusting them to remain intact indefinitely. That's why it's, at once, so marvelously efficient and so deathly fragile. Robust-first computing, on the other hand, is about continuous self-construction and maintenance, so that information structures are automatically refreshed as needed, or at some rate, or both. Sixty-five years ago, von Neumann (1951) predicted that hardware determinism would soon be supplanted, but with design lock-in and network effects it remains virtually unchallenged today. In AI's long-running cross-pollination, let us call it, between the neats and the scruffies, that dominance has given the neats sole possession of the home court advantage--deterministic execution--without anybody really calling them on it. But now the costs of playing on that court are rising too high. It's time for the neats to try playing an away game. Q: Last question! I hate to ask, but who funds this work? A: Eventually, a substantial research and development effort, performed and supported by many people and organizations, will be needed to develop the science and engineering of best-effort computing. Perhaps you will be part of that. For these early stages, support has come from a brave and visionary few. The work summarized here was supported in part by a Google Faculty Research Award, and in part by grant VSUNM201401 from VanDyke Software.



Ackley, D. H. 2013a. Bespoke physics for living technology. Artificial Life 19(3 4):347-364. Ackley, D. H. 2013b. Beyond efficiency. Commun. ACM 56(10):38-40. Author preprint: http://nm8.us/1. Agapie, A.; Andreica, A.; and Giuclea, M. 2014. Probabilistic cellular automata. Journal of Computational Biology 21(9):699-708. Allman, E. 2012. Managing technical debt. Queue 10(3):10:10-10:17. Cappello, F.; Geist, A.; Gropp, B.; Kal, L. V.; Kramer, B.; and Snir, M. 2009. Toward exascale resilience. IJHPCA 23(4):374-388. Cunningham, W. 1992. The WyCash Portfolio Management System. In Addendum to the Proceedings on Objectoriented Programming Systems, Languages, and Applications (Addendum), OOPSLA '92, 29-30. New York, NY, USA: ACM. Grinstein, G.; Jayaprakash, C.; and He, Y. 1985. Statistical mechanics of probabilistic cellular automata. Phys. Rev. Lett. 55:2527-2530. Partridge, C.; Mendez, T.; and Milliken, W. 1993. Host Anycasting Service. RFC 1546 (Informational). von Neumann, J. 1951. The general and logical theory of automata. In Jeffress, L. A., ed., Cerebral Mechanisms in Behaviour: the Hixon Symposium (1948). Wiley. 15-19. Also appears as pages 302-306 in A.H. Taub, editor, John von Neumann Collected Works: Volume V - Design of Computers, Theory of Automata and Numerical Analysis, Pergamon Press, 1963.



References

Abley, J., and Lindqvist, K. 2006. Operation of Anycast Services. RFC 4786 (Best Current Practice). Ackley, D. H., and Ackley, E. S. 2015. Artificial life programming in the robust-first attractor. In Proc. of the European Conference on Artificial Life (ECAL). Ackley, D. H., and Cannon, D. C. 2011. Pursue robust indefinite scalability. In Proc. HotOS XIII. Napa Valley, California, USA: USENIX Association. Ackley, D. H., and Small, T. R. 2014. Indefinitely Scalable Computing = Artificial Life Engineering. In Proceedings of The Fourteenth International Conference on the Synthesis and Simulation of Living Systems (ALIFE 14) 2014, 606- 613. MIT Press. Ackley, D. H.; Cannon, D. C.; and Williams, L. R. 2013. A movable architecture for robust spatial computing. The Computer Journal 56(12):1450-1468.



A Natural Interface and Uni ed Skills for a Mobile Robot

William Adams, Dennis Perzanowski, and Alan C. Schultz Navy Center for Applied Research in Arti cial Intelligence Naval Research Laboratory Washington, DC 20375-5337, U.S.A. adams,dennisp,schultz@aic.nrl.navy.mil

Our research is aimed at developing an independent, cooperative, autonomous agent. Toward this end, we are working on two areas: a natural interface for interacting with the robot, and the basic underlying skills for navigating in previously unknown environments. The interface we are developing combines natural language and gestures 1]. While human communication between individuals occurs on many channels, two of them, natural language and gesture, complement each other fairly regularly in daily communication. Since people interweave them freely during their interations, we assume they might readily do so in their interactions with a mobile robot. Our interface allows the processing of complete or incomplete (fragmentary) commands. To process these types of commands, we keep track of the various goals during human-robot interactions by instantiating \context predicates," which are basically lists of the verbal predicates and their arguments expressed in logical form. By utilizing context predicates, a discourse component of the interface tracks exactly which and to what extent each goal was achieved. With this information and by tracking goal achievement, the robot can continue to achieve unaccomplished goals on its own, no matter at what point or in what state the system is currently. Thus, context predicates permit the system to work independently on achieving previously stated, but as yet uncompleted, goals. This capability ultimately allows the user greater freedom to interact naturally without having to explicitly state or re-state each expected or desired action when an interruption occurs. We hope to extend goal tracking so that the mobile robot can complete semantically related goals which are not initially speci ed or which are unknown to the human at the time when the initial goal is instantiated. This natural interface is currently in use with a moThis work was sponsored by the O ce of Naval Research. bile robot. Navigation goals and locations are speci ed by speech and/or with natural gestures. Commands can be interrupted and subsequently completed with fragmentary utterances. To provide the basic underlying skills for navigating in previously unknown environments, we are working to create a mobile robot system that is robust and adaptive in rapidly changing environments. We view integration of these skills as a basic research issue, studying the combination of di erent, complementary capabilities. One principle that aids integration is the use of unifying representations which allow better communication and interaction among di erent components. Our most recent work uses evidence grids as a common representation to integrate mobile robot exploration, localization, navigation, and planning ?]. In addition, this integrated system includes methods for adapting maps to allow for robust navigation in dynamic environments. As a result, a robot can enter an unknown environment, map it while remaining con dent of its position, and robustly plan and navigate within the environment in real time. We create two types of representations with the evidence grids: short-term perception maps, and longterm metric maps. The short-term maps store very recent sensor data that does not contain signi cant odometry error, and these maps can be used for obstacle avoidance and for localization. The long-term maps represent the environment over time, and can be used for navigation and path-planning. The use of evidence grids requires that the robot be localized within its environment. To overcome odometric drift and errors, we have developed a method for continuous localization, in which the robot continually corrects its position estimates. Continuous localization builds the short-term perception maps, and at frequent intervals registers the oldest short-term map against the long-term map, locating the robot within



the environment. In order for mobile robots to operate in unknown environments, they need the ability to explore and build maps that can be used for navigation. We have developed the frontier-based exploration strategy based on the concept of frontiers, regions on the boundary between open space and unexplored space. When a robot moves to a frontier, about half of its sensors can still see the old, known environment, which can be used by continuous localization to maintain accurate odometry. Its other sensors see into unexplored space and expand the map. By moving to successive frontiers, the robot can constantly increase its knowledge of the world. The new, expanded maps produced by the exploration are passed to continuous localization as its new long-term map. After exploration is complete, changes in the world (blocked passages, moved obstacles, etc) must also be modeled. We have added a learning component to the continuous localization algorithm to allow the longterm map to be updated with recent sensor data from the short-term perception maps, making the long-term map adaptive to the environment. In order to provide robust navigation, we have incorporated Trulla, a propagation-based path planner which uses a navigability grid to describe which areas in the environment are navigable (considering oor properties, obstacles, etc). In our system, we have integrated Trulla by replacing its navigability grid with our long-term metric map. As our long-term map adapts to changes in the environment, Trulla can replan using the robot's current knowledge about the world. Continuous localization's long-term map update method can adapt to somewhat rapid and persistent changes in the environment, but not to very fast changes, such as a person walking through the room. Accordingly, paths generated by Trulla are not su cient to prevent collisions with transient obstacles. We have integrated the Vector Field Histogram (VFH) reactive navigation method to avoid transient obstacles that are not yet represented in the evidence grid. VFH uses an HIMM occupancy grid to model the robot's immediate surroundings. In our integration, we replace the HIMM occupancy grid with the shortterm perception map produced by continuous localization. The short-term perception map allows VFH to consider all sensors, and yields a less noisy picture of the robot's immediate environment. Fig. 1 illustrates the complete architecture. When heading into an unknown environment, the robot autonomously maps the environment while maintaining accurate odometry, producing the initial



long-term map. Each new short-term perception map and long-term map is sent to a Map Server process which in turn makes the sensor-fused perceptions of the environment available to the various processes.

VFH

control



Trulla mapserver



short term map

sensor data Continuous Localization



long term map



exploration



Figure 1: Architecture of integrated system After exploration, the user speci es a navigation goal to Trulla, which consults the Map Server for the current long-term map and computes the vector eld describing the best path from each cell to the goal. Trulla sends the vector eld to VFH, which uses the robot's current position to index the vector eld and get the direction to the goal. VFH retrieves the shortterm map from the Map Server, and steers the robot in the direction closest to that which was planned by Trulla. While VFH is steering the robot, continuous localization continues to correct odometry and produce short-term and adapted long-term maps. With each new long-term map, Trulla replans and sends a new vector eld to VFH which uses it for subsequent navigation.



References

1] Perzanowski, D., Schultz, A., and Adams, W. (1998). \Integrating natural language and gesture in a robotics domain," In Proc. of the IEEE International Symposium on Intelligent Control: ISIC/CIRA/ISIS Joint Conference, Gaithersburg, MD, 247-252. 2] Schultz, A. and Adams, W. (1998). \Continuous localization using evidence grids," In Proc. of the 1998 IEEE International Conference on Robotics and Automation, Leuven, Belgium, 2833-2839.



Learning Bayesian Networks with Incomplete Data by Augmentation

Tameem Adela,, Cassio P. de Camposb

a Machine



arXiv:1608.07734v2 [cs.AI] 9 Oct 2016



b EEECS,



Learning Lab, University of Amsterdam Queen's University Belfast



Abstract We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach.



1. Introduction Missing entries in real-world data exist due to various reasons. For instance, it can be due to damage of the device used to record feature values; a metal detector might fail to produce a signal denoting the existence of a metal due to a certain malfunction. Results can be incomplete in an industrial experiment due to mechanical breakdowns not necessarily related to the performed experiment (Little and Rubin, 1987). Recommendation data can have missing values since participants in the recommendation system did not rate all the available

 Corresponding



author Email address: tameem.hesham@gmail.com (Tameem Adel)



songs, films, books, etc. While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might fail in other examples. People seeking for health insurance might refuse to give an answer to certain questions in order to reduce the costs, e.g. `do you smoke?', and in many cases this can be seen as an indication of one specific answer. In such cases we say that data are missing not at random, or MNAR (see for instance (Van den Broeck et al., 2014)). Given a dataset with categorical random variables, the Bayesian network structure learning problem refers to finding the best network structure (a directed acyclic graph, or DAG) according to a score function based on the data (Heckerman et al., 1995). As well known, learning a Bayesian network from complete data is NP-complete (Chickering, 1996), and the task becomes even harder with incomplete data. In spite of that, the problem of learning a Bayesian network from incomplete data by (an optimistic) augmentation belongs to the same complexity class, as we will show later on. Because of such result, we investigate and obtain a new exact algorithm for the problem, based on reformulating it into a standard structure learning without missing data. This is the first exact algorithm for the problem, to the best of our knowledge. In contrast to previous work, our algorithm performs both tasks, namely structure learning and data imputation, in a single shot rather than learning the Bayesian network and then dealing with the missing data, possibly in an iterative manner (Friedman, 1998; Rancoita et al., 2016). Based on the optimization that is required to solve the problem and on the exact algorithm, we devise a hill-climbing approximate algorithm. The hill-climbing regards the completions of the missing values only, while the structure optimization is performed by any off-the-shelf algorithm for structure learning under complete data. Most previous work to learn the structure of Bayesian networks from incomplete data has focused on MAR. The seminal algorithm in Friedman (1998) introduced an iterative method based on the Expectation-Maximization (EM) 2



technique, referred to as structural EM. Implementation of structural EM begins with an initial graph structure, followed by steps where the probability distribution of variables with missing values is estimated by EM, alternated with steps in which the expectation of the score of each neighbouring graph is computed. After convergence, the graph maximizing the score is chosen. Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005). In Rancoita et al. (2016), structures are learned from incomplete data using a structural EM whose maximization step is performed by an anytime method, and the `expectation' step imputes the missing values using expected means, or modes, of the current estimated joint distribution. By using modes in each iteration (Ramoni and Sebastiani, 1997), the EM method is sometimes called hard EM, and is close to our work. In some sense, we work with a global optimization version of hard EM. While this is not exactly considering data to be MNAR, such approach fits less the observed data and performs well for MNAR missing data when compared to structural EM, as we will empirically show. We emphasize that the actual missingness process is not disclosed to the methods and is not assumed to be somehow known, and that we are mainly interested in structure learning. Given the difficulties of structure learning itself, we assume that the underlying distribution is identifiable (in short terms, provided enough data are available, one could reconstruct such distribution, see for instance (Mohan et al., 2013)). We perform experiments on a set of heterogeneous datasets. We base the evaluation on imputation accuracy in its pure form, as well as in the forms of classification accuracy and semi-supervised learning accuracy. Experiments show the improvements achieved by the proposed algorithms in all scenarios. Regarding the comparison between our exact and approximate methods, experiments suggest that accuracy levels achieved by the approximate algorithm are close to those achieved by the optimal learning algorithm, with the former being much faster and scalable. 3



2. Bayesian Network Structure Learning Let X = (X1 , . . . , Xm ) refer to a vector of categorical random variables, taking values in OX = xi OXi , where OX represents the Cartesian product of the state space, OXi , of each Xi . Denote by D an n-instance dataset where each instance Du = (du,1 , du,2 , . . . , du,m ) is such that du,i is either an observed value ou,i  OXi or a special symbol denoting the entry is missing. Let Zu denote a completion for variables with missing values in instance u and zu,i for the missing value of Xi . A Bayesian network, M, is a probabilistic graphical model based on a structured dependency among random variables to represent a joint probability distribution in a compact and tractable manner. Here, it represents a joint probability distribution PrM over a collection of categorical random variables, X. We define a Bayesian network as a triple M = (G , X, P ), where G = (VG , EG ) is a directed acyclic graph (DAG) with VG a collection of m nodes associated to the random variables X (a node per variable), and EG a collection of arcs; P is a collection of conditional probabilities PrM (Xi |PAi ) where PAi denotes the parents of Xi in the graph (PAi may be empty), corresponding to the relations of EG . In a Bayesian network, the Markov condition states that every variable is conditionally independent of its non-descendants given its parents. This structure induces a joint probability distribution by the expression PrM (X1 , . . . , Xm ) =

i



PrM (Xi |PAi ). We define ri  2 as the number of val-



ues in OXi , i.e. ri = |OXi |, and rPAi as the number of possible realizations of the parent set, that is, rPAi =

Xl PAi



rl . Let R = maxi ri .



Given a complete dataset D with n instances, the structure learning problem in Bayesian networks is to find a DAG G that maximizes a given score function, that is, we look for G  = argmaxGG sD (G ), with G the set of all DAGs over node set X. We consider here the score function sD to be the Bayesian Dirichlet Equivalent Uniform (BDeu) criterion (Buntine, 1991; Cooper and Herskovits, 1992) (other decomposable scores could be used too), so we have sD (G ) =

i sD (Xi , PAi ).



We however have to deal with the missing part of the data,



4



which we treat by completing the missing values in the best possible way (an optimistic completion): (G  , Z  ) = argmax sD (G , Z ) =

GG , ZZ



argmax

GG , ZZ i



sD (Xi , PAi ; Z{Xi }PAi )



(1)



where Z = xu OZu and sD (G , Z ) is the score sD (G ) evaluated for the complete data when its missing values are replaced by Z , while sD (Xi , PAi ; Z{Xi }PAi ) is the local score for a node Xi with parent set PAi (note that such computation only depends on the completion Z{Xi }PAi of the involved variables). We refer to this optimization task as the structure learning problem by optimistic augmentation. It can be applied to MAR data, but we argue that it is particularly suitable to MNAR when compared to the standard techniques such as structural EM. From the optimization viewpoint, this can be seen as a global optimization approach to hard EM, since we complete the data with their mode, but we do it globally instead of in an iterative process such as EM. As well known, hard EM can be seen as a subcase of EM, since it is equivalent to allowing EM to use only degenerate mass functions in its expectation step. Theorem 1. The decision version associated to the structure learning problem by optimistic augmentation is NP-complete. Proof. Hardness is obtained by realizing that this problem generalizes the structure learning problem without missing data, which is NP-hard (Chickering, 1996). Pertinence in NP holds since given G and Z , the score function sD can be computed in polynomial time. Since the problem is a combinatorial optimization over a discrete domain (both DAGs and completions of data are discrete entities), we could resort to enumerating all possible solutions. This is obviously infeasible for both: the number of DAGs grows super-exponentially in the number of variables and the number of completions grows exponentially in the number of missing values. We will now present an exact algorithm for the problem which transforms it 5



into a standard structure learning problem, and later we modify the approach to perform approximate learning. In this respect, we define as a t-local optimal solution for Equation (1) a pair (G , Z ) such that sD (G , Z )  sD (G  , Z  ) for all G  and all Z  with HD(Z , Z  )  t, where HD is the Hamming distance, that is, (G , Z ) is optimal with respect to any other pair whose completion of the data has at most t elements different from Z . A global optimal solution is a -local optimal solution. 2.1. Optimal (Exact) Learning Algorithm We assume that a standard structure learning algorithm for complete data is available to us, which is based on the framework of two main optimizations: (i) parent set identification and (ii) structure optimization. Step (i) concerns building a list of candidate parent sets for each variable, while Step (ii) optimizes the selection of a parent set for each variable in a way to maximize the total score while ensuring that the graph is a DAG. This latter step can be tackled by exact or approximate methods (Bartlett and Cussens, 2013; Scanagatta et al., 2015) (in our experiments we will employ an exact method such that we are sure that the quality of results is only affected/related to the proper treatment of the missing data, but for very large domains any approximate method could be used too). The exact algorithm for solving Equation (1) is based on modifying the parent set identification step. This step has no known polynomial-time solution if we do not impose a maximum number of parents (Koivisto, 2006), so we will assume that such a bound k is given. We compute the candidate list by using one of the available approaches (de Campos and Ji, 2011; Scanagatta et al., 2015) to guide the search, but for each candidate to be evaluated, the corresponding variables in the dataset might contain missing values. The first part of the transformation is to create gadgets composed of some new artificial variables which will be related to the missing values and will enable the inclusion of all possible replacements of missing values by augmenting the original domain. Over all the dataset, for each and every missing value, let us denote it by (u, i) 6



for sample u and variable Xi , we include artificial variables X(u,i),1 , . . . , X(u,i),ri . Each X(u,i),j has two parent set candidates: (i) X  {X(u,i),1+(j

mod ru ) }



with



score zero (assuming all other score values are negative, without loss of generality) and (ii)  with score -, with  a large enough value (e.g. greater than the sum of all other absolute scores). We further illustrate the idea via an example for variable X1 with r1 = 3: Assume m = 3, r1 = 3 and there is one missing value at (u, 1). An artificial variable is included for each possible completion zu,1 , resulting in a total of three new variables, X(u,1),1 , X(u,1),2 , X(u,1),3 . The following gadget, consisting of two parent set candidates per artificial variable, is added to the list of parent set scores (we know that only one parent set per variable will be chosen during the optimization phase later on):



s(X(u,1),1 , {X(u,1),2 , X1 , X2 , X3 }) = 0, s(X(u,1),1 , ) = -, s(X(u,1),2 , {X(u,1),3 , X1 , X2 , X3 }) = 0, s(X(u,1),2 , ) = -, s(X(u,1),3 , {X(u,1),1 , X1 , X2 , X3 }) = 0, s(X(u,1),3 , ) = -. According to this gadget, each artificial variable will either have no parent variables or all other original variables as well as one other artificial variable as its set of parents. The case with no parents leaves open the opportunity to choose the variable representing such completion as a potential parent for all original variables. In contrast, the cases with all variables as parents disables such completion from being chosen as a parent by the original variables, otherwise it would create a cycle. Due to including one artificial variable as a parent of the next artificial variable, at least one parent set among those with score zero cannot be chosen (otherwise a cycle is formed), and because they are all very good scores when compared to -, all but one will certainly be chosen. There is one such gadget per missing value in the original dataset, so we spend 7



time O(R * m * C ), where C is the number of missing values. Finally, we return to the computation of the score for a given variable and parent set. Let Xi be the variable of interest and PAi = {Xi1 , . . . , Xiq } for which the score must be evaluated. At this moment, we consider all possible completions Z{Xi }PAi and compute the scores sD (Xi , PAi ; Z{Xi }PAi ) for each one of them. In order to reduce the problem to a standard structure learning without missing data, we must index these scores somehow. This is made possible via the new artificial variables: sD (Xi , PAi ; Z{Xi }PAi ) = sD (Xi , PAi  {X(u,i),zu,j : zu,j  Z{Xi }PAi }) that is, for each imputed missing value zu,j appearing for variable Xi or PAi we will have an extra parent within the parent set that tells which completion was used for that missing value, according to the completion Z{Xi }PAi . This idea is applied to every evaluation of the score of a parent set, for every possible completion Z{Xi }PAi , so the final list of candidates will include only parent sets for which the completion of the data is `known' at the time that the score is computed. In order to ensure that the completions are compatible among different local score computations, the gadgets explained before are enough, since they force that a certain completion be chosen for each missing value. Theorem 2. The exact algorithm transforms the structure learning problem by augmentation into a standard structure learning without missing data in time O(R * m * C ), plus time O(n * k * Rc ) per parent set evaluation, where C is the total number of missing values and c is the maximum number of missing values appearing in the variable of interest or in variables in the parent set being evaluated (hence polynomial in all parameters but c). There will be many score computations and entries in the list, exponential in the number of missing values involved. So the benefit of this approach is that usually only a few variables are involved in the score computation at the same time. The drawback is that it cannot handle datasets with many missing 8



values for the same variable, since it is Rc times slower than the corresponding parent set evaluation without missing data. Next we address this issue by proposing an approximate method (the exact method is nevertheless useful in small domains and also important to check whether the approximate version achieves reasonable results). 2.2. Approximate Algorithm Albeit locally to the variables involved in the evaluation of a parent set, the exact method considers all possible completions of the data. This is fine with a few missing values per variable, but if there are many missing values, in particular within the same variable, the exact method becomes computationally infeasible. We propose an approximate algorithm based on a hill-climbing idea. We start with an initial guess Z0 (or several different random guesses) for the completion of all missing values in the dataset. Then we execute the very same steps of the exact algorithm, but we restrict the completions only to those which are at most t elements different from the current guess Zh . There are

  at most (R * m)t completions Zh such that HD(Zh , Zh )  t. We proceed as



with the exact method, but applying such constraint during the transformation that was explained in the previous section. After the transformation is done, the structure optimization is run and a new structure and new data completion Zh+1 is obtained. We repeat the process until convergence, that is, until Zh+1 = Zh . Theorem 3. The approximate algorithm transforms the structure learning problem by augmentation into a standard structure learning without missing data in time O(R * m * C ), plus time O(n * k * (R * m)t ) per parent set evaluation (C is the total number of missing values and t is the amount of locality of the approximation, as previously defined), that is, polynomial in all parameters but t. The outcome of the approximate learning algorithm is the network structure as well as the completion of all the missing data values. The approximate algorithm might lead to a locally optimal solution, but on the other hand it is much more scalable than the exact algorithm. 9



Theorem 4. Provided that an optimal structure learning optimization algorithm is available, the approximate algorithm always converges to a t-local optimal solution. If we want to scale to very large domains, we could also resort to an approximate structure learning optimization algorithm (e.g. (Scanagatta et al., 2015)). In this case, our approximate algorithm could be used in domains with hundreds or even thousands of variables (using very small t), but we would lose the guarantee to converge to a t-local optimal solution (it would still be a local optimum, but we would have to define it locally also in terms of the graph structures).



3. Experiments We perform experiments on simulated as well as real-world data. The main evaluation metric used is accuracy of the imputation of missing data values, either in the form of missing values spread throughout the data, or in the form of a binary classification problem where only the class variable can contain missing values. Most of our experiments are with binary data for the sake of exposition, even though the algorithms are general and can be used with any categorical data (as shown in the last experimental setting). To test significance, we perform a paired t-test with significance level at 5%. Throughout all tables of results, a result in bold refers to an accuracy value that is significantly better than its competitors, whereas showing two results belonging to the same experiment in bold means that each of them being significantly better than the rest of the competitors. For structure optimization, we use the exact solver referred to as Gobnilp (Bartlett and Cussens, 2013) with the code available from https://www.cs.york.ac.uk/aig/sw/gobnilp/. We perform comparisons among the two proposed algorithms (exact and approximate) and the structural Expectation-Maximization (EM) algorithm (Friedman, 1998). We compare accuracy of the three algorithms based on the percentage of correct imputations over all missing values. As for the structural EM, we 10



have used the implementation available at https://github.com/cassiopc/csdadataimputation (Rancoita et al., 2016). After convergence, we run the prediction of missing values using a most probable explanation query. We must emphasize that the task of Bayesian network structure learning with missing values is very challenging, since it is already challenging without missing values. Therefore, we have focused on real but controlled experiments where we can effectively run the algorithms and assert their quality. We use maximum number of parents, k = 3, and use t = 1. 3.1. Well-known Bayesian Networks We perform experiments using real but small data sets in order to compare both exact and approximate algorithms. First, we employ the original Bayesian network model for Breast Cancer (Almeida et al., 2014), which contains 8 binary variables, we simulate 100 data instances. That model has been learned from cancer patients of the University of Wisconsin Medical Hospital. Features (Bayesian network nodes) include breast density, mass density, architectural distortion and others, in addition to the diagnosis variable whose binary value refers to benign or malignant (D'Orsi et al., 2003). We include two missing values per variable, resulting in a total of 16 missing values. These missing values are generated in a MNAR manner by randomly removing values that are equal to each other, that is, during the generation we enforce that all missing values are zero, or that all missing values are one. Imputation results of the proposed exact learning algorithm, approximate algorithm and structural EM are displayed in the first row of Table 1 over 100 repetitions of the experiment. Second, we use the Bayesian network that has been learned from the Prostate Cancer data by the Tree Augmented Naive Bayes (TAN) (Friedman et al., 1997), implemented by WEKA (Hall et al., 2009). The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), i.e. during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy. It contains 11 binary variables, and 100 instances are generated. We randomly produce two MNAR missing values 11



per variable, resulting in a total of 22 missing values. Results are shown in the second row of Table 1. Third, the well-known ASIA network is used (Lauritzen and Speigelhalter, 1988). We generate 100 instances according to this model, which contains 8 binary variables. Two missing values are randomly generated according to MNAR. Imputation results are displayed in the third row of Table 1. Results indicate that the algorithms proposed here are significantly better than structural EM, which is expected since in this experiment data are not MAR. More interestingly, results of the proposed exact and approximate BN learning algorithms are not significantly different, which supports the use of the (more efficient) approximate method for larger domains. Table 1: Accuracy of imputation for data simulated from different Bayesian networks with two MNAR missing values per variable. Bayesian net Algorithm Exact learning Breast Cancer Approx. learning Structural EM Exact learning Prostate Cancer Approx. learning Structural EM Exact learning ASIA Approx. learning Structural EM 3.2. (LUng CAncer Simple set) LUCAS Dataset The LUCAS dataset contains data of the LUCAS causal Bayesian network (Fogelman-Soulie, 2008) with 11 binary variables, as well as the binary class variable, and contains 2000 instances. In this experiment we conduct an analysis of both MAR and MNAR missing data, in order to understand whether the benefits that we have seen before are only significant in the MNAR case. 12 Average imputation accuracy 84.38% 80% 50% 91% 86.36% 50% 84.38% 79% 43.75%



Thus, we carry out two experiments: (i) MNAR setting by randomly generating missing values all having the same data value (we repeat that to both zero and one values, one at a time); (ii) MAR setting by randomly generating missing values regardless of their respective original values. These simulations are repeated 100 times. First, we generate two missing values per variable (24 missing values). A comparison between the imputation accuracy values of the approximate algorithm and structural EM is displayed in the first two rows of Table 2 (named `Spread All Over'). Surprisingly, our new algorithm is significantly better than structural EM even when missing data are MAR. Second, we generate 20 missing class values and repeat the experiment to span all instances such that each run involves missing values belonging to different instances (without replacement). For the MNAR experiment, each run consists of 20 identical missing class values (that is, we only make missing values of the same class, and we repeat that for both classes). For the MAR case, there is no such restriction and missing class values are randomly generated. Hence, there are 100 runs in order to cover all 2000 instances. Results of the approximate algorithm, structural EM and SVM using different kernels (for the sake of comparison with a state-of-the-art classifier) are displayed in the bottom rows of Table 2. Results of the proposed algorithm are significantly better when MNAR data are used, while the same cannot be stated for the MAR case (accuracy of the proposed algorithm is nevertheless superior to the others in the MAR case).



13



Table 2: Accuracy of imputation for experiments performed on the Lung Cancer dataset (LUCAS). Spread All Over refers to an imputation of 2 missing values per variable out of the 12 LUCAS variables. Classification refers to a classification problem performed as a cross-validation (100-fold cross-validation in the MNAR setting case) on LUCAS, using SVM, vs. an imputation task on the 20 missing class variables of the same folds, by both the proposed approximate learning algorithm and Structural EM. SVM kernels displayed are those that achieved the highest accuracy in each experiment. MP stands for missingness process, and rbf for radial basis function. MP Algorithm Average imputation accuracy



Exp.: Spread All Over MNAR Approx. learning Structural EM Approx. learning Structural EM Exp.: Classification Approx. learning MNAR Structural EM SVM (rbf) Approx. learning MAR Structural EM SVM (rbf) 97.5% 42.5% 45% 69% 70% 55% 70.83% 45% 70% 50%



MAR



3.3. SPECT Dataset The Single Proton Emission Computed Tomography (SPECT) dataset consists of binary data denoting partial diagnosis from SPECT images (Lichman, 2013). Each patient is classified into one of two categories, normal and abnormal. The SPECT data consists of 267 instances and 23 variables in total (22 binary variables and a binary class variable). We generate MNAR missing data with different proportions, always using only one specific value (missing data 14



proportions over all the data are 3%, 5% and 10%). These randomly generated datasets are given as input to the approximate algorithm as well as to structural EM. We note that there is a large discrepancy in the number of data values holding each of the two binary values: About 67% of the SPECT data has a value 0, whereas merely 33% of the data has a value 1. Due to that, we investigate the average MNAR imputation accuracy within each data value separately, and note as well that there is some discrepancy in such accuracy values. Imputation accuracy of the approximate learning algorithm and structural EM are displayed in Table 3. The new algorithm is significantly better. 3.4. Smoking Cessation Study Dataset The dataset used in this experiment is taken from a smoking cessation study as described in Gruder et al. (1993). It has been further utilized in other works, most notably Hedeker et al. (2007). The smoking cessation dataset is a binary dataset consisting of 489 patient records (instances) with the missing data being inherently therein, i.e. there is no need to simulate missing data. The dataset contains 4 variables including the class variable, which refers to smoking or non-smoking. All the missing values are located in the class variable. There is a total of 372 patient records with observed classes, consisting of 294 smoking and 78 non-smoking records, as well as 117 records with missing class labels. The experiment we perform here is a semi-supervised learning (SSL) experiment where we evaluate the performance of the algorithms as follows: (i) We hide the class labels of a portion of the observed labels; (ii) We apply the approximate learning on the data consisting of the originally missing and artificially hidden labels as missing values, and the rest of the data as observed values. Clearly this is a SSL experiment where the training data consists of the records with observed labels as labeled instances, records with originally missing labels as unlabeled instances, and the test instances are the records with artificially hidden labels. The evaluation metric is the accuracy of the test instances using a crossvalidation approach, as usually done in classification experiments. We compare 15



the performance of the approximate algorithm against an equivalent procedure using structural EM (labels are then chosen based on the posterior distribution), and also against a semi-supervised learner in the form of a Laplacian SVM (Melacci and Belkin, 2011) whose code is available online. Accuracies of the approximate algorithm, structural EM, and the semi-supervised Laplacian SVM are displayed in Table 4. Results suggest that the new algorithm is a very promising approach for SSL. Table 4: MNAR Semi-supervised learning (SSL) results of the Smoking Cessation study data. All test records are Smoking records. The first column refers to the number of missing values in the test set. Accuracy expresses cross-validated accuracy of the test set. # missing values 25 Algorithm Approx. Learning Structural EM Laplacian SVM 50 Approx. Learning Structural EM Laplacian SVM 75 Approx. Learning Structural EM Laplacian SVM 3.5. Car Evaluation Dataset The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class. The 6 attributes refer to the following: buying, maintenance, doors, persons, luggage boots and safety. The class variable refers to the car acceptability and can have exactly one of the following values: unacceptable, acceptable, good, very good. All variables are categorical with 3 or 4 states. The data were derived from a hierarchical decision model originally developed by Bohanec and 16 Avg. Accuracy 90% 15% 76% 88% 10% 73.5% 88% 8% 76%



Rajkovic (1988). Similar to the LUCAS experiment, a MNAR classification task is performed by involving missing values belonging all to one category of the class variable at a time (this is repeated for each label). Due to the class label unbalance (unacceptable: 1210 instances, acceptable: 384, good: 69, v-good: 65), we performed 10 experiments testing only the unacceptable and acceptable labels in five each, where there are 100 randomly chosen instances with a missing label (test set) in each experiment. The proposed algorithm is compared to structural EM and to an SVM classifier. Classification results are displayed in Table 5. Again, the new algorithm is significantly better than the others. Table 5: Accuracy of classification for experiments performed on the Car Evaluation dataset. SVM with an rbf kernel is reported since it leads to best accuracy compared to other 5 experimented kernels. Algorithm Approximate Learning Structural EM SVM (rbf) Avg. Accuracy 87.5% 69.38% 85.96%



4. Conclusions In this paper we discuss the Bayesian network structure learning problem with missing data. We present an approach which performs well even when data are not missing at random. We define an optimization task to tackle the problem and propose a new exact algorithm for it which translates the task into a structure learning problem without missing data. Inspired by the exact procedure, we develop an approximate algorithm which employs structure optimization as a subcall. Experiments show the advantages of such approach. The proposed approximate method can scale to domains with hundreds or even thousands of variables. We intend to investigate such avenue in future work.



17



Table 3: MNAR imputation accuracy for the BN Approximate Learning algorithm and Structural EM on the SPECT dataset with various proportions of missing values, and for both data values. Missing values 3% (overall) Algorithm New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM Average imputation accuracy 81.75% 60% 75.22% 49.27% 81.94% 62.04% 95.65% 56.52% 80.43% 39.13% 92.75% 60.87% 67.83% 63.48% 70% 59.4% 71.13% 63.2%



5% (overall)



10% (overall)



3% (missing value = 0)



5% (missing value = 0)



10% (missing value = 0)



3% (missing value = 1)



5% (missing value = 1)



10% (missing value = 1)



18



References Almeida, E., Ferreira, P., Vinhoza, T., Dutra, I., Wu, Y., Burnside, E., 2014. Expertbayes: Automatically refining manually built Bayesian networks. Machine Learning and Applications (ICMLA) 13, 362-366. Bartlett, M., Cussens, J., 2013. Advances in Bayesian network learning using integer programming. Conference on Uncertainty in artificial intelligence (UAI) 29, 182-191. Blake, C., Merz, C., 1998. UCI machine learning repository of machine learning databases. Bohanec, M., Rajkovic, V., 1988. Knowledge acquisition and explanation for multi-attribute decision making. Intl. Workshop on Expert Systems and their Applications 8, 59-78. Borchani, H., Amor, N. B., Mellouli, K., 2006. Learning Bayesian network equivalence classes from incomplete data. Lecture Notes in Comp. Science, 291-295. Buntine, W., 1991. Theory refinement on Bayesian networks. Conference on Uncertainty in artificial intelligence (UAI) 7, 52-60. Chickering, D., 1996. Learning Bayesian networks is NP-complete. Learning from Data, 121-130. Cooper, G., Herskovits, E., 1992. A Bayesian method for the induction of probabilistic networks from data. Machine Learning 9, 309-347. de Campos, C., Ji, Q., 2011. Efficient structure learning of Bayesian networks using constraints. Journal of Machine Learning Research (JMLR) 12, 663- 689. D'Orsi, C., Bassett, L., Berg, W., Feig, S., Jackson, V., Kopans, D., 2003. BI-RADS: Mammography. American College of Radiology 4. Fogelman-Soulie, F., 2008. Mining massive data sets for security: Advances in data mining, search, social networks and text mining, and their applications to security. Vol. 19. IOS Press.



19



Friedman, N., 1998. The Bayesian structural em algorithm. Conference on Uncertainty in artificial intelligence (UAI) 14, 129-138. Friedman, N., Geiger, D., Goldszmidt, M., 1997. Bayesian network classifiers. Machine Learning 29, 131-163. Gruder, L., Mermelstein, J., Kirkendol, S., D., D. H., Wong, C., Schreckengost, J., Warnecke, R., Burzette, R., Miller, T., 1993. Effects of social support and relapse prevention training as adjuncts to a televised smoking cessation intervention. J Consult Clin Psychol 61, 113-120. Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I., 2009. The WEKA data mining software: An update. SIGKDD Explor. Newsl. 11, 10-18. Heckerman, D., Geiger, D., Chickering, D., 1995. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning 20, 197-243. Hedeker, D., Mermelstein, J., Demirtas, H., 2007. Analysis of binary outcomes with missing data: missing = smoking, last observation carried forward. Addiction 102, 1564-1573. Koivisto, M., 2006. Parent assignment is hard for the mdl, aic, and nml costs. conference on Learning Theory (COLT) 19, 289-303. Lauritzen, S., Speigelhalter, D., 1988. Local computations with probabilities on graphical structures and their application to expert systems. Royal statistical Society B 50, 157-224. Leray, P., Francois, O., 2005. Bayesian network structural learning and incomplete data. Intl. and Interdisc. Conf. on Adaptive Knowledge Repr. and Reasoning (AKRR), 33-40. Lichman, M., 2013. UCI machine learning repository. URL http://archive.ics.uci.edu/ml Little, R., Rubin, D., 1987. Statistical analysis with missing data. John Wiley & Sons.



20



Meila, M., Jordan, M., 1998. Estimating dependency structure as a hidden variable. Advances in Neural Information Processing Systems (NIPS), 584- 590. Melacci, S., Belkin, M., 2011. Laplacian support vector machines trained in the primal. Journal of Machine Learning Research (JMLR) 12, 1149-1184. Mohan, K., Pearl, J., Tian, J., 2013. Graphical models for inference with missing data. Advances in Neural Information Processing Systems (NIPS), 1277-1285. Ramoni, M., Sebastiani, P., 1997. Learning Bayesian networks from incomplete databases. Conference on Uncertainty in artificial intelligence (UAI) 13, 401- 408. Rancoita, P., Zaffalon, M., Zucca, E., Bertoni, F., de Campos, C., 2016. Bayesian network data imputation with application to survival tree analysis. Computational Statistics & Data Analysis 93, 373-387. Riggelsen, C., 2006. Learning Bayesian networks from incomplete data: An efficient method for generating approximate predictive distributions. SDM, 130-140. Riggelsen, C., Feelders, A., 2005. Learning Bayesian network models from incomplete data using importance sampling. AISTATS, 301-308. Sarabando, A., 2011. Um estudo do comportamento de redes Bayesianas no prognstico da sobrevivencia no cancro da prostata. M.Sc. thesis, Universidade do Porto 29, 131-163. Scanagatta, M., de Campos, C., Corani, G., Zaffalon, M., 2015. Learning Bayesian networks with thousands of variables. Advances in Neural Information Processing Systems (NIPS), 1855-1863. Van den Broeck, G., Mohan, K., Choi, A., Pearl, J., 2014. Efficient algorithms for Bayesian network parameter learning from incomplete data. In: Causal Modeling and Machine Learning Workshop at ICML-2014. pp. R-441.



21



The KOJAK Group Finder: Connecting the Dots via Integrated Knowledge-Based and Statistical Reasoning

Jafar Adibi, Hans Chalupsky, Eric Melz and Andre Valente

USC Information Sciences Institute 4676 Admiralty Way, Marina del Rey, CA 90292 Email: {adibi, hans, melz, valente}@isi.edu



Abstract

Link discovery is a new challenge in data mining whose primary concerns are to identify strong links and discover hidden relationships among entities and organizations based on low-level, incomplete and noisy evidence data. To address this challenge, we are developing a hybrid link discovery system called KOJAK that combines state-of-theart knowledge representation and reasoning (KR&R) technology with statistical clustering and analysis techniques from the area of data mining. In this paper we report on the architecture and technology of its first fully completed module called the KOJAK Group Finder. The Group Finder is capable of finding hidden groups and group members in large evidence databases. Our group finding approach addresses a variety of important LD challenges, such as being able to exploit heterogeneous and structurally rich evidence, handling the connectivity curse, noise and corruption as well as the capability to scale up to very large, realistic data sets. The first version of the KOJAK Group Finder has been successfully tested and evaluated on a variety of synthetic datasets.



Introduction

The development of information technology that could aid law enforcement and intelligence organizations in their efforts to detect and prevent illegal and fraudulent activities as well as threats to national security has become an important topic for research and development. Since the amount of relevant information, tips, data and reports increases daily at a rapid pace, analyzing such data manually to its full potential has become impossible. Hence, new automated techniques are needed to take full advantage of all available information. One of the central steps in supporting such analysis is link discovery (LD), which is a relatively new form of data mining. Link discovery can be viewed as the process of identifying complex, multi-relational patterns that indicate potentially illegal or threat activities in large amounts of data. More broadly, it also includes looking for not directly explainable connections that may indicate previously unknown but significant relationships such as new groups or capabilities (Senator, 2002) .

Copyright (c) 2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.



Link discovery presents a variety of difficult challenges. First, data ranges from highly unstructured sources such as reports, news stories, etc. to highly structured sources such as traditional relational databases. Unstructured sources need to be preprocessed first either manually or via natural language extraction methods before they can be used by LD methods. Second, data is complex, multi-relational and contains many mostly irrelevant connections (connectivity curse). Third, data is noisy, incomplete, corrupted and full of unaligned aliases. Finally, relevant data sources are heterogeneous, distributed and can be very high volume. To address these challenges we are developing a hybrid link discovery system called KOJAK that combines stateof-the-art knowledge representation and reasoning (KR&R) technology with statistical techniques from the area of data mining. Using KR&R technology allows us to represent extracted evidence at very high fidelity, build and utilize high quality and reusable ontologies and domain theories, have a natural means to represent abstraction and meta-knowledge such as the interestingness of certain relations, and leverage sophisticated reasoning algorithms to uncover implicit semantic connections. Using data or knowledge mining technology allows us to uncover hidden relationships not explicitly represented in the data or findable by logical inference, for example, entities that seem to be strongly related based on statistical properties of their communication patterns. The full KOJAK system contains a variety of experimental LD components such as an abductive, logicbased Pattern Finder to identify complex patterns of interest in the evidence and a Connection Finder to identify interesting and unusual entities and connections (Lin & Chalupsky 2003). In this paper we only report on the architecture and technology of its first fully completed module called the KOJAK Group Finder (GF). The Group Finder is capable of finding hidden groups and group members in large evidence databases. Our group finding approach addresses a variety of important LD challenges, such as being able to exploit heterogeneous and structurally rich evidence, handling the connectivity curse, noise and corruption as well as the capability to scale up to very large, realistic data sets. The first version of the KOJAK Group Finder has been successfully tested and evaluated on a variety of synthetic datasets.



The Group Detection Problem

A major problem in the area of link discovery is the discovery of hidden organizational structure such as groups and their members. There are of course many organizations and groups visible and detectable in real world data, but we are usually only interested in detecting certain types of groups such as organized crime rings, terrorist groups, etc. Group detection can be further broken down into (1) discovering hidden members of known groups (or group extension) and (2) identifying completely unknown groups. A known group (e.g., a terrorist group such as the RAF) is identified by a given name and a set of known members. The problem then is to discover potential additional hidden members of such a group given evidence of communication events, business transactions, familial relationships, etc. For unknown groups neither name nor known members are available. All we know are certain suspicious individuals ("bad guys") in the database and their connection to certain events of interest. The main task here is to identify additional suspicious individuals and cluster them appropriately to hypothesize new real-world groups, e.g., a new money laundering ring. While our techniques address both questions, we believe group extension to be the more common and important problem. Another important problem characteristic that influenced our solution approach concerns the data. Evidence available to law enforcement organizations is split into primary and secondary sources. Primary evidence is lower volume, high reliability, usually "owned" by the organization and can be searched and processed in arbitrary ways. Secondary evidence is usually not owned by the organization (e.g., might come from news articles or the Web), is higher volume, might only be searchable in restricted ways and might be associated with a cost (e.g., access might require a warrant). Our group detection approach needs to take these different characteristics into account to keep cost at a minimum and properly handle access restrictions to secondary data sources.



extended group. Third, the mutual information model is used to rank these likely members by how strongly connected they are to the seed members. Fourth, the ranked extended group is pruned using a threshold to produce the final output.

Patterns, Constraints Logic-based Group Seed Generator Named Group Seeds Primary Evidence Secondary Evidence



Extend Group



Extended Group



(MI-)Rank Group Threshold Group

Unnamed Group Hypotheses



MI-Ranked Extended Group



Mutual Information Module



Cluster Groups

Named Group Hypotheses



Figure 1: KOJAK Group Finder Architecture. The processing for known and unknown groups is somewhat different at the beginning and end of the process. First, the seed generation for unknown groups is different, since there is less information available. Second, the generation of unknown groups involves an extra step because the extended groups need to be clustered to eliminate duplicates before the thresholding step. The logic-based seed generation module is based upon the PowerLoomTM knowledge representation & reasoning system (PowerLoom, 2003). The mutual information module was implemented in the Matlab programming language. The modules are integrated by combining the C++ translation of PowerLoom and the C translation of the Matlab modules into a single program. Evidence databases are stored in MySQL and accessed from both Matlab and PowerLoom via ODBC. The primary and secondary evidence databases uses a very general evidence schema developed as part of DARPA's Evidence Extraction and Link Discovery (EELD) program (Senator, 2002) which should make it easy to transition to different domains.



The KOJAK Group Finder

The KOJAK Group Finder is a hybrid logicbased/statistical LD component designed to solve group detection problems. It can answer the following questions: * How likely is P a member of group G? * How likely are P and Q members of the same group? * How strongly connected are P and Q? Figure 1 shows the general architecture. The system takes primary and secondary evidence (stored in relational databases) as input and produces group hypotheses (i.e., lists of group members) as output. The system works in four phases. First, a logic-based group seed generator analyzes the primary evidence and outputs a set of seed groups using deductive and abductive reasoning over a set of domain patterns and constraints. Second, an information-theoretic mutual information model finds likely new candidates for each group, producing an



The Need for a Hybrid Approach

Link discovery is a very challenging problem. It requires the successful exploitation of complex evidence that comes in many different types, is fragmented, incomplete, uncertain and very large-scale. LD requires reasoning with abstractions, e.g., that brother-of and husband-of are both subtypes of a family-relation, temporal and spatial reasoning, e.g., that cities are subregions of counties which are subregions of states, etc., common-sense type inferences, e.g., that if two people bought tickets for the same event, they probably were at one point in close spatial proximity in the same city, and constrained search, e.g., one might want to look more closely at people who joined a company around the same time a suspect joined. The knowledge



and ontologies needed for these types of inferences are very naturally modeled in a symbolic, logic-based approach as done in the logic-based seed generator of the KOJAK Group Finder. However, LD also needs detection and reasoning with statistical phenomena such as communication patterns, behavior similarity, etc., which requires cumulative analysis of evidence that cannot be done in logic but is most effectively done in specialized models such as our mutual information component. Such models, on the other hand, are not well-suited for the representation of complex domains and usually assume some data normalization and simplification. Given these characteristics of the problem, using a hybrid approach that combines the strengths of multiple paradigms is a natural choice. How these two approaches work together for the KOJAK Group Finder is described below.



memberAgentsByParticipation formalizes this type of reasoning (memberAgents relates a group and its members; deliberateActors relates groups or people to an event): (DEFRELATION memberAgentsByParticipation ((?g Group) (?p Person)) :<= (AND (Group ?g) (Person ?p) (FAIL (memberAgents ?g ?p)) (EXISTS (?c) (AND (ExploitationCase ?c) (deliberateActors ?c ?g) (deliberateActors ?c ?p)))))



Logic-Based Seed Generation

The first phase of the KOJAK group detection process is the generation of seed groups. Each seed group is intended to be a good hypothesis for one of the actual groups in the evidence data, even though the number of seed members known or inferable for it might be significantly less than its actual members. The reasons for using this logic-based, seeded approach are threefold. First, the information in primary and secondary evidence is incomplete and fragmented. By "connecting the dots" via logical inference we can extract information that is not explicitly stated and our statistical methods would not be able to infer. Second, because the MI model needs to analyze access-restricted secondary data, it needs good initial focus such as seed groups of "bad guys" in order to query the data selectively. The seeded approach therefore dramatically reduces data access cost as well as MIprocessing time. Third, logical reasoning can apply constraints to the information available as well as rule out or merge certain group hypotheses. To generate seed groups we use the PowerLoom KR&R system to scrub every piece of available membership information from primary evidence (which is smaller volume, less noisy and can be searched arbitrarily). Given the size of primary evidence data we are working with (O(10,000) individuals and O(100,000) assertions) we can simply load it directly from the EDB into PowerLoom using its database interface and a set of import axioms. The process of finding seeds is different for known and unknown groups. For known groups, we start with a query to retrieve existing groups and their explicitly declared members. We then employ a number of logic rules to infer additional group members by connecting data that is available but disconnected. For example, in the synthetic datasets available to us members of threat groups participate in exploitation cases (meant to model threat events such as a terrorist attack). To find additional members of a group we can look for exploitations performed by a group that have additional participants not explicitly known to be members of the group. The PowerLoom definition below for the relation



For unknown groups, we use rules to look for patterns on events to find seeds. The basic idea is to find teams participating in threat events that no (known) group is known to be responsible for. Since people who participate in a threat event are part of a threat group, teams of people who are found to jointly participate in a threat event that cannot be attributed to a known group can be used as seeds for unknown groups. Note, however, that such teams may be subsets of one of the known groups or that two or more of the teams may be part of the same unknown group. For that reason, it is vital to use merging techniques later to combine teams (or their extended groups) if appropriate. The logic module can also check constraints to help in the merging of hypotheses. For example, a strong hint that two groups may be the same is that their members participated in the same exploitation events. The rule below finds groups who participated in a given exploitation event indicating a potential duplicate group hypothesis if more than one group is found:

(DEFRELATION groupHasMemberWhoParticipatedInEvent ((?g Group) (?e VulnerabilityExploitationCase)) :<= (AND (Group ?g) (VulnerabilityExploitationCase ?e) (EXISTS ?p (AND (Person ?p) (OR (memberAgents ?g ?p) (memberAgentsByParticipation ?g ?p)) (deliberateActors ?e ?p))))) The use of memberAgentsByParticipation shows that these rules



not only encode complex queries but also interconnect to build a real domain model. There are about 50 complex rules of this type that are specific to group discovery. Even though the synthetic dataset used in our experiments was designed to be relatively poor in link types and attributes, the data is still quite complex. It contains 72 entity types (22 of which are actually instantiated) and 107 relations and attributes (25 of which are actually instantiated in the data). These entity and relation types are further organized by an ontology (developed by Cycorp) whose upward closure from the entity and relation types in the data contains a hierarchy of about 620 concepts (or classes) and 160 relations. Adding this to the O(100,000) assertions representing the evidence we have a fairly large and complex knowledge base to work with. While the examples given above are specific to the synthetic group discovery domain, the approach is general and applicable to other areas. Evidence data will always be fragmented. Such fragmentation is usually easy to handle by a human analyst, but it can be a big obstacle for an automated system. Using a logic-based model of the



domain is a very powerful approach to overcome this problem and connect evidence fragments in useful ways.



E: 2 P: 2 M: 1



E: 2 P: 1 M: 0 P1 E: 1 P: 1 M: 2 P3 MI E: 1 P: 3 M: 4 E: 0 P: 1 M: 1 P4 E: 0 P: 1 M: 1 P2



Time

E: 3 P: 2 M: 2



Action P1 Email P2 P3 Phone Call P4 P2 Meeting P3 P1 Email P2 P2 Email P3 P1 Phone Call P3 P1 Phone Call P2 P2 Meeting P3 P3 Meeting P4 P1 Meeting P3 P2 Phone Call P3



P1 E   E  P P   M 



P2 E  M E E  P M   P



P3  P M  E P  M M M P



P4  P       M  



1 2 3 4 5 6 7 8 9 10 11



Finding Strong Connections Via a Mutual Information Model

After exploiting the various explicit and implicit evidence fragments given in the EDB to generate a seed group, we try to identify additional members by looking for people that are strongly connected with one or more of the seed members. To find two strongly connected entities, we need to aggregate the many other known links between them and statistically contrast those with connections to other entities or the general population. This cannot be done via a logic-based approach and instead is achieved via an information-theoretic mutual information model. The mutual information model can identify entities strongly connected to a given entity or a set of entities and provide a ranked list based on connection strength. To do this it exploits data such as individuals sharing the same property (e.g., having the same address) or being involved in the same action (e.g., sending email to each other). Since such information is usually recorded by an observer we refer to it as evidence. Time is often also an important element of evidence and is also recorded in the EDB. Without loss of generality we only focus on individuals' actions in this paper, but not on their properties. We transform the problem space into a graph in which each node represents an entity (such as a person) and each link between two entities represents the set of actions (e.g., emails, phone calls etc.) they are involved in. For each node we represent the set of its actions with a random variable, which can take values form the set of all possible actions. Figure 2 illustrates this concept. There are four people and three possible actions: sending Email, making a Phone Call and participating in a Meeting. When a person is not involved in any of the above-mentioned actions we indicate that with the empty action . For example, we can represent P1`s actions with the random variable X1 which takes values from the set {E, P, M, } at any given time. Most individuals in the LD evidence space are connected to each other either directly or indirectly. For example, two people may eat at the same restaurant, drink coffee at the same cafe and take the same train to work every day without any strongly meaningful connection. On the other hand, three individuals may be strongly connected if they engage in atypical phone call patterns. To address this problem we measure the mutual information (MI) between the random variables representing individuals' activities. MI is a measure of the dependence between two variables. If the two variables are independent, the MI between them is zero. If the two are strongly dependent, e.g., one is a function of another; the MI between them is large. We therefore believe that two individuals' mutual information is a good indicator whether they are in fact strongly connected to each other or not compared to the rest of the population. There are other interpretations of MI, for example, as the stored information in one variable about another

P3 P3 P2 P3 P2 P1 P1 P4



E: 0 P: 1 M: 1



0.74 0.61 0.49 0.22



Figure 2: MI Example. P1, P2, P3 and P4 represent people. E, P and M stand for Email, Phone Call and Meeting respectively. The table on the right shows activities among individuals and the table on the left shows the MI among them. variable or the degree of predictability of the second variable by knowing the first. Clearly, all these interpretations are related to the same notion of dependence and correlation. The correlation function is another frequently used quantity to measure dependence. The correlation function is usually measured as a function of distance or time delay between two quantities. It has been shown that MI measures the more general (nonlinear) dependence while the correlation function measures linear dependence (Li, 1990). Therefore, MI is the more accurate choice to measure dependence. One of the important characteristics of MI is that it does not need actual variables values to be computed, instead it only depends on the distribution of the two variables. In classical information theory (Shannon, 1948) MI between two random variables X and Y is defined as:  

MI ( X ; Y ) = P( y | x)  P ( x) P( y | x)   P ( x ) P ( y | x ). log   

x y



where P( x) is the Prob ( X = x) , P( y ) is the Prob (Y = y ) and P ( y | x) stands for Prob (Y = y | X = x) . In addition, MI(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y), where the conditional entropy H(X|Y) measures the average uncertainty that remains about X when Y is known (see (Adibi et al. 2004) for more details about the MI model).







x







Group Expansion via Mutual Information

Given that we can use the mutual information calculation to find strongly connected individuals, we can exploit this capability to expand the seed groups provided in phase 1 by the logic-based KR&R module. This expansion is done in the following steps: (1) For each seed member in a seed group we retrieve all activities it participates in from primary and secondary data and add any new individuals found to the group. This step therefore expands the seed group graph by one level. Note, that we obey query restrictions for secondary data and only ask one focused query per seed member. (2) Now we view the expanded group as the universe and compute MI for each connected pair in the graph. (3) Next we look for individuals that either have high MI score with one of the seed members or with all seed



members when viewed as a single "super individual". Members whose score is below a certain (fairly lax) userdefined threshold are dropped from the list. (4) In this step the MI engine repeats the whole procedure by expanding the expanded group from the previous step one more level and recalculates MI for the new graph. For known groups we stop here and pass the result to the final thresholding step. (5) For unknown groups we usually have much smaller seed sets and therefore repeat the previous step one more time to achieve appropriately-sized group hypotheses. The group expansion procedure is performed for each seed group generated by the KR&R module and generates an MI-ranked list of possible additional members for each seed group. This list is initially kept fairly inclusive and needs to undergo proper thresholding before it can be reported to a user or passed on to another LD component.



Signal Phone Call Corruption Report of Phone Call



Negative Noise



Report of Email



Corruption No occurrence of Phone Call Sender

Ground Truth



Report of Meeting



Signal Receiver

Evidence Database



No Report of Phone Call in the database



Figure 3: Noise model for a given "Phone Call" as the "sender" and the evidence database (EDB) as the "receiver". While in a noiseless environment information is recorded in the EDB without error, in a noisy environment we have a noisy channel, which may alter every piece of evidence transmitted through it with some small probability p(noise). For instance, negative noise occurs if there is a phone call in the ground truth but no record of it in the EDB. Corruption occurs, for example, if there is no phone call in the ground truth but a record indicating one in the EDB. The MI framework is a natural fit for such model. Figure 3 illustrates a noisy channel for a given phone call.



Threshold Selection and Thresholding

The result of the process described above is a list of extended groups where members are ranked by their mutual information scores. In order to produce and report a definite result on which members we believe are actually part of the group, we need to cut the ordered list at some threshold. The problem is how to set the threshold so that we get "good" (or even "optimal") recall and precision for a particular application scenario. We used an empirical method that selects a threshold for a dataset based on an empirical analysis of a number of groups in different types of datasets. This method is discussed further in the section describing the experimental results. The good news is that (1) our group detection process generates a very selective ranking (i.e., we reach high recall fairly early) and (2) in real-world situations a good ranking is often more important than picking the best possible cutoff, since human analysts might be willing to accept a certain number of false positives in order to maximize the number of true positives they are after.



Complexity and Dataset Scale

Real-world evidence data sets can be very large and we have to make sure that our techniques scale appropriately. The largest synthetic datasets we have analyzed so far contained O(100,000) events and O(10,000) individuals. Running the KOJAK GF on such a dataset takes roughly 5 minutes for the logic-based seed generation and about 1020 minutes to run the MI engine on a 2Ghz Pentium-IV desktop with 1Gb of RAM. Runtime for the MI engine varies depending on the overall connectivity of the data. While this is perfectly acceptable at the moment, we will eventually need to handle datasets that are at least two orders of magnitude larger, so let us look a bit closer at the architecture and algorithm complexity involved. The complexity of the MI model is relatively low. The MI engine expands only a limited number of nodes in the problem space starting from the seed members of a group. How many individuals are considered depends on how deeply we grow the link graph to build an extended group. So far, one to two levels have been sufficient. Computing MI between two individuals is O(N*M) where N is the average number of people connected to a given individual and M is the average number of links a person is involved in. Unless N and M grow significantly with larger datasets, the overall complexity is primarily dependent on the number of threat groups we are looking for. To be able to handle such large datasets in the logic-based seed generation phase, we built a new database access layer into PowerLoom that allows us to easily and transparently map logic relations onto arbitrary database tables and views. By using these facilities we can keep smaller data portions such as the primary data in main



Handling Noise Via a Noisy Channel model

So far we assumed that we are capable to observe all evidence accurately. However, such accuracy occurs rarely in real world databases. We therefore consider the following kinds of noise in the formulation of our model: Observability (Negative Noise): This factor describes how much of the real data was observable. Not all relevant events that occur in the world will be observed or reported and might therefore not be known to LD components. Corruption: This type of noise varies from typos to misspelled names all the way to intentional misinformation. The negative noise phenomenon has been discussed extensively in the communication literature. We adopt the view of a classical noisy channel scenario where a sender transmits a piece of information to a receiver. The transmission goes through a channel with certain noise properties. In our domain we view the ground truth (GT)



memory for fast access and processing, while keeping potentially very large secondary data sets in an RDBMS from where we page in relevant portions on demand. Particular attention was paid to be able to offload large join processing to the RDBMS wherever possible to avoid doing it inefficiently tuple-by-tuple in PowerLoom. This gives us an architecture where we use a traditional RDBMS for storage and access to very large datasets but enrich it with a deductive layer that allows us to formulate more complex queries where necessary. The complexity of the resulting system depends heavily on the nature of the queries and domain rules used which so far has proven to be manageable. For example, the current system uses an ontology with about 800 concept and relation definitions and about 50 complex, non-taxonomic rules that link evidence fragments without any performance problems.



Number of entities Number of Links Number of Distinct Threat Pattern Lowest Signal to clutter ratio Lowest Signal to Noise Ratio Observability Corruption of Evidence



10,000 100,000 20 0.3(-5 db) .008(-21 db) 50%-100% 0-25%



Table 1: Synthetic Data Characteristics (1) Individual and group information. The existence of most individuals and some of the groups is available directly in the evidence. The groups available in the evidence are known or named groups discussed earlier. (2) Activities from individuals. Individuals participate in activities related to resources, capabilities and events. Much like in the real world, information about those activities is not available directly, but rather indirectly as transactions (e.g., phone calls or email messages). Synthetic Data Characteristics One of the key advantages of using a simulated world is that we are able to test our system against a wide range of datasets. In other words, we are able to create datasets with (almost) arbitrary characteristics, and therefore better understand the potential and limitations of our techniques. Some of the features used in defining the datasets are in Table 1. The values displayed are typical for the datasets we used in our evaluation; each dataset employs different values for each of these features. Of particular interest are observability (how much of the artificial world information is available as evidence), corruption (how much of the evidence is changed before being reported) and clutter (how much irrelevant information that is similar to the information being sought is added to the evidence). Evaluation Metrics The quality of groups we find can be measured with traditional precision and recall metrics defined as follows: Given a proposed group G with g members which matches an answer group A with a members, and given that of the g proposed members only c are correct, precision P=c/g and recall R=c/a. Another metric that helps us analyze precision and recall in aggregate is the F-measure: (b 2 + 1) PR F =- 2 b P+R The F-measure both requires and allows us to specify the desired trade-off between precision and recall through the b variable. A value of b=1 indicates that precision and recall are equally important; b = 2 means that recall is twice as important as precision, etc. That is, using the Fmeasure allows users of our module to specify their own desired trade-offs in terms of b.



Experimental Set-Up

We have applied the KOJAK Group Finder to a wide variety of synthetic data. Access to real world databases has been a main concern in AI, machine learning and data mining communities in the past. The LD community is not an exception in this matter. In particular, since the LD goal is to relate people, place and entities, it triggers privacy concerns. The balance between privacy concerns and the need to explore large volumes of data for LD is a difficult problem. These issues motivate employing synthetic data for performance evaluation of LD techniques. Synthetic Data For the purpose of evaluating and validating our techniques, we tested them on synthetic datasets developed by Information Extraction & Transport, Inc. within the EELD Program (Silk 2003, Schrag 2003). These synthetic datasets were created by running a simulation of an artificial world. The main focus in designing the world was to produce datasets with large amounts of relationships between agents as opposed to complex domains with a large number of entity properties. From the point of view of group detection, the artificial world consists of individuals that belong to groups. Groups can be threat groups (that cause threat events) or non-threat-groups. Targets can be exploited (in threat and non-threat ways) using specific combinations of resources and capabilities; each such combination is called a mode. Individuals may have any number of capabilities or resources, belong to any number of groups, and participate in any number of exploitations at the same time. Individuals are threat individuals or non-threat individuals. Every threat individual belongs to at least one threat group. Non-threat individuals belong only to nonthreat groups. Threat groups have only threat individuals as members. Threat individuals can belong to non-threat groups as well. A group will have at least one member qualified for any capability required by any of its modes. Non-threat groups carry out only non-threat modes. The evidence available in the dataset for our analysis consists of two main types of information:



1.00 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 1 35 69 103 137 171 205 239 273 307 341 375 409 443 477 511 545 579 613 647 Number of members 681 B=0.5 B=1.0 B=1.5 B=2.0



Logic Module Data set Plain High clutter Low observability Both Number Avg. Avg. of Precision Recall Groups 14 11 16 19 1 1 1 1 0.53 0.53 0.52 0.50



KOJAK Group Finder Avg. Avg. Precision Avg. Recall F-Measure Precision Variance Recall Variance (b=1.5) 0.81 0.59 0.70 0.88 0.005 0.010 0.004 0.005 0.87 0.86 0.72 0.66 0.010 0.014 0.026 0.011 0.85 0.74 0.71 0.75



Table 2: Scores for applying the KOJAK Group Finder to datasets of increasing complexity (known groups only). Results We have applied KOJAK to 26 datasets of varying complexity and characteristics. Table 2 shows some sample metrics for four datasets. Since there are many groups in each dataset we provide mean and variance values for precision and recall among all groups in a dataset. The average F-measure for known groups varies between 0.71 and 0.85. Note that the differences in the properties of the datasets cause the best F-measure to be obtained with different recall and precision values. This shows that "harder" datasets, where precision drops more steeply require lower thresholds that yield lower recalls and higher precision values. A more detailed analysis with ROC curves is presented in (Adibi et al. 2004). Table 2 also compares the KOJAK results against a baseline of using only the logic module. The results show that the logic module is very accurate (precision = 1), meaning all members found are provably correct. However, since the evidence is incomplete the logic module achieves a maximum recall of about 50%. We also evaluated our threshold prediction model. We found that the average F-measure for these datasets compares to the optimum F-measure obtained by using the best possible threshold for each group would result only in a difference of around 6%. In other words, the threshold model only "misses" 6% of whatever was available in the extended groups.



Figure 4: F-measure curves for different thresholds for a typical group.



Experimental Results

Overall empirical properties Figure 4 shows a typical set of F-measure curves for different thresholds. An important property is that our Fmeasure curves have maximums (and thus optimums). Notice also that F-measure curves for higher values of b have wider "peaks", which means they are more "forgiving" in threshold selection (a given variation of threshold provokes a smaller variation in F-measure.) Threshold Analysis Focusing on the F-measure, we defined an empirical model that allowed us to predict good threshold values for a given type of dataset. Datasets vary in many dimensions, in particular on their levels of observability, corruption, and clutter. Our goal was to define a model parametric on these dataset dimensions. One key initial step is to define the base for the model. Possible bases include the average size of the groups we are looking for (if sufficiently known), the size of extended group and the size of the seed group. Our empirical analysis indicated that the best alternative is to use the size of extended group as a basis for defining the threshold. We found that the ratio between the real size of the group we would be looking for and the size of the extended group we created as a hypothesis varies little and is usually around 11%-14%. Another advantage is that this measure is organic to the mutual information model, that is, no additional information is needed. The empirical model consists of defining one specific threshold (as a percentage of the extended group size) for each type of dataset. We used thirteen types of datasets that employed combinations of different values for the parameters in Table 1. We then analyzed the F-measure curves to find optimums for each b-value (i.e., trade-off between precision and recall) and type of dataset. For example, for a b of 1, we predicted a threshold of 8% for a baseline dataset, 6% for a dataset with more clutter, 9% for a dataset with low observability and 3% for a dataset with both additional clutter and low observability. These thresholds are then used to predict the best threshold for a new dataset of a particular type.



Related Work

Link discovery (LD) can be distinguished from other techniques that attempt to infer the structure of data, such as classification and outlier detection. Classification and clustering approaches such as that of Getoor et al. (2001) try to maximize individual similarity within classes and minimize individual similarity between classes. In contrast, LD focuses on detecting groups with strongly connected entities that are not necessarily similar. Outlier detection methods attempt to find abnormal individuals. LD, on the other hand, identifies important individuals based on networks of relationships. Additionally, outlier techniques require large amounts of data including normal and abnormal cases, and positive and negative noise. This is inappropriate for LD applications that need to detect threats with few or no available prior cases. Mutual information has also been used in other domains such as finding functional genomic clusters in RNA expression data and measuring the agreement of object models for image processing (Butte, 2000).



Our work can be distinguished from other group detection approaches such as Gibson, (1998) and Ng, (2001) by three major characteristics. First, GF is unique since it is based on a hybrid model of semantic KR&R and statistical inference. There are very few approaches that use semantic information. Second, in our approach each type of relation (link) is valuable and treated differently, in contrast to work in fields such as Web analysis and social networks. Third, with our technique, multiple paths between individuals or groups (direct or indirect) imply a strong connection which is different from techniques which focus on finding chains of entities. The work closest to our own is that of Jeremy Kubica et al. (Kubica, 2002; Kubica, 2003) that uses a probabilistic model of link generation based on group membership. The parameters of the model are learned via a maximum likelihood search that finds a Gannt Chart that best explains the observed evolution of group membership. The approach has a strong probabilistic foundation that makes it robust in the face of very low signal-to- noise ratios. Another recent approach to the LD problem is the use of probabilistic models (Cohn, 2001; Friedman, 1999; Getoor, 2001). Kubica et al. (2001) present a model of link generation where links are generated from a single underlying group and then have noise added. These models differ significantly from ours since we do not assume a generative model of group formation, but rather probabilistically determine each entity's membership.



conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFRL or the U.S. Government.



References

Adibi, J., Valente, A., Chalupsky, H. & Melz, E. (2004). Group detection via a mutual information model. Submitted to KDD 2004. Butte, A. & Kohane, I. (2000). Mutual information relevance networks: Functional genomic clustering using pairwise entropy measurements. Pacific Symposium on Biocomputing. Honolulu, Hawaii. Cohn, D. & Hofmann, T. (2001). The missing link: a probabilistic model of document content and hypertext connectivity. Advances in Neural Information Processing Systems 13: 430-436. Friedman, N., Getoor, L., Koller, D. & Pfeffer, A. (1999). Learning probabilistic relational models. IJCAI 1999, San Francisco, Morgan Kaufmann Publishers. Getoor, L., Segal, E., Taskar, B., & Koller, D. (2001). Probabilistic models of text and link structure for hypertext classification. IJCAI 2001 Workshop on Text Learning: Beyond Supervision. Seattle, Washington. Gibson, D., Kleinberg, J. & Raghavan, P. (1998). Inferring Web communities from link topology. In Proceedings of the Ninth ACM Conference on Hypertext and Hypermedia. New York, ACM Press. Kubica, J., Moore, A., Cohn, D. & Schneider, J. (2003). Finding underlying connections: a fast method for link analysis and collaboration queries. International Conference on Machine Learning (ICML). Kubica, J., Moore, A., Schneider, J. & Yang, Y. (2002). Stochastic link and group detection. Eighteenth National Conference on Artificial Intelligence (AAAI). Li, W. (1990). Mutual information functions versus correlation functions. Journal of Statistical Physics 60: 823-837. Lin, S. & Chalupsky, H.. Using unsupervised link discovery methods to find interesting facts and connections in a bibliography dataset. SIGKDD Explorations, 5(2): 173-178, December 2003 Ng, A., Zheng, A. & Jordan, M. (2001). Link analysis, eigenvectors and stability. IJCAI 2001. PowerLoom (2003). www.isi.edu/isd/LOOM/PowerLoom. Schrag, R. et. al. (2003). EELD Y2 LD-PL Performance Evaluation, Information Extraction and Transport, Inc. Senator, T. (2002). Evidence Extraction and Link Discovery, DARPA Tech 2002. Shannon, C. (1948). A Mathematical Theory of Communication. Bell System Tech. Journal 27: 379-423. Silk, B. & Bergert, B. (2003). EELD Evidence Database Description, Information Extraction and Transport, Inc.



Conclusion and Future Work

In this paper we introduced the KOJAK Group Finder (GF) as a hybrid model of logic-based and statistical reasoning. GF is capable of finding potential groups and group members in large evidence data sets. It uses a logicbased model to generate group seeds and a multi-relational mutual information model to compute link strength between individuals and group seeds. Noise and corruption are handled via a noisy channel model. Our GF framework is scalable and robust, and exhibits graceful degradation in the presence of increased data access cost and decreased relational information. The Group Finder is best-suited for problems where some initial information or group structure is available (e.g. finding hidden members of existing groups vs. detecting completely new groups) which is a common case in many real world applications. Group detection is useful for law enforcement, fraud detection, homeland security, business intelligence as well as analysis of social groups such as Web communities. There are several lines of ongoing and future work, such as, determining group leaders by measuring their entropy, use of temporal information for more focused access to relevant information as well as employing sampling and data streaming techniques to deal with very large datasets.

Acknowledgments. This research was sponsored by the Defense Advance Research Projects Agency and Air Force Research Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-01-2-0583. The views and



Sensitivity of Diffusion Dynamics to Network Uncertainty

Abhijin Adiga, Chris Kuhlman, Henning S. Mortveit and Anil Kumar S. Vullikanti

Network Dynamics and Simulation Science Laboratory, Virginia Bioinformatics Institute, Virginia Tech, VA 24061 {abhijin, ckuhlman, hmortvei, akumar}@vbi.vt.edu



Abstract

Simple diffusion processes on networks have been used to model, analyze and predict diverse phenomena such as spread of diseases, information and memes. More often than not, the underlying network data is noisy and sampled. This prompts the following natural question: how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure? In this paper, we consider two popular diffusion models: Independent cascades (IC) model and Linear threshold (LT) model. We study how the expected number of vertices that are influenced/infected, given some initial conditions, are affected by network perturbation. By rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results. (1) For the IC model, we characterize the susceptibility to network perturbation in terms of the critical probability for phase transition of the network. We find the expected number of infections is quite stable, unless the the transmission probability is close to the critical probability. (2) We show that the standard LT model with uniform edge weights is relatively stable under network perturbations. (3) Empirically, the transient behavior, i.e., the time series of the number of infections, in both models appears to be more sensitive to network perturbations. We also study these questions using extensive simulations on diverse real world networks, and find that our theoretical predictions for both models match the empirical observations quite closely.



1



Introduction



A number of diverse phenomena are modeled by simple diffusion processes on graphs, such as the spread of epidemics (Newman 2003), viral marketing (Kempe, Kleinberg, and Tardos 2005; Goldenberg, Libai, and Muller 2001) and memes in online social media (Romero, Meeder, and Kleinberg 2011; Bakshy et al. 2011). It is common to associate with each vertex a state of 0 (denoting "not infected" or "not influenced") or state 1 (denoting "infected" or "influenced") in these models; each neighbor of a node in state 1 switches to state 1 based on a probabilistic rule. We focus on two such models, referred to as the independent cascades (IC) model (which is a special case of the SIR process), and linear threshold (LT) model. In most applications, however, the underlying networks are inherently noisy

Copyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.



and incomplete, since they are often inferred by indirect measurements, for instance: (i) networks based on Twitter data (e.g., (Gonzlez-Bailn et al. 2011; Bakshy et al. 2011; Galuba 2010)) are constructed by limited samples available through public APIs, (ii) biological networks are inferred by experimental correlations, e.g., (Hagmann 2008; Schwab et al. 2010), which might be incomplete, and (iii) the Internet router/AS level graphs are constructed using traceroutes, e.g., (Faloutsos, Faloutsos, and Faloutsos 1999), which are known to give a biased and incomplete structure (see, e.g., (Achlioptas et al. 2009)). This raises a fundamental issue for diffusion processes on networks: How does the uncertainty in the network affect the conclusions drawn from a study of the diffusion dynamics? For instance, how robust is an inference that there will be no large outbreak in the network, in the face of noise/uncertainty in the network? Recent statistical and simulation based studies involving perturbation of the network by "rewiring" pairs of edges (which preserves the degree sequence) show that changes in the network structure significantly alter the dynamics, and the efficacy of intervention mechanisms, even when aggregate structural properties, such as the degree distribution and assortativity are preserved (Eubank 2010; Chen 2010). Surprisingly, there is limited mathematically rigorous work to explain the empirical findings in a systematic manner, despite a large body of research on diffusion models. Our work is motivated by these considerations of sensitivity of the dynamics to noise and the adequacy of sampling of a network G = (V, E ). Since there is very limited understanding of how noise should be modeled, we consider a simple Random Edge Perturbation model for noise, in which each pair u, v of vertices is selected for addition/deletion with probability n , where > 0 is a parameter, and n is the number of vertices; thus, on average, only n edges are altered. This model has been used quite extensively both in social network analysis and computer science for understanding the sensitivity to graph properties, e.g., (Costenbader and Valente 2003; Borgatti, Carley, and Krackhardt 2006; Flaxman and Frieze 2004; Flaxman 2007). Let R( ) denote the random set of edges selected by this process; we denote the perturbed graph by G  R( ). We study how the expected number of infections, given some initial conditions, is affected by the extent of perturbation, .



Our contributions. 1. The Independent cascades model. We consider networks G which exhibit a phase transition in their component sizes, with a critical probability pc , and transmission probability p (see Section 3 for definitions). In Theorem 1, we characterize the expected number of infections in the perturbed graph in terms of p and pc : when p < pc , we show that there exist constants c  c , and a threshold t such that for < c t , the expected number of infections in the perturbed graph remains close to that in G; however, for > c t , there is a phase transition, and the expected number of infections after perturbation is much larger than that in G. The main implication is that the dynamics are quite robust to perturbations, unless the transmission probability is close to pc . We find this to be consistent with extensive simulations on a large number of real networks--the sensitivity to perturbations is maximized at a point which approximately matches the threshold t in many networks. We also examine the transient behavior (i.e., the time series of the number of infections), and find it to be more sensitive than the expected total number of infections. 2. The Linear threshold model. In Theorem 2, we show formally that for any network G with maximum degree D = O(n/ log n), the expected number of infections after perturbation, starting at s random initial infections, is bounded by O(s(D + + log n) log n). This implies that the dynamics is quite stable for low s and . Our result is based on the analysis of the random graph model in which each node selects a random in-edge (which is shown to "correspond" to the LT model by (Kempe, Kleinberg, and Tardos 2003)). We first show that the diameter is bounded by O(D log n), where D is the maximum degree of the perturbed graph, and then prove that the expected number of infections, starting at a random source, is bounded by the diameter. Our theoretical bounds corroborate well with our experimental observations on a large class of real networks, which show a gradual variation with . We find that the expected number of infections grows more sharply with , as the number of sources is increased. Further, as in the IC model, we find the transient behavior is more sensitive to . Discussion and implications. From the point of view of dynamical system theory, our work may be regarded as a study of stability of dynamics over a network with respect to the edge structure. The existence of the critical value for the parameter in the IC model can be thought of as a bifurcation point. Admittedly, our results only hold for the specific random edge perturbation model of noise; uncertainty in networks is a much more complex process, and might involve dependencies arising out of the network evolution. Although we focus on specific dynamical properties and the random edge perturbation model, our results give the first rigorous theoretical and empirical analysis of the noise susceptibility of these diffusion models. Further, our analytical and empirical techniques, based on the random graph characterization, are likely to help in the analysis of other, more complex, noise models, which take dependencies into account. Organization. Because of space limitations, we omit the details of some of our proofs and experimental results; these will be available in the complete version of the paper.



2



Related work



Noise and issues of sampling are well recognized as fundamental challenges in complex networks, and there has been some work on characterizing it and the sensitivity to different parameters, especially in network properties, such as: (i) (Costenbader and Valente 2003; Borgatti, Carley, and Krackhardt 2006) show that certain centrality measures are robust to random edge and node perturbations, and (ii) (Achlioptas et al. 2009) show that there is an inherent bias in traceroute based inference of the Internet router network, which might give incorrect degree distributions. Flaxman and Frieze (Flaxman and Frieze 2004; Flaxman 2007) formally characterize conditions under which the graph expansion and diameter is highly sensitive to random edge additions; these are among the few analytical results of this type. Some of the approaches to address noise include: (i) the prediction of missing links using clustering properties, e.g., (Clauset, Moore, and Newman 2008), and (ii) approaches such as "property testing" algorithms, e.g., (Ron 2010) and "smoothed analysis", e.g., (Spielman 2009) for efficient computation of graph properties. To our knowledge, most work on the sensitivity of graph dynamical systems to noise in the network is empirical. However, for regular networks such as rings, topics such as synchronization and bifurcations have been studied (Kaneko 1985; Wu 2005). As discussed earlier, (Eubank 2010; Chen 2010) study the effect of changes in the network by edge rewirings on the epidemic properties. (Lahiri et al. 2008) study the effect of stochastic changes in the network on influence maximization problems. They find, using simulations, that in the LT model, the spread size is quite robust; our techniques help explain some of these observations.



3



Preliminaries



Noise models There is no consensus on the best way to model uncertainty/noise, and we consider a simple model of random edge additions that has been studied quite extensively in social network analysis (Costenbader and Valente 2003; Borgatti, Carley, and Krackhardt 2006); some problems have also been studied analytically in this model (Flaxman and Frieze 2004; Flaxman 2007). Let G = (V, E ) be the unperturbed graph. Let R( ) = (V, E ( )) be a random graph on V in which each pair u, v  V is connected with probability n . The perturbation graph G = G  R( ) is a graph constructed in the following manner: each pair u, v  V is connected in G if (u, v )  R( ) - E or (u, v )  E - R( ). In other words, each pair u, v is selected for addition/deletion with probability n . We also consider perturbations involving just addition of edges: this is denoted by G + R( ), and consists of all edges (u, v )  E  R( ). Network diffusion models. Let G = (V, E ) denote an undirected network. Here we define the diffusion models we study. In each model, each vertex v  V can be in state xv  {0, 1}, with state 0 denoting "inactive/uninfected/uninfluenced" and state 1 denoting "active/infected/influenced", depending on the application. We restrict ourselves to monotone or progressive processes, i.e., an infected node stays infected. Each node is associated with an activation function whose inputs



include the states of its neighbors. This function computes the next state of the node. The diffusion process starts with a few vertices becoming active/infected; we refer to this set as the initial set or the seed set. For an initial set of active nodes S , let  (S ) denote the expected number of active nodes at termination; these models always reach fixed points. We consider the following models: 1. Independent Cascade (IC) Model (Kempe, Kleinberg, and Tardos 2003). This model is a special case of the SIR model for epidemics. An infected node v infects each neighbor w with probability p (referred to as the transmission probability). Equivalently, each edge (v, w) can be live with probability p, independently of all other edges. All those nodes which are connected to the initial set through a live path are considered infected. In the perturbed graph G = G + R( ), suppose (v, x) is a newly added edge, then, v tries to infect x with probability p and vice versa. 2. Linear Threshold (LT) Model. (Kempe, Kleinberg, and Tardos 2005) Each node v is associated with a threshold v  [0, 1], chosen uniformly at random. v is influenced by its neighbor w according to weight bv,w such that wN (v ) bv,w  1. Node v becomes infected if wA(v ) bv,w  v , where A(v )  N (v ) is the set of neighbors of v which are currently infected. In our analysis and experiments, we assume that bv,w = 1/deg (v, G) for all w  N (v ), where deg (v, G) is the degree of v in G. This means, v is influenced equally by all its neighbors. This model was considered in (Kempe, Kleinberg, and Tardos 2003). In the perturbed graph G = G + R( ), bv,w = 1/deg (v, G ), where deg (v, G ) is the new degree of v.



the closer is N2 to n- this follows from the rough estimates of n2 /N N2 are given in Table 1. We also show in Lemma 3 in the Appendix that this holds in the case of Erdos-Renyi random graphs. This assumption will play a role in Theorem 1. Theorem 1. Consider the IC model on a family of graphs G exhibiting the following properties: (1) it undergoes a phase transition with critical probability pc , with the additional assumption that for p < pc , all components in G(p) are o( n), with high probability and there are two functions N = N (n, p) and N2 = N2 (n, p) such that in the graph resulting from percolation in G at probability p, (2) the number of components is within (N, (1 + )N ) and (3) the sum of the square of the component sizes is within (N2 , (1+ )N2 ), with probability 1 - o(1), for a constant  > 0. Let G = G + R( ) denote the perturbed graph. If p < pc , then, there is a threshN old perturbation factor t = pn , such that for (i) < c(p) t ,

n where c(p) = N N2 , the expected number of infections in G starting at a random initial node is o(n), and for (ii) 1/p > > 2(1 +  ) t , for any constant  > 0, the expected number of infections in G starting at a random initial node is (n) as n  .

2



4



Analyzing the sensitivity of the independent cascade model



Proof. First we note that N2 /n  n/N and therefore, c(p)  1. Let {Ci |i  N } be the set of connected components of G(p). Let ni denote the size of Ci . The probability that components Ci and Cj are conn n p nected by at least one edge is at most i nj in G (p). Consider an instance H of the Chung-Lu random graph model (Chung and Lu 2002) with N nodes with weights w1 , . . . , wN , such that wi = ni p. The probability of w wj (n p)*(n p) n n p edge (i, j ) in H equals i w = i nk jp = i nj  k

k k



We now discuss the sensitivity of the IC model for graphs that exhibit a phase transition, which is discussed informally here. Given a graph G with n vertices, let G(p) denote the random spanning subgraph of G obtained by retaining each edge of G independently with probability p. Many graphs (e.g., the complete graph, random regular graphs) exhibit the following property: there is a critical probability pc such that if p < pc , all components in G(p) are "small", namely of size o(n), while if p > pc , there exists a giant component of size (n). Similar threshold phenomena has been observed (empirically) in the real-world graphs which we study. Let N denote the number of components in G(p) when p < pc . We note that if the number of nodes in G of degree  d is at least cn for a constant d (a property satisfied by scale free networks), then N = (n). This follows from the fact that the expected number of isolated vertices in G(p) is  c(1 - p)d n = c n, under this assumption. Using Chebychev inequality, it can be shown that with high probability, the number of isolated vertices is very close to the expected value. Hence, N  c n with high probability. Next we consider the function N2 : the sum of the square of component sizes in G(p). Clearly, n  N2  n2 . However, in all the networks which we analyzed, for p < pc , N2 happens to be of the order of n and the farther p is from pc ,



Pr[Ci and Cj are connected in G (p)], since k=1 nk = n. Thus, the connectivity in the Chung-Lu instance H dominates that in G . The second order average degree w avg

p i p) for H is w avg = i wi = i (nn = N2 p n . From the i i connectivity threshold in the Chung-Lu model, it follows that H has no giant component if w avg < 1, which gives n < pN = t c(p). 2 Next, suppose 1/p > > t . By inclusion-exclusion, it follows that the probability that components Ci and Cj are connected in G (p) by at least one edge is at least ni nj p (n nj p)2 n nj p - i2n  i2n , because of the assumption n 2 that ni = o( n) and p < 1. Next, consider another instance H of the Chung-Lu model with N nodes with weights w1 , . . . , wN , such that wi = ni p/2. The probability of edge w wj (n p/2)*(nj p/2) n nj p (i, j ) in H equals i w = i = i2n  k k k nk p/2 Pr[Ci and Cj are connected in G (p)]. Thus, the connectivity in the Chung-Lu instance H is dominated by that in i G . The average degree wavg for H is wavg = i w N = ni p n p i 2N = 2N . From the connectivity threshold in the Chung-Lu model, it follows that H has a giant component if wavg > 1 +  , for any constant  > 0, which gives  )N > 2(1+ = 2(1 +  ) t . Therefore, with high probapn bility G has a component with (n) vertices. Since there w2

2



N



1 2 4 6 3 5 7 2 4 6



1 3 5 7



incoming edge from some vertex u  Ti-1 . The set T0 is a singleton consisting of the root vertex r. The incoming edge for r is from some neighbor in k i=1 Ti . All of this is illustrated in Figure 1. First, we show the following: Lemma 1. In the LT model, let  = minvV,wN (v) bv,w . Each tree in the random subgraph HLT has depth 1 O 1  log n , with probability at least 1 - n3 .



Figure 1: A graph and an instance of the random graph HLT corresponding to the LT model. For the component T induced by {1, 2, 3, 4, 5}, 1 is chosen as the root and as a result, T0 = {1}, T1 = {3}, T2 = {2, 5} and T3 = {4}.



is a constant probability that the seed belongs to the giant component, it follows that the expected number of infections in this case is (n). Remark 1. We note that if < t c(p), for any seed set of size s (not necessarily random), the expected number of infections in G is o(sn).



Proof. Consider a tree T in HLT , which is partitioned into sets T0 , . . . , Tk , as mentioned above. For any i = 1, . . . , k - 1, a vertex v  Ti would become a root if it chooses an incoming edge from one of its descendants. The probability of this event is at least minwN (v) bv,w   . Therefore, the probability that none of the vertices in Ti becomes a root is at most 1 -  , which in turn implies that the probability that none of the vertices in Ti , i = k - 1, . . . , 1 becomes a root is at most (1 -  )k-2 . Hence, the probability that T has depth more than k = c * 1  log n + 2 for a constant c is at most n 1 (1 -  ) k -2  n 1 4 . Since there are at most n kc*  log n+2 such trees in HLT , the probability that any of these has depth 1 more than O 1  log n is at most n3 .



5



Analyzing the sensitivity of the linear threshold model



We now analyze the impact of edge perturbations on the LT model on a graph G = (V, E ). Recall that in the specific version of the LT model we consider here, we set bv,w = 1/deg (v ) for each node v  V and neighbor w  N (v ). (Kempe, Kleinberg, and Tardos 2003) show that the fixed points and the number of infected nodes they have, can be studied through an elegant random graph model, which we describe here. Construct a random directed graph HLT = (V, E ) in the following manner: For each node v  V , a neighbor w is chosen with probability bv,w and a directed edge is added from w to v . Figure 1 illustrates a graph G and an instance of HLT . Note that even though G is undirected, HLT is a directed graph. For a set S  V , let  (S, HLT ) denote the number of nodes reachable from S in HLT (including those in S ). Then, (Kempe, Kleinberg, and Tardos 2003) show that  (S ), the expected number of infections with a starting set S , satisfies  (S ) = HLT Pr[HLT ] (S, HLT ). We use this characterization to analyze the impact of edge perturbations. The random graph HLT constructed by the above process has the following structure: In each connected component T of HLT , every vertex has one incoming edge and therefore, there exists exactly one directed cycle. If we choose a vertex in the cycle as the root r and remove its incoming edge, then, T remains connected and corresponds to a tree rooted at r with all edges oriented away from r. In the rest of this section, we loosely refer to such a component as a "tree" with one cycle or sometimes just tree. T can be partitioned into sets T0 , . . . , Tk such that for each i > 0, a vertex v  Ti has an



Consider a vertex v contained in a tree T . Let n(v, T ) denote the number of vertices reachable from v in T . Then, the number of infections resulting from v is the expected value of n(v, T ), averaged over all random subgraphs HLT and trees containing v . Define A(T ) as 1 A(T ) = |T v T n(v, T ). Conditioned on a random | subgraph HLT , the average number of infections start|T | ing at a random source is T HLT A(T ) n ; the average number of infections starting at a random source is |T | HLT Pr[HLT ] T HLT A(T ) n . Lemma 2. For each tree T in a random subgraph HLT , A(T )  2d, where d is the depth of T .



 to be the tree obtained by removing the Proof. Define T  incoming edge for the root in T . As described above, T   is an out-tree. For each v  T , we define n(v, T ) as the -- this corresponds number of vertices reachable from v in T . We define A(T ) = to the size of the subtree rooted at v in T 1    n(v, T ), and prove that A(T )  d. We prove this | v T |T by induction on the depth of the out-tree. The base case is a leaf node u, for which A(u) = 1. . Suppose it has children v1 , . . . , va . Let r be the root of T i be the subtree rooted at vi , and let ni be the number of Let T i . By induction, A(T i ) = 1  vertices in T i n(v, Ti )  v T ni



d - 1. ) A( T = 1 | |T ) n(v, T

 v T a



=



1 ) + n(r, T | |T

a



i=1



1 | |T



i ) n(v, T

i v T a



= 



1+

i=1



ni i )  1 + A(T | |T



i=1



ni (d - 1) | |T



| - 1 |T 1+ (d - 1)  d | |T



) = |T |, and by The third equality follows because n(r, T  i ) = 1 n ( v, T ) definition, A(T . The first inequali i v T ni ity follows by the induction hypothesis, since the depth of i  d - 1. The second inequality follows because each T a  i=1 ni = |T | - 1. Next, we consider A(T ). We recall that T is a tree with a cycle of length at most d. Let the cycle consist of vertices u0 = r, u1 , . . . , ub , with b  d - 1. For each ui , n(ui , T ) = |T |, since there is a path from ui to r. For every other vertex ). This implies, A(T )  u = ui in T , n(u, T ) = n(u, T d|T |  |T | + A(T )  2d. Finally, we bound the number of infections in the perturbed graph below; empirically, we find that the expected number of infections in the LT model is not very sensitive to , which is consistent with the bound below, which is linear in . Theorem 2. Let G(V, E ) be a graph with maximum degree D. For the LT model where bv,w = 1/deg (v ) for each node v  V and w  N (v ), the expected number of infected vertices starting with a initial random seed set of size s in the perturbed graph G + R( ) is O(s(D + + log n) log n). Proof. By a direct application of Chernoff bound, it can be 1 shown that with probability at least 1 - n 3 , the maximum degree in G = G + R( ) is at most D + + c * log n 1 for a constant c; with the remaining probability of n 3 , the maximum degree is O(n). We consider the random graph process to generate a subgraph HLT of G . Since bv,w = 1/deg (v ) for each node v  V and w  N (v ), for this model, the value of  of Lemma 1 is 1/D and therefore, each tree in HLT has depth at most O((D + + log n) log n), with 1 probability at least 1 - n 3 . Conditioned on HLT satisfying this bound on the depth, A(T ) = O((D + + log n) log n) for all T  HLT . For HLT that does not satisfy the depth bound, we have A(T ) = O(n) for all T  HLT . Therefore, the expected number of infections for a single random seed is n O((D + +log n) log n)+O( n 3 ) = O ((D + +log n) log n). Hence proved.



6



Experimental results



We study the sensitivity to edge perturbations empirically on twenty diverse real-world networks (from (Leskovec 2011)) with varying degrees of perturbation and other factors for both IC and LT models. Our main conclusions are the following:



1. Sensitivity in the IC model: we find that our empirical results match quite well with Theorem 1-- the expected number of infections IC model is well-behaved with , unless p is close to pc . Further, in most networks, the sensitivity is maximized at a point which approximately matches the threshold t . Though Theorem 1 strictly holds for graphs showing a phase transition, we find that most of the networks we study exhibit such a phenomenon. 2. LT model: we find that the expected number of infections in the LT model is not very sensitive to , especially for low number of seeds (e.g., less than 10), confirming the general bound derived from Theorem 2. When the number of seeds is large, the rate of increase of the expected number of infections seems to be higher initially. 3. Sensitivity of transients/temporal characteristics: our preliminary results suggest that the transient behavior (the time series of #infections versus time) is more sensitive than the expected #infections to , in both models. 4. Additions vs deletions: we find that perturbations involving both edge additions and deletions do not alter the results by much, compared to perturbations involving just edge additions. This follows from the sparsity of the graphs, and corroborates our analytical results, to some extent. Because of space limitations, we only discuss a sample of the results, and omit the results involving edge deletions; the remaining will be available in the complete version of the paper. We consider twenty different networks from (Leskovec 2011), with values of ranging from 0 to 100, with results averaged over 10 independent simulation runs. A simulation runs consists of 100 separate diffusion instances on one graph instance. A diffusion instance is a computation of the state of every node as a function of time, from time t=0 to the specified maximum time. The Independent Cascades Model. Figure 2 shows the the expected fraction of infected nodes vs. for two networks (namely, the astrophysics co-authorship and epinions networks)-- they both show low sensitivity for a broad range of values. For each of the settings, the expected number of infections rises sharply; further, the networks show differences in the plots for different parameter values. Some of the results for other networks are summarized in Table 1, which shows two sets of results for each network. Both are estimates of t , the threshold perturbation factor, using two methods. (i) In Method 1 we estimate N , the number of connected components in a random subgraph from the simulations, and use Theorem 1 to determine t = N/np. (ii) In Method 2, we consider the plot of infection size with respect to for a particular transmission probability p (as in Figure 2), and choose t to be the point of maximum slope of the curve on the X -axis. We note that both methods seem to give similar estimates of t . We empirically observe that the standard plot of infection size vs. transmission probability p for all the networks (without perturbations) exhibits some kind of phase transition; these results are omitted here because of space. Linear Threshold Model. Figure 3 shows the expected number of infections for two representative networks-- the slashdot and wiki networks. They both seem to follow the



Network



n



|E | 100000 12572 22002 31180 196972 91286 13422 117619 24806 420877 352285 180811 364481 405739 469180 504230 59898 100736 39994 65369



 pc 0.05 0.05 0.07 - 0.02 0.05 0.12 0.01 0.1 0.02 0.02 0.01 0.05 0.02 0.02 0.02 0.04 0.01 0.09 0.12



2 p < pc : n



N



t



p=0.001 990.0 998.0 997.9 997.1 989.0 996.0 997.0 989.0 997.0 988.0 988.0 995.0 999.0 995.0 994.0 993.8 997.0 985.0 996.2 997.4



by Method 1 0.01 0.05 90.1 98.1 97.8 97.1 89.0 95.7 96.8 90.3 97.2 87.7 87.5 95.1 98.6 94.8 93.9 93.8 97.3 86.0 96.3 97.5 10.0 18.1 17.9 17.3 11.6 15.8 16.9 13.8 17.1 10.1 10.6 16.1 18.7 16.5 15.6 15.5 17.5 12.3 16.3 17.5



t



0.1 1.5 8.2 8.078 7.715 4.1 6.1 7.2 5.5 7.2 3.1 3.5 6.8 8.8 7.3 6.6 6.5 7.8 5.0 6.389 7.529



p=0.001 > 100 > 100 > 100 > 100 > > > > > 100 100 100 100 100



by Method 2 0.01 0.05 90 90 90 90 80 90 90 90 90 70 80 90 90 90 90 90 90 80 90 90 10 20 20 20 3 10 10 8 10 4 0 10 20 10 10 9 10 0 10 10



0.1 8 8 8 8 0 4 3 0 5 0 0 0 9 7 6 6 8 0 2 4



Synthetic graphs Regular (d = 20) 10000 Autonomous Systems As20000102 6474 Oregon1010331 10670 Oregon2010331 10900 Co-authorship Astroph 17903 Condmat 21363 Grqc 4158 11204 Hepph Hepth 8638 Citation HepPh 34546 27770 HepTh Communication Email-Enron 33696 265214 Email-EuAll Social Epinion 75877 Slashdot0811 77360 82168 Slashdot0902 Twitter 22405 Wiki-Vote 7066 Internet peer-to-peer Gnutella04 10876 Gnutella24 26518



0.01: 1.1 0.01: 1.1 0.01: 1.2 - 0.01: 2.2 0.01: 1.1 0.05: 2.96 0.001: 1 0.05: 1.84 0.01: 1.7 0.01: 7.2 0.01: 3.9 0.01: 1.6 0.01: 6.1 0.01: 18 0.01: 28 0.01: 5 0.001: 1 0.05: 0.581 0.05: 0.75



> 100 > 100 > 100 > 100 > > > > > 100 100 100 100 100



> 100 > 100



Table 1: Estimates of t for the IC model in several real-world networks: Columns 2 and 3 contain the number of nodes n and edges |E | respectively. Column 4 contains approximate values of critical probability pc (See Figure 5(a) in Appendix) and Column 5 contains empirically assessed values of n2 /N N2 for a value p < pc . There are two sets of measurements of t corresponding to the two methods described in the experiments section. Each set is comprised of 4 values corresponding to different values of transmission probability p. In Method 2, the column corresponding to p = 0.001 has entries "> 100" because it was not possible to estimate the maximum conclusively, as we only considered  100 in our simulations.

0.6 0.45 0.4 0.35 0.3 0.25 0.3 0.2 0.2 0.1 0.05 0 0 10 20 30 40 50 60 70 80 90 100  0 0 10 20 30 40 50 60 70 80 90 100  0.15 0.1



1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0



0.5



(0.001,10) (0.001,100) (0.01,10) (0.01,100) (0.05,10) (0.05,100) (0.1,10) (0.1,100)



0.0001 0.001 0.01



0.0001 0.001 0.01



0.4



10 20 30 40 50 60 70 80 90 100 



(a) Ca-Astroph



(b) Epinion



Figure 3: Expected #infections vs in the LT model for different seed probabilities s = 0.0001, 0.001, 0.01 (seed nodes chosen uniformly at random). Plot (1) Slashdot0811 and (2) Wiki.



Figure 2: Expected #infections vs in the IC model for different pairs of transmission probability p and seed set size, with the seed nodes being chosen randomly.



general bounds of Theorem 2. We have also studied the LT model on all the 20 networks, as in the IC model; these are omitted here because of space limitations. Figure 4 shows the sensitivity in the transient behavior, i.e., the fraction of infections by time for the LT model-- as mentioned earlier, this shows a greater sensitivity to .



7



Conclusions and open problems



to other models of noise, especially those involving dependencies, sensitivity to the number of sources, and to examine the sensitivity of other dynamical properties in more general diffusion models (including the IC and LT models with heterogeneous probabilities/weights) are natural open problems for future research. Acknowledgments. This work has been partially supported by the following grants: DTRA Grant HDTRA1-11-10016, DTRA CNIMS Contract HDTRA1-11-D-0016-0010, NSF Career CNS 0845700, NSF ICES CCF-1216000, NSF NETSE Grant CNS-1011769 and DOE DE-SC0003957.



We give the first rigorous results on the stability of the independent cascades and linear threshold models, with respect to edge perturbations. These help explain our empirical observations on 20 diverse real networks. Extending our results



1e-04



1e-03



5e-05



5e-04



0e+00 0



10



20



30



40



50



0e+00 0



10



20



30



40



50



(a) s=0.0001



(b) s=0.001



Figure 4: LT model: Plots of infection size over time for Slashdot network for = 0, 1, 10, 100. Here s corresponds to the seed probability.



References

Achlioptas, D.; Clauset, A.; Kempe, D.; and Moore, C. 2009. On the bias of traceroute sampling. J. ACM 56(4):21:1-21:28. Alon, N.; Spencer, J. H.; and Erd os, P. 1992. The probabilistic method. John Wiley & Sons, Inc. Bakshy, E.; Hofman, J.; Mason, W.; and Watts, D. 2011. Everyones an influencer: Quantifying influence on twitter. In WSDM. Borgatti, S.; Carley, K.; and Krackhardt, D. 2006. On the robustness of centrality measures under conditions of imperfect data. Social Networks 28:124-136. Chen, J. 2010. The effects of demographic and spatial variability on epidemics: A comparison between beijing, delhi and los angeles. In Conf. on Crit. Inf. Chung, F., and Lu, L. 2002. Connected components in random graphs with given expected degree sequences. Annals of Combinatorics 6:125-145. Clauset, A.; Moore, C.; and Newman, M. 2008. Hierarchical structure and the prediction of missing links in networks. Nature 453:98-101. Costenbader, E., and Valente, T. 2003. The stability of centrality measures when networks are sampled. Social Networks 25:283-307. Eubank, S. 2010. Detail in network models of epidemiology: are we there yet? Journal of Biological Dynamics 446-455. Faloutsos, M.; Faloutsos, P.; and Faloutsos, C. 1999. On power-law relationships of the internet topology. In SIGCOMM, volume 29, 251-262. Flaxman, A., and Frieze, A. M. 2004. The diameter of randomly perturbed digraphs and some applications. In APPROX-RANDOM, 345-356. Flaxman, A. 2007. Expansion and lack thereof in randomly perturbed graphs. Internet Mathematics 4(2-3):131-147. Galuba, W. 2010. Outtweeting the twitterers - predicting information cascades in microblogs. In WOSN.



Goldenberg, J.; Libai, B.; and Muller, E. 2001. Talk of the network: A complex systems look at the underlying process of word-of-mouth. Marketing Letters. Gonzlez-Bailn, S.; Borge-Holthoefer, J.; Rivero, A.; and Moreno, Y. 2011. The dynamics of protest recruitment through an online network. Scientific Reports 1. Hagmann, P. 2008. Mapping the structural core of human cerebral cortex. PLoS Biol. Kaneko, K. 1985. Spatiotemporal intermittency in coupled map lattices. Progress of Theoretical Physics 74(5):1033- 1044.  2003. MaxiKempe, D.; Kleinberg, J. M.; and Tardos, E. mizing the spread of influence through a social network. In KDD, 137-146. ACM.  2005. Influential Kempe, D.; Kleinberg, J. M.; and Tardos, E. nodes in a diffusion model for social networks. In ICALP. Lahiri, M.; Maiya, A. S.; Caceres, R. S.; Habiba; and BergerWolf, T. Y. 2008. The impact of structural changes on predictions of diffusion in networks. In ICDM, 939-948. Leskovec, J. 2011. Stanford network analysis project. Newman, M. 2003. The structure and function of complex networks. SIAM Review 45(2):167-256. Romero, D.; Meeder, B.; and Kleinberg, J. 2011. Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter. In Proc. of WWW, 695-704. ACM. Ron, D. 2010. Algorithmic and analysis techniques in property testing. Foundations and Trends in TCS 5(2):73205. Schwab, D. J.; Bruinsma, R. F.; Feldman, J. L.; and Levine, A. J. 2010. Rhythmogenic neuronal networks, emergent leaders, and k -cores. Phys. Rev. E 82:051911. Spielman, D. 2009. Smoothed analysis: An attempt to explain the behavior of algorithms in practice. Communications of the ACM 76-84. Wu, C. W. 2005. Synchronization in networks of nonlinear dynamical systems coupled via a directed graph. Nonlinearity 18:1057-1064.



A



Appendix



Lemma 3. For G  G(n, p), if p  /n for a constant  < 1, N2 = (n) with high probability. Proof. For a vertex v , let Tv denote the number of vertices reachable from any vertex v in G(p) (including v ). It is well-known that (see (Alon, Spencer, and Erd os 1992)), for  < 1, P (Tv  t)  P (B [n, t/n]  t)  e-t , where  = () is some constant and B [a, b] is the standard Binomial random variable with a trials and probabil2 ity of success b. Using linearity of expectation, E [Tv ] = 2 2 t P ( T = t )  t P ( T  t ) . Since  is v v t>0 t>0 a constant, there exists a constant  such that for t   , t - 2 log t   t, where  is another constant. Applying 2 this, E [Tv ]   3 + t e- t = (1). Using a similar 2 technique, we can show that var[Tv ] = (1).



Note that N2  v T 2 and by linearity of expectation, 2 E [N2 ]  nE [Tv ] = (n). Further, since Tv are i.i.d. ran2 2 dom variables, var v Tv = v var[Tv ] = (n). Applying Chebychev inequality it follows that with high probability N2 = (n).



Independent Cascades: (for a fixed seed set size) 1 0.9 0.8 Infection size (fraction of nodes) 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.047 0.094 0.141 0.188 0.235 p 0.282 as20000102 ca-astroph.giant ca-condmat.giant ca-grqc.giant ca-hepph.giant ca-hepth.giant cit-hepph.giant cit-HepTh email-Enron email-EuAll epin.giant oregon1-010331 p2p-Gnutella04 p2p-Gnutella24 regular-20 slashdot0811 slashdot0902 tweet wiki.giant 0.329 0.376 0.423 0.47 Infection size (fraction of nodes) 0.5 0.6



Linear threshold (uniform weights)



0.4



0.3



0.2



0.1



0 0 0.008 0.016 0.024 0.032 0.04 p 0.048



as20000102 ca-astroph.giant ca-condmat.giant ca-grqc.giant ca-hepph.giant ca-hepth.giant cit-hepph.giant cit-HepTh email-Enron email-EuAll epin.giant oregon1010331 oregon1-010331 oregon2010331 oregon2-010331 p2p-Gnutella04 p2p-Gnutella24 regular-20 slashdot0811 slashdot0902 tweet wiki.giant 0.056 0.064 0.072 0.08



(a) Independent cascades



(b) Linear threshold



Figure 5: Dynamics on several real-world graphs (unperturbed).



1e-04 Fraction of Nodes

0 1 10 100



0.03 Fraction of Nodes 0.02 0.01 0.00 0 Fraction of Nodes

0 1 10 100



1.0

0 1 10 100



5e-05



0.5



0e+00 0



5



10 Time



15



20



10



20



30



40



50



0.0 0



5



10 Time



15



20



Time



(a) p=0.001



(b) p=0.01



(c) p=0.1



Figure 6: IC model: Plots of infection size over time for Slashdot network for = 0, 1, 10, 100.



import os



fdata = open("text.txt","w")

path = os.listdir(os.getcwd())

for p in path:

    ftxt = open(p,"r")

    for each in ftxt.readlines():

        print >>fdata,each

Solving QBF Instances With Nested SAT Solvers



Bart Bogaerts and Tomi Janhunen and Shahab Tasharrofi



Helsinki Institute for Information Technology HIIT Department of Computer Science Aalto University, FI-00076 AALTO, Finland







Abstract



We present a new approach towards solving quantified Boolean formulas (QBFs) using nested SAT solvers with lazy clause generation. The approach has been implemented on top of the Glucose solver by adding mechanisms for nesting solvers as well as clause learning. Our preliminary experiments show that nested SAT solving performs (out of the box) relatively well on QBF, when taking into account that no particular QBF-oriented solving techniques were incorporated. The most important contribution of this work is that it provides a systematic way of lifting advances in SAT solvers to QBFs with low implementation effort.







ing's readability and succinctness, or enhance the strength of propagation via specialized propagators. Recent work by Janhunen, Tasharrofi, and Ternovska (2016) started from the following observation: "if SAT solvers are this efficient, then why not to use a SAT solver as a smart (in the sense that it learns good clauses) oracle for a SAT solver itself?". The idea here is to solve satisfiability problems for theories of the form T =   x : , where x is a sequence of propositional variables and  and  are CNF-theories. Propagation for T combines unit propagation on  with an oracle call to a SAT solver for  . From the result of this oracle call, a learned clause is generated and added to .1 Advantages of this approach are manifold: (1) It is a modular approach that allows plugging in any SAT solver as innermost solver and only requires minor modifications to the outermost solver, hence progress in SAT will automatically translate to solvers of this richer formalism; (2) It can be immediately combined with other SAT extensions (such as integer variables, acyclicity, or any other theory propagator); (3) No dedicated propagators need to be developed for the new extension because the nested solver is (automatically) used as a propagator for its internal theory; for example, it was shown by Janhunen, Tasharrofi, and Ternovska (2016) how using an internal SAT solver to propagate reachability constraints leads to a simple encoding of Hamiltonian paths that performs much better when compared to a direct encoding (i.e., a SAT encoding without second-order structure). A solver, called SAT- TO -SAT, that implements this idea was presented (Janhunen, Tasharrofi, and Ternovska 2016). Since in principle any solver2 can be nested, it is also possible to nest SAT- TO -SAT in itself, as an oracle. By allowing such arbitrarily deep nesting, we essentially obtain a QBF solver. This paper presents how SAT- TO -SAT can be used for QBF solving. We evaluate how this technique performs with respect to state-of-the art QBF solvers and conclude that SAT- TO -SAT is still slower than the best QBF solver around. However, it deserves to be mentioned that our implementation is generic and performs no optimizations



1 2







1







Introduction







Since the addition of conflict-driven clause learning (Marques-Silva and Sakallah 1999), SAT solvers have made huge leaps forward in two respects: their popularity for tackling real-life problems and their efficiency. Now that these highly-performant SAT-solvers exist, research often stretches beyond SAT, either because of trying to tackle problems of a complexity higher than NP or because the input format of SAT solvers (propositional logic) is too limited to concisely and naturally express certain domain specific constraints, such as graph properties. For this reason, researchers have extended the SAT language with new language constructs, sometimes also called constraints, and have extended SAT solvers with dedicated propagators for those constraints that communicate with the underlying SAT solver through clauses. This has happened for example in the field of constraint programming (Apt 2003) in the form of solvers with lazy clause generation (Ohrimenko, Stuckey, and Codish 2009), in SAT modulo theories (Barrett et al. 2009) in the form of DPLL(T) solvers (Ganzinger et al. 2004), and in answer set programming (Marek and Truszczy nski 1999) where all modern solvers use this architecture (Gebser, Kaufmann, and Schaub 2012; 2013; De Cat et al. 2013; Alviano et al. 2015). Several further extensions to SAT use the same approach (Gebser, Janhunen, and Rintanen 2014; Bayless et al. 2015). Such extensions may (1) increase complexity (for applications in which tasks of a higher complexity than NP are required to be tackled), or (2) remain in NP but either improve an encodCopyright c 2015, The authors. All rights reserved.







See Section 3 for a detailed explanation. Any solver that respects the interface given in Definition 3.3.







designed for QBF specifically. Furthermore, our current implementation is built on the popular SAT solver G LUCOSE (Audemard and Simon 2009). In principle, any SAT solver can be plugged in, resulting in a strongly improved performance. The ideas presented here also shed new light on techniques used in QBF. For example, conflict-driven clauselearning and solution-driven cube-learning (Giunchiglia, Narizzano, and Tacchella 2002; Letz 2002; Zhang and Malik 2002; Chu and Stuckey 2014) are completely unified in our framework as clause-learning occurring in even (respectively odd) levels of nesting depth. The main contributions of this paper are as follows: (1) We show how SAT- TO -SAT can be extended to a QBFsolver. This results in a principled, low-cost way to transfer improvements from SAT to QBF; (2) Furthermore, since the nesting idea is completely orthogonal to other language extensions, we can also lift extensions of SAT to QBF, for example resulting in QBF modulo theories (QBF(T)) or solvers for QBF modulo acyclicity (Acyc-QBF).







is called a QBF sentence. We use  :  to abbreviate p1 . . . pn :  if  = {p1 , . . . , pn }. If  is a propositional formula, we use ( ) to denote that the free symbols of  are all in  , i.e., that  is a  -QBF. A prenex QBF is a QBF in which all quantifiers are in the front, i.e., a set of quantifiers followed by a propositional formula. The QDIMACS format is a numerical format to describe a prenex QBF in which the propositional formula is a CNF formula. The format is the de-facto standard for representing QBF instances. Satisfiability Relation for QBFs. The satisfiability relation between  -interpretations I and a  -QBFs , denoted by I |= , is defined recursively in the standard way: * I |= p if I (p) = t. * I |=  if I |= ; * I |=    (resp. I |=    ) if I |=  and (resp. or) I |=  ; * I |= x :  (resp. I |= x : ) if (I  {xt }) |=  and (resp. or) (I  {xf }) |= . Let I be a (partial)  -interpretation. We call a  -QBF  I -satisfiable if there exists a model of  more precise than I and I -unsatisfiable otherwise.







2







Background







Vocabularies and Interpretations. A vocabulary is a set of symbols, also called atoms; we use , ,  to refer to vocabularies. If  is a vocabulary, a (two-valued)  interpretation is a mapping   {t, f } where t denotes true and f false. A partial  -interpretation is a mapping   {t, f , u}, where u denotes unknown. We often identify a partial  -interpretation J with a set of tuples pv with p  , v  {t, f } (with each atom occurring at most once), meaning that J sends all atoms occurring in this set to their corresponding values, and all others to unknown. This allows us to define the "union" of two interpretation. E.g., if  and  are disjoint, I is a (partial)  -interpretation and J a (partial)  -interpretation, we use I  J to interpret symbols in  in the same way as I and symbols in  in the same way as J . If I and J are two  -interpretations, we will use the expression I J only if I J indeed defines a partial interpretation (i.e., contains not both pt and pf ). The truth order <t on truth values is induced by f <t u <t t. The precision order <p on truth values is induced by u <p t, u <p f . This order is extended pointwise to partial interpretations: I <p J if I (q ) <p J (q ) for all q in  . If I is a  -interpretation and    , any (partial)  -interpretation is identified with the partial  -interpretation equal to I on  and mapping all symbols in  \  to u. Formulas. A propositional formula is recursively built from propositional atoms p, q, r, . . . using connectives ,  and . A propositional formula is a  -formula if its atoms are in  . A literal is an atom or its negation. A clause is a disjunction of literals. A CNF is a conjunction of clauses. A sub-formula occurs positively (resp. negatively) if it is within the scope of an even (resp. odd) number of negations. A quantified Boolean formula (QBF) is built using the same recursive rules, but with added quantifiers  and  to quantify over propositional atoms. A  -QBF is a QBF with free symbols belonging to  . A  -QBF with  = {}







3 SAT- TO -SAT In order to discuss the working of SAT- TO -SAT, we first present a formalisation of SAT-solvers for our purposes.



SAT solving. The principal goal of a SAT solver is to find a model for a CNF, i.e., to check the validity of a formula  : , where  is a CNF. Many modern SAT solvers do more than that: they explain their answer in terms of a set of so-called assumptions (Nadel and Ryvchin 2012). In this text, we assume3 that  is the disjoint union of two vocabularies  and  , an assumption vocabulary  and an internal vocabulary  . A solver not only returns a model or UNSAT but also explains this in terms of the assumptions. Definition 3.1 (Explaining Satisfiability). Let  be any  formula and  =    where  and  are disjoint. Let J be a partial  -interpretation and M be a partial  -interpretation. We say that (J, M ) explains the satisfiability of  if each  interpretation more precise than J  M is a model of  Example 3.2. Let  = {o, p, q },  = {r} and 1 = (p  q  r)  (o  r). Furthermore, let J be the partial  -interpretation {pt } and M the  -interpretation {rf }. In this case (J, M ) explains the satisfiability of 1 . Indeed, J guarantees that the first clause of 1 is satisfied, while M guarantees that the second is. Definition 3.3 (SAT-solver). Suppose that  =    . A SAT-solver is a procedure that takes as input a  -CNF T and a two-valued  -interpretation I . * If T is I -satisfiable, it returns (SAT, J, M ) such that J p I and (J, M ) explains the satisfiability of .



3







This assumption is not vital but simplifies the presentation







* Otherwise, it returns (UNSAT, J ) where J p I is such that T is J -unsatisfiable. Hence a SAT solver solves the problem  : T under assumptions I . Note that in formalisations of SAT solvers, often a J that explains the answer of the solver is not present. In this case, J can always be equal to I . Several SAT solvers, such as MiniSAT (E en and S orensson 2003), support smart reasoning methods to generate better (less precise) J . In order to solve problems of form  : T , state-of-the-art SAT solvers use the conflict-driven clause learning (CDCL) algorithm (Silva, Lynce, and Malik 2009). The CDCL algorithm works by maintaining a state that represents a partial  -interpretation. We use S(S ) to denote the state of a solver S . The algorithm uses operations of propagation, decision, backjumping and restart to manipulate its state. Propagation takes a state S(S ) and either returns a (possibly) more precise state that is the consequence of its previous state or returns a conflict clause showing no model can extend the current state. The decision operation takes a non-conflicting state S(S ) and branches the search on a variable v (decision variable) that is currently unassigned in S(S ). Backjumping takes a conflicting state S(S ), learns a clause from it and returns to a less precise non-conflicting state. The restart operation restarts the search while remembering learnt clauses. SAT- TO -SAT. Recently, Janhunen, Tasharrofi, and Ternovska (2016) introduced SAT- TO -SAT, a framework for combining SAT solvers so that, together, they solve QBF problems. Essentially, this framework performs lazy clause generation (Ohrimenko, Stuckey, and Codish 2009) where clauses are obtained from calls to another SAT solver. In this section, we recall how SAT- TO -SAT works in a slightly generalised setting. The input for SAT- TO -SAT is an QBF of the form T =  :   (1 : 1 )  * * *  (n : n ), where  is a  -CNF and the i 's are i -CNFs with i =   i . Without loss of generality, from now on, we assume that n = 1 and use  for 1 ,  for 1 and  for    . SAT- TO -SAT checks validity of T , i.e., it returns SAT iff there exists a  -interpretation I that satisfies  such that  is I -unsatisfiable and returns UNSAT otherwise. To explain how SAT- TO -SAT works, we need some terminology: Definition 3.4 (Lowerbound/Upperbound Mapping). The LU-mapping of vocabulary  is lu = {pu | p   }  {pl | p   } with pu (resp. pl ) representing upper- (resp. lower) bound of p. The LU-mapping of a partial interpretation I , denoted as Ilu , is a 2-valued lu -interpretation so that Ilu (pu ) = t if and only if I (p) = f and Ilu (pl ) = t if and only if I (p) = t. Note that for each atom p in the vocabulary  , Ilu satisfies Ilu (pl ) t I (p) t Ilu (pu ), i.e., pl (respectively pu ) is a lower (respectively upper) bound on the truth of p. Definition 3.5 ( -under-approximation). Let  be a    formula. A  -under-approximation of  is any lu   formula  such that for all interpretations I : (1) if I is a two-valued  -interpretation, then  :  is satisfied in Ilu iff  :  is satisfied in I , and







(2) if I is a partial  -interpretation, then Ilu |=  :  implies that every two-valued  -interpretation more precise than I satisfies  :  . The first condition guarantees that in two-valued interpretations the approximation coincides with the original formula. The second states that if Ilu can be expanded to a model of  , then I can be expanded to a model of  . Example 3.6 (Example 3.2 continued). Let 1 = (pl  ql  r)  (ou  r). In this case 1 is a  -under-approximation of 1 . We show this for some partial  -interpretations. * When I is 2-valued, Ilu (xl ) = I (x) = Ilu (xu ) for all x   . Hence Condition (1) in Definition 3.5 is satisfied. * Let I0 be the partial  -interpretation that maps all atoms to u. In this case, 1 is (I0 )lu -unsatisfiable, hence Condition (2) in Definition 3.5 is clearly satisfied as well. * Now, let I1 be the partial  -interpretation {pt }. Since (I1 )lu (pl ) = t, M = {rt } is a model of  : 1 . For each two-valued interpretation I p I1 , it holds that I  M is a model of the original formula 1 . Assuming a  -under-approximation  for  , the SAT- TO -SAT algorithm instantiates two CDCL-solvers S (tasked with solving ) and S (tasked with solving  ). After each unit propagation phase of S , solver S is called with assumptions S(S )lu . * If S returns (SAT, J, M ), it then follows from the fact that  is a  -under-approximation of  that  :  (and hence also T ) is I -unsatisfiable. In this case, J is used to create a clause that falsifies S 's current assignment; this clause is added to . * If S returns (UNSAT, J ), nothing can be concluded. Literals in J are used as watched literals to avoid calling S again as long as S(S )lu is more precise than J . The use of the under-approximation has the effect that if S(S ) is not exact, the call to the nested solver S remains sound in the sense that whenever S finds a model, a conflict clause can be added to . In case S is unsatisfiable (with the given assumption), nothing final can be concluded yet. In this case, the explanation of unsatisfiability can be used to avoid calling the nested solver too often. The use of the under-approximation roughly has the same effect as calling a SAT-solver for  , with assumptions S(S ), but obliging the nested solver to keep all variables in  that are unassigned in S(S ) unassigned. This would require us to modify the internals of the solver S to find models in a partial context, which is something we try to avoid. As such, the under-approximation serves two purposes (1) it allows us to call the nested solver after each unit propagation, (2) it ensures we can use the nested SAT solver as a blackbox. Example 3.7 (Example 3.6 continued). Let T be the QBF 1   : 1 , where 1 = p  q. SAT- TO -SAT solves the satisfiability task for T as follows: * SAT- TO -SAT starts from the partial  -interpretation I0 in which all atoms in  are unknown. In this case, as







shown in Example 3.6, 1 is unsatisfiable. A SAT-solver f t for 1 can return (UNSAT, {pf l , ql , ou }). SAT- TO -SAT interprets this result by watching literals p, q , and o. As soon as one of these literals becomes false, the subsolver will be called again. * SAT- TO -SAT chooses I1 = {pf }. None of the watches fire hence the internal solver is not called. * SAT- TO -SAT chooses I2 = {pf , q t }. In this case 1 has a model (M = {rf }) more precise than (I2 )lu . The internal t solver returns (SAT, {ql }, M ). SAT- TO -SAT interprets this result by adding the clause q to 1 . * The solver for 1 now finds a conflict, backjumps and continues search. The only thing that remains to be explained is how SAT- TO -SAT obtains a  -approximation of  . This is done by the following syntactical transformation. Lemma 3.8. Let  be a    -CNF. Let lu be the lu   CNF obtained from  by 1. replacing each literal p in  with p   by pu , and 2. replacing each literal p in  with p   by pl . Then, lu is a  -under-approximation of  . This lemma follows the fact that, for each partial  interpretation I , we have Ilu (pl ) t I (p) t Ilu (pu ). Example 3.9 (Example 3.6 continued). The formula 1 equals (1 )lu as defined in Lemma 3.8.







Example 4.2 (Example 3.7 continued). Let 1 {p, o}, 2 = {q } and T (1 ) = 2 : (1   : 1 ).







=







Furthermore, let J = {pf } and J = {pf , q f , ot }. In this case, J explains satisfiability of 1 and J explains unsatisfiability of 1 . Theorem 4.1 shows that J = {pf , ot } explains satisfiability of T (1 ). The case of unsatisfiability is easier: every time SAT- TO -SAT finds a model for its nested expression, a clause is added to  that invalidates the current partial interpretation. Hence, in the end, the only way for T to become unsatisfiable is that  becomes unsatisfiable. Theorem 4.3. Let T be a QBF of the form T ( ) =  :   ( :  ), where  is a CNF and  is an arbitrary (QBF) formula. Let I be a  -interpretation. Suppose  is J -unsatisfiable and J p I , then T is J -unsatisfiable as well. We now show how Lemma 3.8 extends to general QBFs. Lemma 4.4. Let  be a    -QBF. Let lu be the lu   QBF obtained from  by 1. replacing each negative occurrences of p   by pu , and 2. replacing each positive occurrences of p   by pl . Then, lu is a  -under-approximation of  . Again, this lemma follows from the fact that, for all partial  -interpretations I , we have Ilu (pl ) t I (p) t Ilu (pu ). Lemma 4.4 shows how to obtain under-approximations for QBFs in general. Together with Theorems 4.1 and 4.3, we obtain all ingredients to extend SAT- TO -SAT for general QBFs. The resulting solver is a SAT solver S1 extended with an oracle S2 , also an instantiation of SAT- TO -SAT. The results from calls to S2 are used either as learnt clauses in the theory of S1 or watches that avoid unnecessary calls to S2 . The solver works exactly as described in Section 3 except that the "black box" nested solver is now another instance of SAT- TO -SAT rather than an instance of a SAT solver.







4







Solving QBF With SAT- TO -SAT







The previous section discussed how SAT- TO -SAT solves QBF validity problems. These ideas easily generalise to QBF validity problems: instead of nesting a SAT-solver in another SAT-solver, we can nest SAT- TO -SAT inside a SAT-solver (recursively). In order to do this, two obstacles are to be overcome. First, we must extend SAT- TO -SAT so that it not only outputs SAT or UNSAT, but also explains this result in terms of assumptions given to it, i.e., it should respect the interface we specified in Definition 3.3. Second, it is necessary to define how an approximation of a QBF theory can be obtained, i.e., extend Lemma 3.8 to general QBFs. For the first, we use the following theorem. Theorem 4.1. Let T be a QBF of the form T ( ) =  :   ( :  ), where  is a CNF and  is an arbitrary QBF. Let I be a  -interpretation. Suppose the following hold: * J1 p I is a partial  -interpretation, and M a 2-valued  interpretation s.t. (J1 , M ) explains 's satisfiability, and * J2 is a partial (   )-interpretation, J2 p I  Mlu and  is J2 -unsatisfiable. Then, with J = J1  J2 | , it holds that J p I and (J, M ) explains the satisfiability of   ( :  ). Theorem 4.1 shows how an explanation of satisfiability of  and one of unsatisfiability of  can be combined to provide an explanation of satisfiability of the combined expression. This can be directly used to extend SAT- TO -SAT's output in case of satisfiability.







Translating QDIMACS into SAT- TO -SAT Input



Minor syntactical differences aside, a QDIMACS specification could be fed directly to SAT- TO -SAT. The only difference is that QDIMACS supports universal quantifications, while SAT- TO -SAT supports existential quantifications under negation, a difference that can be trivially eliminated. However, contrary to QDIMACS, SAT- TO -SAT does not require theories to be in prenex normal form. When feeding QDIMACS input to SAT- TO -SAT, all clauses are in the innermost solver. During search, other solver instances will gradually learn clauses as well. However, some clauses can be pulled out to prior to any search. This will speed up search, since SAT- TO -SAT will not waste time rediscovering information that was present in the first place. Starting from a QDIMACS specification, we perform two preprocessing steps before feeding it to SAT- TO -SAT. 1. Remove tautological clauses. We remove all clauses containing both a literal and its negation from the theory.







2. Pull clauses outwards. When certain clauses only use certain variables, they can be pulled out. We iteratively apply the following lemma to obtain an equivalent theory that no longer is in prenex normal form and in which each clause is located at the "right" level. Lemma 4.5. Let  denote the QBF (0 ) = 1 : 1 2 : 3 : 3  (1 (0 , 1 )  2 (2 )), where the i 's are arbitrary formulas and the i 's are clauses. If 2 is no tautology, then (0 ) is equivalent to 1 : 1 (0 , 1 )  1  2 : 3 : 3 .







5







Evaluation & Future Work







We implemented the aforementioned techniques on top of the Glucose solver (Audemard and Simon 2009). We evaluated the resulting solver on 276 instances from the QBFLIB problems suite (Giunchiglia, Narizzano, and Tacchella 2001) , namely those that were used in the latest QBF competition (QBFEVAL 2014). We compared running times of SAT- TO -SAT with GhostQ (Klieber et al. 2010), the winner of the competition on the QBFLIB track. We only compared a plain version of our solver with the plain version of GhostQ. All tests were ran with a time limit of 900 seconds on an Intel c Xeon c E5-4652 CPU clocked at 2.70GHz with 260Gb of RAM running Ubuntu 14.04LTS. Table 1: The numbers of satisfiable and unsatisfiable instances solved by SAT- TO -SAT and GhostQ. GhostQ SAT- TO -SAT SAT 66 28 UNSAT 57 43 Total 123 71







These kind of observations allow us to reduce the nesting depth or to pull variables and clauses higher in the nesting hierarchy. Since SAT- TO -SAT is designed to gradually pass more and more information upwards in the hierarchy, doing this in advance could save precious time; in this case, without any memory overhead. Reverse Tseitin engineering is used for example by Goultiaeva and Bacchus (2013). Implementing such techniques is a topic for future work. (ii) Sometimes when the internal solver in SAT- TO -SAT finds a model, there are several choices on how to construct a conflict clause. More formally, for a given model M of the internal theory, there might be multiple J 's such that (J, M ) explains satisfiability of the internal theory, as defined in Definition 3.1. Each of these J 's gives rise to a different learned clause in the outermost solver. Example 5.1. Let T be the following theory:   o, p, q :     (p  q ) T =    r :  (p  q  r)  (r  q )  (o  q  r) If a SAT-solver for the internal theory is called with assumptions I = {ot , pt , q t }, then M = {rf } is a model. If J1 = {ot , pt } and J2 = {ot , q t }, then both (J1 , M ) and (J2 , M ) explain satisfiability of (p  q  r)  (r  q ). Returning the first of these would result in the addition of a clause o  p to the outermost theory, while the second would result in the addition of a clause o  q . In principle, both clauses found in Example 5.1 are valid consequences and could be added to the top theory. However, there could be exponentially many such clauses. Using ideas from extended resolution (Tseitin 1968), we can summarise all of these clauses in linear size in terms of the original theory by introducing new variables. In the example, this would boil down to introducing a variable t and adding an encoding of the following definition t  p  q and the clause to to invalidate the current assignment. This generalises both of the above clauses with the cost of introducing an extra variable. We need to research the impact of such learned clauses both on time and memory consumption. (iii) Several of the preprocessing techniques discussed here involved transforming a prenex normal form QBF into a non-prenex normal form sentence. Sometimes this even involves "rediscovering" problem structure that was probably present at the time of the encoding, but that was lost because of the low-level format used. An example of this is the case (i) above, where we need to do clever reasoning to rediscover that one variable is defined functionally in terms of other variables, since CNF's have no native language construct to express this definition. A richer language could directly present this information in the encoding. Hence, we intend to generalise SAT- TO -SAT to accept non-prenex and non-CNF input format such as QCIR (QBF Gallery 2014).







Table 1 depicts the number of instances solved by SAT- TO -SAT and GhostQ. As can be seen, SAT- TO -SAT's performance still lags behind the best available QBF solver. The difference seems big. However, some remarks, that also define our future work directions, need to be made. Firstly, combining these results with the results from the latest competition (QBFEVAL 2014) puts SAT- TO -SAT approximately on-par with the plain version of RAReQS (Janota et al. 2012), while a version of RAReQS with QBF preprocessors ended up in the third place of this competition. Finding out which preprocessors boost SAT- TO -SAT's performance is a topic for future work. Secondly, our implementation was built on top of Glucose. It is not hard to replace Glucose with any SAT solver that implements the interface given in Definition 3.3. Related, SAT- TO -SAT allows us to lift all future improvements in SAT solving to QBF. Thirdly, our solver was not optimised for QBF solving. Several optimisations are possible. We discuss three of them below. (i) One particularly useful optimisation would be to detect Tseitin variables. QBF specifications in the QBFLIB often contain patterns of the form  :  :  : (p  (,  ))  , with p   , which is equivalent to  : , p : (p  (,  ))  ( \ {p}) : .







6







Related Work







There exist many CDCL-based algorithms to solve QBF instances. Our approach differs from most of them in three aspects. (1) Using under-approximations, we circumvent a







common limitation in QBF solving that variables must be chosen in accordance to the quantifier prefix. Thus, nested solvers can be called earlier, during the search process of a solver, before a solver has found a complete assignment. This leads to faster propagation. (2) We treat existential and universal quantifiers symmetrically. That is, all our algorithms are defined for theories of the form  :    :  , where neither the structure of  , nor the context in which this formula occurs matters. (3) Given that our approach is applicable to any SAT solver, we gain an engineering advantage over many existing QBF solving techniques. The idea that using an NP oracle inside an NP-solver results in a solver for P 2 (and, similarly, for the rest of polynomial hierarchy) is not new and directly follows the definition of these complexity classes. This idea has also been applied before to obtain a QBF solver (Ranjan, Tang, and Malik 2004). Our approach is different in two main ways from Ranjan, Tang, and Malik (2004): (1) we use of underapproximations, and (2) we are not limited to 2QBF . These ideas have been integrated with a new learning technique known as Counterexample Guided Abstraction Refinement (CEGAR) (Janota et al. 2012). CEGAR enables gradual expansion of a QBF instance and, in that sense, our approach is similar to CEGAR. However, unlike CEGAR, our approach generates only one solver per quantification level and maintains the states of those solvers for faster search. Recently, Rabe and Tentrup (2015) introduced a new extension of the CEGAR approach that also uses one solver per quantifier level as well as a form of clause selection. Our work is different from theirs because of our underapproximation technique and our uniform treatment of existential and universal quantifiers. Also, recently, Janota and Marques-Silva (2015) presented a QBF solver based on clause selection which, again, is different from our algorithm due to our underapproximation technique and our uniform treatment of quantifiers. Another point where our approach differs from theirs relates to the learnt clauses. Their approach uses an extended language of learning where each clause is associated with variables that express whether that clause is selected (i.e., should be satisfied in lower levels) or deselected (i.e., is already satisfied) at a certain level. Learnt clauses are always over these new, so-called clause selection variables. Their method allows to succinctly represent many clauses invalidating different J 's that explain satisfiability at once (similar to the technique presented in Example 5.1). We are convinced that the extended language of learning is beneficial to QBF solving. Investigating the exact effect of this aspect on solver performance is a topic for future work. Several QBF solvers combine CDCL with solution-driven cube learning (Zhang and Malik 2002; Goultiaeva, Seidl, and Biere 2013). A cube is a conjunction of literals; a formula is in Disjunctive Normal Form (DNF) if it is a disjunction of cubes. Zhang and Malik (2002) introduced Augmented CNF (ACNF): a formula is an ACNF if it is of the form    where  is a CNF and  a DNF. They also introduced solution-driven cube learning: a technique where a QBF solver reasons on an ACNF and gradually adds cubes to 2 . Currently, many QBF solvers implement solution-







driven cube learning in addition to conflict-driven clause learning. Goultiaeva, Seidl, and Biere (2013) show that dual propagation for QBF can be represented using the existing clause-learning and cube-learning methods in QBF solvers. Let us give the outermost solver in SAT- TO -SAT level one and each nested solver level one plus its parent's level. We show that cube learning and clause learning in QBF both boil down to clause learning in SAT- TO -SAT. That is, clauses learnt in a QDPLL algorithm correspond to learnt clauses in odd levels of SAT- TO -SAT and cubes learnt in a QDPLL algorithm correspond to SAT- TO -SAT clauses in even levels. Informally, this follows easily from the fact that a cube is the negation of a clause and vice versa. Since the clauses in even levels of SAT- TO -SAT have an odd number of negations preceding them, they indeed represent a negated clause, i.e., a cube. Similarly, the even number of negations that precede a clause in an odd level of SAT- TO -SAT cancel each other out to represent a normal QBF clause. Formally, If T = 1 : 2 : 3 : . . . n-1 : n :  is a QBF with  in ACNF, then all learned cubes C have the property that T is equivalent to 1 : 2 : 3 : . . . n-1 : n : C  . (1) Lemma 6.1. Suppose T = 1 : 2 : 3 : . . . n-1 : n :  is a QBF with  in ACNF and C = C  Cn is a cube with C a i<n i -cube and Cn a n -cube. Suppose also that T is equivalent to (1). Then T is also equivalent to 1 : 2 : 3 : . . . n-1 : C  n :  and to 1 : 2 : 3 : . . . n-1 : C  n : . From Lemma 6.1, we indeed see that (since C is a clause) learned cubes in QBF correspond to learned clauses in the corresponding SAT- TO -SAT specification.







7







Conclusion







This paper introduced an extension of SAT- TO -SAT with an arbitrary nesting depth and showed how it can be used to solve the validity problem of Quantified Boolean Formulas. Our experiments show that, even without any QBF-specific optimizations, SAT- TO -SAT performs relatively well on QBF instances from the latest QBF competition. Moreover, the generic architecture of SAT- TO -SAT with respect to its underlying SAT solvers allows us to uniformly lift SAT-related optimizations to QBF. In particular, G LUCOSE, the SAT-solver SAT- TO -SAT is currently based on, can be easily replaced with other SAT solvers. In addition, in Section 5, we have identified several topics for future work: (i) detecting when variables are completely defined in terms of variables form higher levels in order to pull larger parts of the theory to higher levels (ii) learning stronger clauses based on extended resolution when conflicts arise because the internal solver finds a model, and (iii) moving towards a richer input language.







Acknowledgments



This work is supported by the Finnish Center of Excellence in Computational Inference Research (COIN) funded by the Academy of Finland (under grant #251170).







References



Alviano, M.; Dodaro, C.; Leone, N.; and Ricca, F. 2015. Advances in WASP. In Logic Programming and Nonmonotonic Reasoning 13th International Conference, LPNMR 2015. Proceedings, 40-54. Apt, K. R. 2003. Principles of Constraint Programming. Cambridge University Press. Audemard, G., and Simon, L. 2009. Predicting learnt clauses quality in modern SAT solvers. In Boutilier, C., ed., IJCAI, 399-404. Barrett, C. W.; Sebastiani, R.; Seshia, S. A.; and Tinelli, C. 2009. Satisfiability modulo theories. In Biere, A.; Heule, M.; van Maaren, H.; and Walsh, T., eds., Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications. IOS Press. 825-885. Bayless, S.; Bayless, N.; Hoos, H. H.; and Hu, A. J. 2015. SAT modulo monotonic theories. In Bonet, B., and Koenig, S., eds., Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015., 3702-3709. AAAI Press. Chu, G., and Stuckey, P. J. 2014. Nested constraint programs. In O'Sullivan, B., ed., Principles and Practice of Constraint Programming - CP 2014. Proceedings, volume 8656 of Lecture Notes in Computer Science, 240-255. Springer. De Cat, B.; Bogaerts, B.; Devriendt, J.; and Denecker, M. 2013. Model expansion in the presence of function symbols using constraint programming. In 2013 IEEE 25th International Conference on Tools with Artificial Intelligence, 2013, 1068-1075. IEEE Computer Society. E en, N., and S orensson, N. 2003. An extensible SAT-solver. In Giunchiglia, E., and Tacchella, A., eds., SAT, volume 2919 of LNCS, 502-518. Springer. Ganzinger, H.; Hagen, G.; Nieuwenhuis, R.; Oliveras, A.; and Tinelli, C. 2004. DPLL(T): Fast decision procedures. In Alur, R., and Peled, D., eds., CAV, volume 3114 of LNCS, 175-188. Springer. Gebser, M.; Janhunen, T.; and Rintanen, J. 2014. SAT modulo graphs: Acyclicity. In Logics in Artificial Intelligence - 14th European Conference, JELIA 2014. Proceedings, 137-151. Gebser, M.; Kaufmann, B.; and Schaub, T. 2012. Conflict-driven answer set solving: From theory to practice. Artif. Intell. 187:52- 89. Gebser, M.; Kaufmann, B.; and Schaub, T. 2013. Advanced conflict-driven disjunctive answer set solving. In IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence. Giunchiglia, E.; Narizzano, M.; and Tacchella, A. 2001. Quantified Boolean formulas satisfiability library (QBFLIB). http://www. qbflib.org. Giunchiglia, E.; Narizzano, M.; and Tacchella, A. 2002. Learning for quantified boolean logic satisfiability. In Proceedings of the Eighteenth National Conference on Artificial Intelligence and Fourteenth Conference on Innovative Applications of Artificial Intelligence., 649-654. Goultiaeva, A., and Bacchus, F. 2013. Recovering and utilizing partial duality in QBF. In Theory and Applications of Satisfiability Testing - SAT 2013 - 16th International Conference, Helsinki, Finland, July 8-12, 2013. Proceedings, 83-99. Goultiaeva, A.; Seidl, M.; and Biere, A. 2013. Bridging the gap between dual propagation and cnf-based qbf solving. In Design, Automation Test in Europe Conference Exhibition (DATE), 2013, 811-814.







Janhunen, T.; Tasharrofi, S.; and Ternovska, E. 2016. SAT- TO -SAT: Declarative extension of SAT solvers with new propagators. In To Appear in the Proceedings of 30th AAAI Conference on Artificial Intelligence (AAAI-16). Preprint available on https://www.cs.sfu.ca/sta44/personal/ files/jtt-aaai-2016.pdf. Janota, M., and Marques-Silva, J. 2015. Solving QBF by clause selection. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015., 325-331. Janota, M.; Klieber, W.; Marques-Silva, J.; and Clarke, E. M. 2012. Solving QBF with counterexample guided refinement. In Theory and Applications of Satisfiability Testing - SAT 2012 - 15th International Conference, 2012. Proceedings, 114-128. Klieber, W.; Sapra, S.; Gao, S.; and Clarke, E. M. 2010. A nonprenex, non-clausal QBF solver with game-state learning. In Theory and Applications of Satisfiability Testing - SAT 2010, 13th International Conference, Proceedings, 128-142. Letz, R. 2002. Lemma and model caching in decision procedures for quantified boolean formulas. In Automated Reasoning with Analytic Tableaux and Related Methods, International Conference, TABLEAUX 2002, Proceedings, 160-175. Marek, V., and Truszczy nski, M. 1999. Stable models and an alternative logic programming paradigm. In Apt, K. R.; Marek, V.; Truszczy nski, M.; and Warren, D. S., eds., The Logic Programming Paradigm: A 25-Year Perspective. Springer-Verlag. 375-398. Marques-Silva, J. P., and Sakallah, K. A. 1999. GRASP: A search algorithm for propositional satisfiability. IEEE Transactions on Computers 48(5):506-521. Nadel, A., and Ryvchin, V. 2012. Efficient SAT solving under assumptions. In Theory and Applications of Satisfiability Testing - SAT 2012 - 15th International Conference, 2012. Proceedings, 242-255. Ohrimenko, O.; Stuckey, P. J.; and Codish, M. 2009. Propagation via lazy clause generation. Constraints 14(3):357-391. QBF Gallery. 2014. QCIR-G14: A non-prenex non-CNF format for quantified boolean formulas. Technical report. 2014. QBF gallery 2014 (competition). http://qbf. satisfiability.org/gallery/index.html. Rabe, M. N., and Tentrup, L. 2015. Caqe: A certifying qbf solver. In Proceedings of the 15th Conference on Formal Methods in Computer-aided Design (FMCAD'15), 136-143. Ranjan, D. P.; Tang, D.; and Malik, S. 2004. A comparative study of 2qbf algorithms. In SAT 2004 - The Seventh International Conference on Theory and Applications of Satisfiability Testing, Online Proceedings. Silva, J. P. M.; Lynce, I.; and Malik, S. 2009. Conflict-driven clause learning SAT solvers. In Biere, A.; Heule, M.; van Maaren, H.; and Walsh, T., eds., Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications. IOS Press. 131-153. Tseitin, G. S. 1968. On the complexity of derivation in the propositional calculus, Zapiski nauchnykh seminarov. LOMI 8:234-259. English translation of this volume: Studies in Constructive Mathematics and Mathematical Logic, Part 2, A. O. Slisenko, eds. Consultants Bureau, N.Y., 1970, pp. 115-125. Zhang, L., and Malik, S. 2002. Towards a symmetric treatment of satisfaction and conflicts in quantified boolean formula evaluation. In Principles and Practice of Constraint Programming - CP 2002, 8th International Conference, CP 2002, Proceedings, 200-215.







Multi-Agent Path Finding with Delay Probabilities



Hang Ma



Department of Computer Science University of Southern California hangma@usc.edu







T. K. Satish Kumar



Department of Computer Science University of Southern California tkskwork@gmail.com







Sven Koenig



Department of Computer Science University of Southern California skoenig@usc.edu







Abstract



Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.







Start State Goal State







v1 v2 s2 s1 v3 g1 v4 g2 v5







Figure 1: A MAPF-DP instance. an undirected graph (that models the environment) to move from its start vertex to its goal vertex. At any discrete time step, the agent can either execute 1) a wait action, resulting in it staying in its current vertex, or 2) a move action with the intent of traversing an outgoing edge of its current vertex, resulting in it staying in its current vertex with the delay probability and traversing the edge otherwise. The MAPFDP problem is the problem of finding 1) a MAPF-DP plan that consists of a path for each agent from its start vertex to its goal vertex (given by a sequence of wait and move actions) and 2) a plan-execution policy that controls with GO or STOP commands how each agent proceeds along its path such that no collisions occur during plan execution. There are 2 kinds of collisions, namely vertex collisions (where 2 agents occupy the same vertex at the same time step) and edge collisions (where 2 agents traverse the same edge in opposite directions at the same time step). We make the following contributions to solve the MAPFDP problem with small average makespans: First, we formalize the MAPF-DP problem, define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.







Introduction



Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths for a given number of agents from their given start locations to their given goal locations in a given environment. MAPF problems arise for aircraft towing vehicles (Morris et al. 2016), office robots (Veloso et al. 2015), video game characters (Silver 2005) and warehouse robots (Wurman, D'Andrea, and Mountz 2008), among others. Several recently developed MAPF solvers scale to large MAPF instances. However, agents typically cannot execute their MAPF plans perfectly since they often traverse their paths more slowly than intended. Their delay probabilities can be estimated but current MAPF solvers do not use this information, which often leads to frequent and runtimeintensive replanning or plan-execution failures. We thus formalize the MAPF Problem with Delay Probabilities (MAPF-DP), where each agent traverses edges on



Our research was supported by NSF under grant numbers 1409987 and 1319966. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the sponsoring organizations, agencies or the U.S. government. Copyright c 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.











Background and Related Work



The MAPF problem is NP-hard to solve optimally for flowtime minimization and to approximate within any constant factor less than 4/3 for makespan minimization (Ma et al. 2016). Search-based MAPF solvers can be optimal, bounded suboptimal or suboptimal (Standley 2010; Luna







and Bekris 2011; Wang and Botea 2011; Goldenberg et al. 2014; Sharon et al. 2013; 2015; Boyarski et al. 2015; Wagner and Choset 2015; Ma and Koenig 2016; Cohen et al. 2016). Current MAPF solvers typically assume perfect plan execution. However, utilizing probabilistic information about imperfect plan execution can reduce frequent and time-intensive replanning and plan-execution failures. Partially Observable Markov Decision Processes (POMDPs) are a general probabilistic planning framework. The MAPF-DP problem can be solved with POMDPs but this is tractable only for very few agents in very small environments since the size of the state space is proportional to the size of the environment to the power of the number of agents and the size of the belief space is proportional to the size of the state space to the power of the length of the planning horizon (Kurniawati, Hsu, and Lee 2008; Ma and Pineau 2015). Several specialized probabilistic planning frameworks, such as transition-independent decentralized Markov Decision Processes (DecMDPs) (Becker et al. 2004) and Multi-Agent Markov Decision Processes (MMDPs) (Boutilier 1996) can solve larger probabilistic planning problems than POMDPs. In transition-independent Dec-MDPs, the local state of each agent depends only on its previous local state and the action taken by it (Goldman and Zilberstein 2004). MAPF-DP is indeed transition independent. However, there are interactions among agents since the reward of each agent depends on whether it is involved in a collision and thus on the local states of other agents and the actions taken by them. Fully decentralized probabilistic planning frameworks thus cannot prevent collisions. Fully centralized probabilistic planning frameworks can prevent collisions but are more runtime-intensive and can thus scale poorly. For example, the MAPF-DP problem can be solved with transition-independent MMDPs (Scharpff et al. 2016). In fact, the most closely related research to ours is that on approximating MMDPs (Liu and Michael 2016) although it handles different types of dynamics than we do. The runtime of probabilistic planning frameworks can be reduced by exploiting the problem structure, including when interactions among agents are sparse. For example, decentralized sparse-interaction Markov Decision Processes (Dec-SIMDPs) (Melo and Veloso 2011) assume that interactions among agents occur only in well-defined interaction areas in the environment (which is not the case for MAPF-DP in general), but typically still do not scale to more than 10 agents. The model shaping technique for decentralized POMDPs (Velagapudi et al. 2011) can compute policies for hundreds of agents greedily and UM* (Wagner 2015) scales to larger numbers of agents (with identical delay probabilities), but the plan execution for both approaches is completely decentralized and thus cannot prevent collisions.







ai has a unique start vertex si  V , a unique goal vertex gi  V and a delay probability pi  (0, 1). A path for agent ai is expressed by a function li that maps each time index x = 0, 1 . . . Xi to a vertex li (x)  V such that li (0) = si , consecutive vertices li (x) and li (x + 1) are either identical (when agent ai is scheduled to execute a wait action) or connected by an edge (when agent ai is scheduled to execute a move action from vertex li (x) to vertex li (x + 1)) and li (Xi ) = gi . A MAPF plan consists of a path for each agent.







Problem Definition: Plan Execution



The local state xt i of agent i at time step t = 0, 1 . . .  during plan execution is a time index. We set x0 i := 0 and always update its local state such that it is in vertex li (xt i ) at time step t. The agent knows its current local state and receives messages from some of the other agents about their local states. At each time step, its plan-execution policy maps this knowledge to one of the commands GO or ST OP that control how it proceeds along its path. 1. If the command is GO at time step t: (a) If xt i = Xi , then agent ai executes no action and remains in its current vertex li (xt i ) since it has entered its last local state (and thus the end of its path). We thus +1 update its local state to xt := xt i. i t t (b) If xt = X and l ( x ) = l i i i i (xi + 1), then agent ai i executes a wait action to remain in its current vertex li (xt i ). The execution of wait actions never fails. We +1 thus update its local state to xt := xt i + 1 (success). i t t t (c) If xi = Xi and li (xi ) = li (xi + 1), then agent ai executes a move action from its current vertex li (xt i ) to vertex li (xt + 1) . The execution of move actions fails i with delay probability pi with the effect that the agent executes no action and remains delayed in its current t+1 vertex li (xt := i ). We thus update its local state to xi t +1 t t xi with probability pi (failure) and xi := xi + 1 with probability 1 - pi (success). 2. If the command is ST OP at time step t, then agent ai executes no action and remains in its current vertex li (xt i ). +1 t We thus update its local state to xt := x . i i Our objective is to find a combination of a MAPF plan and a plan-execution policy with small average makespan, which is the average earliest time step during plan execution when all agents have entered their last local states. The MAPF problem is a special case where the delay probabilities of all agents are zero and the plan-execution policies always provide GO commands.







Valid MAPF-DP Plans



Definition 1. A valid MAPF-DP plan is a plan with 2 properties: 1. i, j, x with i = j : li (x) = lj (x) [two agents are never scheduled to be in the same vertex at the same time index, that is, the vertices of two agents in the same local state are different]. 2. i, j, x with i = j : li (x + 1) = lj (x) [an agent is never scheduled to be in a vertex at a time index x + 1 when any other agent is scheduled to be in the same vertex at time index x, that is, the vertex of an agent in a local state x +1







Problem Definition: Planning



A MAPF-DP instance is characterized by an undirected graph G = (V, E ) whose vertices V correspond to locations and whose edges E correspond to transitions between locations. We are given m agents a1 , a2 . . . am . Each agent







has to be different from the vertex of any other agent in local state x]. Figure 1 shows a sample MAPF-DP instance where the blue agent a1 has to move from its start vertex v3 to its goal vertex v4 and the red agent a2 has to move from its start vertex v2 to its goal vertex v5 . Agent a1 has to move north to let agent a2 pass. The paths l1 = v3 , v1 , v1 , v1 , v3 , v4 and l2 = v2 , v2 , v3 , v4 , v5 form a valid MAPF-DP plan. However, the paths l1 = v3 , v1 , v1 , v3 , v4 and l2 = v2 , v3 , v4 , v5 a valid MAPF plan but not a valid MAPF-DP plan since l2 (1) = l1 (0) = v3 violates Property 2. Property 1 of Definition 1 is necessary to be able to execute valid MAPF-DP plans without vertex collisions because two agents could otherwise be in the same vertex at the same time step (under perfect or imperfect plan execution). Property 2 is also necessary because an agent could otherwise enter the vertex of some other agent that unsuccessfully tries to leave the same vertex at the same time step (under imperfect plan execution). Property 2 is also necessary to be able to execute valid MAPF-DP plans without edge collisions (under perfect or imperfect plan execution).







Robust Plan-Execution Policies



We study 2 kinds of decentralized robust plan-execution policies for valid MAPF-DP plans, which are plan-execution policies that prevent all collisions during the imperfect plan execution of valid MAPF-DP plans.







Fully Synchronized Policies (FSPs)



Fully Synchronized Policies (FSPs) attempt to keep all agents in lockstep as much as possible by providing a GO command to an agent if and only if the agent has not yet entered its last local state and all other agents have either entered their last local states or have left all local states that precede the local state of the agent itself. FSPs can be implemented easily if each agent sends a message to all other agents when it enters a new local state. An agent can implement its FSP simply by counting how many messages it has received from each other agent and providing a GO command to itself in local state x if and only if it has not yet entered its last local state and has received x messages over the course of plan execution from each other agent.







Minimal Communication Policies (MCPs)



FSPs have 2 drawbacks. First, agents wait unnecessarily, which results in large average makespans. Second, each agent always needs to know the local states of all other agents, which results in many sent messages. Property 2 of Definition 1 suggests that robust plan-execution policies for valid MAPF-DP plans could provide a GO command to an agent if and only if the agent has not yet entered its last local state and all other agents have left all local states that precede the local state of the agent itself and whose vertices are the same as the vertex of the next local state of the agent itself. This way, it is guaranteed that the vertex of the next local state of the agent is different from the vertices of all other agents in their current local states. Minimal Communication







Policies (MCPs) address these drawbacks by identifying such critical dependencies between agents and obeying them during plan execution, an idea that originated in the context of centralized non-robust plan-execution policies (H onig et al. 2016). The local state of an agent ai at any time step during plan execution is a time index x. Since we need to relate the local states of different agents, we use li (x) in the following not only to refer to the vertex assigned to local state x of agent ai but also to the local state x of agent ai itself (instead of x), depending on the context. Every valid MAPF-DP plan defines a total order on the local states of all agents, which we relax to a partial order  as follows: 1. i, x : li (x)  li (x + 1) [agent ai enters a local state x + 1 during plan execution only after it enters local state x]. 2. i, j, x, x with i = j , x < x and l = lj (x ) = li (x + 1) : lj (x + 1)  li (x + 1) [agent ai enters a local state x + 1 with a vertex l during plan execution only after agent aj has left a local state x with vertex l (and thus entered local state x + 1) that precedes local state x]. Property 1 of the partial order enforces that each agent visits its locations in the same order as in the MAPFDP plan. Property 2 enforces that any two agents visit the same location in the same order as in the MAPFDP plan. We can express the partial order with a directed graph G = (V , E ) whose vertices correspond to local states and whose edges correspond to the partial order given by the two properties above. Property 2 specifies the critical dependencies between agents. Edges are redundant and can then be removed from the directed graph when they are implied by the other edges due to transitivity. A transitive reduction of the directed graph minimizes the number of remaining edges. It can be computed in time O(|V||E|) (Aho, Garey, and Ullman 1972), is unique, contains all edges between local states of the same agent (since they are never redundant) and thus minimizes the number of edges between the local states of different agents. MCPs can be implemented easily if each agent aj sends a message to each other agent ai when agent aj enters a new local state x  (= x + 1 in Property 2) if and only if the transitive reduction contains an edge lj ( x )  li ( x) for some local state x  (= x + 1 in Property 2) of agent ai . Since the transitive reduction minimizes the number of edges between the local states of different agents, it also minimizes the number of sent messages. An agent ai can implement its MCP simply by counting how many messages it has received from each other agent and providing a GO command to itself in local state x if and only if it has not yet entered its last local state and has received a number of messages over the course of plan execution from each other agent aj that corresponds to the number of incoming edges from local states of agent aj to its local states 0, 1 . . . x + 1. Figure 2 shows a sample partial order on the local states for the MAPF-DP instance from Figure 1 and its valid MAPF-DP plan l1 = v3 , v1 , v3 , v1 , v1 , v1 , v3 , v4 and l2 = v2 , v2 , v2 , v2 , v3 , v4 , v5 . l1 (1)  l2 (4), for example, is implied by l1 (1)  l1 (2)  l1 (3)  l2 (4) and can thus







v3



l1 (0)







v1



l1 (1)







v3



l1 (2)







v1



l1 (3)







v1



l1 (4)







v1



l1 (5)







v3



l1 (6)







v4



l1 (7)







v2



l2 (0)







v2



l2 (1)







v2



l2 (2)







v2



l2 (3)







v3



l2 (4)







v4



l2 (5)







v5



l2 (6)







Figure 2: A directed graph that specifies a partial order on the local states for the MAPF-DP instance from Figure 1 and its valid MAPF-DP plan l1 = v3 , v1 , v3 , v1 , v1 , v1 , v3 , v4 and l2 = v2 , v2 , v2 , v2 , v3 , v4 , v5 .



v3



l1 (0)







li (x) = lj (y + 1) = lj (x + 1), which is a contradiction with the State Property. Case 2) If x < y , then li (x + 1)  lj (y +1) according to Property 2 of the partial order  since li (x) = lj (y +1) according to our edge collision assumption and x < y according to the case assumption. Thus, agent aj can leave local state y only when agent ai reaches local state x + 1, which is a contradiction with the edge collision assumption.







v1



l1 (1)







v3



l1 (2)







v1



l1 (3)







v1



l1 (4)







v1



l1 (5)







v3



l1 (6)







v4



l1 (7)







Approximate Minimization in Expectation



MCPs are robust plan-execution policies for valid MAPFDP plans that do not stop agents unnecessarily and result in few sent messages. We present a MAPF-DP solver, called Approximate Minimization in Expectation (AME), that determines valid MAPF-DP plans so that their combination with MCPs results in small average makespans. AME is a 2-level MAPF-DP solver that is based on Conflict-Based Search (CBS) (Sharon et al. 2015). Its highlevel search imposes constraints on the low-level search that resolve violations of Properties 1 and 2 of Definition 1 (called conflicts). Its low-level search plans paths for single agents that obey these constraints and result in small average makespans. The average makespan of a MAPF-DP plan is the expectation of the maximum of (one or more) random variables that represent the time steps when all agents enter their last local states. Moreover, the average time step when an agent enters a local state is the expectation of the maximum of random variables as well. It is often difficult to obtain good closed-form approximations of the expectation of the maximum of random variables. AME thus approximates it with the maximum over the expectations of the random variables, which typically results in an underestimate but, according to our experimental results, a close approximation. The approximate average time step  li ( x ) when agent ai enters a local state x for a given MAPF-DP plan is 0 for x = 0 and



max( li (x - 1), maxj,x = maxj,x



:i=j,x <x,lj (x )li (x) (lj (x







v2



l2 (0)







v2



l2 (1)







v2



l2 (2)







v2



l2 (3)







v3



l2 (4)







v4



l2 (5)







v5



l2 (6)







Figure 3: The transitive reduction for Figure 2. be removed from the directed graph. Figure 3 shows the resulting transitive reduction, which implies that agent a2 has to wait in local state 3 until it has received one message from agent a1 during the course of plan execution but can then proceed through all future local states without waiting.







Properties of FSPs and MCPs



Both FSPs and MCPs do not result in deadlocks during the plan execution of valid MAPF-DP plans because there always exists at least one agent that is provided a GO command before all agents have entered their last local states (namely an agent with the smallest local state among all agents that have not yet entered their last local states since an agent can wait only for other agents with smaller local states). Both FSPs and MCPs are robust plan-execution policies due to Properties 1 and 2 of valid MAPF-DP plans. We now provide a proof sketch for the robustness of MCPs. First, consider a valid MAPF-DF plan and assume that li (x) = lj (y ) for two agents ai and aj with i = j . Then, 1) y = x since li (x) = lj (x) according to Property 1 of Definition 1 and 2) y = x + 1 since lj (x + 1) = li (x) according to Property 2 of Definition 1 (State Property). Second, we show by contradiction that no vertex collisions can occur during plan execution. Assume that a vertex collision occurs between agents ai and aj with i = j when agent ai is in local state x and agent aj is in local state y . Assume without loss of generality that x  y . Then, li (x +1)  lj (y ) according to Property 2 of the partial order  since li (x) = lj (y ) according to our vertex collision assumption and x < y - 1 according to the State Property. Thus, agent aj can leave local state y - 1 only when agent ai reaches local state x + 1, which is a contradiction with the vertex collision assumption. Third, we show by contradiction that no edge collisions can occur during plan execution. Assume that an edge collision between agents ai and aj with i = j occurs when agent ai changes its local state from x to x + 1 and agent aj changes its local state from y to y + 1. Assume without loss of generality that x  y . Case 1) If x = y , then















i ))) + t (1)







:x <x,lj (x







  )li (x) (lj (x )) + ti







otherwise since agent ai first enters local state x - 1 at approximate average time step  li (x - 1), then might have to wait for messages from other agents aj that they send when they enter their local states x at approximate average time steps  lj (x ) and finally has to successfully execute one action (perhaps repeatedly) to enter local state i of time steps that it needs for x. The average number t the successful execution of the action is 1 (for a wait action) if li (x) = li (x - 1) and 1/(1 - pi ) (for a move action) otherwise. The approximate average makespan of the given MAPF-DP plan is then maxi  li (Xi ) since all agents need to enter their last local states. One might be able to obtain better approximations with more runtime-intensive importance sampling or dynamic programming methods but the runtime of the resulting AME variant would be large since it needs to compute many such approximations.







Algorithm 1: High-Level Search of AME.



1 Root.constraints := ; 2 Root.plan := ; 3 for each agent ai do 4 if LowLevelSearch(ai , Root, 0) returns no path (nor its labels) then 5 return "No solution exists"; 6 Add the returned path (and its labels) to Root.plan;







the second child node [Line 18], thus preventing the conflict in both cases.







Low-Level Search



LowLevelSearch(ai , N , key) finds a new path for agent ai and the labels  li (x) of this path. It uses the paths of the other agents and their labels in N.plan but does not update them. (The paths are empty directly after the execution of Line 2.) It performs a focal search with re-expansions in a state space whose states correspond to pairs of vertices and local states (except for those pairs ruled out by constraints in N.constraints that pertain to agent ai ) and whose edges connect state (l, x) to state (l , x +1) if and only if l = l (for a wait action) or (l, l )  E (for a move action). The g-value of a state (l, x) approximates (sic!) the approximate average time step  li (x). The start state is (si , 0) and its g-value is 0. When the low-level search expands state (l, x - 1), it sets the g-value of its successor (l , x) according to Equation (1) to the minimum of its current g-value g ((l , x)) and



max(g ((l, x - 1)), maxj,x



:i=j,x <x,lj (x )li (x) (lj (x







7 Root.key := ApproximateAverageMakespan(Root.plan); 8 Priorityqueue := {Root}; 9 while Priorityqueue =  do 10 N := Priorityqueue.pop(); 11 if FindConflicts(N.plan) returns no conflicts then 12 return "Solution is" N.plan; 13 14 15 16 17 18 19 20 21 22 Conflict := earliest returned conflict; for each agent ai involved in Conflict do N := new node with parent node N ; N .constraints := N.constraints; N .plan := N.plan; Add one new constraint for agent ai to N .constraints (see main text); if LowLevelSearch(ai , N , N.key) returns a path (and its labels) then Replace the path (and its labels) of agent ai in N .plan with the returned path (and its labels); N .key := ApproximateAverageMakespan(N .plan); Priorityqueue.insert(N );







23 return "No solution exists";















i , ))) + t







High-Level Search



Algorithm 1 shows the high-level search of AME, which is similar to the high-level search of CBS. In the following, we point out the differences. Each high-level node N contains the following items: 1. A set N.constraints of constraints of the form (ai , l, x) that states that the vertex of agent ai in local state x has to be different from vertex l. 2. A (labeled) MAPF-DP plan N.plan that contains a path li for each agent ai (that obeys the constraints N.constraints) and an approximation  li (x) (called label) of each average time step when agent ai enters local state x during plan execution with MCPs. 3. The key N.key of high-level node N that encodes its priority (smaller keys have higher priority) and is equal to the approximate average makespan of MAPF-DP plan N.plan given by ApproximateAverageMakespan(N.plan) = maxi  li (Xi ). When a conflict exists in MAPF-DP plan N.plan, then the high-level search creates 2 child nodes of node N [Line 15] whose constraints are initially set to the constraints N.constraints [Line 16] and whose MAPF-DP plan is initially set to MAPF-DP plan N.plan [Line 17]. Assume that the earliest conflict is a violation of Property 1 in Definition 1, in which case the vertices of two agents ai and aj in a local state x are both identical to a vertex l. In this case, AME adds the constraint (ai , l, x) to the constraints of the first child node and the constraint (aj , l, x) to the constraints of the second child node [Line 18], thus preventing the conflict in both cases. Assume that the earliest conflict is a violation of Property 2 in Definition 1, in which case the vertex of an agent ai in a local state x + 1 and the vertex of some other agent aj in the immediately preceding local state x are both identical to a vertex l. In this case, AME adds the constraint (ai , l, x + 1) to the constraints of the first child node and the constraint (aj , l, x) to the constraints of







i is 1 if l = l and 1/(1 - pi ) otherwise. The lowwhere t level search decides which state (l, x) to expand next based on 1) the f-value of the state, which is the sum of its g-value and its h-value, where the h-value is 1/(1 - pi ) times the distance from location l to location gi in graph G (which is an optimistic estimate of the average number of time steps required to move from location l to location gi ) and 2) the number of conflicts of the path for agent ai that corresponds to the locations in the states on the found path from the start state to (l, x) with the paths of other agents. The low-level search starts in Phase 1. The objective in this phase is to find a path for agent ai so that it enters its last local state with a reasonably small approximate average number of time steps, namely one that is no larger than the approximate average makespan key of the MAPF-DP plan in the parent node of node N in the high-level search, and has a small number of conflicts. The first part of the objective tries to ensure that the approximate average makespan of the resulting MAPF-DP plan in node N is no larger than the one of the MAPF-DP plan in the parent node of node N , and the second part tries to ensure that the resulting MAPF-DP plan has a small number of conflicts so that the high-level search has a small runtime since it needs to resolve only a small number of conflicts. The low-level search thus repeatedly expands a state with the smallest number of conflicts among all states in the priority queue whose f-values are no larger than key. If no such state exists, then the low-level search switches to Phase 2. The objective in this phase is to find a path for agent ai so that it enters its last local state with a small approximate average number of time steps. This objective tries to ensure that the approximate average makespan of the resulting MAPF-DP plan in node N is not much larger than the one of the MAPF-DP plan in the parent node of node N . The low-level search thus repeatedly expands a state with the smallest f-value among all states in the priority queue.







The low-level search terminates successfully when it is about to expand a state (l, x) with l = gi and N.constraints contains no constraints of the form (ai , gi , x ) with x > x. It then sets Xi := x, the locations li (x) that form the path of agent ai to the corresponding locations in the states on the found path from the start state to (l, x) and the approximate average time steps  li (x) to the corresponding g-values of these states. The low-level search terminates unsuccessfully when the priority queue becomes empty. The low-level search currently does not terminate otherwise but we might be able to make it complete by using an upper bound on the smallest average makespan of any valid MAPF-DP plan, similar to upper bounds in the context of valid MAPF plans (Kornhauser, Miller, and Spirakis 1984).







Future Work



The low-level search is currently the weakest part of AME due to the many approximations to keep its runtime small which is important since the high-level search runs many low-level searches. We expect that future work will be able to improve the low-level search substantially. For example, the approximate average time steps  lj (x) for agents aj different from agent ai could be updated before, during or after the local search, which would provide more accurate values for the current and future low-level searches as well as the current high-level search. Once the low-level search finds a path for agent ai and the high-level search replaces the path of agent ai in the MAPF-DP plan in the current high-level node with this path, it could update the approximate average time steps of all agents to the ideal approximate average time steps given by Equations (1), for example as part of the execution of ApproximateAverageMakespan on Lines 7 and 21. Many other improvements are possible as well.







Figure 4: Two MAPF-DP instances: random 1 (top) and warehouse 1 (bottom). Blocked cells are shown in black. The start and goal cells for each agent are represented by a solid circle and a hollow circle of the same color, respectively. Table 1 reports for each MAPF-DP instance the runtime, the approximate average makespan calculated by AME, the average makespan over 1,000 plan-execution runs with MCPs together with 95%-confidence intervals and the number of sent messages. Dashes indicate that the MAPFDP instance was not solved within a runtime limit of 5 minutes. There is no obvious difference in the numbers of sent messages of the 3 MAPF(-DP) solvers. However, AME seems to find MAPF-DP plans with smaller average makespans than Adapted CBS, which seems to find MAPFDP plans with smaller average makespans than Push and Swap. The approximate average makespans calculated by AME are underestimates but reasonably close to the average makespans. AME and Push and Swap seem to run faster than Adapted CBS. In fact, Adapted CBS did not solve MAPFDP instances with more than 35 agents within the runtime limit while AME and Push and Swap seem to scale to larger numbers of agents than reported here (see also Experiment 3).







Experiments



We evaluate AME with MCPs on a 2.50 GHz Intel Core i52450M PC with 6 GB RAM.







Experiment 1: MAPF Solvers



We compare AME to 2 MAPF solvers, namely 1) Adapted CBS, a CBS variant that assumes perfect plan execution and computes valid MAPF-DP plans, minimizes maxi Xi and breaks ties toward paths with smaller Xi and thus fewer actions and 2) Push and Swap (Luna and Bekris 2011), a MAPF solver that assumes perfect plan execution and computes valid MAPF-DP plans where exactly one agent executes a move action at each time step and all other agents execute wait actions. We generate 10 MAPFDP instances (labeled random 1-10) in 30x30 4-neighbor grids with 10% randomly blocked cells and random but unique start and unique goal cells for 35 agents whose delay probabilities for AME are sampled uniformly at random from the delay probability range (0, 1/2). In the same way, we generate 10 MAPF-DP instances (labeled warehouse 110) in a simulated warehouse environment with random but unique start and unique goal cells on the left and right sides. Figure 4 shows two MAPF-DP instances: random 1 (top) and warehouse 1 (bottom).







Experiment 2: Delay Probability Ranges



We use AME with different delay probability ranges. We repeat Experiment 1 with 19 MAPF-DP instances generated from the MAPF-DP instance labeled "random 1" in Experimax = 2, 3 . . . 20. For each MAPFment 1, one for each t DP instance, the delay probabilities pi of all agents are max ) by sampled from the delay probability range (0, 1 - 1/t  sampling the average number of time steps ti = 1/(1 - pi ) needed for the successful execution of single move actions max ) and then calculating uniformly at random from (1, t i . pi = 1 - 1/t Table 2 reports the same measures as used in Experiment







800







Table 1: Results of different MAPF(-DP) solvers for MAPFDP instances with 35 agents and delay probability range (0, 1/2).



AME Push and Swap Adapted CBS approxruntime imate average mess- runtime average messaverage mess- runtime id (s) makespan ages (s) makespan ages average makespan ages (s) makespan random 1 0.058 63.15 71.28  0.34 267 0.031 812.41  0.40 287 random 2 0.052 66.22 73.02  0.29 257 0.025 768.30  0.43 257 random 3 0.080 78.44 84.90  0.40 373 0.052 934.59  0.33 387 random 4 0.063 67.00 72.89  0.37 251 0.028 755.95  0.33 255 random 5 0.050 65.13 73.98  0.31 255 0.029 875.48  0.47 318 282.079 84.11  0.40 282 random 6 0.052 62.89 66.98  0.36 257 0.031 830.77  0.32 290 random 7 0.495 67.22 71.34  0.36 269 0.038 785.55  0.46 274 random 8 0.042 49.33 51.72  0.35 164 0.024 648.80  0.35 199 197.911 52.35  0.37 163 random 9 0.051 56.27 61.30  0.27 247 0.052 780.60  0.30 294 random 10 0.487 60.06 64.77  0.38 234 0.032 750.12  0.35 284 warehouse 1 0.124 114.32 124.18  0.44 705 0.055 1,399.14  0.43 703 warehouse 2 0.106 119.74 124.63  0.51 762 0.055 1,620.03  0.60 810 warehouse 3 0.107 112.96 117.00  0.53 609 0.032 1,295.75  0.53 616 warehouse 4 0.090 114.90 117.31  0.52 541 0.043 1,246.47  0.67 571 warehouse 5 0.060 1,453.36  0.54 783 warehouse 6 0.111 127.65 131.10  0.59 710 0.037 1,437.01  0.58 664 warehouse 7 0.142 87.45 96.54  0.34 488 0.028 1,154.21  0.60 403 warehouse 8 0.024 1,233.13  0.58 401 warehouse 9 0.087 103.51 107.33  0.42 462 0.024 1,088.53  0.44 422 warehouse 10 0.183 120.76 127.36  0.53 909 0.057 1,541.56  0.62 678 -







700







600



500 400 300







200



100 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20







Figure 5: Visualization of Table 2, where the x-axis shows max needed for the the average number of time steps t successful execution of single move actions. The average makespans are shown in red, and the approximate average makespans calculated by AME are shown in blue. The grey max . line corresponds to 30t







Table 2: Results of AME for MAPF-DP instances with 35 agents on a 30x30 4-neighbor grid with 10% randomly blocked cells and different delay probability ranges (0, 1 - 1 ).  t



max







Table 3: Results of AME for MAPF-DP instances with different numbers of agents on 30x30 4-neighbor grids with 10% randomly blocked cells and delay probability range (0, 1/2).



agents 50 100 150 200 solved (%) 0.94 0.68 0.10 0 runtime (s) 0.166 4.668 134.155 approximate average makespan 69.32 78.48 81.77 average makespan 75.19 87.29 96.43 messages 474.62 1,554.71 2,940.40







max t 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20







runtime (s) 0.073 0.525 0.356 0.311 0.623 0.346 0.236 0.779 1.751 2.528 1.374 0.683 2.583 1.414 7.072 2.116 3.410 5.708 7.812







approximate average makespan 77.92 123.92 144.61 133.55 168.51 264.78 333.09 260.58 307.63 337.15 323.87 381.63 440.94 470.06 554.32 451.32 763.44 462.71 490.26







average makespan 84.30  0.42 131.12  0.79 157.88  0.96 157.00  0.98 192.76  1.46 279.51  2.05 349.72  2.69 271.71  2.28 336.95  2.26 375.46  2.74 383.25  2.53 413.18  3.19 498.30  3.32 524.94  3.95 607.20  4.26 570.15  3.90 782.40  6.08 666.42  5.29 591.35  3.73







messages 251 301 287 278 299 289 293 294 305 312 300 282 278 295 316 275 306 309 323







Experiment 4: Plan-Execution Policies



We use AME with 3 plan-execution policies, namely 1) MCPs, 2) FSPs and 3) dummy (non-robust) plan-execution policies that always provide GO commands. We repeat Experiment 1 for each plan-execution policy. Table 4 reports for each solved MAPF-DP instance and plan-execution policy the average makespan over 1,000 plan-execution runs together with 95%-confidence intervals, the number of sent messages for MCPs and FSPs and the average number of collisions for dummy plan-execution policies. The number of sent messages is zero (and thus not shown) for dummy plan-execution policies since, different from MCPs and FSPs, they do not prevent collisions. The average makespan for MCPs seems to be only slightly larger than that for dummy plan-execution policies, and the average makespan and number of sent messages for MCPs seem to be smaller than those for FSPs.







1, and Figure 5 visualizes the results. Larger delay probability ranges seem to result in larger runtimes, approximate average makespans calculated by AME and average makespans (although there is lots of noise). The differences between the approximate average makespans calculated by AME and average makespans are larger as well but remain reasonable.







Experiment 3: Numbers of Agents



We use AME with different numbers of agents. We repeat Experiment 1 with 50 MAPF-DP instances in 30x30 4neighbor grids generated as in Experiment 1 for each number of agents. Table 3 reports the same measures as used in Experiment 1, averaged over all MAPF-DP instances that were solved within a runtime limit of 5 minutes. AME solves most MAPF-DP instances with 50 agents and then degrades gracefully with the number of agents.







Conclusions



In this paper, we formalized the Multi-Agent Path-Finding Problem with Delay Probabilities (MAPF-DP) to account for imperfect plan execution and then developed an efficient way of solving it with small average makespans, namely with Approximate Minimization in Expectation (a 2-level MAPF-DP solver for generating valid MAPF-DP plans) and Minimal Communication Policies (decentralized robust plan-execution policies for executing valid MAPF-DP plans without collisions).







Table 4: Results of AME for the 18 solved MAPF-DP instances from Experiment 1 and different plan-execution policies.



Dummy Plan-Execution Policies average average average average id messages messages makespan makespan makespan collisions random 1 71.28  0.34 267 140.29  0.50 23,109 67.82  0.35 16.68 random 2 73.02  0.29 257 143.55  0.55 19,316 71.96  0.31 14.27 84.90  0.40 373 160.43  0.59 24,098 81.20  0.37 27.71 random 3 random 4 72.89  0.37 251 141.71  0.52 19,587 69.16  0.36 25.38 random 5 73.98  0.31 255 141.49  0.54 20,794 69.59  0.32 14.98 random 6 66.98  0.36 257 115.98  0.51 20,597 66.76  0.37 15.19 random 7 71.34  0.36 269 124.03  0.54 20,481 70.79  0.38 16.53 random 8 51.72  0.35 164 96.04  0.46 16,665 51.65  0.38 8.81 61.30  0.27 247 113.76  0.46 20,976 58.52  0.23 10.33 random 9 random 10 64.77  0.38 234 114.04  0.50 19,834 64.00  0.38 17.51 warehouse 1 124.18  0.44 705 219.63  0.65 28,794 122.42  0.42 34.59 762 235.35  0.72 34,154 124.40  0.60 68.68 warehouse 2 124.63  0.51 warehouse 3 117.00  0.53 609 206.29  0.65 26,647 117.89  0.54 29.61 warehouse 4 117.31  0.52 541 194.07  0.59 24,889 116.02  0.53 28.09 warehouse 6 131.10  0.59 710 205.54  0.71 29,462 131.54  0.60 37.41 warehouse 7 96.54  0.34 488 187.90  0.59 22,401 95.80  0.35 24.91 warehouse 9 107.33  0.42 462 187.80  0.56 18,950 105.63  0.45 22.21 warehouse 10 127.36  0.53 909 226.95  0.73 32,903 127.59  0.55 43.78 MCPs FSPs







References



Aho, A. V.; Garey, M. R.; and Ullman, J. D. 1972. The transitive reduction of a directed graph. SIAM Journal on Computing 1(2):131-137. Becker, R.; Zilberstein, S.; Lesser, V.; and Goldman, C. V. 2004. Solving transition independent decentralized Markov decision processes. Journal of Artificial Intelligence Research 22(1):423-455. Boutilier, C. 1996. Planning, learning and coordination in multiagent decision processes. In Conference on Theoretical Aspects of Rationality and Knowledge, 195-210. Boyarski, E.; Felner, A.; Stern, R.; Sharon, G.; Tolpin, D.; Betzalel, O.; and Shimony, S. E. 2015. ICBS: Improved conflict-based search algorithm for multi-agent pathfinding. In International Joint Conference on Artificial Intelligence, 740-746. Cohen, L.; Uras, T.; Kumar, T. K. S.; Xu, H.; Ayanian, N.; and Koenig, S. 2016. Improved solvers for bounded-suboptimal multiagent path finding. In International Joint Conference on Artificial Intelligence, 3067-3074. Goldenberg, M.; Felner, A.; Stern, R.; Sharon, G.; Sturtevant, N. R.; Holte, R. C.; and Schaeffer, J. 2014. Enhanced Partial Expansion A*. Journal of Artificial Intelligence Research 50:141- 187. Goldman, C. V., and Zilberstein, S. 2004. Decentralized control of cooperative systems: Categorization and complexity analysis. Journal of Artificial Intelligence Research 22:143-174. H onig, W.; Kumar, T. K. S.; Cohen, L.; Ma, H.; Xu, H.; Ayanian, N.; and Koenig, S. 2016. Multi-agent path finding with kinematic constraints. In International Conference on Automated Planning and Scheduling, 477-485. Kornhauser, D.; Miller, G.; and Spirakis, P. 1984. Coordinating pebble motion on graphs, the diameter of permutation groups, and applications. In Annual Symposium on Foundations of Computer Science, 241-250. Kurniawati, H.; Hsu, D.; and Lee, W. S. 2008. SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Robotics: Science and Systems, 65-72. Liu, L., and Michael, N. 2016. An MDP-based approximation method for goal constrained multi-MAV planning under action uncertainty. In IEEE International Conference on Robotics and Automation, 56-62.







Luna, R., and Bekris, K. E. 2011. Push and Swap: Fast cooperative path-finding with completeness guarantees. In International Joint Conference on Artificial Intelligence, 294-300. Ma, H., and Koenig, S. 2016. Optimal target assignment and path finding for teams of agents. In International Conference on Autonomous Agents and Multiagent Systems, 1144-1152. Ma, H., and Pineau, J. 2015. Information gathering and reward exploitation of subgoals for POMDPs. In AAAI Conference on Artificial Intelligence, 3320-3326. Ma, H.; Tovey, C.; Sharon, G.; Kumar, T. K. S.; and Koenig, S. 2016. Multi-agent path finding with payload transfers and the package-exchange robot-routing problem. In AAAI Conference on Artificial Intelligence, 3166-3173. Melo, F. S., and Veloso, M. 2011. Decentralized MDPs with sparse interactions. Artificial Intelligence 175(11):1757-1789. Morris, R.; Pasareanu, C.; Luckow, K.; Malik, W.; Ma, H.; Kumar, S.; and Koenig, S. 2016. Planning, scheduling and monitoring for airport surface operations. In AAAI-16 Workshop on Planning for Hybrid Systems, 608-614. Scharpff, J.; Roijers, D. M.; Oliehoek, F. A.; Spaan, M. T. J.; and de Weerdt, M. M. 2016. Solving transition-independent multiagent MDPs with sparse interactions. In AAAI Conference on Artificial Intelligence, 3174-3180. Sharon, G.; Stern, R.; Goldenberg, M.; and Felner, A. 2013. The increasing cost tree search for optimal multi-agent pathfinding. Artificial Intelligence 195:470-495. Sharon, G.; Stern, R.; Felner, A.; and Sturtevant, N. R. 2015. Conflict-based search for optimal multi-agent pathfinding. Artificial Intelligence 219:40-66. Silver, D. 2005. Cooperative pathfinding. In Artificial Intelligence and Interactive Digital Entertainment, 117-122. Standley, T. S. 2010. Finding optimal solutions to cooperative pathfinding problems. In AAAI Conference on Artificial Intelligence, 173-178. Velagapudi, P.; Varakantham, P.; Sycara, K. P.; and Scerri, P. 2011. Distributed model shaping for scaling to decentralized POMDPs with hundreds of agents. In International Conference on Autonomous Agents and Multi-agent Systems, 955-962. Veloso, M.; Biswas, J.; Coltin, B.; and Rosenthal, S. 2015. CoBots: Robust symbiotic autonomous mobile service robots. In International Joint Conference on Artificial Intelligence, 4423- 4429. Wagner, G., and Choset, H. 2015. Subdimensional expansion for multirobot path planning. Artificial Intelligence 219:1-24. Wagner, G. 2015. Subdimensional Expansion: A Framework for Computationally Tractable Multirobot Path Planning. Ph.D. Dissertation, Carnegie Mellon University. Wang, K., and Botea, A. 2011. MAPP: a scalable multi-agent path planning algorithm with tractability and completeness guarantees. Journal of Artificial Intelligence Research 42:55-90. Wurman, P. R.; D'Andrea, R.; and Mountz, M. 2008. Coordinating hundreds of cooperative, autonomous vehicles in warehouses. AI Magazine 29(1):9-20.







Multiscale Manifold Learning



Chang Wang



IBM T. J. Watson Research Lab 1101 Kitchawan Rd Yorktown Heights, New York 10598 wangchan@us.ibm.com







Sridhar Mahadevan



Computer Science Department University of Massachusetts Amherst, Massachusetts 01003 mahadeva@cs.umass.edu







Abstract



Many high-dimensional data sets that lie on a lowdimensional manifold exhibit nontrivial regularities at multiple scales. Most work in manifold learning ignores this multiscale structure. In this paper, we propose approaches to explore the deep structure of manifolds. The proposed approaches are based on the diffusion wavelets framework, data driven, and able to directly process directional neighborhood relationships without ad-hoc symmetrization. The proposed multiscale algorithms are evaluated using both synthetic and real-world data sets, and shown to outperform previous manifold learning methods.







Introduction



In many application domains of interest, from information retrieval and natural language processing to perception and robotics, data appears high dimensional, but often lies near or on low-dimensional structures, such as a manifold or a graph. By explicitly modeling and recovering the underlying structure, manifold learning methods (Belkin and Niyogi 2003; Roweis and Saul 2000; He and Niyogi 2003) have been shown to be significantly more effective than previous dimensionality reduction methods. Many existing manifold learning approaches are largely based on extending classical Fourier analysis to graphs and manifolds. In particular, spectral graph theory (Chung 1997) combined with classical differential geometry and global analysis on manifolds forms the theoretical basis for "Laplacian" techniques for function approximation and learning on graphs and manifolds, using the eigenfunctions of a Laplace operator naturally defined on the data manifold to reveal hidden structure. While Fourier analysis is a powerful tool for global analysis of functions, it is known to be poor at recovering multiscale regularities across data and for modeling local or transient properties (Mallat 1998). Consequently, one limitation of these techniques is that they only yield a "flat" embedding but not a multiscale embedding. However, when humans try to solve a particular problem (such as natural language processing), they often exploit their intuition about how to decompose the problem into sub-problems and construct multiple levels of representation. As a consequence, there has been rapidly



Copyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







growing interest in the problem of "deep learning", wherein learning methods are designed that construct multiple layers of latent representations from data (Hinton and Salakhutdinov 2006; Hinton, Osindero, and Teh 2006; Lee et al. 2007; Bengio 2009). Another problem with such Fourier analysis based methods is that they cannot handle the relationships characterized by directed graphs without some ad-hoc symmetrization. Some typical examples where non-symmetric matrices arise are when k -nearest neighbor relationships are used, in information retrieval/data mining applications based on network topology (Shin, Hill, and Raetsch 2006), and state space transitions in a Markov decision process. For a general weight matrix W representing the edge weights on a directed graph, its eigenvalues and eigenvectors are not guaranteed to be real. Many current approaches to this problem convert the directed graphs to undirected graphs. A simple solution is setting W to be W + W T or W W T . It is more desirable to find an approach that handles directed graphs without the need for symmetrization. To address the need for multiscale analysis and directional neighborhood relationships, we explore multiscale extensions of Fourier analysis based approaches using wavelet analysis (Mallat 1998). Classical wavelets in Euclidean spaces allow a very efficient multiscale analysis much like a highly flexible tunable microscope probing the properties of a function at different locations and scales. Diffusion wavelets (DWT) (Coifman and Maggioni 2006) extends the strengths of classical wavelets to data that lie on graphs and manifolds. The term diffusion wavelets is used because it is associated with a diffusion process that defines the different scales, allows a multiscale analysis of functions on manifolds and graphs. We focus on multiscale extensions of Laplacian eigenmaps (Belkin and Niyogi 2003) and LPP (He and Niyogi 2003). Laplacian eigenmaps constructs embeddings of data using the low-order eigenvectors of the graph Laplacian as a new coordinate basis (Chung 1997), which extends Fourier analysis to graphs and manifolds. Locality Preserving Projections (LPP) is a linear approximation of Laplacian eigenmaps. Our paper makes the following specific contributions: (1) We investigate the relationships between DWT and (multiscale) Laplacian eigenmaps and LPP. To extend LPP to a multiscale variant requires solving a generalized eigenvalue







problem using diffusion wavelets. This extension requires processing two matrices, and was not addressed in previous work on diffusion wavelets. (2) We also show how to apply the method to directed (non-symmetric) graphs. Previous applications of diffusion wavelets did not focus on nonsymmetric weight matrices. Similar to Laplacian eigenmaps and LPP, our approach represents the set of instances by vertices of a graph, where an edge is used to connect instances x and y using a distance measure, such as if y is among the k -nearest neighbors of x. The weight of the edge is specified typically using either a symmetric measure, such as the heat kernel or a non-symmetric measure, such as a directional relationship induced by non-symmetric actions in a Markov decision process. Such pairwise similarities generate a transition probability matrix for a random walk P = D-1 W , where W is the weight matrix, and D is a diagonal "valency" matrix of the row-sums of W . In contrast to almost all previous graph-based eigenvector methods, we do not require W to be symmetric. In Laplacian eigenmaps and LPP, dimensionality reduction is achieved using eigenvectors of the graph Laplacian. In the new approach, we use diffusion scaling functions, which are defined at multiple scales. In the special case of symmetric matrices, these span the same space as selected spectral bands of eigenvectors. The remainder of this paper is organized as follows. The next section discusses the diffusion wavelets model. Then, we explain the main multiscale manifold learning algorithms and the rationale underlying our approaches. We finish with a presentation of the experimental results and conclusions.







{j , Tj } = DW T (T, 0 , QR, J, ) //INPUT: //T : Diffusion operator. 0 : Initial (unit vector) basis matrix. QR: A modified QR decomposition. //J : Max step number. This is optional, since the algorithm automatically terminates. //: Desired precision, which can be set to a small number or simply machine precision. //OUTPUT : j : Diffusion scaling functions at scale j . Tj = j  [T 2 ]j . j F or j = 0 to J - 1{ j j +1 j  ([j +1 ]j , [T 2 ]j )  QR([T 2 ]j ,  ); j [T 2 }



j +1 j +1 j +1 = ([T 2 ]j [j +1 ]j )2 ; ] j +1















j















Figure 1: Diffusion Wavelets construct multiscale representations



denotes matrix T whose colat different scales. The notation [T ]b a umn space is represented using basis b at scale b, and row space is represented using basis a at scale a. The notation [b ]a denotes basis b represented on the basis a . At an arbitrary scale j , we  have pj basis functions, and length of each function is lj . [T ]b is a a pb x la matrix, [b ]a is an la x pb matrix. Typically the initial basis for the algorithm 0 is assumed to be the delta functions (represented by an identity matrix), but this is not strictly necessary.











j 0







T







2







j







QR Decomposition [j+1] [T ] 



2



j







Extended Bases



[j+1]



0







j+1 j







j







.







Diffusion Wavelets Model



The procedure for performing multiscale decompositions using diffusion wavelets and the relevant notation are explained in Figure 1. The main procedure can be explained as follows: an input matrix T is orthogonalized using an approximate QR decomposition in the first step. T 's QR decomposition is written as T = QR, where Q is an orthogonal matrix and R is an upper triangular matrix. The orthogonal columns of Q are the scaling functions. They span the column space of matrix T . The upper triangular matrix R is the representation of T on the basis Q. In the second step, we compute T 2 . Note this is not done simply by multiplying T by itself. Rather, T 2 is represented on the new basis Q: T 2 = (RQ)2 . This result is based on matrix invariant subspace theory (Stewart and Sun 1990). Since Q may have fewer columns than T , T 2 may be a smaller square matrix. The above process is repeated at the next level, generating j compressed dyadic powers T 2 , until the maximum level is reached or its effective size is a 1 x 1 matrix. Small powers of T correspond to short-term behavior in the diffusion process and large powers correspond to long-term behavior. Scaling functions are naturally multiscale basis functions because they account for increasing powers of T (in particular, j the dyadic powers 2j ). At scale j , the representation of T 2 is compressed based on the amount of remaining information and the precision we want to keep. Figure 2 illustrates this procedure.



1







.







.



J-1







Figure 2: Multiscale diffusion analysis.







We use the "Olivetti Faces" data to illustrate the difference between eigenvector basis and diffusion wavelets basis (scaling functions). The dataset contains 200 face images represented over pixels. The well-known eigenface approach (Turk and Pentland 1991) first computes the pixelpixel covariance matrix, and then computes the corresponding eigenvectors. Each eigenvector is an "eigenface". Using this approach, each image can be written as a linear combination of eigenfaces. In our approach, we start with the same covariance matrix, but we use diffusion wavelets instead of eigenvectors. Each column of [j ]0 is used as a "diffusion face". Diffusion wavelets model identifies a 4 level hierar-







Figure 3: All 9 Diffusion Wavelets Basis Functions at Level 3.







1. Construct diffusion matrix T characterizing the given data set: * T = I - L is an n x n diffusion matrix. 2. Construct multiscale basis functions using diffusion wavelets: * {j , Tj } = DW T (T, I, QR, J, ). * The resulting [j ]0 is an n x pj matrix (Equation (1)). 3. Compute lower dimensional embedding (at level j ): * The embedding xi  yi = row i of [j ]0 .







Figure 4: Selected Diffusion Wavelets Basis Functions at Level 1.







1. Construct relationship matrix T characterizing the given data set: * T = (F + X LX T (F T )+ )+ is an r x r matrix.. 2. Apply diffusion wavelets to explore the intrinsic structure of the data: * {j , Tj } = DW T (T, I, QR, J, ). * The resulting [j ]0 is an r x pj matrix (Equation (1)). 3. Compute lower dimensional embedding (at level j ): * The embedding xi  yi = ((F T )+ [j ]0 ) xi .



T







Figure 5: Eigenfaces.



199 Bases 52 Bases 9 Bases 2 Bases







Figure 7: Top: Multiscale Laplacian Eigenmaps; Bottom: Multiscale LPP.







Figure 6: Image Reconstruction at Different Scales chy of diffusionfaces, and dimensionality of each level is: 199, 52, 9, 2. We plot all 9 diffusionfaces at level 3 in Figure 3, and the top 24 diffusionfaces at level 1 in Figure 4. We also plot the top 24 eigenfaces in Figure 5. It is clear that these two types of basis are quite different: eigenvectors are global, and almost all such bases model the whole face. Diffusion faces are defined at multiple scales, where the finer scale (e.g. Figure 4) characterizes the details about each image, while the coarser scales (e.g. Figure 3) skip some of the details and only keep the lower frequency information. Scaling functions (especially those at low levels) are usually sparse, and most of them focus on just one particular feature on the face, like eyes and noses. Given an image written as a summation of diffusionfaces, we can estimate what the image looks like based on the coefficients (contributions) of each type of eyes, noses, etc. Figure 6 shows the face reconstruction results using diffusion faces at different scales.







Multiscale Manifold Learning



In this section, we discuss how to extend Laplacian eigenmaps and LPP to multiple scales using diffusion wavelets. Notation: X = [x1 , * * * , xn ] be an p x n matrix representing n instances defined in a p dimensional space. W







is an n x n weight matrix, where Wi,j represents the sim2 ilarity of xi and xj (Wi,j can be defined by e- xi -xj ). D is a diagonal valency matrix, where Di,i = j Wi,j . -0.5 -0.5 W = D WD . L = I - W , where L is the normalized Laplacian matrix and I is an identity matrix. XX T = F F T , where F is a p x r matrix of rank r. One way to compute F from X is singular value decomposition. (*)+ represents the Moore-Penrose pseudo inverse. (1) Laplacian eigenmaps minimizes the cost function 2 i,j (yi - yj ) Wi,j , which encourages the neighbors in the original space to be neighbors in the new space. The c dimensional embedding is provided by eigenvectors of Lx = x corresponding to the c smallest non-zero eigenvalues. The cost function for multiscale Laplacian eigenmaps 1 n is defined as follows: given X , compute Yk = [yk , * * * , yk ] i at level k (Yk is a pk x n matrix) to minimize i,j (yk - j 2 yk ) Wi,j . Here k = 1, * * * , J represents each level of the underlying manifold hierarchy. (2) LPP is a linear approximation of Laplacian eigenT maps. LPP minimizes the cost function i,j (f xi - f T xj )2 Wi,j , where the mapping function f constructs a c dimensional embedding, and is defined by the eigenvectors of X LX T x = XX T x corresponding to the c smallest non-zero eigenvalues. Similar to multiscale Laplacian eigenmaps, multiscale LPP learns linear mapping functions defined at multiple scales to achieve multilevel decompositions.







The Multiscale Algorithms



Multiscale Laplacian eigenmaps and multiscale LPP algorithms are shown in Figure 7, where [j ]0 is used to compute a lower dimensional embedding. As shown in Figure 1, the scaling functions [j +1 ]j are the orthonormal bases to span the column space of T at different levels. They define a set of new coordinate systems revealing the information in the original system at different scales. The scaling functions also provide a mapping between the data at longer spatial/temporal scales and smaller scales. Using the scaling functions, the basis functions at level j can be represented in terms of the basis functions at the next lower level. In this manner, the extended basis functions can be expressed in terms of the basis functions at the finest scale using:



[j ]0 = [j ]j -1 [j -1 ]0 = [j ]j -1 * * * [1 ]0 [0 ]0 , (1)







Since the columns of both V1:pj and [j ]0 are orthonormal, it is easy to verify that



T T V1: pj V1:pj = [j ]0 [j ]0 = I,







where I is a pj x pj identity matrix. So



T T T V1:pj = V1:pj V1: pj V1:pj = [j ]0 [j ]0 V1:pj = [j ]0 ([j ]0 V1:pj ).







Next, we show Q = [j ]T 0 V1:pj is a rotation matrix.



T T T T QT Q = V1: pj [j ]0 [j ]0 V1:pj = V1:pj V1:pj V1:pj V1:pj = I.



T T T QQT = [j ]T 0 V1:pj V1:pj [j ]0 = [j ]0 [j ]0 [j ]0 [j ]0 = I.







det(QT Q) = (det(Q))2 = 1 = det(Q) = 1







So Q is a rotation matrix. The embeddings constructed by LPP reduces to solving the generalized eigenvalue decomposition X LX T x = XX T x, where we have two input matrices X LX T and XX T to process. However, using the DW T procedure requires converting the generalized eigenvalue decomposition to a regular eigenvalue decomposition problem (with one input matrix). Theorem 2: Solution to generalized eigenvalue decomposition X LX T v = XX T v is given by ((F T )+ x, ), where x and  are eigenvector and eigenvalue of F + X LX T (F T )+ x = x. Proof: We know XX T = F F T , where F is a p x r matrix of rank r. Case 1: When XX T is positive definite: It follows immediately that r = p. This implies that F is an p x p full rank matrix: F -1 = F + .



X LX T v = XX T v = X LX T v = F F T v = X LX T v = F F T (F T )-1 F T v = X LX T v = F (F T v ) = X LX T (F T )-1 (F T v ) = F (F T v ) = F -1 X LX T (F T )-1 (F T v ) = (F T v )







where each element on the right hand side of the equation is created by the procedure shown in Figure 1. In our approach, [j ]0 is used to compute lower dimensional embeddings at multiple scales. Given [j ]0 , any vector/function on the compressed large scale space can be extended naturally to the finest scale space or vice versa. The connection between vector v at the finest scale space and its compressed representation at scale j is computed using the equation [v ]0 = ([j ]0 )[v ]j . The elements in [j ]0 are usually much coarser and smoother than the initial elements in [0 ]0 , which is why they can be represented in a compressed form.







Theoretical Analysis



It is well-known that regular Laplacian eigenmaps and LPP both return the optimal dimensionality reduction results with respect to the cost functions described at the beginning of this section (Belkin and Niyogi 2003; He and Niyogi 2003). If the input matrix is symmetric, there is an interesting connection between our algorithms and regular approaches. Theorem 1 and 3 below prove that the dimensionality reduction results produced by the proposed approaches at level k and the results from Laplacian eigenmaps and LPP (with top pk eigenvectors) are the same up to a rotation and a precision. So the proposed approaches are also optimal with respect to the same cost functions up to a precision. Theorems 2 proves some intermediate results, which are subsequently used in Theorem 3. One significant advantage of our approach is that it also directly generalizes to non-symmetric input matrices. Theorem 1: Laplacian eigenmaps (with eigenvectors corresponding to pj smallest non-zero eigenvalues) and Multiscale Laplacian eigenmaps (at level j ) return the same pj dimensional embedding up to a rotation Q and a precision. Proof: In Laplacian eigenmaps, we use row i of V1:pj to represent pj dimensional embedding of xi , where V1:pj is an n x pj matrix representing the pj smallest eigenvectors of L. When T = I - L, the largest eigenvectors of T are the smallest eigenvectors of L. Let [j ]0 represent the scaling functions of T at level j , then V1:pj and [j ]0 span the same space up to a precision (Coifman and Maggioni 2006), i.e.



T T V1:pj V1: pj = [j ]0 [j ]0 .







So solution to X LX T v = XX T v is given by ((F T )+ x, ), where x and  are eigenvector and eigenvalue of



F + X LX T (F T )+ x = x.







Case 2: When XX T is positive semi-definite, but not positive definite: In this case, r < p and F is a p x r matrix of rank r. Since X is a p x n matrix, F is a p x r matrix, there exits a matrix G such that X = F G. This implies G = F +X .



X LX T v = XX T v = F GLGT F T v = F F T v = F GLGT (F T v ) = F (F T v ) = (F + F )GLGT (F T v ) = (F T v ) = GLGT (F T v ) = (F T v ) = F + X LX T (F T )+ (F T v ) = (F T v )







So one solution to X LX T v = XX T v is ((F T )+ x, ), where x and  are eigenvector and eigenvalue of



F + X LX T (F T )+ x = x.







Note that the eigenvector solution to Case 2 is not unique.







Theorem 3: For any instance u, its embedding under LPP (using the top pj eigenvectors) is the same as its embedding under multiscale LPP (at level j ) up to a rotation and a precision. Proof: It is well known that the normalized graph Laplacian L is positive semi-definite (PSD), so F + X LX T (F T )+ is also PSD, and all its eigenvalues are  0. This implies that eigenvectors corresponding to F + X LX T (F T )+ 's smallest non-zero eigenvalues are the same as eigenvectors corresponding to (F + X LX T (F T )+ )+ 's largest eigenvalues. Let T = (F + X LX T (F T )+ )+ , [j ]0 (a p x pj matrix) represent the diffusion scaling functions of T at level j . From Theorem 1, it follows that V1:pj = [j ]0 Q where V1:pj is a p x pj matrix, represents the pj smallest eigenvectors of F + X LX T (F T )+ and Q is a rotation. Given an instance u (p x 1 vector): its embedding result with LPP is



T + ((F T )+ V1:pj )T u = V1: pj F u;







its embedding result with multiscale LPP is



+ + T ((F T )+ [j ]0 )T u = [j ]T 0 F u = QV1:pj F u.







eigenmaps with the original weight matrix W reconstructs the original structure, while both approaches based on symmetrized W fail. The reason that symmetrization does not work is that for the points (red) on the rim of the sphere, their 20 neighbors are mostly red points. For the points (yellow) in the middle, some of their 20 neighbors are red, since the yellow points are sparse. Symmetrizing the relationship matrix will add links from the red to the yellow. This is equal to reinforcing the relationship between the red and yellow, which further forces the red to be close to the yellow in the low dimensional space. The above process weakens the relationship between the red points. So in the 3D embedding, we see some red points are far away from each other, while the red-yellow relationship is well preserved. Directed Laplacian also fails to generate good embeddings in this task. Finally, all three linear dimensionality reduction approaches (LPP, multiscale LPP with W and W ) can reconstruct the original structure. A possible reason for this is that the strong linear mapping constraint prevents overfitting from happening for this task.



1



2 2







So, the two embeddings are the same up to a rotation Q and a precision.







(T)



1.5







0.8







(T2) (T )



4







1







1







0.6







0







Experimental Results



To test the effectiveness of our multiscale manifold learning approaches, we compared "flat" and "deep" multiscale approaches using dimensionality reduction tasks on both synthetic and real-wold data sets. It is useful to emphasize that the intrinsic structure of the data set does not depend on the parameters. The structure only depends on the given data. The input parameters decide the way to explore the structure. The time complexity of the proposed approaches are similar to the corresponding eigenvector based approaches.







0.5







-1







0.4



0 1 0.5 0 -0.5 -1 -1 -0.5 0.5 0 1 -2 2







0.2







1 0 -1 1 0 -1 -2 -2







2







0 0







200







400







600







800







(A)



0.06 0.04 0.05 0.02 0 -0.02 -0.05 -0.04 -0.06 0.1 0.05 0 -0.05 -0.1 -0.1 -0.05 0.05 0 0.1 -0.1 0.1 0.05 0 -0.05 0 0.1







(B)



0.1 0.05 0







(C)







-0.05







-0.1 0.1 0.1 0.05 0 -0.05 -0.1 -0.1 0.05 0 -0.05 -0.1 -0.1 -0.06 -0.08 -0.02 -0.04 0







(D)



1.5 0.06 0.04 0.02 1 0.5 0 -0.5 0







(E)



0 -0.01 -0.02 -0.03 -0.02 -0.04 -0.06 0.06 -0.04 -0.05 0.1 0.04 0.02 -0.05 0 -0.1 0 0.1 0.05 0.05 0 -0.05







(F)







Punctured Sphere Example



Consider the punctured sphere in Figure 8(A) based on 800 samples. We use the heat kernel to generate its weight matrix, and for each point, we compute the weights for 20 nearest neighbors (in each row). This results in a non-symmetric matrix W . To apply Laplacian eigenmaps and LPP, we symmetrize W : W = (W + W T )/2. Figure 8(B) shows the spectrums of W and its higher powers. The high powers have a spectrum that decays much more rapidly than the low powers. This spectral decay property is characteristic of "diffusion-like" matrices, particularly those generated by the k nearest neighbor similarity metric. The embedding results are in Figure 8(C)-(I). The results verify Theorem 1 and 3, showing multiscale approaches (using diffusion scaling functions at level j ) and eigenmap approaches (using top pj eigenvectors) result in the same embeddings up to a rotation and a precision. Furthermore, multiscale Laplacian eigenmaps can successfully identify the intrinsic structures of the data set. Dimensionality of the finest scales of multiscale Laplacian eigenmaps is 3, which is the intrinsic dimensionality of the given data. Also, among all four nonlinear dimensionality reduction approaches (Direct Laplacian (Chung 2005), Laplacian eigenmaps, Multiscale Laplacian eigenmaps with W and W ), only Multiscale Laplacian







-1 -1.5 0 -0.5 -1 -1.5 -2 -2 -1 1 0 2







0.1 0.05 0 -0.05 -0.1 -0.1







(G)







(H)







(I)







Figure 8: Punctured Sphere Example: (A) Puncture Sphere; (B)



Spectrum of W ; (C) Directed Laplacian with W ; (D) Laplacian eigenmaps with W ; (E) Multiscale Laplacian eigenmaps with W (finest scale); (F) Multiscale Laplacian eigenmaps with W (finest scale); (G) LPP with W ; (H) Multiscale LPP with W ; (I) Multiscale LPP with W .







Citation Graph Mining



The citation data set in KDD Cup 2003 consists of scientific papers (about 29, 000 documents) from arXiv.org. These papers are from high-energy physics. They cover the period from 1992 through 2003. We sampled 3,369 documents from the data set and created a citation graph, i.e. a set of pairs of documents, showing that one paper references another. To evaluate the methods, we need to assign each document a class type. To compute this, we first represent each paper using a TF-IDF vector based on the text of its abstract and the title, then we use the dot product to







0.8







NSF Research Awards Abstracts Data



We also ran a test on a selected set of the NSF research awards abstracts (Frank and Asuncion 2010), which includes 5,000 abstracts describing NSF awards for basic research. The data set is represented by bag-of-words and has already been cleaned. Each abstract has a corresponding label: "division" (37 different values). Using Multiscale Laplacian eigenmaps, a 9 level manifold hierarchy was automatically constructed. Dimensionality discovered at each level was: 5000, 3069, 3052, 2524, 570, 54, 20, 13, 9. We applied the same quantitative comparison approach as that used in the previous section to compare Multiscale Laplacian eigenmaps (level 5) and regular Laplacian eigenmaps (with varying numbers of eigenvectors: 100, 1200, 1600, 2000). The results are summarized in Figure 10. The 570 dimensional embedding returned the best results. From the figures, we can see that choosing an appropriate scale for embedding can help improve learning performance. Using too many or too few bases may result in a redundant feature space or loss of valuable information. Finding an appropriate value for dimensionality is quite difficult. In previous approaches, the users need to specify this value. Generally speaking, even though a given problem may have tens of thousands of instances, the number of levels identified by the new approach is a much smaller number (often < 10). Also, some levels are defined by either too many or too few features. This eliminates from consideration additional levels, usually leaving a handful of levels as possible candidates. In this example, we chose the space defined by 570 features, since the levels below and above this have too few or too many features, respectively. Manifold hierarchy is task independent. For different tasks, users can select the most appropriate level by testing his/her data at different levels. For simplicity, the paper focuses on selecting scaling functions at a single level, but the methods can be extended to use multiple levels together.







0.7







0.6







Correctness







0.5







0.4







0.3







0.2







Multiscale Laplacian Projections (directed) Laplacian Eigenmaps







0.1 1







2







3







4







5 K







6







7







8







9







10







Figure 9: Comparison of citation graph embeddings.







0.75







0.7







Probability of Matching







0.65







0.6







0.55







0.5







0.45







0.4







0.35







d=570 (multiscale) Laplacian eigenmaps, d=100 Laplacian eigenmaps, d=1200 Laplacian eigenmaps, d=1600 Laplacian eigenmaps, d=2000



2 3 4 5 6 7 8 9 10







1







K







Figure 10: Comparison of NSF abstracts embeddings (using



`division' as label).







compute the similarity between any two papers. Hierarchical clustering is used to assign each document with a class. As a result, we get 7 classes. We apply both Multiscale Laplacian eigenmaps and regular Laplacian eigenmaps to the citation graph (without using document contents). Since the input is a graph, LPP and multiscale LPP can not be used. Multiscale approach results in a 8 level hierarchy. Dimensionality of each level is: 3369, 1442, 586, 251, 125, 105, 94, 7. From the result, we can see that multiscale approach successfully identifies the real intrinsic dimensionality at the highest level: 7 classes. Obviously, the citation graph is non-symmetric, and to apply Laplacian eigenmaps, we symmetrize the graph as before. A leave-one-out test is used to compare the low dimensional embeddings. We first map the data to a d dimensional space (we run 10 tests: d = 10, 20, 30 * * * 100) using both multiscale approach (using basis functions at level 6) and regular Laplacian eigenmaps. For each document in the new space, we check whether at least one document from the same class is among its K nearest neighbors. The multiscale approach using a non-symmetric graph performs much better than regular Laplacian eigenmaps with a symmetric graph in all 10 tests. We plot the average performance of these tests in Figure 9. Laplacian eigenmaps is less effective because the citation relationship is directed, and a paper that is cited by many other papers should be treated as completely different from a paper that cites many others but is not cited by others.







Conclusions



This paper presents manifold learning techniques that yield a multiscale decomposition of high-dimensional data. The proposed approaches are based on the diffusion wavelets framework, and are data driven. In contrast to "flat" eigenvector based approaches, which can only deal with symmetric relationships, our approach is able to analyze non-symmetric relationship matrices without ad-hoc symmetrization. The superior performance of the multiscale techniques and some of their advantages are illustrated using both synthetic and real-world data sets.







Acknowledgments



This research is supported in part by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-101-0383, and the National Science Foundation under Grant Nos. NSF CCF-1025120, IIS-0534999, IIS-0803288, and IIS-1216467. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the AFOSR or the NSF.







References



Belkin, M., and Niyogi, P. 2003. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation 15:1373-1396. Bengio, Y. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning 2(1):1-127. Chung, F. 1997. Spectral graph theory. Regional Conference Series in Mathematics 92. Chung, F. 2005. Laplacians and the Cheeger inequality for directed graphs. Annals of Combinatorics 9. Coifman, R., and Maggioni, M. 2006. Diffusion wavelets. Applied and Computational Harmonic Analysis 21:53-94. Frank, A., and Asuncion, A. 2010. UCI machine learning repository. [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. He, X., and Niyogi, P. 2003. Locality preserving projections. In Proceedings of the Advances in Neural Information Processing Systems (NIPS). Hinton, G. E., and Salakhutdinov, R. R. 2006. Reducing the dimensionality of data with neural networks. Science 313:504-507. Hinton, G. E.; Osindero, S.; and Teh, Y. W. 2006. A fast learning algorithm for deep belief nets. Neural Computation 18:1527-1554. Lee, H.; Battle, A.; Raina, R.; and Ng, A. 2007. Efficient sparse coding algorithms. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), 801-8084. Mallat, S. 1998. A wavelet tour in signal processing. Academic Press. Roweis, S., and Saul, L. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science 290:2323- 2326. Shin, H.; Hill, N.; and Raetsch, G. 2006. Graph-based semisupervised learning with sharper edges. In Proceedings of the European Conference on Machine Learning, 401-412. Stewart, G. W., and Sun, J. 1990. Matrix perturbation theory. Academic Press. Turk, M., and Pentland, A. 1991. Eigenfaces for recognition. Journal of Cognitive Neuroscience 3:71-86.







Online Transfer Learning for Differential Diagnosis Determination



Jie Xu , Daby Sow , Deepak Turaga , Mihaela van der Schaar



 







University of California at Los Angeles, Los Angeles, California 90095 IBM T. J. Watson Research Center, Yorktown Heights, New York 10598







Abstract



In this paper we present a novel online transfer learning approach to determine the set of tests to perform, and the sequence in which they need to be performed, in order to develop an accurate diagnosis while minimizing the cost of performing the tests. Our learning approach can be incorporated as part of a clinical decision support system (CDSS) with which clinicians can interact. The approach builds on a contextual bandit framework and uses online transfer learning to overcome limitations with the availability of rich training data sets that capture different conditions, context, test results as well as outcomes. We provide confidence bounds for our recommended policies, which is essential in order to build the trust of clinicians. We evaluate the algorithm against different transfer learning approaches on real-world patient alarm datasets collected from Neurological Intensive Care Units (with reduced costs by 20%).







Introduction



Recent advances in sensing and measurement technologies are enabling us to monitor complex human, engineered, physical, biological and chemical systems and processes in many sophisticated ways. This enables improved ability to understand the state of health of these systems, diagnose problems, and use this to design interventions to maximize health at varying timescales. However, while several such measurements can be made (e.g. by performing different tests on a patient), the decision on which test to perform and when to perform it remains a very challenging problem. Challenges stem from multiple factors: i) There are complex relationships between different attributes that are being measured. ii) Tests have varying degrees of costs associated with them (e.g. some tests are very expensive). iii) Tests are significantly impacted by context, i.e. the best set of tests, measurements and interventions may be different depending on the context in which it takes place. iv) The determination of tests is often challenged by the limited access to relevant data. For instance, existing patients datasets often have distributions that do not necessarily capture the information needed for the accurate diagnosis in a



c 2014, Association for the Advancement of Artificial Copyright  Intelligence (www.aaai.org). All rights reserved.







novel problem domain. Thus, the resulting diagnosis policies may perform poorly. This prompts the need for a system that can effectively perform context-specific diagnosis that maximizes diagnosis accuracy and minimizes test costs even when highly relevant data pertaining to the diagnosis decision is missing. In this paper, we present a novel decision support system that addresses these challenges by transferring knowledge from multiple related problem domains and incrementally learning the best policies (i.e. sequences of test) to adopt depending on the context of the diagnosis problem. These contexts can be exogenous facts or meta-data about the problem. In the medical setting, they can be patient's age, gender and weight. Note that the contexts are different than the endogenous testing results. The use of multiple related problem domains enables transferring knowledge from the most relevant domains for different diagnosis contexts; it also creates a way to measure the semantic similarity between contexts: contexts are similar if their most related existing domains are the same. The learned semantic similarity is then used to develop context-specific solutions in the novel problem domain. The proposed approach is able to provide diagnosis confidence bounds which are important to ensure the trust of domain professionals.







Related Work



Support systems for decision making have been extensively studied. A first strand of related research focuses on costsensitive learning (Turney 2000)(Greiner, Grove, and Roth 2002)(Zubek, Dietterich, and others 2004). A disadvantage of these approaches is that they rely on training datasets to learn the appropriate model. Our focus in this paper is on how to overcome the lack of initial training data by using transfer of knowledge from relevant datasets. The majority of the transfer learning literature assumes a single source from which knowledge can be transferred (Marx et al. 2005). Transfer learning from multiple sources is much more challenging; most works aiming to address this problem focus on classification problems (Duan et al. 2009)(Yao and Doretto 2010). In our considered setting, the target data arrives sequentially and the features are not given but need to be discovered (by performing various tests). Imitationtype transfer learning techniques are often adopted where source policies are applied to the target task initially, while the target task solution is learned gradually (Fern andez and







Entity (e.g. patient)







Test Results t x te n o C n tio a m r fo n i Recommend Diagnostic Diagnostic Action Policy Selection Source Policy 1 Source Policy K



Domain Expert (e.g. doctor)







Which further test to execute







?



Diagnose Case closed (Subsequent actions follow) Update







...







Target Policy







Figure 1: Computer-aided diagnosis system Veloso 2006). However, such works only consider the availability of a single source, while our work focuses on multiple sources. Our solution builds on the contextual bandit framework (Slivkins 2009). While conventional works on multiarmed bandits focus on learning the best policy (or policies) among an fixed set, our algorithm uses the learned semantic similarity between contexts to produce new context-specific target policies (which may be distinct from existing policies) using the data accumulated so far for the target domain.







At any point in time, the execution of tests on entities provides additional information on the entities. Such state transitions are probabilistic and specific to the domain. Let p(s |s, q ) denote the transition probability from state s to s when test q is executed. Since taking an action d  D always leads to a diagnosis and closes the current case, we have p(|s, d) = 1, d  D. Let c(q |s) denote the expected cost of performing test q on entities in state s. Let c(d|s) denote the expected cost of making a diagnose d on entities in state s. We unify these two types of costs in a cost function c : AxS  R as a mapping from the action space and the state space to a real value. In sum, we call the set of transition probabilities p and the diagnosis cost function c the problem parameters. These parameters are Markovian; they depend only on the last state. This is a reasonable approximation since a state represents all the knowledge revealed about the entity so far. The optimal diagnostic policy that minimizes the expected diagnostic cost in each state is defined using the Bellman equation:J (a|s) = c(a|s) + p(s |s, a)V (s ) where V (s ) = mina J (s |a ). Thus  opt = {aopt (s)}sS such that s, aopt (s) = arg mina J (a|s). Since the entity comes with the initial state sinit , the expected diagnostic reward is V (sinit ). With abuse of notation, we let V ( ) = V (sinit | ) denote the diagnostic cost by using  . If the problem parameters were known, then the optimal diagnostic policy making problem can be solved by backward induction using the estimated problem parameters from an existing dataset. Since state space size is exponential in the test set size, the complexity grows as the number of tests increases. Reducing the solution complexity of this problem is not the main focus of the present paper; we refer readers interested in this topic to existing work that provides efficient heuristics algorithms such as (Zubek, Dietterich, and others 2004).



s







Computer-aided Diagnosis System



We use    to denote the initial exogenous context information about the entity being analyzed, such as the patient's basic symptoms and personal medical profile (e.g. gender, age, weight, medical history etc.). Let Q = {1, 2, ..., N } denote the set of possible tests, N < . We assume that each test q  Q has a finite set of possible results, denoted by Oq . We also define an "unknown" test result to be assigned to tests that have not been performed. At any point in time, an entity is assigned with a state s that represents the known test results that have been performed. This state does not reflect the patient's medical condition but rather the knowledge about the patient with respect to the medical tests. This state evolves as more medical tests are executed. Let S denote the state space. The initial state of an entity is sinit q = unknown, q  Q. Depending on the current entity state s, the computer-aided diagnostic system either recommends new tests to be performed to extract more knowledge or recommends a diagnosis decision if it has enough information about the entity. Let the action space be A = {Q, D} where D represents the diagnosis space; they are kept fixed. We assume that if the expert follows a  D, then the diagnosis for the current entity case is closed and subsequent intervention actions follow. Let  be a special terminal state which denotes that the case is k k , ..., qn closed. For an entity k , let {q1 k } be the sequence of k tests that are executed and d be the final diagnosis decision. The diagnosis cost ck for this entity is defined as ck =  k k ck (qi ) + ck (dk ) where ck (qi ), i = 1, ..., nk



i{1,...,nk }







Transfer Learning in Diagnosis



One of the key challenges for many diagnosis systems is that access to relevant data is limited. In a medical setting, existing patient datasets often have distributions that do not necessarily capture the information needed for the accurate diagnosis in a novel problem domain. The resulting diagnosis policies constructed may perform poorly. To address this issue, we propose to efficiently reuse and transfer knowledge from other older domains to minimize as much as possible the diagnosis cost in the new domain. In what follows, we call the diagnosis problem in the new domain the target problem and the diagnosis problem in the old domain the source problem.







Algorithm



We consider an online setting where data on entities in the target domain are received in sequence, indexed by {1, 2, ..., k, ...}. Due to the lack of a training dataset in the new domain, it is initially impossible to construct a good policy for the target problem. Instead, we have a set of K source policies  constructed for K related source problems (e.g. similar diseases or datasets of patients with a similar demography). However, the exact relationship and the effectiveness of these source policies on the target problem







are the costs incurred by executing the tests, c (d ) is the costs due to incorrect diagnosis and   [0, 1] is a tradeoff factor. A diagnostic policy is defined as a set of actions that are recommended to the domain expert in the various states. Specifically, a policy is denoted by  = {a(s)}sS . Hence, given a diagnostic policy, after observing the entity state, the diagnostic system can recommend an action to the domain expert. Our goal is to develop diagnosis policies that minimize the diagnosis cost.







k







k







are unknown a priori. Our algorithm begins by exploring the source policies for entities in the target domain. After accumulating sufficient data on entities for the target problem, it builds the target policy using the information extracted from applying the source policies. The algorithm is provided next in Algorithm 1. The parameter k  [0, 1] is used to control when to adopt source policies and when to use the newly built target policies; it is decreasing in k and lim k = 0.



k







Algorithm 2 Policy Selection and Adaptive Clustering 1: Initialize H = , r  ( ) = 0, M ( ) = 0,   . 2: for each entity kt do 3: Determine active cluster C  Ht such that t  C 4: Case 1:    such that MC ( ) <  (t) 5: Randomly select among such policies  t =  6: Case 2:   , MC ( )   (t) 7: Select  t = arg min r C ( ).



8: Set MC ( t )  MC ( t ) + 1 9: (The diagnosis reward rt is observed.) 10: Update r C ( t )  11: Update   MC ( ) using all past cases.  12: if  MC ( )   (l) then 13: Uniformly partition C into 2W level-(l + 1) 14: clusters. 15: Update the set of active clusters Ht . 16: Update the counters and cost estimates for all 17: new clusters using the entity cases received 18: so far. 19: end if 20: end for



 







Algorithm 1 Transfer Learning with Multiple Sources 1: for each entity k do 2: With probability k , select a source policy to apply 3: With probability 1 - k , apply the target policy 4: After the current case is closed 5: Build the target policy using received data 6: end for In Algorithm 1, there are two major questions that remain to be addressed: which source policy to apply (line 3) and how to build the target policy (line 7). We discuss them next. Let (k1 , k2 , ..., kt , ...) be the subsequence of received entity cases where a source policy is adopted according to Algorithm 1. Without loss of generality, we normalize the entity context space to be   [0, 1]W where W is the context space dimension. We introduce some concepts of the algorithm as follows: 1) Entity cluster. An entity cluster is represented by the range of context information that is associated with entities in the cluster. In this paper, we will consider clusters with the form [iw 2-(l-1)w , (iw + 1)2-(l-1)w ] where iw  {0, 1, ..., 2(l-1)w - 1} for each context dimension w = 1, ..., W for some positive integer l. Such a cluster is called a level-l cluster. At each time kt when source policies are applied, the algorithm keeps a set of mutually exclusive clusters that cover the entire context space. We call these clusters the active clusters, and denote this set by Ht . Clearly, we have C Ht = , t. 2) Counters. For each active cluster C , the algorithm maintains several counters: for each source policy   , MC ( ) records the number of entity cases so far in which  is applied. 3) Diagnosis cost estimates. For each active cluster C , the algorithm also maintains the sample mean diagnosis cost estimate r C ( ) for each source policy   , using the observed diagnosis costs of cases that belong to C so far. The algorithm is described in Algorithm 2. When an entity case kt is received, the algorithm first checks which active cluster C  Ht it belongs to. Then it investigates counter MC ( ) for all    to see if there exists any under-explored source policy  such that MC ( )   (t, l) where  (t, l) is a time- and level-dependent control function. If there exists such an under-explored policy  , then the algorithm selects this policy for the current entity case. This is called an exploration step. If there does not exist any under-explored policy, then the algorithm selects the policy with the lowest cost estimate arg min r C ( ). This is called



 







is a level-dependent control function, the current cluster C is partitioned in to 2W level-(l + 1) clusters. From the next entity case on, C is deactivated and the new level-(l + 1) clusters are activated. We will show how to select the control functions  (t, l) and  (l) in the next section. Entity clusters for which the estimated best source policies are the same are considered to be similar and hence, they are grouped together to form a dataset from which the problem parameters can be estimated. Using these K set of parameters, we can produce K context-specific target policies.







Confidence Bound



We make the following widely adopted technical assumption below; however, this is not needed for running the algorithm. Assumption. (Lipschitz) For each   , there exists L > 0,  > 0 such that for all ,   , we have |V ( ) - V (  )|  L,   . The above assumption states that if the entity context information is similar, then the expected diagnosis cost by selecting the same diagnostic policy is also similar. We can derive a confidence level of the learned effectiveness of diagnosis policies as follows: Proposition. For any active level-l context cluster C , at any time when policy  has been adopted for MC times on entities which belong to C , then for any individual context   C , the following confidence relation between the estimated diagnosis reward reward holds  and the true diagnosis 2 P (|r C - V | > L( W /2-l ) + ) < e-2 MC .







Experiments



Real-world patient dataset



We test our proposed algorithm using an alarm data set obtained from the Columbia Medical Center neurological







an exploitation step. After the diagnosis cost of the current entity case is observed, the cost  estimate of the selected policy is updated. Moreover, if MC ( )   (l), where  (l)



 







Intensive Care Unit (ICU). This dataset contains over a million alarm events produced by patient monitoring systems for 581 patients.







K = 2, c1 = 0.3 K = 2, c1 = 0.5 K = 3, c1 = 0.3







Methodology



We artificially treat the patient alarm dataset on each day as a separate dataset. K such datasets are picked as the source datasets and another one is picked as the target dataset. Moreover, the target data is made available to the system in sequence. We treat each alarm as a medical test. Hence, only when the test is performed, the corresponding alarm status is revealed. In the experiments, we focus on predicting whether the patient will have at least one of the two secondary complications: Pneumonia and Respiratory failure. The set of tests (alarms) that we consider includes Bradycardia, Tachycardia, High Blood Pressure, Low Blood Pressure, High Respiratory Rate. We assign different costs to differen types of prediction errors. Specifically, we normalized the cost of a miss detection to be 1 and the cost of a false alarm to be c1 . A uniform cost c2 is assigned for the execution of any test. In the experiments, we use a single patient context: the APACHE II ("Acute Physiology and Chronic Health Evaluation II") score that evaluates the severity of illness of our patients upon admission in the ICU.







c2 0.01 0.005 0.001 0.01 0.005 0.001 0.01 0.005 0.001







EM 0.299 0.268 0.245 0.357 0.332 0.302 0.274 0.258 0.250







AT 0.259 0.241 0.233 0.309 0.290 0.277 0.260 0.239 0.226







MSTAB 0.246 0.231 0.224 0.312 0.282 0.262 0.265 0.232 0.224







Proposed 0.212 0.193 0.187 0.292 0.267 0.260 0.218 0.191 0.192







Table 1: Diagnosis cost comparison



Context Diag. cost Age 0.222 GCS ApacheII 0.225 0.193 ApachePhys 0.205







Table 2: Impact of contexts







Conclusion



In this paper, we proposed an online transfer learning approach for differential diagnosis determination and showed how it can be incorporated as part of a clinical decision support system to improve the diagnosis performance. We envision that the proposed methodology can also be used in other complex diagnosis systems besides clinical diagnosis, such as cyber-security, biological and mechanical diagnosis.







Baseline approaches



1) Empirical Diagnosis (EM): All medical tests are executed and all alarms are revealed. It predicts the complication if any of the alarms are positive. 2) Average Transfer (AT): In this approach, we combine all source datasets to estimate the average problem parameters and construct a new diagnostic policy. This average source policy is applied to the target cases while the target policy is gradually learnt. 3) MultiSourceTriAdaBoost (MSTAB): This is a modified version of the state-of-the-art MultiSourceTriAdaBoost algorithm in (Yao and Doretto 2010) for transfer learning with multiple sources. We modified the weight update structure to incorporate the diagnosis cost instead of a plain diagnosis accuracy. Since the original algorithm is an offline algorithm that assumes a training set for the target problem, we also extended it to produce an online version using batch updates as more target cases are received.







References



Duan, L.; Tsang, I. W.; Xu, D.; and Chua, T.-S. 2009. Domain adaptation from multiple sources via auxiliary classifiers. In Proceedings of the 26th Annual International Conference on Machine Learning, 289-296. ACM. Fern andez, F., and Veloso, M. 2006. Probabilistic policy reuse in a reinforcement learning agent. In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, 720-727. ACM. Greiner, R.; Grove, A. J.; and Roth, D. 2002. Learning cost-sensitive active classifiers. Artificial Intelligence 139(2):137-174. Marx, Z.; Rosenstein, M. T.; Kaelbling, L. P.; and Dietterich, T. G. 2005. Transfer learning with an ensemble of background tasks. Inductive Transfer 10. Slivkins, A. 2009. Contextual bandits with similarity information. arXiv preprint arXiv:0907.3986. Turney, P. 2000. Types of cost in inductive concept learning. In Workshop on Cost-Sensitive Learning at 17th International Conference on Machine learning. Yao, Y., and Doretto, G. 2010. Boosting for transfer learning with multiple sources. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 1855- 1862. IEEE. Zubek, V. B.; Dietterich, T. G.; et al. 2004. Pruning improves heuristic search for cost-sensitive learning. Technical report, Corvallis, OR: Oregon State University, Dept. of Computer Science.







Results



We report the diagnosis cost results for three sets of experiments in Table 1 for various parameters. The diagnosis cost is computed by averaging the diagnosis cost of patient cases from the target patient case set. In all experiments, the proposed transfer learning algorithm significantly outperforms the baseline approaches by reducing the diagnosis cost up to 20% against the best baseline approach. We also investigate the impact of different choices of contexts on the diagnosis performance. Table 2 shows the achieved diagnosis cost by using different contexts for K = 2, c1 = 0.3 and c2 = 0.005. The experiment results indicate that the ApacheII score is the best context in our problem. Nevertheless, the diagnosis cost by using any context is lower than those achieved by the baseline approaches.







Role-aware Conformity Influence Modeling and Analysis in Social Networks



Jing Zhang , Jie Tang , Honglei Zhuang , Cane Wing-Ki Leung and Juanzi Li



Department of Computer Science and Technology, Tsinghua University Department of Computer Science, University of Illinois at Urbana-Champaign Huawei Noah's Ark Lab zhangjing12@mails.tsinghua.edu.cn, {jietang, lijuanzi}@tsinghua.edu.cn, hzhuang3@illinois.edu, cane.leung@huawei.com



 







Abstract



Conformity influence is the inclination of a person to be influenced by others. In this paper, we study how the conformity tendency of a person changes with her role, as defined by her structural properties in a social network. We first formalize conformity influence using a utility function based on the conformity theory from social psychology, and then propose a probabilistic graphical model, referred to as Role-Conformity Model (RCM), for modeling the role-aware conformity influence between users by incorporating the utility function. We apply the proposed RCM to several academic research networks, and discover that people with higher degree and lower clustering coefficient are more likely to conform to others. We also evaluate RCM through the task of word usage prediction in academic publications, and show significant improvements over baseline models.







1







Introduction







In social networks, conformity influence is the inclination of a person to be influenced by others by yielding to perceived group pressure and copying the behavior and beliefs of others. The earliest study on conformity influence dates back to the 1930's by social psychologists (Jenness 1932; Sherif 1935). Since then, precedent work extensively studied how conformity affects individuals' actions. The wellknown experiments in (Asch 1955) showed that over 75% of people tend to conform to others in varying degrees. Existing work (Bernheim 1994; Cialdini and Goldstein 2003; Kelman 1958; Aronson, Wilson, and Akert 2007) has repeatedly verified the significant effect of conformity influence in our social life. With the rapid proliferation of online social networks such as Facebook and Twitter, quantitatively estimating the conformity tendency of each individual becomes more and more critical for applications such as viral marketing, social influence maximization, etc. Yet, research on conformity influence in online social networks is just beginning. For instance, Li et al. (Li, Bhowmick, and Sun 2011; 2013) studied the interplay between the influence and conformity of an individual, while Tang et al. (Tang, Wu, and



Copyright c 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







Sun 2013) proposed a probabilistic factor graph model that takes the effect of conformity into account when predicting user behavior. Both focus on modeling the effect of conformity influence at the individual level. In this paper, we explore whether it is possible to summarize several "roles", i.e., prototypes, to concisely describe the correlation between the conformity tendency of an individual and her structural features in a network. Such rolebased modeling and analysis of conformity influence is beneficial for applications such as recommendations, where data sparsity or the cold-start problem cannot be overlooked. It is also the key difference from existing social contagion studies (Kempe, Kleinberg, and Tardos 2003; Gruhl et al. 2004), which ignore the effect of roles in information diffusion. There are several questions to address when modeling the conformity tendency of individuals based on their roles. First, how to formalize the conformity theory in social psychology? Second, how can roles be incorporated into such formalization to model user actions? Furthermore, how can role-based conformity be used to real applications? We summarize our answers to these questions and our main results in what follows. Results We formalize conformity influence in terms of a utility function based on the conformity theory (Bernheim 1994), and justify the proposed utility function by proving the existence of Nash equilibria when users conduct actions according to it. We further incorporate the utility function into an application-oriented probabilistic model, known as the Role-Conformity Model (RCM), to describe user behaviors. To the best of our knowledge, this is the first attempt to formally connect the conformity theory from social psychology to a computational model. We apply the proposed model on several academic networks to observe correlations between people's latent roles and their conformity tendency. Interestingly, people with higher degree and lower clustering coefficient are more likely to conform to others. The phenomenon may be explained as that when a person has more collaborators with the structure among them more diverse (i.e., the collaborations between the neighbors in the local network are infrequent), she may become more open-minded and tend to accept new ideas from others. However, when the social circle is restricted to a few collaborators, the person will limit her mind and tend not to accept other ideas.







We evaluate the proposed RCM through the task of word usage prediction, and results indicate that our model performs much better (+3.6-4.1% in terms of average MAP and +7.1-47.4% in terms of average AUC) than the basic TF-IDF and PLSA.







2







Formalizing Conformity







In this section, we formalize conformity influence based on the conformity theory from social psychology in terms of a utility function, and prove the existence of Nash equilibria if all users in a network behave according to it. Conformity utility function The conformity theory suggests that heterogeneous preferences do result in heterogeneous behaviors (Bernheim 1994). Everyone in a group expresses her own individuality. Yet, even individualists succumb somewhat to the desire for status (esteem or popularity) and shade their choices toward the social norm. This is because people seeking status care about how someone else feels about them through their actions. They are therefore willing to suppress their individuality and conform to the social norm, worrying that even small departures from the social norm may seriously impair their popularity. We formalize the conformity theory in terms of a utility function. We use a binary value to represent whether a user vi adopts an action (yi = 1) or not (yi = 0). Given the decision yi , we model the utility vi obtained from her decision from two aspects. One is the individual's intrinsic utility in the absence of all other neighbors, the other is the esteem acquired through conforming: f (yi ) = (1 - i ) d(yi , y i ) + i



j N (i)







when vk+1 joined a k -network that has already arrived at a Nash equilibrium. We assume that the preferred selection of vk+1 is 1, i.e., y k+1 = 1. The proof is the same when y k+1 = 0. Given an existing k -network, we denote the number of vk+1 's neighbors with y = 1 as N 1 , and the number of vk+1 's neighbors with y = 0 as N 0 . Thus, the utility of vk+1 is calculated as: (1 - ) + N 1 if yk+1 = 1 f (vk+1 ) = N 0 if yk+1 = 0 The utility of a neighbor vi of vk+1 is represented as: f (yi ) = (1 - ) d(yi , y i )+ 



j N (i)







d(yi , yj )+  d(yi , yk+1 )







d(yi , yj )







(1)







where y i represents the intrinsic preferred selection of user vi , i represents the conformity tendency of vi , and N (i) denotes the neighbors of vi at the time when vi makes the decision. d(., .) is a metric that gives a utility of 1 when two decisions are the same, and 0 otherwise. Nash equilibria We provide an induction method to prove that there exists Nash equilibria if all users in a network make the decisions for a given action according to the utility function defined by Eq. (1). For brevity, we assume that the parameter  in Eq. (1) is fixed for different users. The proof is the same for different . The proof is straightforward when there is only one user in a network. For a network with two users, when their intrinsic preferred selections are the same, a Nash equilibrium exists because they will make the same decision. When their preferred selections are different,  determines the final selection. If  < 0.5, a Nash equilibrium exists because they will select their own preferences respectively. If  > 0.5, two Nash equilibria exist because they will both select y 1 or y 2 . Finally, we prove that if a Nash equilibrium exists in a network with k users (k -network), a Nash equilibrium will definitely exist in any (k +1)-network obtained by adding an additional user, vk+1 , to it. The general idea is to investigate whether the neighbors of vk+1 will change their decisions







Suppose (1 - ) + N 1 > N 0 , vk+1 will decide to adopt the action (i.e., yk+1 = 1). The proof is the same when (1 - ) + N 1 < N 0 . We observe that the neighbors with yi = 1 will not change their decisions. Otherwise, the utility obtained from the k network will decrease because the Nash equilibrium is damaged, and the utility obtained from vk+1 will also decrease because yi is changed differently from yk+1 . For the neighbors with yi = 0, if they change their decisions, the marginal utility is  - ci , where -ci is the decreased utility triggered from the k -network because the Nash equilibrium is damaged.  is the increased utility caused by vk+1 because yi is changed to be the same as yk+1 . If   ci , none of the neighbors will change their decisions. If  > ci , the neighbors will change their decisions. However, in such situation, vk+1 will not change back to 0, because the utility will be reduced from (1 - ) + (N 1 + 1) to (N 0 - 1). To summarize, we can find a Nash equilibrium when an additional user vk+1 is added to any k -network with a Nash equilibrium already arrived.







3







Role-Conformity Model (RCM)







The aforementioned conformity utility function presents elegant theoretical properties, although it is too simple for real cases. In this section, we further extend it into an application-oriented probabilistic model, named RoleConformity Model (RCM), to describe user behaviors. We introduce in the model discrete time slices from t = 1 to T , and two hidden variables for characterizing the "role" of a user as well as the "topic" of a certain action. Definition 1 Individual attributes At time slice t, each user vi is associated with an attribute vector of length H , where the h-th attribute's value is denoted by xi,t,h . Different network properties of vi , such as clustering coefficient and degree, can be used as individual attributes, with the choice of which being application-dependent. Definition 2 Role distribution We adopt the concept of "role" to summarize user attributes into several clusters. A user can play different roles at different time slices. Formally, we associate each user vi at each time slice t with a vector i,t  RR , where R is the number of roles in the R r model ( r=1 r i,t = 1). Each element i,t is the probability that user vi belongs to role r at t.







Definition 3 Topic distribution In social networks, a user is usually interested in multiple topics. Formally, each user vi is associated with a vector i  RK , where K is the K z z number of topics ( z=1 i = 1). Each i is the probability (intrinsic preference) of user vi choosing topic z . Model description Based on the above definitions, we explain the proposed Role-Conformity Model. The basic idea is that users' role distribution is determined by not only attributes but also actions. We use users' attributes to determine her role distribution, which is then used as a prior to guide the sampling process for users' actions. Specifically, the model consists of two parts. The first part models the generation of individual attributes. For an individual attribute, we first draw a role r from a multinomial distribution, and then draw the value of the attribute from a normal distribution with respect to r. The second part models the total utility of generating all the actions. Specifically, we extend the utility function in Eq. (1) to further incorporate the role and topic distributions of a user. Instead of binary actions, each user is now allowed to take a set of actions, denoted by W = {w}. When in role r, the utility function of vi taking an action w is defined as:   K K 1 w z w z w i,r = (1 - r ) i z + r j z (2) | N | i z =1 z =1



j Ni







A







CAd







Ad











AXT







j



S=1







i



S=0







r







x



T A







, 



H 







z







s











R











K







w



Nd D







Figure 1: Role-conformity model. Application example: We are given a bibliographic dynamic network Gt = (V t , E t ), where V t is the set of authors up to time t and E t is the set of coauthor relationships among them up to time t. Each author vi  V t is associated with an attribute vector xi , containing her individual attributes in the network. There is also a set of documents D, where each d  D can be represented by (Ad , Wd , Cd , t). Ad  V t stands for the author set of d, Wd is the list of words w in d, Cd  D is the set of documents cited by d, and t is the time slice when d is published. By regarding each word as an action, we can naturally plug this data set into RCM. The only concern is that we usually do not know which author wrote down which word w in a document d with multiple authors, thus we assume that each word is generated from an author randomly chosen from Ad . We also need to prudently define the neighborhood Ni of each author. Since the author is more likely to be influenced by the documents she is citing, we model Ni for author vi in document d by CAd = d Cd Ad . In this application, the individual features are defined based on the co-author network. The neighbors an author conforms with can also be their coauthors. However, we discover that conformity influence caused by coauthor relationships is not as significant as that by citation relationships. Thus we investigate conformity influence caused by citation relationships in this paper. We omit the analysis result for space limitation. Without loss of generality, we continue our discussions based on the bibliographic data set to provide more technical details about how to apply our model in a real application. Figure 1 summarizes the RCM on a bibliographic data set. Model learning We adopt the probabilistic interpretation of Eq. (2) and use maximum-likelihood estimation (MLE) for model learning. The likelihood for individual attribute generation can be written as:



A T H R







is a non-negative score of taking action w under where topic z , satisfying w w z = 1, Ni is the (directed) neighborhood of vi in the social network, and r is a weighting factor similar to i in Eq. (1). Note that we utilize a unified r for all the users in role r, to reduce the number of parameters. Since the neighborhood of different users in r can be different, we normalize the gain a user obtained from her neighbors by her neighborhood size. This modification does not affect the conclusion of Section 2 since it is equivalent to directly assigning the individual conformity tendency of r each user vi as i = |Ni |-(| Ni |-1)r . The extended utility function is more general than the one described in Eq. (1). It also has another probabilistic interpretation and can be regarded as the likelihood of generating action w, by tossing a coin s with distribution Bern(r ). Then, if s = 1, w is determined by the individual's intrinsic z w topic distribution and is drawn from P (w|i) = z  i z . Otherwise, w is influenced by the neighbors' topic distribuz w tion and is drawn from P (w|Ni ) = j Ni z j z /|Ni |. The complete generative process is summarized as: * For the h-th attribute of user vi at time slice t: - Draw a role r from multinomial distribution i,t ; - Draw the value of the attribute xi,t,h  N (r,h , r,h ). * For an action w conducted by user vi at time t: - Draw vi 's role r from multinomial distribution i,t ; w - Obtain the utility of action w denoted by i,r (or apply the probabilistic interpretation here). For a given data set, we need to learn the parameters i,t , r,h , r,h , as well as i , z and r . We provide an application example of RCM in what follows.







w z







L1 =



i=1 t=1 h=1 r =1







r i,t



2 2r,h







exp -







(xi,t,h - r,h ) 2 2r,h







2







The likelihood of action generation can be written as: R r w r =1 i,t r,i L2 = |Ad |



d,w iAd







The unified likelihood function is L = L1 L2 . It is intractable to directly solve L. Thus we optimize L1 and L2







by EM algorithm respectively at each iteration. The product operation provides a theoretical guarantee that the product of lower bounds is also the lower bound of the product. We explain the EM steps for L1 and L2 respectively as below. To optimize L1 , we first estimate the posterior distribution over r for each individual attribute xi,t,h in the E-step: 



r qi,t,h r i,t



2 2r,h







derived from both L1 and L2 : r i,t =



H r h=1 qi,t,h + R H r r =1 ( h=1 qi,t,h d,w







ar d,w,i



d,w







+







ar d,w,i )







exp -







=







(xi,t,h -r,h ) 2 2r,h







2







Please refer to the supplementary materials for derivation details.







R r =1















r i,t



2 2r,h







exp -







(xi,t,h -r,h )2 2 2r,h







4







Experiments







Then in M-step, we update parameters r,h , r,h by: r,h =



A T r i=1 t=1 qi,t,h xi,t,h A T r i=1 t=1 qi,t,h A i=1 T r t=1 qi,t,h (xi,t,h - A T r i=1 t=1 qi,t,h







In this section, we apply our proposed RCM on a public available academic research data set1 to investigate the conformity tendency of authors when they write papers.







4.1



r,h )2







Experimental Setup







r,h =







where r,h and r,h are the mean and variance of the h-th attribute in role r. In order to optimize the likelihood of action generation L2 , we first apply the E-step as: ar d,w,i = bd,w,i,r = cz d,w,i,r =



w r i,t r,i R r =1 w r i,t r,i 1 |CAd | K z w z =1 i z z w i z K z w z =1 i z j CAd 1 |CAd | K z w z =1 j z K z w j CAd z =1 j z







+







where for each w in d conducted by user vi , ar d,w,i is the posterior distribution over r, bd,w,i,r is the posterior distribution of conforming, and cz d,w,i,r is the posterior distribution over z topic z . And then in M-step, we update i , w z , and r as:



D z i  d=1 wNd r =1 D R R z ar d,w,i bd,w,i,r cd,w,i,r







 ar d,w,i (1 - bd,w,i,r )



j CAd







  cz d,w,j,r







+



d=1 wNd j Ad r =1







D







R z ar d,w,i bd,w,i,r cd,w,i,r







w z 



d=1 iAd r =1 D R







 ar d,w,i (1 - bd,w,i,r )



j CAd R







  cz d,w,j,r







+



d=1 iAd r =1 D







r =



d=1 wNd iAd r =1







ar d,w,i bd,w,i,r



w r w z = 1. The parameter i,t is







Data sets We collect the data sets as follows. We first select eight domains from computer science, including database and data mining (DB&DM), human computer interaction (HCI), system and high performance computing (HP), software engineering (SE), computational theory (CT), artificial intelligence and machine learning (AI&ML), computer networks (CN), as well as computer vision and multimedia (CV&MM). For each domain, we then collect all the papers from the well-known journals and conferences in the domain and the citation relationships among them. There are in total 231,728 papers, 269,508 authors and 347,735 citation relationships, where each author has on average 3.44 papers and each paper has on average 1.68 citation relationships. We design a task of word usage prediction to evaluate our proposed model. The objective of the task is to predict whether a user will write a given word in her paper title in a given time period. Using word usage patterns to study social behaviors has been adopted in existing literature such as (Danescu-Niculescu-Mizil et al. 2013). Specifically, we split each data set into training and test set. The training set contains the papers published in or before 2009, and the test set contains the papers published after 2009. We construct a coauthor network at each time slice and use the degree and clustering coefficient as the individual attributes at each time. Each paper can be viewed as a document with a list of words as the actions performed by the authors. We run our model on the training set and then predict the candidate words (all the words appeared in both the training and test set with stop words removed) that will be used for each candidate user (the user appeared in both the training and test set). The probability of one user using a word, P (w|i), is w calculated as the expectation of i,r in Eq. (2) over role r at time t, where t is the ending time of the training set, i.e., 2009 in our setting, and Ni in Eq. (2) is the collection of authors whose papers are cited by user vi within [t - , t]. We empirically set  as 3 years. Since the word usage prediction is more like a ranking problem, precision at top ranked results is preferred in evaluating the results. Specifically, given a candidate user vi , we rank all the candidate words based on P (w|i). We view the co-occurrence of word and user pairs in the test set as the ground truth and use P@5 (Precision of top-5 predictions),



1







where







z z i







= 1 and







http://arnetminer.org/citation/







P@10, Mean Average Precision (MAP), and area under the ROC curve (AUC) to evaluate the ranking results for each user and then aggregate the results for all the users together. Baselines We compare our model with TF-IDF, the traditional probabilistic latent semantic analysis (PLSA) (Hofmann 1999) and the citation influence model (CIM) (Dietz, Bickel, and Scheffer 2007). TF-IDF: In TF-IDF, the probability of a user using a word in the test set is calculated as the TF-IDF value of the user writing the word in the training set. We view a document as the aggregation of all the paper titles of a user to calculate TF-IDF value. PLSA: In PLSA, the probability of a user using a word is K z w calculated as P (w|i) = z=1 i z . This method ignores conformity influence and assumes that users write words only based on their intrinsic preferences. CTM: In CTM, the probability of one user using a word w is calculated as i,r in Eq. (2), where i is directly learned for each user in CTM, instead of for each role in RCM.







4.2







Experimental Results







Table 1 shows the performance of word usage prediction in the collected data sets from eight different domains. Better performance From Table 1, we can see that RCM clearly outperforms TF-IDF and PLSA on all the eight data sets (+3.6-4.1% in terms of average MAP and +7.1-47.4% in terms of average AUC). TF-IDF and PLSA predict word usage only based on the intrinsic preference of a given user, and ignore the situation where a user's topic distribution may change and become closer to her neighbors' topic distribution over time. TF-IDF performs worse because it directly counts the frequency of words, which may be sparse in paper titles. RCM also outperforms CIM on almost all the data sets. Although CTM also considers both the intrinsic preference of a user and her conformity tendency, it suffers from the problem of data sparsity. Specifically, CTM directly learns the conformity tendency of each user, which is very difficult to be estimated accurately when very few historical actions of the user and/or her neighbors are available for model learning. In contrast, our model clusters similar users (with similar individual attributes) into roles, and then learns the conformity tendency of each role. The data sparsity problem can be well avoided by RCM. Parameter Analysis There are two tunable parameters, K and R, in RCM. K is the number of topics, which has been analyzed in many previous research (Blei, Ng, and Jordan 2003). We experiment with different values of K, and observe that perplexity first rises and then stabilizes as K gets large. We then fix K = 30 where the perplexity is stable and then analyze the number of roles R. Figure 2 plots the correlation between MAP/AUC and the number of roles on the eight data sets. We find that both MAP and AUC present increasing trend at the very beginning and soon become stable when R gets large. The results indicate that our RCM model is insensitive to the number of roles. Finally, we empirically set R = 13 in our experiments.







Table 1: Performance of word usage prediction (%). Query Method P@5 P@10 MAP AUC TF-IDF 15.84 12.67 6.68 36.20 PLSA 20.10 15.49 9.26 77.61 DB&DM CIM 22.26 17.98 11.59 85.50 RCM 30.40 24.94 14.16 86.90 TF-IDF 13.57 11.42 5.40 27.59 PLSA 14.25 11.65 5.71 67.37 HCI CIM 18.67 15.34 8.12 73.39 RCM 19.16 15.40 8.92 75.32 TF-IDF 15.71 12.95 7.08 38.70 HP PLSA 17.33 14.39 8.47 79.96 CIM 19.62 16.25 10.83 88.67 RCM 20.57 17.12 11.37 89.21 TF-IDF 16.81 13.21 7.82 38.07 SE PLSA 4.20 2.60 2.60 81.15 CIM 21.43 16.42 12.16 85.55 RCM 25.31 19.98 12.54 85.27 TF-IDF 19.18 15.10 11.56 46.80 CT PLSA 17.52 13.37 9.88 81.09 CIM 19.36 14.50 11.04 85.31 RCM 20.13 15.20 11.46 85.93 TF-IDF 19.14 15.39 8.25 42.02 PLSA 19.92 15.50 9.40 84.10 AI&ML CIM 21.24 16.41 10.85 90.70 RCM 23.60 18.02 11.41 90.92 TF-IDF 20.03 17.51 8.71 37.23 CN PLSA 26.68 20.33 12.99 80.63 CTM 29.36 21.62 14.75 86.92 RCM 31.20 23.35 15.22 88.41 TF-IDF 17.19 14.19 8.18 41.65 CV&MM PLSA 19.88 14.78 09.64 78.85 CTM 22.09 16.12 11.10 85.02 RCM 24.49 18.37 11.50 85.63 TF-IDF 17.18 14.06 7.96 38.53 Avg PLSA 17.49 13.51 8.49 78.85 CTM 21.75 16.83 11.31 85.13 RCM 24.36 19.05 12.07 85.95







Correlation between role and conformity influence The learned parameter  by RCM represents the conformity tendency for different roles. The model also learns the mean value of each individual attribute for a role, i.e., r,h . Thus we can represent each role as a vector of the mean values of different network attributes and analyze the correlation between role and conformity tendency. We select two domains, DB&DM and HP for further discussions. Figure 3 shows the correlation between a role's mean degree and its conformity probability. We discover that the correlation follows a logarithm function. When fitting the data points, we first remove the roles with a small number of related users, where the number of related users with respect to a role r is estimated by summing up the probability r i,t over all vi and t. We try different forms of functions to fit the remaining data points and select logarithm function with the largest R2 . Figure 4 shows the correlation between a role's mean clustering coef-







2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20







R







2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20







0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08 0.07







0.95



Conformity probability







1.0



Conformity probability



Clustering coefficient







1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 1.2



Clustering coefficient







0.90



AUC(%)







0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 1.2







MAP(%)







0.85



DB&DM HCI HP SE CT AI&ML CN CV&MM







DB&DM HCI HP SE CT AI&ML CN CV&MM







0.80 0.75 0.70



R







(a) MAP







(b) AUC







(a) DB&DM







(b) HP







Figure 2: Role number analysis.



1.0



Conformity probability



Conformity probability







1.0 0.8 0.6 0.4 0.2 0.00 20 40 60 80 100 120 140 160



Degree







Figure 4: The correlation between mean clustering coefficient of role and conformity probability. from social psychology to a computational model. Social influence has been studied throughly. Kempe et al. (Kempe, Kleinberg, and Tardos 2003) first proposed two fundamental diffusion models to estimate the expected influence of given seed users. Bakshy et al. (Bakshy et al. 2012) and Bond et al. (Bond et al. 2012) conducted randomized controlled trials to identify the effect of social influence. Dietz et al. (Dietz, Bickel, and Scheffer 2007) and Liu et al. (Liu et al. 2012) used topic models to learn the influential strength between papers or users. Tang et al. (Tang et al. 2009) and Tan et al. (Tan et al. 2011) used discriminative models to learn the weights of different influence factors. Gruhl et al. (Gruhl et al. 2004), Saito et al. (Kimura et al. 2011) and Goyal et al. (Goyal, Bonchi, and Lakshmanan 2010) learned the influence probabilities of the time-decayed diffusion models. However, they all focus on modeling how users influence others, and ignore the inclination of the users to be influenced.







0.8 0.6 0.4 0.2 0.00 50 100 150 200 250 300



Degree







(a) DB&DM







(b) HP







Figure 3: The correlation between mean degree of role and conformity probability. ficient and its conformity probability. We discover that one kind of roles have clustering coefficient close to 0, and the other kind of roles follows an exponential function. Since papers are publicly published, anyone could read others' papers and have almost the same opportunity to be influenced by others. Thus the phenomenon in the two figures may be explained as: when a person collaborates with more authors and the coauthors are more structurally diverse (i.e., with a small clustering coefficient), she may become more openminded and tend to accept new ideas from others. However, when the social circle of the user is restricted to a few coauthors forming a dense collaboration network, the person will be more conservative and tend not to accept other ideas.







6







Conclusion







5







Related work







Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Considerable research (Asch 1955; Bernheim 1994; Cialdini and Goldstein 2003; Kelman 1958) has been conducted on the issue of conformity in social psychology. Recently, several studies on conformity have been conducted on large social networks. For example, Li et al. (Li, Bhowmick, and Sun 2011; 2013) studied the interplay between the influence and conformity. Tang et al. (Tang, Wu, and Sun 2013) proposed a factor graph model to quantify the effects of different conformity factors. However, both the studies do not consider the problem of data sparsity. An individual's conformity cannot be estimated accurately if her historical actions are few. To overcome this problem, we assign hidden roles to users and then learn the correlation between roles and conformity tendency. Besides, to the best of our knowledge, this is the first attempt to formally connect the conformity theory







We present the first attempt to connect the conformity theory from social psychology to a computational model. We first formalize conformity theory in terms of a utility function, and validate the utility function by proving the existence of Nash equilibria. We then extend and incorporate the utility function into a probabilistic topic model that takes the role and topic distributions of users into account. Our model allows for mining the correlation between users' hidden roles and conformity tendency. Our experiments on academic research networks show an interesting result that people with higher degree and lower clustering coefficient are more likely to conform to others. In addition, our method also outperforms several baselines on the task of word usage prediction in academic papers.







ACKNOWLEDGMENTS



The work is supported by the National High-tech R&D Program (No. 2014AA015103), National Basic Research Program of China (No. 2014CB340500, No. 2012CB316006), NSFC (No. 61222212, No. 61035004), NSFC-ANR (No. 61261130588), the Tsinghua University Initiative Scientific Research Program (20121088096), a research fund supported by Huawei Technologies Co. Ltd and Beijing key lab of networked multimedia.







References



Aronson, E.; Wilson, T.; and Akert, R. 2007. Social Psychology. Prentice Hall. Asch, S. 1955. Opinions and social pressure. Bakshy, E.; Eckles, D.; Yan, R.; and Rosenn, I. 2012. Social influence in social advertising: evidence from field experiments. In EC'12, 146-161. Bernheim, B. D. 1994. A theory of conformity. Journal of Political Economy 1027(5):841-877. Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. JMLR 3:993-1022. Bond, R. M.; Fariss, C. J.; Jones, J. J.; Kramer, A. D. I.; Marlow, C.; Settle, J. E.; and Fowler, J. H. 2012. A 61million-person experiment in social influence and political mobilization. Nature 489:295-298. Cialdini, R., and Goldstein, N. 2003. Social influence: Complicance and conformity. Annual Review of Psychology 55. Danescu-Niculescu-Mizil, C.; West, R.; Jurafsky, D.; Leskovec, J.; and Potts, C. 2013. No country for old members: User lifecycle and linguistic change in online communities. In WWW'13, 307-318. Dietz, L.; Bickel, S.; and Scheffer, T. 2007. Unsupervised prediction of citation influences. In ICML'07, 233-240. Goyal, A.; Bonchi, F.; and Lakshmanan, L. V. 2010. Learning influence probabilities in social networks. In WSDM'10, 241-250. Gruhl, D.; Guha, R.; Liben-Nowell, D.; and Tomkins, A. 2004. Information diffusion through blogspace. In WWW'04, 491-501. Hofmann, T. 1999. Probabilistic latent semantic indexing. In SIGIR'99, 50-57.







Jenness, A. 1932. The role of discussion in changing opinion regarding matter of fact. Abnormal and Social Psychology 279-296. Kelman, H. C. 1958. Compliance, identification, and internalization: Three processes of attitude change. Journal of Conflict Resolution 2(1):51-60. Kempe, D.; Kleinberg, J.; and Tardos, E. 2003. Maximizing the spread of influence through a social network. In KDD'03, 137-146. Kimura, M.; Saito, K.; Ohara, K.; and Motoda, H. 2011. Learning information diffusion model in a social network for predicting influence of nodes. Intell. Data Anal. 15:633- 652. Li, H.; Bhowmick, S. S.; and Sun, A. 2011. Casino: Towards conformity-aware social influence analysis in online social networks. In CIKM'11, 1007-1012. Li, H.; Bhowmick, S. S.; and Sun, A. 2013. Cinema: conformity-aware greedy algorithm for influence maximization in online social networks. In EDBT'13, 323-334. Liu, L.; Tang, J.; Han, J.; and Yang, S. 2012. Learning influence from heterogeneous social networks. DataMKD 25(3):511-544. Sherif, M. 1935. A study of some social factors in perception. Archives of Psychology 187. Tan, C.; Lee, L.; Tang, J.; Jiang, L.; Zhou, M.; and Li, P. 2011. User-level sentiment analysis incorporating social networks. In KDD'11, 1049-1058. Tang, J.; Sun, J.; Wang, C.; and Yang, Z. 2009. Social influence analysis in large-scale networks. In KDD'09, 807-816. Tang, J.; Wu, S.; and Sun, J. 2013. Confluence: Conformity influence in large social networks. In KDD'13, 347-355.







Grounded Fixpoints



Bart Bogaerts



Department of Computer Science KU Leuven 3001 Heverlee, Belgium bart.bogaerts@cs.kuleuven.be







Joost Vennekens



Department of Computer Science Campus De Nayer, KU Leuven 2860 Sint-Katelijne-Waver, Belgium joost.vennekens@cs.kuleuven.be







Marc Denecker



Department of Computer Science KU Leuven 3001 Heverlee, Belgium marc.denecker@cs.kuleuven.be







Abstract



Algebraical fixpoint theory is an invaluable instrument for studying semantics of logics. For example, all major semantics of logic programming, autoepistemic logic, default logic and more recently, abstract argumentation have been shown to be induced by the different types of fixpoints defined in approximation fixpoint theory (AFT). In this paper, we add a new type of fixpoint to AFT: a grounded fixpoint of lattice operator O : L  L is defined as a lattice element x  L such that O(x) = x and for all v  L such that O(v  x)  v , it holds that x  v . On the algebraical level, we show that all grounded fixpoints are minimal fixpoints approximated by the well-founded fixpoint and that all stable fixpoints are grounded. On the logical level, grounded fixpoints provide a new mathematically simple and compact type of semantics for any logic with a (possibly non-monotone) semantic operator. We explain the intuition underlying this semantics in the context of logic programming by pointing out that grounded fixpoints of the immediate consequence operator are interpretations that have no non-trivial unfounded sets. We also analyse the complexity of the induced semantics. Summarised, grounded fixpoint semantics is a new, probably the simplest and most compact, element in the family of semantics that capture basic intuitions and principles of various non-monotonic logics.







1







Introduction







Motivated by structural analogies in the semantics of several non-monotonic logics, Denecker, Marek, and Truszczy nski (2000) developed an algebraic theory that defines different types of fixpoints for a so-called approximating bilattice operator, called supported, Kripke-Kleene, stable and wellfounded fixpoints. In the context of logic programming, they found that Fitting's immediate consequence operator is an approximating operator of the two-valued immediate consequence operator and that its four different types of fixpoints correspond exactly with the four major, equally named semantics of logic programs. They also identified approximating operators for default logic (DL) and autoepistemic logic (AEL) and showed that the fixpoint theory induces all main



Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







and some new semantics in these fields (Denecker, Marek, and Truszczy nski 2003). By showing that Konolige's mapping from DL to AEL preserves the approximating operator, they resolved an old research question regarding the nature of these two logics: AEL and DL are "just" two different dialects of autoepistemic reasoning (Denecker, Marek, and Truszczy nski 2011). The study of these approximating operators is called approximation fixpoint theory (AFT). It is now commonly used to define semantics of extensions of logic programs, such as logic programs with aggregates (Pelov, Denecker, and Bruynooghe 2007) and HEX logic programs (Antic, Eiter, and Fink 2013). Vennekens, Gilis, and Denecker (2006) used AFT in an algebraic modularity study for logic programming, AEL and DL. Recently, Strass (2013) showed that many semantics from Dung's argumentation frameworks (AFs) and abstract dialectical frameworks (ADFs) can be obtained by direct applications of AFT and Bogaerts et al. (2014) defined the causal logic FO(C) as an instantiation of AFT. This work suggests that fixpoint theory, despite its high level of abstraction, captures certain fundamental intuitions and cognitive principles in a range of logics and sorts of human knowledge. It is this observation that provides the basic motivation for the present study. In Section 3, we extend AFT with a new type of fixpoint: a point x in a lattice L is a grounded fixpoint of operator O : L  L if O(x) = x and for all v  L such that O(x  v )  v , it holds that x  v . In Section 4, we discuss the relation between grounded fixpoints and the other fixpoints defined by AFT. In particular, we show that all (ultimate) stable fixpoints are grounded and that all grounded fixpoints are minimal fixpoints approximated by the (ultimate) well-founded fixpoint in the bilattice. In general there are minimal fixpoints that are not grounded, and grounded fixpoints that are not stable. If the well-founded fixpoint is "exact", the well-founded fixpoint is the unique grounded and stable fixpoint of O. Grounded fixpoints have several appealing properties. First of all, a grounded fixpoint is a purely algebraical concept. As such, it can be used in all fields where AFT is applied. Secondly, a first step in the application of AFT for a lattice operator O is to choose a bilattice operator that approximates O. In contrast, grounded fixpoints are defined directly in terms of the original operator O. Thirdly, their







definition formalises and generalises well-known intuitions. In the context of logic programming, which we discuss in Section 5, the algebraic results show that grounded fixpoints induce a semantics that is slightly more "liberal" than stable semantics: all stable models are grounded (i.e., we identified a property all stable models have in common) but also every grounded fixpoint is approximated by the well-founded model; the differences collapse in case the well-founded model is two-valued. We will see that for logic programming, this semantics is simple and intuitive: we show that the grounded fixpoints can be characterised in terms of a generalised notion of unfounded set. Contrary to the more common semantics of logic programs, grounded fixpoint semantics does not rely on any form of three-valued logic: It is defined directly in terms of the (twovalued!) immediate consequence operator. The grounded fixpoint semantics is very flexible towards language extensions. Currently, much research is being conducted in order to extend stable and well-founded semantics for logic programs with new language constructs (Pelov, Denecker, and Bruynooghe 2007; Faber, Pfeifer, and Leone 2011; Marek, Niemel a, and Truszczy nski 2008; Balduccini 2013). Since the grounded fixpoint semantics is completely defined using the two-valued immediate consequence operator, it suffices to extend this operator to obtain an extended semantics; this is often trivial. These nice properties come at a cost: we show that in general, determining whether a logic program has a grounded fixpoint is P 2 -complete. However, for large classes of programs, grounded fixpoint semantics coincides with stable semantics. For those programs, we obtained a simple, concise, purely 2-valued and algebraical, extensible reformulation of the existing semantics. Due to space limitations, we do not elaborate on how our theory applies to AEL, DL or ADFs and refer to (Bogaerts, Vennekens, and Denecker 2014) for proofs. Summarised, the main contributions of this paper are as follows. We extend AFT with the notion of a grounded fixpoint, a fixpoint closely related to stable fixpoints with similar properties, but that is determined by O, not by the choice of an approximator. Applied to logic programming this yields an intuitive, purely two-valued, semantics that is easily extensible and that formalises well-known intuitions.







We call L,  a complete lattice if every subset of L has a least upper bound and a greatest lower bound. A complete lattice has both a least element  and a greatest element . An operator O : L  L is monotone if x  y implies that O(x)  O(y ). An element x  L is a prefixpoint, a fixpoint, a postfixpoint if O(x)  x, respectively O(x) = x, x  O(x). Every monotone operator O in a complete lattice has a least fixpoint, denoted lfp(O), which is also O's least prefixpoint and the limit (the least upper bound) of the increasing sequence (xi )i0 defined by * x 0 = , * xi+1 = O(xi ), * x = lub({xi | i < }), for limit ordinals .







2.2







Logic Programming







In the following sections, we illustrate our abstract results in the context of logic programming. We recall some preliminaries. We restrict ourselves to propositional logic programs, but allow arbitrary propositional formulas in rule bodies. However, AFT has been applied in a much broader context (Denecker, Bruynooghe, and Vennekens 2012; Pelov, Denecker, and Bruynooghe 2007; Antic, Eiter, and Fink 2013) and our results can also be applied in these extensions of logic programming. Let  be an alphabet, i.e., a collection of symbols which are called atoms. A literal is an atom p or the negation q of an atom q . A logic program P is a set of rules r of the form h  , where h is an atom called the head of r, denoted head(r), and  is a propositional formula called the body of r, denoted body (r). An interpretation I of the alphabet  is an element of 2 , i.e., a subset of . The set of interpretations 2 forms a lattice equipped with the order . The truth value (t or f ) of a propositional formula  in a structure I , denoted I is defined as usual. With a logic program P , we associate an immediate consequence operator (van Emden and Kowalski 1976) TP that maps a structure I to TP (I ) = {p | r  P : head(r) = p  body (r)I = t}.







3







Grounded Fixpoints







2



2.1







Preliminaries







Lattices and Operators







Let L,  be a complete lattice and O : L  L a lattice operator, fixed throughout this entire section. We start by giving the most central definition of this text, namely the notion of groundedness. Definition 3.1 (Grounded). We call x  L grounded for O if for each v  L such that O(v  x)  v , it holds that x  v . The intuition behind this concept is easiest to explain if we assume that the elements of L are sets of "facts" of some kind and the  relation is the subset relation between such sets. In this case, a point x is grounded if it contains only facts that are sanctioned by the operator O, in the sense that if we remove them from x, then the operator will add at least one of them again. The above definition captures this idea, by using a set v  L to remove all elements not in v from x. If their removal is not contradicted by O, i.e., O does not re-derive any removed element (O(x  v )  v ), then these elements cannot be part of the grounded point (x  v ).







A poset L,  is a set L equipped with a partial order , i.e., a reflexive antisymmetric, transitive relation. If S is a subset of L, then x is an upper bound, respectively a lower bound of S if for every s  S , it holds that s  x respectively x  s. An element x is a least upper bound, respectively greatest lower bound of S if it is an upper bound that is smaller than every other upper bound, respectively a lower bound that is greater than every other lower bound. If S has a least upper bound, respectively a greatest lower bound, we denote it lub(S ), respectively glb(S ). As is custom, we sometimes call a greatest lower bound a meet, and a least upper bound a join and use the related notations S = glb(S ), xy = glb({x, y }), S = lub(S ) and xy = lub({x, y }).







Proposition 3.2. If O is a monotone operator and x is grounded for O then x is a postfixpoint of O that is less than or equal to lfp(O), i.e., x  O(x) and x  lfp(O). Example 3.3. The converse of Proposition 3.2 does not hold. Consider the following logic program P : p. q  p  q. Its immediate consequence operator TP is represented by the following graph, where full edges express the order relation (to be precise, the  relation is the reflexive transitive closure of these edges) and the dotted edges represent the operator: q} 5 6 = {p, W h i {p} h c = TP is a monotone operator with least fixpoint . Also, {q } is a postfixpoint of TP since TP ({q }) =  {q }. However, {q } is not grounded since TP ({q }  {p}) = TP () = {p}  {p}, while {q }  {p}. Proposition 3.4. All grounded fixpoints of O are minimal fixpoints of O. Example 3.5. The converse of Proposition 3.4 does not hold. Consider the logic program P : p  p. q  p  q. This logic program corresponds has as immediate consequence operator TP : 6 + {p} h = In this case, {p} is a minimal fixpoint of TP , but {p} is not grounded since TP ({p}  {q }) = TP () = {q }. Proposition 3.6. A monotone operator has exactly one grounded fixpoint, namely its least fixpoint. = {p, q } W h 62 {q } s 6 {q }







denote the set of consistent elements. Elements (x, x)  Lc are called exact. We sometimes abuse notation and use the tuple (x, y ) and the interval [x, y ] interchangeably. The precision ordering on L2 is defined as (x, y ) p (u, v ) if x  u and v  y . In case (u, v ) is consistent, this means that (x, y ) approximates all elements approximated by (u, v ), or in other words that [u, v ]  [x, y ]. If L is a complete lattice, then L2 , p is also a complete lattice. AFT studies fixpoints of lattice operators O : L  L through operators approximating O. An operator A : L2  L2 is an approximator of O if it is p -monotone, and has the property that for all x, O(x)  A(x, x). Approximators are internal in Lc (i.e., map Lc into Lc ). As usual, we restrict our attention to symmetric approximators: approximators A such that for all x and y , A(x, y )1 = A(y, x)2 . Denecker, Marek, and Truszczy nski (2004) showed that the consistent fixpoints of interest (supported, stable, well-founded) are uniquely determined by an approximator's restriction to Lc , hence, sometimes we only define approximators on Lc . AFT studies fixpoints of O using fixpoints of A. The AKripke-Kleene fixpoint is the p -least fixpoint of A and has the property that it approximates all fixpoints of O. A partial A-stable fixpoint is a pair (x, y ) such that x = lfp(A(*, y )1 ) and y = lfp(A(x, *)2 ), where A(*, y )1 denotes the operator L  L : x  A(x, y )1 and analogously for A(x, *)2 . The A-well-founded fixpoint is the least precise partial Astable fixpoint. An A-stable fixpoint of O is a fixpoint x of O such that (x, x) is a partial A-stable fixpoint. This is equivalent with the condition that x = lfp(A(*, x)1 ). The A-Kripke-Kleene fixpoint of O can be constructed by iteratively applying A, starting from (, ). For the A-wellfounded fixpoint, a similar constructive characterisation has been worked out by Denecker and Vennekens (2007). In general, a lattice operator O : L  L has a family of approximators of different precision. Denecker, Marek, and Truszczy nski (2004) showed that there exists a most precise approximator, UO , called the ultimate approximator of O. This operator is defined by UO : Lc  Lc : (x, y )  ( O([x, y ]), O([x, y ])). Semantics defined using the ultimate approximator have as advantage that they only depend on O since the approximator can be derived from O. It was shown that for any approximator A, all A-stable fixpoints are UO -stable fixpoints, and the UO -well-founded fixpoint is always more precise than the A-well-founded fixpoint. We refer to UO -stable fixpoints as ultimate stable fixpoints of O and to the UO -well-founded fixpoint as the ultimate well-founded fixpoint of O. AFT and Logic Programming In the context of logic 2 programming, elements of the bilattice 2 are partial interpretations, pairs I = (I1 , I2 ) of interpretations. The pair (I1 , I2 ) approximates all interpretations I with I1  I  I2 . We are mostly concerned with consistent (or, threevalued) interpretations: tuples I = (I1 , I2 ) with I1  I2 . For such an interpretation, the atoms in I1 are true (t) in I , the atoms in I2 \ I1 are unknown (u) in I and the other atoms are false (f ) in I . If I is a three-valued interpretation, and  a formula, we write I for the standard three-valued valuation based on the Kleene truth tables (Kleene 1938). An in-







4



4.1







Grounded Fixpoints and AFT







Preliminaries: AFT







Given a lattice L, approximation fixpoint theory makes uses of the bilattice L2 . We define two projection functions for pairs as usual: (x, y )1 = x and (x, y )2 = y . Pairs (x, y )  L2 are used to approximate all elements in the interval [x, y ] = {z | x  z  z  y }. We call (x, y )  L2 consistent if x  y , that is, if [x, y ] is non-empty. We use Lc to







terpretation I corresponds to the partial interpretation (I, I ). If I = (I1 , I2 ) is a (partial) interpretation, and U  , we write I [U : f ] for the (partial) interpretation that equals I on all elements not in U and that interprets all elements in U as f , i.e., the interpretation (I1 \ U, I2 \ U ). Several approximators have been defined for logic programs. The most common is Fitting's immediate consequence operator P (Fitting 2002), a direct generalisation of TP to partial interpretations. Denecker, Marek, and Truszczy nski (2000) showed that the well-founded fixpoint of P is the well-founded model of P (Van Gelder, Ross, and Schlipf 1991) and that P -stable fixpoints are exactly the stable models of P (Gelfond and Lifschitz 1988). Contrary to classical stable and well-founded semantics, their ultimate counterparts have the nice property that they are insensitive to equivalence preserving rewritings of the bodies of rules. If two logic programs P and P have the same immediate consequence operator, then their ultimate stable (respectively ultimate well-founded) models are the same. For example consider programs P = {p  p  p} and P = {p.}. Even though the body of the rule defining p in P is a tautology, {p} is not a stable model of P (while it is a stable model of P ). However, the ultimate stable semantics treats these two programs identically. However, this property comes at a cost. Denecker, Marek, and Truszczy nski (2004) showed that deciding whether P has an ultimate stable model is 2 P -complete, while that same task is only NP-complete for classical stable models.







Corollary 4.3. If A is an approximator of O, then all Astable fixpoints are grounded fixpoints of O. Theorem 4.4. The well-founded fixpoint (u, v ) of a symmetric approximator A of O approximates all grounded fixpoints of O. Corollary 4.5. If the well-founded fixpoint of a symmetric approximator A of O is exact, then this point is the unique grounded fixpoint of O.







5







Grounded Fixpoints of Logic Programs







4.2







Grounded Fixpoints and AFT







In this section, we discuss how groundedness relates to AFT. More concretely, we show that all (ultimate) stable fixpoints are grounded and that all grounded fixpoints are approximated by the (ultimate) well-founded fixpoint. Proposition 4.1. All ultimate stable fixpoints of O are grounded. Example 4.2. The converse of Proposition 4.1 does not always hold. Consider the logic program P : p  p  q. q  q  p. This logic program has as immediate consequence operator TP : = {p, q } 6 O W h {p} hr = is grounded for TP , since the only v with TP (  v ) = TP (v )  v is itself. However, since TP ([, ]) = L \ {} and {p}  {q } = , it follows that (TP [, ]) = . Thus, lfp( TP ([*, ])) = . Therefore, is not an ultimate stable fixpoint of TP . The fact that all A-stable fixpoints are ultimate stable fixpoints (Denecker, Marek, and Truszczy nski 2004) yields: 62 {q }







In this section, we discuss grounded fixpoints in the context of logic programming. It follows immediately from the algebraical results (Corollary 4.3 and Theorem 4.4) that stable models are grounded fixpoints of the immediate consequence operator and that grounded fixpoints are minimal fixpoints approximated by the well-founded model. Grounded fixpoints can be explained in terms of unfounded sets. Intuitively, an unfounded set is a set of atoms that might circularly support themselves, but have no support from outside. Stated differently, an unfounded set of a logic program P with respect to an interpretation I is a set U of atoms such that P provides no support for the truth of any atom in U , except possibly support based on the truth of other atoms in U . Since TP maps an interpretation I to the set of atoms supported by P in I , the above intuitions are directly formalised as follows. Definition 5.1 (2-Unfounded set). Let P be a logic program, TP the corresponding direct consequence operator and I  2 an interpretation. A set U   is a 2-unfounded set of P with respect to I if TP (I [U : f ])  U = . Thus, U is a 2-unfounded set of P with respect to I if removing all elements of U from I results in a state I [U : f ] where no atom in U is supported, i.e., TP (I [U : f ]) contains no atoms from U . Definition 5.1 slightly differs from the original definition of unfounded set by Van Gelder, Ross, and Schlipf (1991) but it formalises the same intuitions. The most important difference is that we work in a two-valued setting, while van Gelder et al. defined unfounded sets in a three-valued setting. For clarity, we refer to our unfounded sets as "2-unfounded sets" and to the original definition as "GRS-unfounded sets". Our theory does not require any form of three-valued logic. In Section 6, we extend our definition to a three-valued context and show that the different notions of unfounded set are equivalent in the context of the well-founded model construction. Example 5.2. Let P be the following program:      p  q  r.  q  p.   t  s  r.   Let I be the interpretation {p, q, s, t}. Then U1 = {p, q } is an unfounded set of P with respect to I since I [U1 : f ] = {s, t} and in this structure, the bodies of rules defining p and q are false. More formally, TP (I [U1 : f ])  U1 =  U1 = . U2 = {s, t} is not a 2-unfounded set of P with respect to I since TP (I [U2 : f ])  U2 = {p, q, t}  U2 = {t} = .







In what follows, we use U c for the set complement of U , i.e., U c =  \ U . Proposition 5.3. Let P be a logic program, TP the corresponding direct consequence operator and I  2 an interpretation. A set U   is a 2-unfounded set of P with respect to I if and only if TP (I  U c )  U c . Proposition 5.3 shows that U is a 2-unfounded set if and only if its complement satisfies the condition on v in Definition 3.1! This allows us to reformulate the condition that I is grounded as follows. Proposition 5.4. A structure I is grounded for TP if and only if I does not contain any atoms that belong to a 2unfounded set U of P with respect to I . If I is a fixpoint of TP , then all sets U  I c are 2unfounded sets. We call these 2-unfounded sets trivial. With this terminology, we find: Corollary 5.5. A structure I is a grounded fixpoint of TP if and only if it is a fixpoint of TP and P has no non-trivial 2-unfounded sets with respect to I . Similarly to ultimate semantics, grounded fixpoints are insensitive to equivalence-preserving rewritings in the bodies of rules: if P and P are such that TP = TP , then the grounded fixpoints of P and P coincide. Also similar to ultimate semantics, the above property comes at a cost. Theorem 5.6. The problem "given a finite propositional logic program P , decide whether P has a grounded fixpoint" is P 2 -complete. Let us briefly compare grounded fixpoint semantics with the two most frequently used semantics of logic programming: well-founded and stable semantics. Firstly, it deserves to be stressed that the three semantics provide different formalisations of a similar intuition: a certain minimality criterion for fixpoints (which we called groundedness). Consequently it is to be expected that they often coincide. We established that for programs with a two-valued wellfounded model, the three semantics coincide. This sort of programs is common in applications for deductive databases (Datalog and extensions (Abiteboul and Vianu 1991)) and for representing inductive definitions (Denecker and Vennekens 2014). In contrast, well-founded semantics coincides only seldom with stable semantics in the context of answer set programming (ASP). We illustrated in Example 4.2 that in this case, also stable and grounded fixpoint semantics may disagree. This example is quite unwieldy, as are all such programs that we found. It led us to expect that for large classes of ASP programs, both semantics still coincide. For those programs, we have defined a an elegant, intuitive and concise reformulation of the existing semantics. It is a topic for future research to search for characteristics of ASP programs that guarantee that both semantics agree or disagree. Grounded fixpoint semantics is, to the best of our knowledge, the first purely two-valued and algebraical semantics. The well-founded semantics explicitly uses three-valued interpretations in the well-founded model construction. Stable semantics uses three-valued logic implicitly: the GelfondLifschitz reduct corresponds to an evaluation in a partial interpretation. The ultimate versions of these semantics are







purely algebraical but still refer to three-valued interpretations (replacing Kleene valuation by supervaluation). Due to this, ultimate stable and well-founded models are relatively complex to understand. Logic Programs with Abstract Constraint Atoms. The fact that grounded fixpoints semantics is two-valued and algebraical makes it not only easier to understand, but also to extend the semantics. To illustrate this, we consider logic programs with abstract constraint atoms as defined by Marek, Niemel a, and Truszczy nski (2008). An abstract constraint is a collection C  2 . A constraint atom is an expression of the form C (X ), where X   and C is an abstract constraint. The goal of such an atom is to model constraints on subsets of X . The truth value of C (X ) in interpretation I is t if I  X  C and f otherwise. Abstract constraints are a generalisation of pseudo-Boolean constraints, cardinality constraints, and much more. A deterministic logic program is a set of rules of the form1 p  a1  * * *  an  b1  * * *  bm , where p is an atom and the ai and bi are constraint atoms. The intuition behind such a rule is that p is justified if the constraints ai are satisfied while the bi are not. This intuition is captured in an extended immediate consequence operator: TP (I ) = {p | r  P : head(r) = p  body (r)I = t}. Grounded fixpoints of this operator still represent the same intuitions: an interpretation I is grounded if it contains no unfounded sets, or said differently, no atoms without external support. Thus, if it contains no set U of atoms such that TP (I [U : f ])  U = . Example 5.7. Let  be the alphabet {a, b, c, d} For every i, let Ci be the cardinality constraint {X   | |X |  i}.. Consider the following logic program P over : a. c  C4 (). b  C1 (). d  C4 ().







Any interpretation in which d holds is not grounded since for every I , C4 ()I [d:f ] = f and thus d  TP (I [d : f ]). It can easily be verified that {a, b, c} is the only grounded fixpoint of P . This example illustrates that even for complex, abstract extensions of logic programs, groundedness is an intuitive property: for any extension, a point is grounded if it contains no self-supporting atoms. Also, it often possible to derive common properties of all grounded fixpoints such as the fact that d cannot be contained in any of them. Lastly, groundedness easily extends to these rich formalisms (defining grounded fixpoints takes one line given the immediate consequence operator). This is in sharp contrast with more common semantics of logic programming (such as stable



Here, we limit ourselves to deterministic programs. In general, Marek, Niemel a, and Truszczy nski also described nondeterministic programs. We come back to this issue in Section 6.



1







and well-founded semantics) which are often hard(er) to extend to richer formalisms, as can be observed by the many different versions of those semantics that exist for logic programs with aggregates (Ferraris 2005; Son, Pontelli, and Elkabani 2006; Pelov, Denecker, and Bruynooghe 2007; Faber, Pfeifer, and Leone 2011; Gelfond and Zhang 2014).







GRS-unfounded sets (Lifschitz 2008). This again shows that many of the intuitions used in Answer Set Programming are also closely related to the notion of groundedness. Groundedness and Nondeterminism. In Section 5, we restricted ourselves to logic programs with abstract constraint atoms in the bodies of rules. As argued by Marek, Niemel a, and Truszczy nski (2008), allowing them as well in heads gives rise to a nondeterministic generalisation of the immediate consequence operator. A consistent nondeterministic operator maps every point x  L to a non-empty set O(x)  L. The definition of groundedness can straightforwardly be extended to this nondeterministic setting: a point x  L is grounded for nondeterministic operator O, if x  v for all v such that O(x  v )  v , where we define for a set X  L that X  v if x  v for every x  X . A thorough study of groundedness for nondeterministic operators is out of the scope of this paper. Other Definitions of Groundedness. The terminology "grounded" is heavily overloaded in the literature. This is not a coincidence since this term often represents similar intuitions. For example Denecker, Marek, and Truszczy nski (2002) argued that ultimate stable models satisfy "some groundedness condition" without defining this condition. We formally defined groundedness and showed in Proposition 4.1 that with this definition, their claim indeed holds. In 1988, Konolige defined notions of weak, moderate and strong groundedness in order to formalise some of his intuitions regarding "good" models of autoepistemic theories. However, as he mentions himself, the closest he got to formalising these intuitions was strong groundedness, a syntactical criterion that depends on how a theory is rewritten to a normal form. We now claim2 that our notion of groundedness formalises his intuitions, or at least, that it works for all examples he gave! In Dung's argumentation frameworks, the grounded semantics is also defined. Since this is defined as the least fixpoint of the (monotone) characteristic operator, in this case this is the unique grounded fixpoint. However, Strass (2013) showed that this does not generalise to abstract dialectical frameworks, where the grounded extension corresponds to the (ultimate) Kripke-Kleene fixpoint.







6







Discussion







Unfounded Sets. Unfounded sets were first defined by Van Gelder, Ross, and Schlipf (1991) in their seminal paper introducing the well-founded semantics. Their definition slightly differs from Definition 5.1. Definition 6.1 (GRS-Unfounded set). Let P be a logic program and I a three-valued interpretation. A set U   is a GRS-unfounded set of P with respect to I , if for each rule r with head(r)  U , body (r)I = f or body (r)I [U :f ] = f . The first difference between 2-unfounded sets and GRSunfounded sets is that GRS-unfounded sets are defined for three-valued interpretations, while we restricted our attention to (total) interpretations. Our definition easily generalises to three-valued interpretations using Fitting's operator: Definition 6.2 (3-Unfounded set). Let P be a logic program, P Fitting's immediate consequence operator and I a threevalued interpretation. A set U   is a 3-unfounded set of P with respect to I if P (I [U : f ])2  U = . This definition formalises the same intuitions as Definition 5.1: U is a 3-unfounded set if making all atoms in U false results in a state where none of them can be derived. The following proposition relates the two notions of unfounded sets. Proposition 6.3. Let P be a logic program, I a three-valued interpretation and U  . The following hold. * If U is a 3-unfounded set, then U is a GRS-unfounded set. * If I [U : f ] is more precise than I , then U is a GRSunfounded set if and only if U is a 3-unfounded set. We showed that for a certain class of interpretations, the two notions of unfounded sets coincide. Furthermore, Van Gelder et al. only use unfounded sets to define the wellfounded model construction. It follows immediately from Lemma 3.4 in (Van Gelder, Ross, and Schlipf 1991) that every partial interpretation I in that construction with GRSunfounded set U satisfies the condition in the second claim in Proposition 6.3. This means that 3-unfounded sets and GRS-unfounded sets are equivalent for all interpretations that are relevant in the original work! Essentially, we provided a new formalisation of unfounded sets that correctly formalises the underlying intuitions, and that coincides with the old definition on all interpretations used in the original work. Furthermore, our definition is simpler and translates easily to algebra. Corollary 5.5, which states that grounded fixpoints are fixpoints of TP that permit no non-trivial 2-unfounded sets, might sound familiar. Indeed, it has been shown that an interpretation is a stable model of a logic program if and only if it is a fixpoint of TP and it permits no non-trivial







7







Conclusion







In this paper, we defined a new algebraical concept, namely groundedness. We showed that grounded fixpoints behave well with respect to other fixpoints studied in approximation fixpoint theory: given an operator O and an approximator A of O, all A-stable fixpoints are grounded for O and all grounded fixpoints of O are approximated by the A-wellfounded fixpoint. Moreover, grounded fixpoints free us from the need of choosing such an approximator: they are defined directly in terms of the original lattice operator.



2 The journal version of this paper will formally describe the application of our theory to various fields, including a more detailed discussion about the different notions of groundedness, Konolige's intuitions and the relation to grounded fixpoints.







Grounded fixpoint semantics is the first purely two-valued and algebraical semantics for logic programming. Moreover, this semantics is compact, intuitive (directly based on the notion of unfounded sets) and easily extensible: as long as the (two-valued) immediate consequence operator is defined, the grounded fixpoint semantics is obtained for free. Our theory can also be applied to AEL, DL, Dung's argumentation frameworks and ADFs where it also results in a semantics with attractive properties.2 Acknowledgements This work was supported by the KU Leuven under project GOA 13/010 and by the Research Foundation - Flanders (FWO-Vlaanderen).







References



Abiteboul, S., and Vianu, V. 1991. Datalog extensions for database queries and updates. J. Comput. Syst. Sci. 43(1):62-124. Antic, C.; Eiter, T.; and Fink, M. 2013. Hex semantics via approximation fixpoint theory. In Cabalar, P., and Son, T. C., eds., LPNMR, volume 8148 of LNCS, 102-115. Springer. Balduccini, M. 2013. ASP with non-herbrand partial functions: A language and system for practical use. TPLP 13(45):547-561. Bogaerts, B.; Vennekens, J.; Denecker, M.; and Van den Bussche, J. 2014. FO(C): A knowledge representation language of causality. TPLP 14(4-5-Online-Supplement):60- 69. Bogaerts, B.; Vennekens, J.; and Denecker, M. 2014. Grounded fixpoints. Technical Report CW 677, Departement of Computer Science, Katholieke Universiteit Leuven. Denecker, M., and Vennekens, J. 2007. Well-founded semantics and the algebraic theory of non-monotone inductive definitions. In Baral, C.; Brewka, G.; and Schlipf, J. S., eds., LPNMR, volume 4483 of LNCS, 84-96. Springer. Denecker, M., and Vennekens, J. 2014. The well-founded semantics is the principle of inductive definition, revisited. In Baral, C.; De Giacomo, G.; and Eiter, T., eds., KR, 22-31. AAAI Press. Denecker, M.; Bruynooghe, M.; and Vennekens, J. 2012. Approximation fixpoint theory and the semantics of logic and answers set programs. In Erdem, E.; Lee, J.; Lierler, Y.; and Pearce, D., eds., Correct Reasoning, volume 7265 of LNCS. Springer. 178-194. Denecker, M.; Marek, V.; and Truszczy nski, M. 2000. Approximations, stable operators, well-founded fixpoints and applications in nonmonotonic reasoning. In Minker, J., ed., Logic-Based Artificial Intelligence, volume 597 of The Springer International Series in Engineering and Computer Science. Springer US. 127-144. Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2002. Ultimate approximations in nonmonotonic knowledge representation systems. In Fensel, D.; Giunchiglia, F.; McGuinness, D. L.; and Williams, M.-A., eds., KR, 177-190. Morgan Kaufmann.







Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2003. Uniform semantic treatment of default and autoepistemic logics. Artif. Intell. 143(1):79-122. Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2004. Ultimate approximation and its application in nonmonotonic knowledge representation systems. Information and Computation 192(1):84-121. Denecker, M.; Marek, V. W.; and Truszczy nski, M. 2011. Reiter's default logic is a logic of autoepistemic reasoning and a good one, too. In Brewka, G.; Marek, V.; and Truszczy nski, M., eds., Nonmonotonic Reasoning - Essays Celebrating Its 30th Anniversary. College Publications. 111-144. Faber, W.; Pfeifer, G.; and Leone, N. 2011. Semantics and complexity of recursive aggregates in answer set programming. Artif. Intell. 175(1):278-298. Ferraris, P. 2005. Answer sets for propositional theories. In Proceedings of International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR), 119-131. Fitting, M. 2002. Fixpoint semantics for logic programming a survey. Theoretical Computer Science 278(1-2):25-51. Gelfond, M., and Lifschitz, V. 1988. The stable model semantics for logic programming. In Kowalski, R. A., and Bowen, K. A., eds., ICLP/SLP, 1070-1080. MIT Press. Gelfond, M., and Zhang, Y. 2014. Vicious circle principle and logic programs with aggregates. TPLP 14(4-5):587- 601. Kleene, S. C. 1938. On notation for ordinal numbers. The Journal of Symbolic Logic 3(4):pp. 150-155. Konolige, K. 1988. On the relation between default and autoepistemic logic. Artif. Intell. 35:343-382. Lifschitz, V. 2008. Twelve definitions of a stable model. In Garc ia de la Banda, M., and Pontelli, E., eds., ICLP, volume 5366 of LNCS, 37-51. Springer. Marek, V. W.; Niemel a, I.; and Truszczy nski, M. 2008. Logic programs with monotone abstract constraint atoms. TPLP 8(2):167-199. Pelov, N.; Denecker, M.; and Bruynooghe, M. 2007. Wellfounded and stable semantics of logic programs with aggregates. TPLP 7(3):301-353. Son, T. C.; Pontelli, E.; and Elkabani, I. 2006. An unfoldingbased semantics for logic programming with aggregates. CoRR abs/cs/0605038. Strass, H. 2013. Approximating operators and semantics for abstract dialectical frameworks. Artif. Intell. 205:39-70. van Emden, M. H., and Kowalski, R. A. 1976. The semantics of predicate logic as a programming language. J. ACM 23(4):733-742. Van Gelder, A.; Ross, K. A.; and Schlipf, J. S. 1991. The well-founded semantics for general logic programs. J. ACM 38(3):620-650. Vennekens, J.; Gilis, D.; and Denecker, M. 2006. Splitting an operator: Algebraic modularity results for logics with fixpoint semantics. ACM Trans. Comput. Log. 7(4):765-797.







Robust Subspace Clustering via Thresholding Ridge Regression



Xi Peng1 , Zhang Yi2, , and Huajin Tang1,2,



1







Institute for Infocomm Research, Agency for Science, Technology and Research (A*STAR), Singapore 138632 2 College of Computer Science, Sichuan University, Chengdu 610065, P.R. China. pangsaai@gmail.com, zhangyi@scu.edu.cn, htang@i2r.a-star.edu.sg.







In the follow analysis, lower-case bold letters represent column vectors and upper-case bold ones represent matrices. AT and A-1 denote the transpose and pseudo-inverse of the matrix A, respectively. I denotes the identity matrix. Table 1 summarizes some notations used throughout the material.







Table 1: Some used notations.



Notation n m r x  Rm c  Rn D = [d1 , d2 , . . . , dn ] Dx  D D -x Definition the size of the dictionary the dimensionality of sample the rank of a given matrix a data point the representation of x over D a given dictionary x and Dx belong to the same cluster the data points of D except Dx







Intra-subspace Projection Dominance



In this material, we provide the theoretical analyses to show that the trivial coefficients always correspond to the codes over errors. Lemmas 1-3 show that our errors-removing strategy will perform well when the p -norm is enforced over the representation, where p = {1, 2, }. Let x = 0 be a data point in the union of subspaces SD that is spanned by D = [Dx D-x ], where Dx and D-x consist of the intra-cluster and inter-cluster data points, respectively. Note that, noise and outlier could be regarded as a kind of inter-cluster data point of x. Without loss of generality, let SDx and SD-x be the subspace spanned by Dx and D-x , respectively. Hence, there are only two possibilities for the location of x, i.e., in the intersection between SDx and SD-x (denoted by x  {S|S = SDx  SD-x }), or in SDx except the intersection (denoted by x  {S|S = SDx \SD-x }).  Let c x and c-x be the optimal solutions of min c



p







Lemma 1. For any nonzero data point x in the subspace SDx except the intersection between SDx and SD-x , i.e., x  {S|S = SDx \SD-x }, the optimal solution of (1) over D is given by c which is partitioned according to the c sets Dx and D-x , i.e., c = x . Thus, we must have c-x  [c x ]rx ,1 > [c-x ]1,1 . Proof. For the nonzero data point x, suppose there exists a nonzero vector c -x such that



 x = Dx c x + D-x c-x ,







(2)







s.t. x = Dc,







(1)







over Dx and D-x , respectively. * p denotes the p -norm and p = {1, 2, }. We aim to investigate the conditions under which, for every nonzero data point x  SDx , if the p  norm of c x is smaller than that of c-x , then the coefficients over intra-subspace data points are larger than those over  inter-subspace data points, i.e., [c x ]rx ,1 > [c-x ]1,1 (intra subspace projection dominance). Here, [cx ]rx ,1 denotes the rx -th largest absolute value of the entries of c x and rx is the dimensionality of SD . In the following analysis, Lemma 1 and Lemma 3 show  [c x ]rx ,1 > [c-x ]1,1 when x  {S|S = SDx \SD-x } and x  {S|S = SDx  SD-x }, respectively. And Lemma 2 is a preliminary step toward Lemma 3.



*Corresponding Authors Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







 x - Dx c (3) x = D-x c-x ,  Since x  SDx , then x - Dx cx  SDx , i.e., D-x c -x  SDx . Since SDx  SD-x = 0 and x = 0, then c -x = 0 and  cx = 0. This contradicts the assumption c -x = 0. Then,  we must have c x = 0 and c-x = 0 which implies that   [cx ]r0 ,1 > [c-x ]1,1 . This completes the proof.







then







Lemma 2. Consider a nonzero data point x in the intersection between SDx and SD-x , i.e., x  {S|S = SDx  SD-x }. Let c , zx , and z-x be the optimal solution of min c



p







s.t. x = Dc c x c -x







(4)







over D, Dx , and D-x . c =







is partitioned accordp







ing to the sets D = [Dx D-x ]. If zx  [c x ]rx ,1 > [c-x ]1,1 .







< z-x







p,







then







Proof. (=) We prove the result using contradiction. Assume c -x = 0, then Note that, the left side and the right side of (5) correspond a data point from SDx and SD-x , respectively. Then, we must have x = Dx c (6) x + Dx z0 , and x = Dx c (7) x + D-x ze ,   c + z0 c Clearly, x and x are feasible solutions of (4) 0 ze over [Dx D-x ]. According to the triangle inequality and the condition z0 p < ze p , we have c x + z0 0  c x



p  x - Dx c x = D-x c-x .







(5)







+ z0



p







p







< c x



p







p







+ ze







p.







(8)







p







Proof. Since x  {S|S = SDx  SD-x }, we could write T T is the skinny z , where Dx = Ur0 r0 Vr x = Ur0 r0 Vr 0 0 0 SVD of Dx , r0 = diag(1 (Dx ), 2 (Dx ), * * * , r0 (Dx )), r0 is the rank of Dx , and z0 is the optimal solution of (10) 1 T over Dx . Thus, z0 = Vr0 - r0 Ur0 x. From the propositions of p-norm, i.e., z   z 1  n z  , z   z 2  n z  , and z 2  z 1   n z 2 , we have   1 T z0 p  z0 1  r0 z0 2 = r0 Vr0 - r0 Ur0 x 2 . (12) Since the Frobenius norm is subordinate to the Euclidean vector norm, we must have  1 T z0 p  r0 Vr0 - r0 Ur0 F x 2  r0 = x 2 2 (D ) + * * * +  2 (D ) 1 x x r0 where min (Dx ) = r0 (Dx ) is the smallest nonzero singular value of Dx . Moreover, x could be represented as a linear combination of D-x since it lies in the intersection between SDx and SD-x , i.e., x = D-x ze , where ze is the optimal solution of (10) over D-x . Multiplying two sides of the equation with xT , it gives x 2 = xT D-x ze . According to the H older's inequality, we have x



2 2 -1  min (Dx ) x 2







as ze p is the opc x + z0 timal solution of (4) over D-x . Then, < 0 p c c x x . It contradicts the fact that is the opti c c -x - x p p mal solution of (4) over D. (=) We prove the result using contradiction. For a nonzero data point x  {S|S = SDx  SD-x }, assume z0 p  ze p . Thus, for the data point x = x, it is possible that (4) will only choose the points from SD-x to represent x. This contradicts the assumption that c x = 0 and c -x = 0 . This completes the proof. Definition 1 (The First Principal Angle). Let  be a Euclidean vector-space, and consider the two subspaces W , V with dim(W ) := rW  V := rV . There exists a set of anW gles {i }r i=1 called the principal angles, the first one being defined as: min := min arccos T   2  ,



2







From (7), we have ze







 c -x







(13)







According to the definition of the first principal angles (Definition 1), we have DT -x x



 T = max [D-x ]T 1 x , [D-x ]2 x , * * *







 DT -x x















ze 1 ,







(14)







(9)







where [D-x ]i denotes the ith column of D-x , min is the first principal angle between SDx and SD-x , and D-x 1,2 denotes the maximum 2 -norm of the columns of D-x . Note that the smallest principal angle between any two subspaces always greater than zero, hence, cos min  [0, 1). Combining (14) and (15), it gives x hence,



2 2







 cos min D-x







1,2







x 2,







(15)







where   W and   V . Lemma 3. Consider the nonzero data point x in the intersection between SDx and SD-x , i.e., x  {S|S = SDx  SD-x }, where SDx and SD-x denote the subspace spanned by Dx and D-x , respectively. The dimensionality of SDx is rx and that of SD-x is r-x . Let c be the optimal solution of min c p s.t. x = Dc (10) over D = [Dx D-x ] and c = cording to the sets Dx and D-x . If



 then [c x ]rx ,1 > [c-x ]1,1 . Here, min (Dx ) is the smallest nonzero singular value of Dx , min is the first principal angle between Dx and D-x , D-x 1,2 is the maximum 2 norm of the columns of D-x and [c]r,1 denotes the r-th largest absolute value of the entries of c.







 cos min D-x ze



1







1,2







x







2







ze 1 ,







(16) (17)







x 2 . cos min [D-x ]1,2 From the propositions of p-norm, we have  ze Let z0



p p p,







c x be partitioned acc -x



1,2 ,















x 2 . cos min [D-x ]1,2 x 2 , cos min [D-x ]1,2







(18)







< ze







then



2







min (Dx )  r-x cos min D-x







(11) then,







-1 min (Dx ) x







<







(19)







min (Dx ) > cos min [D-x ]1,2 . (20)   It is the sufficient condition for [cx ]r0 ,1 > [c-x ]1,1 since  it implies c x = 0 and c-x = 0 (Lemma 2). This completes the proof.







It should be pointed out that the above proofs are motivated by the theoretical analysis in (Elhamifar and Vidal 2013), but they are different. (Elhamifar and Vidal 2013) provides the conditions under which SSC (1 -norm) will only chose the intra-subspace data points to represent the input when data lies onto the union of independent subspaces or disjoint subspaces, while our theoretical analyses investigate the conditions under which p -norm-based coefficients with small value correspond to the projections over the errors even though the data come from dependent subspaces, where p = 1, 2, . Moreover, the most different point between these two works may be that, we theoretically show that the effect of errors could be eliminated by removing trivial coefficients in the projection space, whereas (Elhamifar and Vidal 2013) aims to prove the success of 1 -normbased representation in the input data space.







References



Elhamifar, E., and Vidal, R. 2013. Sparse subspace clustering: Algorithm, theory, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence 35(11):2765- 2781.







IBM Research, Ireland







University of California, Irvine







Anytime Best+Depth-First Search for Bounding Marginal MAP



Radu Marinescu



IBM Research - Ireland







Junkyu Lee, Alex Ihler and Rina Dechter



University of California, Irvine







AAAI 2017 Technical Session: RU: Reasoning under Uncertainty Feb. 8th. 2017 10:00 am - 11:00 am Oral Presentation Paper 2066







IBM Research, Ireland







University of California, Irvine







Motivation and Contribution











Marginal MAP Inference



-







Probabilistic inference query











Optimal partial configuration after marginalizing hidden/latent variables in a probability distribution







- -







Complexity: NPpp complete Often it is the right task on various applications











Probabilistic conformant planning [Lee, Marinescu, Dechter, 2015] Natural language processing task [Bird, Klein, Loper, 2009] Image completion task [Xue, Li, Ermon, Gomes, Selman, 2016]































Contributions



- -







Anytime hybrid (best+depth-first) search for MMAP Improvement of anytime performance for finding upper and lower bounds











Upper-bound: estimate of optimal solution from a partial solution Lower-bound: sub-optimal solution















2







IBM Research, Ireland







University of California, Irvine







Outline











Background



- - -







Graphical model AND/OR search space & WMB heuristic Previous MMAP search algorithms















Best+Depth-First search for MMAP



- - -







LAOBF (Best-First AND/OR Search with Depth-First Lookaheads) AAOBF (Alternating Best-First and Depth-First AND/OR search) LnDFS (Learning Depth-First AND/OR search)















Experiments Conclusion



3















IBM Research, Ireland







University of California, Irvine







Background - graphical model











Graphical model



- - -















Primal graph



- -







variables domains functions







nodes are variables two nodes are connected if they appear in the same function E C















Marginal Map task



-







A







B







H







F Max and sum not commute







D







G







-







4







IBM Research, Ireland







University of California, Irvine







Background - AND/OR search space











Bucket elimination



MAX







[Dechter, 1999]















AND/OR search graph



A



0 1







[Mateescu, Dechter, 2007]







B



0 1 0







B



1







C



0 1 0







D



1 0







C



1 0







D



1 0







C



1 0







D



1 0







C



1 0







D



1







E



0 1 0







E



1







G



0 1 0







F







E



1 0 1 0







E



1







G



0 1 0







F



1







E



0 1 0







E



1







G



0 1 0







F







E



1 0 1 0







E



1







G



0 1 0







F



1







H



0







H



1 0 1







H



0







H



1 0 1







SUM











Pseudo tree



[Freuder, Quinn, 1985]







5







IBM Research, Ireland







University of California, Irvine







Background - WMB heuristics











Mini-bucket elimination [Dechter, Rish 2001]



-















Weighted Mini-bucket [Liu, Ihler, 2012]



-







"i-bound", limit on the number of variables in a single mini-bucket







Holder's inequality







MAX















WMB Moment Matching



-







MAP variables







[Liu, Ihler, 2011] [Marinescu,Ihler,Decther, 2014]







SUM



-







-







SUM variables







Mini-bucket upper bound







6







IBM Research, Ireland







University of California, Irvine







Previous MMAP search algorithms



Park, Darwiche Depth-First BnB Join-tree upper bound (relaxed variable ordering)



- depth-first search - dynamic heuristic







Marinescu, Decther, Ihler Depth-First BnB AND/OR Search WMB Heuristic



- compact AND/OR search space - more accurate WMB heuristics







Lee,Marinescu, Decther, Ihler Weighted Best-First Anytime Depth-First AND/OR WMB heuristic



- anytime solutions - infrequent solution updates - still memory intensive







2003 2009







2014 2015







2016 2017







Yuan, Hansen Depth-First BnB Incremental Join-tree upper bound



- static heuristic







Marinescu, Decther, Ihler Best-First/Recursive BF AND/OR Search WMB heuristic







Marinescu, Lee, Iihler, Decther Best+Depth-First - high quality upper/lower bounds - more frequent solution updates - memory efficiency







- BF avoids solving summation problems - very memory intensive - no anytime, return optimal solution or no solution 7







IBM Research, Ireland







University of California, Irvine







Outline











Background



- -







Graphical model AND/OR search space & WMB heuristic















Best+Depth-First search for MMAP



- - -







LAOBF (Best-First AND/OR Search with Depth-First Lookaheads) AAOBF (Alternating Best-First and Depth-First AND/OR search) LnDFS (Learning Depth-First AND/OR search)















Experiments Conclusion



8















IBM Research, Ireland







University of California, Irvine







Best+Depth-First Search











Depth-First search











Better guidance for depth-first dives using improved heuristics Frequent solution updates







Best-First search























Lower bound







Cutoff frontier of best-first search using improved lower bounds Learn accurate heuristics by depth-first lookahead







When Global UB = Global LB, Optimal Solution Discovered



9







IBM Research, Ireland







University of California, Irvine







Notations - solution tree



MAX



OR AND OR AND OR AND OR AND OR AND







partial solution tree







tip of partial solution tree







solution tree







10







IBM Research, Ireland







University of California, Irvine







Notations - basic operations



OR AND OR AND OR AND OR AND OR AND







MAX



q(n), l(n)



 







Expand(n)







Update(n)







q(n) : upper bound at n q(root) : global upper bound l(n) : lower bound at n l(root) : global lower bound : best partial solution tree (partial solution tree where OR nodes direct the child m with best q(m)







 























backup q and l values















re-direct best partial solution tree



11







IBM Research, Ireland







University of California, Irvine







LAOBF (best-first AND/OR search with depth-first lookaheads)



Best-first selection Depth-first lookahead Best-first expansion & update







  







depth-first dive at the tip of compute global lower bound cache summation subproblems







 







Select a tip node n Expand and Update n







cutoff parameter: control depth-first lookahead (at every







number of node expansions.)







12







IBM Research, Ireland







University of California, Irvine







AAOBF (alternating best-first with depth-first AND/OR search)



Depth-first greedy expansion Best-first re-direct Depth-first selection Best-first selection Depth-first re-direct Best-first expansion & update















Expand(n) and Update(n) depth-first greedy search















redirect from explicated search graph from the root with updated q and l select Expand and Update a tip node



13























IBM Research, Ireland







University of California, Irvine







LnDFS (learning depth-first AND/OR search)



Best-first update Best-first selection Depth-first expansion







Keep expanding tips nodes of Update values from tip nodes of



14







IBM Research, Ireland







University of California, Irvine







Outline











Background



- -







Graphical model AND/OR search space & WMB heuristic















Best+Depth-First search for MMAP



- - -







LAOBF (Best-First AND/OR Search with Depth-First Lookaheads) AAOBF (Alternating Best-First and Depth-First AND/OR search) LnDFS (Learning Depth-First AND/OR search)















Experiments Conclusion



15















IBM Research, Ireland







University of California, Irvine







Experiments











Anytime Algorithms



-







Presented Best+Depth-First Search











LAOBF AAOBF LnDFS Weighted Recursive Best-First AND/OR Search with Overestimation Breadth Rotate AND/OR Branch and Bound Anytime Factor Set Elimination



[Lee, Marinescu, Ihler, Dechter, 2016]























-







State-of-the-art



















[Lee, Marinescu, Ihler, Dechter, 2016]















[Maua, Campos, 2012]















Memory



- - -







total 24 GB WMB-MM(i) i-bound: 20 or the largest within 4 GB caching for AND/OR search graph max 4 GB



16







IBM Research, Ireland







University of California, Irvine







Experiment











Benchmark



- - - -







derived from UAI inference competitions for MPE query randomly choose 50% of the variables as MAP variables generate 4 random MMAP instances Grid, Pedigree, Promedas domain















Problem instance parameters







Domain (#. instances) Grid (128) Pedigree (88) Promedas (100) 144,649,2500 144,649,2500 2,2 3,3 25,163,814 42,189,834







334,917,1289







334,917,1289







3,7







4,5







35,127,289







63,152,312







381,1064,1997







385,1077,2024







2,2







3,3







11,137,552







33,171,577







N: number of variables, W: constrained induced width,







F: number of functions, H: constrained pseudo tree height







K: domain size,







S: scope size



17







IBM Research, Ireland







University of California, Irvine







Experiment - individual instances











Anytime search status for individual instances







N:2500 F:2500 K:2 S:3 W:788 H:817







N:1183 F:1183 K:5 S:5 W:272 H:290







N:1675 F:1701 K:2 S:3 W:259 H:298







- search: LAOBF (lab), AAOBF (aab), LnDFS (ldt), BRAOBB (bra) - heuristic: WMB-MM (20) - memory: 24 GB Other algorithms couldn't find any solution due to memory out



18







IBM Research, Ireland







University of California, Irvine







Experiment - average solution quality











Average solution quality



- -







anytime quality of lower bound normalized by optimal solution when optimal solution is not available, used best-known solution















Result



- -







How the quality of solution improves over time LAOBF, AAOBF, LnDFS











improved upon WRBFAOO on 3 domains best on promedas domain, second worst on pedigree domain



19







-







BRAOBB











-







AFSE: worst performance on 3 domains







IBM Research, Ireland







University of California, Irvine







Experiment - average gap quality











Average gap quality



-







anytime gap (difference between upper and lower bound) normalized by upper bound (If no lower bound available, gap = 1)















Result



- -







How the gap between lower/upper bound decreases over time (gap=0 optimal) LAOBF, AAOBF, LnDFS











All similar improvements over time, especially at shorter time bounds AAOBF was overall best















-







AFSE: worst performance on 3 domains



20







IBM Research, Ireland







University of California, Irvine







Experiment - memory robustness











Memory robustness







- - - - -







How search algorithm effectively utilized the memory and improves gap within the memory limit % of instances terminated by memory limit % of instances terminated by memory limit and no solution found at all average gap computed from out of memory instances only average search time computed from out of memory instances















Result



- - -







LnDFS is the most memory robust algorithm AAOBF (LAOBF) improved memory robustness compared to WRBFAOO AFSE is the worst among 5 algorithms



21







IBM Research, Ireland







University of California, Irvine







Conclusion











Anytime Best+Depth-First search algorithms improved upon the state-of-the-art algorithms



- - -







higher quality anytime solutions tighter anytime upper bounds more effective use of memory















Future work



-







New anytime search + approximate summation inference



 







variational bounds with search probabilistic bounds from sampling







22







Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence







Bayesian Approach to Modeling and Detecting Communities in Signed Network



Bo Yang, Xuehua Zhao, and Xueyan Liu



School of Computer Science and Technology, Jilin University, Changchun, China Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, China ybo@jlu.edu.cn







Abstract



There has been an increasing interest in exploring signed networks with positive and negative links in that they contain more information than unsigned networks. As fundamental problems of signed network analysis, community detection and sign (or attitude) prediction are still primary challenges. To address them, we propose a generative Bayesian approach, in which 1) a signed stochastic blockmodel is proposed to characterize the community structure in context of signed networks, by means of explicitly formulating the distributions of both density and frustration of signed links from a stochastic perspective, and 2) a model learning algorithm is proposed by theoretically deriving a variational Bayes EM for parameter estimation and a variation based approximate evidence for model selection. Through the comparisons with state-of-the-art methods on synthetic and real-world networks, the proposed approach shows its superiority in both community detection and sign prediction for exploratory networks.







Introduction



In recent years, the study of signed networks becomes a burgeoning research area. In contrast to the extensively studied unsigned networks only encoding whether relationships exist or not, signed networks contain more information by extending the single relationship to positive and negative relationships, wherein positive ones represent to like, trust, support or collaborate and negative ones represent to dislike, distrust, oppose or compete, among others. For signed networks, community detection is of considerable importance for understanding the basic patterns of structure and dynamics. This task is trying to identify K antagonistic communities, so that most links within communities are positive while most links between communities are negative. In this sense, communities are consistent with the clusters defined in balance theory in social science (Cartwright and Harary 1956; Davis 1967), where a strongly (or weakly) balanced network can be divided into two (or K ) clusters, so that all links within clusters are positive and all links between clusters are negative. Note that, real-world signed networks are usually



Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







unbalanced due to the frustration in them, i.e., negative links within clusters and positive links between clusters. Although many methods have been proposed to address community detection since Girvan and Newman's work (Girvan and Newman 2002), however, most of them are exclusively designed for unsigned networks, which focus on link density rather than link sign to define and detect communities.Therefore, the primary techniques adopted by them cannot be directly applied to signed networks, such as modularity optimization (Newman 2004), Markov random walk (Zhou 2003), clique percolation model (Palla et al. 2005), spectral analysis (Mitrovi c and Tadi c 2009), evolutionary optimization (Pizzuti 2008), among many others. In view of this, new methods have been proposed for signed community detection. On one hand, some of them are studied from the perspective of social science. For instance, based on the social balance theory, Doreiian and Mrvar proposed a frustration-optimization based method, referred to as DM, which partitions a signed network by minimizing the sum of negative link quantity within communities and positive link quantity between communities (Doreian and Mrvar 1996). Thereafter, Larusso et al improved the same idea to partition weighted signed networks (Larusso, Bogdanov, and Singh 2010). Very similarly, Bansal et al proposed a correlation clustering method to maximize the agreement (i.e. the number of positive intra-cluster links and negative intercluster links) or to minimize the disagreement (i.e. the number of negative intra-cluster links and positive inter-cluster links) among nodes (Bansal, Blum, and Chawla 2004). On the other hand, some of them are proposed by means of generalizing the current techniques of partitioning unsigned networks as mentioned above. For examples, based on potts model Traag et al deduced an improved modularity function for signed networks and then proposed a modularity-optimization based partition algorithm PSA (Traag and Bruggeman 2009). Yang et al generalized the Markov stochastic process on unsigned network to signed network and then proposed an improved random-walk based method FEC (Yang, Cheung, and Liu 2007). Huang et al improved the clique percoltion model to detect overlapping signed communities (Huang and Qiu 2010). Anchuri et al proposed a generalized spectral method for signed network partition (Anchuri and Magdon-Ismail 2012). Very recently, multi-objective evolutionary methods have been applied to







1952







signed network decomposition, by simultaneously optimizing two objectives defined in terms of not only link density but also link sign, e.g. internal similarity versus external similarity (Liu, Liu, and Jiang 2014) and kernel k-means versus ratio-cut (Gong et al. 2014). All the aforementioned methods can be seemed as discriminative, which just focused on looking for a way to distinguish notes by clustering them into different groups, based on either predefined optimization objectives (such as modularity) or heuristics (such as random walk model). However, they are not concerned with how the real-world signed networks containing community structures are generated. Distinctly, in this work we plan to propose a generative approach. Compared with discriminative methods, generative methods are more expected because they can be applied to not only community detection but network modeling, generation, as well as link and attitude prediction. Being one important statistical network model, stochastic blockmodel (SBM) is a good generative model. As it enables us to reasonably decompose and then properly analyze an exploratory network without a priori knowledge about its intrinsic structure, SBM has attracted more and more attention since it was originally proposed by Holland and Leinhardt (Holland and Leinhardt 1981). Although various extensions of SBM have been proposed to address different tasks of network analysis, such as multiple role SBM (Airoldi et al. 2008), overlapping SBM (Latouche et al. 2011), dynamical SBM (Yang et al. 2011) and hierarchical SBM (Yang, Liu, and Liu 2012), however, to the best of our knowledge, all the existing SBMs are designed for unsigned networks and thereby incompetent for handling signed networks. In view of this, we are motivated to propose a novel generative Bayesian approach. More specifically, our main contributions are two-fold: (1) We proposed a signed stochastic blockmodel to characterize and generate the block structures of signed networks by means of explicitly formulating both link density and link sign from a stochastic perspective. (2) We proposed an effective algorithm for learning this model from exploratory networks based on variational Bayes techniques, which can automatically detect block numbers and assignments.







Given a signed network, one can deduce a latent n x K matrix Z , indicating the relationship between node and block assignment. zik = 1 if node i is assigned to block k , otherwise zik = 0. Moreover, zi follows the following multinomial distribution with a parameter : zi  M (1,  = {1 , 2 , ..., K }) Given Z , aij follows the following multinomial distribution with parameters  and : aij  M (1,  = {1 , -1 , 0 }), ziq zjl = 1 and q = l aij  M (1,  = {1 , -1 , 0 }), ziq zjl = 1 and q = l According to SSBM, one can generate a synthetic signed network with a block structure by following steps: 1) assign nodes to blocks according to . 2) generate positive and negative links between nodes within the same blocks according to . 3) generate positive and negative links between nodes belonging to different blocks according to . Accordingly, we have proofed that the log-likelihood of complete data is as follows:



n K







logp(N, Z |K ) =



i=1 q =1







ziq logq +



i<j q,l







(ziq zjl x







(2)







logM (aij ; ) + (1 - ziq zjl ) log M (aij ; )) We now describe the aforementioned SSBM in a full Bayesian framework. In the framework, we need specify the priors for the model parameters (, , ). Since p(zi |), p(aij |Z, ) and p(aij |Z, ) satisfy multinomial distribution, respectively, we select Dirichlet distribution as their conjugate prior distributions, as follows:



0 0 p(| 0 = {0 1 , ..., K }) = Dir(;  ) 0 0 0 p(| 0 = {1 , -1 , 0 }) = Dir(;  0 ) 0 0 0 p(| 0 = {0 1 , -1 , 0 }) = Dir(;  ) 0 0 where q :0 q , h: h , h:h are hyperparameters, which are interpreted as an effective pseudo-occupations of respective blocks in the prior, pseudo-observations of three types of links (positive, negative, no-link) within or between blocks in the prior, respectively. In other words, in the full Bayesian framework, parameters , , and  are regarded as random variables, the distributions of which depend on their respective hyperparameters. Being a generation of standard SBM (Snijders and Nowicki 1997), SSBM is much more flexible and it is able to depict more structural patterns of unsigned or signed networks, as defined in terms of either link density or link sign or both of them, from a stochastic perspective. For examples: (1) in the case of -1 = 0 and -1 = 0, SSBM is able to characterize either the community structure (when 1 > 1 ) or the multipartite structure (when 1 < 1 ) of unsigned networks in terms of link density; (2) in the case of -1 = 0 while 1 = 0, SSBM is able to characterize a balanced signed network in terms of link sign; (3) in the most general case of -1 = 0 while 1 = 0, SSBM is able







Signed Stochastic Blockmodel



Let A denote the adjacency matrix of a signed network N containing n nodes. aij is equal to 1 or -1 if node i is connected to node j by a positive or negative link. Otherwise, aij will be zero. The signed stochastic blockmodel (SSBM for short) of N is defined as a 4-tuple: X = (K, , , ) (1) K is the number of blocks.  is a K -dimension vector, wherein q denotes the prior probability that a node is assigned to block q .  = (1 ,-1 , 0 ) is a 3-dimension vector, in which each component denotes the probability that there is a positive link, negative link, or no link between a pair of nodes within the same block, respective. Similarly, we define  = (1 ,-1 ,0 ), in which each component denotes the probability that there is a positive link, negative link, or no link between a pair of nodes belonging to different blocks.







1953







to characterize the frustration of an unbalanced signed network in terms of both density and sign, in which there are a small fraction of negative links within communities and a small fraction of positive links between communities.







Variational Bayes SSBM Learning



Now we introduce SSBM learning algorithm (SSL for short). SSL adopts a variational Bayes EM algorithm to estimate parameters and an approximate Bayesian model evidence for model selection. We adopt such variational techniques due to two main reasons. First of all, standard EM algorithm cannot be directly used for SSBM in that the components of latent variable Z are correlated and thus the posterior distribution of Z , under the condition of data and model parameters, cannot be explicitly derived as an input required by standard EM. More specifically, component zi is correlated to others means that the computation of its posterior distribution P (zi |N, , , ) is recursively dependent on the distribution of zj for any j = i. Using variational Bayes EM, one can infer an approximate posterior distribution of Z in terms of estimated superparameters. Note that, in the literature, variational EM has ever been adopted for SBM learning (Daudin, Picard, and Robin 2008). Unlike variational EM based on point estimation, variational Bayes EM infers the distribution of Z based on the distributions of parameters instead of their point estimation values (or maximum likelihood estimation values). Therefore, compared with variational EM, variational Bayes EM is more robust and is expected to infer a better posterior distribution close to the truth from real-world networks usually containing much noise. In addition, although the Bayesian model evidence of network N (i.e. log p(N |K )) can be obtained by computing the marginal integration of log p(N, Z |K ) (see Eq. 2) over Z , however, this computation involves a summation of K n terms, which will quickly becomes prohibitively intractable. By taking the model parameters (,,) as random variables, bases on variational Bayes techniques one can readily compute a lower bound of the marginal likelihood in terms of their superparamters, as an approximation of true evidence, for model selection. Superparameter estimation The log-likelihood of N (or the marginal log-likelihood of complete data) can be decomposed into two terms: log p(N ) = L(q (*)) + KL(q (*)||p(*|N )) where L(q (*)) =



Z







In Eqs.3 and 5, KL denotes the Kullback-Leibler divergence between q (Z, , , ) and p(Z, , , |N ). To minimizing Eq.5 with respect to q (Z, , , ) is equivalent to maximizing the lower bound Eq.4. To obtain a computationally tractable algorithm, we use mean field approximation, one of the most popular forms of variational inference, in which we assume the posterior q (Z, , , ) is a fully factorized approximation. Formally, we have:



n







q (Z, , , ) = q ()q ()q ()



i=1







q (zi )







(6)







where q (), q (), q () and q (zi ) denote the posteriors of variables , ,  and Z , respectively, which will be inferred by a variational Bayes EM. Specifically, in its E-step, each distribution q (zi ) is optimized; and in its M-step, q (), q () and q () are optimized, respectively. We first derive the optimal approximation at node i. According to variational Bayes, the optimal posterior q (zi ) is: log q (zi ) = EZ \i ,,, log p(N, Z, , , ) + const = EZ \i ,, log p(N |Z, , ) + EZ \i , log p(Z |) + const



K







=



q =1







ziq



j =i







jq



h







 (aij , h)( (h ) -  (



h







h )) h ))







(7)







+



l=q







jl



h







 (aij , h)( (h ) -  (



h







+ ( (q ) -  (



k







k )) + const







where Z \i denotes Z of all nodes except node i,  (a, h) =1 * I{a=h} +0 * I{a=h} , and h  {1, -1, 0}. When y  Dir(y ; a1 , a2 , ..., aK ), Ey [log(y )] =  (aq ) -  ( aq ) where q  {1, 2, ..., K } and  (*) is Digamma function. To simplify calculations, the terms that do not depend on Zi have been absorbed into the constant. After taking the exponential of Eq.7 and normalization, the optimal approximation at node i is the following multinomial distribution: q (zi ) = M (zi ; 1, i1 , ..., iK ) (8)







(3)







where iq is the probability of node i belonging to block q , and satisfies:



n







q (Z, , , ) x log p(N, Z, , , ) d d d q (Z, , , ) q (Z, , , )x



Z







iq e (4)







 (q )- (







k







k ) j =i







ejq



K







h







 (aij ,h)( (h )- (







h







h ))







x



l=1







ejl







h







 (aij ,h)( (h )- (







h







h ))







KL(q (*)||p(*|N )) = -







(9) (5) Then, we derive the posteriors q (), q (), q () by optimizing the lower bound (see Eq.4), respectively. According







p(Z, , , |N ) log d d d q (Z, , , )







1954







n







to variational Bayes, the optimal distribution q () is: log q () = EZ,, log p(N, Z, , , ) + const = EZ log p(Z |) + log p() + const



K n







+ E logp() + E logp() -



i







Ezi logq (zi )







- E logq () - E logq () - E logq ()



n K







(10) iq log q + const







=



h







0 h - h + i<j q =1 n K







iq jq  (aij , h)  (h ) -  (



h







h)







=



q =1







0 q -1+



i=1







+







After taking the exponential of Eq.10 and normalization, we obtain the optimal approximation of q (), i.e., a Dirichlet distribution, which is the same form as its prior p().



n







0 h - h +



h K i<j q =l







iq jl  (aij , h)  (h ) -  (



h n







h )







+



q =1 n K







0 q - q +



i=1







iq ( (



K q =1 K q =1







 (q ) -  (



q







q )







q () = Dir(;  ),







q = 0 q +



i=1







iq







(11)







-



i=1 q =1







iq log iq + log ( (



h 0 h ) h h )







0 q) q )



h







K q =1 K q =1 h h







(q ) (0 q) (h ) (0 h)







In the same way, we obtain q () and q (), two Dirichlet distributions, which are the same form as their priors.



n K







+ log







(h ) 0 h (h )



h







( (







0 h) h h )







q () = Dir(;  ),







0 h = h + i<j q =1







iq jq  (aij , h) (12)



n K







According 0 h - h +



n i<j







to



n i<j







Eqs.11,12 and 13, the terms K 0 q =1 iq iq  (aij , h), h - h +



n







q () = Dir(;  ), h = 0 h + Eh -



i<j q =1







iq jq (aij , h) (13)







and 0 q - q + i=1 iq in the lower bound vanish. So, finally the low bound is: L(q (*)) = log ( ( ( (



h







K q =l iq jl  (aij , h),







( (







where Eh denotes the number of the positive, negative and no link in the network, respectively. Evidence approximation and model selection So far we have derived the approximated posteriors of model parameters and latent variables. However, the problem of automatically determining block number K has not been addressed, which is significant for exploring realworld networks, in that we usually have not a prior knowledge about K . According to Bayesian theory, an optimal model should be the one with the largest evidence. Formally, the evidence of SSBM is log P (N |K ) = log{ Z P (N, Z, , , |K )d d d }. Unluckly, the computation of SSBM evidence is intractable in that for each value of K , it involves a multiple integration over all possible values of parameters and latent variables. To tackle this issue, we plan to approximate the evidence by its lower bound, as suggested by (Hofman and Wiggins 2008). Recall Eq.3, an evidence is the sum of lower bound (Eq.4) with respect to q (*) and KL divergence (Eq.5). After the convergence of minimizing KL divergence by variational Bayes EM, q (*) is expected to be close to true posterior distribution, or in other words, the KL divergence is expected to be much smaller than the lower bound, thereby the evidence can be approximated by its lower bound with a small error, which can be seemed as the model selection criterion of SSBM. The formula of calculating the lower bound in terms of estimated posteriors of latent variables and parameters is derived as follows:



L(q (*)) =



Z







K q =1 K q =1 h







0 q) q )







K q =1 (q ) K 0 q =1 (q )







+ log







0 h )  ) h h







(h ) 0 h (h )



h (h ) 0 h (h ) n K







(14) -



i=1 q =1







+log







0 h) h h )



h







iq log iq







SSL Algorithm In summary, the algorithm of SSBM learning based on variational Bayes approach is given in Table 1, which can automatically detect the block structure of a given signed network. Next, we analyze its time complexity. Updating the posterior of Z by for loop in line 07-09 takes O(Kn2 ). Updating the posterior of  by for loop in line 10-11 takes O(Kn). Updating the posteriors of  and  by for loop in line 12-14 takes O(Kn2 ). Consequently, when K is given, the time of SSL is O(IKn2 ), where I denotes the iterations of repeat loop until convergence. Calculating the lower bound LK in 16 takes O(Kn). So, when K is unknown, the total time of SSL is O(In2 (Kmax - Kmin )2 ).







Validation



In this section, we test the proposed SSBM and SSL toward two main tasks: community detection and sign prediction.







Validation on community detection



In showing the superiority of SSBM and SSL, three representative algorithms for signed community detection are selected to compare. They are the frustration-optimization based DM (Doreian and Mrvar 1996), the random-walk based FEC (Yang, Cheung, and Liu 2007), and the modularity-optimization based PSA (Traag and Bruggeman 2009), respectively. We use both synthetic networks and







q (Z, , , )log







p(N, Z, , , ) d d d q (Z, , , )







= EZ,, logp(N |Z, , ) + EZ, logp(Z |) + E logp()







1955







i oc oe oe oe oe oi UUY UO I IIO i







i oe oe oi oi UUY UO I IIO i







i oc oe oe UUY UO I IIO i







ioe ioe ioi ioi ioi







 i







i







oi oi oi oi oi oe oe oe oe oc *







 oi oi oi oi oe oe oe oe oc o







oe oi oi oi oi oe oe oe oe oc o







i







i







O







e







e







i







(a)







(b)







(c)







(d)







Figure 1: Performance comparisons of community detection. models are used, i.e. SG(4, 300, 100, 0.8, p- , 0.2) and SG(4, 300, 100, 0.8, 0.2, p+ ), in which p- and p+ gradually increase from 0 to 1 stepping by 0.1, respectively. The two models are used to test the influence of two types of noise on the performance of community detection. For each model mentioned above, we generate 100 random networks. Fig. 1(a) shows the performance of four algorithms on balanced networks. As we can see, SSL and PSA perform the best. For all pin , the detections provided by these two algorithms are exactly the same as ground truth (i.e. NMI=1). Compared with DM and FEC, this result implies a good feature of SSL and PSA. That is, when handling balanced networks, the performance of these two algorithms will be not affected by the link density within communities. Figs. 1(b) and 1(c) show the performance of four algorithms on unbalanced networks. In Fig. 1(b), p+ is fixed and the noisy level within communities augments as p- increasing. As we see, the performance of SSL is significantly better than other three, and the detections provided by it are exactly the same as ground truth except for p- > 0.9. In Fig. 1(c), p- is fixed and the noisy level outside communities augments as p+ increasing. In this case, SSL, FEC and PSA, particularly the first two, performs much better than DM. The main reason is, as the fraction of positive links across communities (i.e. p+ ) increasing, the signed network being handled gradually turn into an unsigned network, in which community structure are dominated by link density. Compared with DM that focuses on optimizing the frustration of signs, the other three consider not only link sign but also link density when they are partitioning a network, thereby leading to a much better performance in this case. From these results, one notes that SSL performs the best when handling unbalanced networks contain different types and different levels of noise. The rationale is two told: 1) SSBM explicitly models such noise with parameters such as -1 and 1 ; and 2) SSL adopts variational Bayes to estimate the distributions rather than point values of such parameters. Fig. 1(d) shows the model selection process of SSL, in which y -axis denotes the minus evidence corresponding to different K . As an example, we just show the interval of K from 1 to 10. As we see, the evidence reaches its biggest value when K = 4, exactly the same as the truth. Modularity-optimization based methods such as PSA will suffer the problem of resolution limitation. That is, such methods tend to detect a small number of bigger commu-







Table 1: SSL Algorithm



X=SSL(N ,Kmin ,Kmax ) 01 Input: N, Kmin , Kmax 02 Output: Z 03 initialize L 04 for K = Kmin to Kmax 05 initialize , , , ; 06 repeat 07 for i = 1 to N // Update the posterior over each zi 08 for q = 1 to K 09 update iq according to Eq. 9; 10 for q = 1 to K // Update the posterior over  11 update q according to Eq. 11; 12 for h  {1, -1, 0} // Update the posterior over ,  13 update h according to Eq. 12; 14 update h according to Eq. 13; 15 until convergence 16 update LK according to Eq. 14; // Update the lower bound 17 if LK > L then L = LK ; Zp =  ; 18 calculate Z according to Zp ;







real-world networks to test the four algorithms. Since all test networks contain ground truth community structures, the NMI criterion (Kuncheva and Hadjitodorov 2004) is adopted to quantitatively measure the accuracy of community detections. Intuitively, the larger NMI, the closer to ground truth. Synthetic signed networks We first use synthetic networks to test. Although the proposed SSBM is a generation model of signed networks, for the sake of fairness, here we choose a widely used model (Yang, Cheung, and Liu 2007) to produce synthetic signed networks, which is defined as: SG(c, n, k, pin , p- , p+ ) where c is the number of communities, n is the number of nodes in each community, k is the average degree of node, pin is the probability of each node connecting other nodes in the same community. p- and p+ regulate noise levels, denoting the probabilities of negative links within communities and positive links across communities, respectively. First, we generate two types of synthetic signed networks: balanced networks and unbalanced networks. For balanced networks, the generation model is SG(4, 300, 100, pin , 0, 0), in which pin increases from 0.1 to 1 stepping by 0.1. For unbalanced networks, two







1956







I(R) IIO I







15 16 17 18 19 20 21 22 23 24 25 15 16 17 18 19 20 21 22 23 24 25 15 16 17 18 19 17 14 10 9 10 27







Figure 2: Resolution limitation test. nities. Next, we will test whether the proposed Bayesian approach is able to fix this issue. In this experiment, the network to be tested is similar to the one suggested by (Hofman and Wiggins 2008), which consists of a ring of complete cliques, as shown in Fig. 2. Each clique stands for a community. The links within cliques are positive (see solid lines), and those between cliques are negative (see dotted lines). As the number of cliques in the ring (denoted by Ktrue ) increasing, it gets more and more challenge to precisely detect them all. Fig.2 show the performance of SSL and PSA. As we see, SSL performs perfectly in all cases, much better than PSA in the cases of larger Ktrue , although PSA already takes effort to weaken the effect of resolution limitation. Real-world signed networks We use Slovene parliamentary party network (Kropivnik and Mrvar 1996), GahukuGama subtribes network (Read 1954) and monastery network (Doreian and Mrvar 1996) to further validate SSL. The three real-world networks are chosen because they all have ground truth community structures and thereby have been the benchmarks for testing the performance of signed community detection (Yang, Cheung, and Liu 2007; Doreian and Mrvar 1996). In all cases, the detections of SSL are exactly the same as the ground truth. Note that, before applying SSL to Slovene parliamentary party network, we first turn it into a binary network by setting zero as the threshold of positive and negative links.







connected and balanced signed network, where there are five communities that contain 100, 200, 300, 400 and 500 nodes, respectively. A subnetwork N is constructed by sampling links from N with a sampling rate s. N is regarded as an observed network for training and the rest links in N - N as incoming links for prediction. s takes the values 0.005, 0.01,0.02,0.03,0.05,0.07 and 0.1, alternatively. For each value, 100 subnetworks are sampled to calculate the prediction accuracy on average. Fig. 3(a) shows the performance of five algorithms. As we see, SSL, PSA and LR perform very good and stable; SSL provides the best prediction when s > 0.01.



i oc oe







i oc oe







OI ONY IIO I ONx







oe oe







OI ONY IIO I ONx







oe oe oi







oe oi oi oi oe oe U(R)1/2*  3/4(R)1/4 *







oi







oie







oi oie oi oie U(R)1/2*  * *







oi







(a)







(b)







Figure 3: Performance comparisons of sign prediction. Next, we test the five algorithms in a more challenge way by injecting different levels of noise into the abovegenerated balanced networks. In this validation, we first construct a subnetwork N with a sampling rate s = 0.1, and thereafter we change the signs of randomly selected links within or between communities with a noise rate , varying from 0.1 to 0.4 with an increment 0.03. Similarly, for each configuration we generate 100 subnetworks to calculate prediction accuracy on average. Fig. 3(b) shows the performance of five algorithms on such unbalanced networks. As we see, SSL works still better, particularly for the unbalanced networks containing more noise (i.e. > 0.25). Note that, both SSL and PSA, the two community detection based methods, perform quite good among the five competitors for both balanced and unbalanced networks. This is probably because these methods implicitly take more information, provided by the global community structure in terms of both link density and sign, into account for prediction making.







Validation on sign (or attitude) prediction



In showing the superiority of SSL for sign prediction, three representative algorithms are selected to compare. They are the balance theory based MOI (Chiang et al. 2011), the supervised learning based HOC (Leskovec, Huttenlocher, and Kleinberg 2010), and the matrix factorization based LR (Chiang et al. 2013). Distinctly, SSL predicts link signs based on community detection. Provided that we have a community structure of a network, SSL predicts the sign of an incoming link based on the following rule: the link is positive if both end nodes fall into the same community, otherwise it is negative. Based on the same idea, the aforementioned PSA is also selected to join the comparison. The fraction of correct prediction is used to measure the performance of sign prediction, which is defined as Rp = Ep /Et , where Ep is the number of links being correctly predicted and Et is the total number of links to be predicted. In this experiment, we follow the same way as suggested by (Chiang et al. 2013) to test the sign prediction performance of five algorithms, in which the learning data and testing data are generated as follows. Let N be a fully-







Conclusion



Community detection and sign prediction are important for signed network analysis. Most of the existing methods are discriminative, which are depend on either predefined optimization objectives or heuristics. Distinctly, we propose a generative Bayesian approach to addressing these tasks, in which a signed stochastic blockmodel is proposed to characterize the block structures of signed networks in terms of both link density and sign and a variational Bayes method is proposed for model learning. To the best of our knowledge, this is the first effort in the literature to generalize the current SBM to address signed networks.







1957







Acknowledgements



This work was funded by the Program for New Century Excellent Talents in University under Grant NCET-11-0204, and the National Science Foundation of China under Grants 61133011, 61373053, and 61300146.







References



Airoldi, E. M.; Blei, D. M.; Fienberg, S. E.; and Xing, E. P. 2008. Mixed membership stochastic blockmodels. Journal of Machine Learning Research 9:1981-2014. Anchuri, P., and Magdon-Ismail, M. 2012. Communities and balance in signed networks: A spectral approach. In Proceedings of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012), 235-242. IEEE Computer Society. Bansal, N.; Blum, A.; and Chawla, S. 2004. Correlation clustering. Machine Learning 56(1-3):89-113. Cartwright, D., and Harary, F. 1956. Structural balance: a generalization of heider's theory. Psychological review 63(5):277-293. Chiang, K.-Y.; Natarajan, N.; Tewari, A.; and Dhillon, I. S. 2011. Exploiting longer cycles for link prediction in signed networks. In Proceedings of the 20th ACM international conference on Information and knowledge management, 1157-1162. ACM. Chiang, K.-Y.; Hsieh, C.-J.; Natarajan, N.; Tewari, A.; and Dhillon, I. S. 2013. Prediction and clustering in signed networks: A local to global perspective. arXiv preprint arXiv:1302.5145. Daudin, J.-J.; Picard, F.; and Robin, S. 2008. A mixture model for random graphs. Statistics and computing 18(2):173-183. Davis, J. A. 1967. Clustering and structural balance in graphs. Human relations 20(2):181-187. Doreian, P., and Mrvar, A. 1996. A partitioning approach to structural balance. Social networks 18(2):149-168. Girvan, M., and Newman, M. E. 2002. Community structure in social and biological networks. Proceedings of the National Academy of Sciences 99(12):7821-7826. Gong, M.; Cai, Q.; Chen, X.; and Ma, L. 2014. Complex network clustering by multiobjective discrete particle swarm optimization based on decomposition. Evolutionary Computation, IEEE Transactions on 18(1):82-97. Hofman, J. M., and Wiggins, C. H. 2008. Bayesian approach to network modularity. Physical review letters 100(25):258701. Holland, P. W., and Leinhardt, S. 1981. An exponential family of probability distributions for directed graphs. Journal of the american Statistical association 76(373):33-50. Huang, Z., and Qiu, Y. 2010. A multiple-perspective approach to constructing and aggregating citation semantic link network. Future Generation Computer Systems 26(3):400-407. Kropivnik, S., and Mrvar, A. 1996. An analysis of the slovene parliamentary parties network. Developments in Statistics and Methodology 209-216.







Kuncheva, L. I., and Hadjitodorov, S. T. 2004. Using diversity in cluster ensembles. In Systems, man and cybernetics, 2004 IEEE international conference on, volume 2, 1214- 1219. IEEE. Larusso, N.; Bogdanov, P.; and Singh, A. 2010. Identifying communities with coherent and opposing views. In Proc. of the 15th Annual Graduate Student Workshop in Computing. Santa Barbara: UCSB, 31-32. Latouche, P.; Birmel e, E.; Ambroise, C.; et al. 2011. Overlapping stochastic block models with application to the french political blogosphere. The Annals of Applied Statistics 5(1):309-336. Leskovec, J.; Huttenlocher, D.; and Kleinberg, J. 2010. Predicting positive and negative links in online social networks. In Proceedings of the 19th international conference on World wide web, 641-650. ACM. Liu, C.; Liu, J.; and Jiang, Z. 2014. A multiobjective evolutionary algorithm based on similarity for community detection from signed social networks. Cybernetics, IEEE Transactions on PP(99):1-1. Mitrovi c, M., and Tadi c, B. 2009. Spectral and dynamical properties in classes of sparse networks with mesoscopic inhomogeneities. Physical Review E 80(2):026123. Newman, M. E. 2004. Fast algorithm for detecting community structure in networks. Physical review E 69(6):066133. Palla, G.; Der enyi, I.; Farkas, I.; and Vicsek, T. 2005. Uncovering the overlapping community structure of complex networks in nature and society. Nature 435(7043):814-818. Pizzuti, C. 2008. Ga-net: A genetic algorithm for community detection in social networks. In Parallel Problem Solving from Nature-PPSN X. Springer. 1081-1090. Read, K. E. 1954. Cultures of the central highlands, new guinea. Southwestern Journal of Anthropology 1-43. Snijders, T. A., and Nowicki, K. 1997. Estimation and prediction for stochastic blockmodels for graphs with latent block structure. Journal of classification 14(1):75-100. Traag, V., and Bruggeman, J. 2009. Community detection in networks with positive and negative links. Physical Review E 80(3):036115. Yang, T.; Chi, Y.; Zhu, S.; Gong, Y.; and Jin, R. 2011. Detecting communities and their evolutions in dynamic social networksa bayesian approach. Machine learning 82(2):157-189. Yang, B.; Cheung, W. K.; and Liu, J. 2007. Community mining from signed social networks. Knowledge and Data Engineering, IEEE Transactions on 19(10):1333-1348. Yang, B.; Liu, J.; and Liu, D. 2012. Characterizing and extracting multiplex patterns in complex networks. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 42(2):469-481. Zhou, H. 2003. Distance, dissimilarity index, and network community structure. Physical review e 67(6):061901.







1958







Near-Optimal Active Learning of Halfspaces via Query Synthesis in the Noisy Setting



Lin Chen1,2 and Hamed Hassani3 and Amin Karbasi1,2



1







Department of Electrical Engineering, 2 Yale Institute for Network Science, Yale University 3 Computer Science Department, ETH Zurich {lin.chen, amin.karbasi}@yale.edu, hamed@inf.ethz.ch







arXiv:1603.03515v2 [cs.AI] 12 Nov 2016







Abstract



In this paper, we consider the problem of actively learning a linear classifier through query synthesis where the learner can construct artificial queries in order to estimate the true decision boundaries. This problem has recently gained a lot of interest in automated science and adversarial reverse engineering for which only heuristic algorithms are known. In such applications, queries can be constructed de novo to elicit information (e.g., automated science) or to evade detection with minimal cost (e.g., adversarial reverse engineering). We develop a general framework, called dimension coupling (DC), that 1) reduces a d-dimensional learning problem to d - 1 lowdimensional sub-problems, 2) solves each sub-problem efficiently, 3) appropriately aggregates the results and outputs a linear classifier, and 4) provides a theoretical guarantee for all possible schemes of aggregation. The proposed method is proved resilient to noise. We show that the DC framework avoids the curse of dimensionality: its computational complexity scales linearly with the dimension. Moreover, we show that the query complexity of DC is near optimal (within a constant factor of the optimum algorithm). To further support our theoretical analysis, we compare the performance of DC with the existing work. We observe that DC consistently outperforms the prior arts in terms of query complexity while often running orders of magnitude faster.







1 Introduction



In contrast to the passive model of supervised learning, where all the labels are provided without any interactions with the learning mechanism, the key insight in active learning is that the learning algorithm can perform significantly better if it is allowed to choose which data points to label. This approach has found farreaching applications, including the classical problems in AI (e.g., classification (Tong and Koller 2002), information retrieval (Tong and Chang 2001), speech recognition (Hakkani-Tur, Riccardi, and Gorin 2002)) as well as the modern ones (e.g., interactive recommender systems (Karbasi, Ioannidis, and Massoulie 2012) and optimal decision making (Javdani et al. 2014)). In all the above applications, the unlabeled data are usually abundant and easy to obtain, but training labels are either time-consuming or expensive to acquire (as they require asking an expert). Throughout this paper, our objective is to actively learn an unknown halfspace H  = {x  Rd  h , x > 0}







via query synthesis (a.k.a. membership queries), where ,  denotes the standard inner product of the Euclidean space and h is the unit normal vector of the halfspace we want to learn. We would like to note that learning the halfspace H  is mathematically equivalent to learning its unit normal vector h ; therefore we focus on learning h hereinafter. In addition, it should be noted that using the kernel trick we can easily extend the halfspace learning to more complex (e.g., non-linear) decision boundaries (Shawe-Taylor and Cristianini 2004). The hypothesis space H, which consists of all possibilities of unit normal vectors, is the unit sphere S d-1 = {x  Rd  x = 1}, where  denotes the standard Euclidean norm. In active learning of halfspaces via query synthesis, the algorithm is allowed to query whether any point x in Rd resides in the true halfspace. When the algorithm queries x, the true outcome is sign(h , x)  {1, -1}. When sign(h , x) = 1, it means that x  H  ; otherwise, x  H  . We should note here that the only information we obtain from a query is the sign of the inner product rather than the value. For example, the queries of the form sign(h , ei ), where ei is the ith standard basis vector, will only reveal the sign of the ith component of h (and nothing further about its value). In the noiseless setting, we observe the true outcome of the query, i.e. sign h , x  {1, -1}. In the noisy setting, the outcome is a flipped version of the true sign with independent flip probability . That is, denoting the outcome by y we have y  {-1, 1} and Pr[y  sign h , x]   < 1 2. Since the length of the selected vector x will not affect the outcome of the query, we only query the points on the unit sphere S d-1 = {x  Rd  x = 1}. Hence, we term X = S d-1 as the query space. Given ,  > 0, we would like to seek an active learning algorithm that (i) adaptively selects vectors x1 , x2 , . . .  X , (ii) observes the (noisy) responses to each query signh , xi , (iii) and outputs, using as few queries as possible, an esti of h such that h  - h <  with probability at least mate h 1 - . Our main contribution in this paper is to develop a noise resilient active learning algorithm that has access to noisy membership queries. To the best of our knowledge, we are the first to show a near-optimal algorithm that outperforms in theory and practice the naive repe-







tition mechanism and the recent spectral heuristic methods (Alabdulmohsin, Gao, and Zhang 2015). Specifically, we develop a framework, called Dimension Coupling (DC), with the following guarantees. Its query complexity is 1 O(d(log 1 + log  )) and its computational complexity is  1 1 2 O(d(log  + log  ) ). In particular, in the noiseless setting ( = 0), both its computational complexity and query complexity are O(d log 1 ). Note that in both settings the com putational complexity scales linearly with the dimension. Moreover, the query complexity in both settings is nearoptimal. Our empirical experiments demonstrate that DC runs orders of magnitude faster than the existing methods. The rest of the paper is structured as follows. In Section 2, we start with investigating this problem in the 2-dimensional case and present an algorithm called DC2 . Then in Section 3 we generalize it to the d-dimensional case and present a general framework called DC. Empirical results are shown in Section 4. We extensively review related literature in Section 5.







Algorithm 1 DC 2 Input: orthonormal vectors e1 , e2 , estimation error at most , success probability at least 1 -  . Output: a unit vector e  which is an estimate for the normalized orthogonal projection of h onto span{e1 , e2 }.



1: Set p0 (h) to be uniform, i.e., h  S 1  p0 (h) = 1 2 . 2: for m = 1 to T, do 3: Find a vector xm  S 1 which is a solution to the fol-







4: 5: 6: 7:







lowing equation: S 1 sign x, h pm-1 (h)dh = 0. If there are multiple solutions, choose one arbitrarily. Ask from the oracle the value of sign xm , h . Based on the response obtained from the oracle, update the distribution pm-1 (h) to pm (h). end for return e  = arg maxhS 1 pT, (h).







2







DC : Solving the 2-Dimensional Problem







2







To gain more intuition before studying the general ddimensional problem, it might be beneficial to study a special case where the dimension is two. In other words, we study in this section how to learn the normalized projection of the true unit normal vector h  Rd onto span{e1 , e2 }, where e1 , e2  Rd are two orthonormal vectors and span{e1 , e2 } is the linear subspace spanned by e1 and e2 . We should note here that the underlying space is still d-dimensional (i.e., Rd ) but our goal is not to learn h per se but its normalized projection onto a 2-dimensional subspace. Formally, given two orthonormal vectors e1 , e2 we denote the (normalized) projection of h onto span{e1 , e2 } by h , i.e., h , e1  e1 + h , e2  e2 h = . (1) h , e1  e1 + h , e2  e2 2 Our objective is to find a unit vector e   span{e1 , e2 } such that e  - h < . In fact, we require the latter to hold with probability at least 1 -  . We should emphasize that noise, characterized by independent flip probability , is generally present. In the 2dimensional problem, one may propose to use the simple binary search (a detailed discussion with examples is presented in Appendix B) to find a unit vector e  that resides -close to h . To make it noise-tolerant, when the binary search algorithm queries a point, say xi , we query it R times to obtain R noisy versions of sign h , xi  and view the majority vote of the noisy versions as the true outcome (Kaariainen 2006; Karp and Kleinberg 2007; Nowak 2011). We call this method repetitive querying. However, its query complexity is O(log(1 )(log log(1 ) + log(1  )), which is suboptimal both theoretically (we will prove this bound in Appendix C) and empirically (referred to as R EPETITIVE -DC in Section 4). As a result, instead, we will present a Bayesian algorithm termed DC2 that solves this 2-dimensional problem with query complexity O(log(1 ) + log(1  )). Recall that any unit vector inside span{e1 , e2 }, e.g., h , can equivalently be







We take a Bayesian approach. In the beginning, when no queries have been performed, DC 2 assumes no prior information about the vector h . Therefore, it takes the uniform distribution on S 1 (with pdf p0 (h) = 21 ) as its prior belief  about h . After performing each query, the posterior (belief) about h will be updated according to the observation. We let pm (h) denote the (pdf of the) posterior after performing the first m queries. In this manner, DC2 runs in total of T, rounds, where in each round a specific query is selected and posed to the oracle. The number T, will be specified later (see Theorem 1). Upon the completion of round T, , the algorithm returns as its final output a vector e   S 1 that maximises the posterior pdf pT, (h). If there are multiple such maximisers, it picks one arbitrarily. We now proceed with a detailed description of DC 2 (a formal description is provided in Algorithm 1). As shown in Algorithm 1, at each round, say round m + 1, the algorithm maintains and updates the distribution pm that encodes its current belief in the true location of h . We should note here that these distributions can be stored efficiently and as a result the vector xm+1 can be computed efficiently. Indeed, (the pdf of) pm is piecewise constant on the unit circle (see Figure 1). More precisely, at any round m, there are at most 2m points u1 , u2 , , u2m that are ordered clock-wise on the unit-circle and pm is constant when restricted to each of the sectors [ui , ui+1 ). The piecewise constant property of the pdf of pm can be established by induction on m. Recall that the initial distribution p0 is uniform and thus piecewise constant. The Bayesian update step (line 5 of Algorithm 1) preserves this property when the algorithm updates the distribution pm (h) to pm+1 (h). We will show why this is true when we discuss the Bayesian update step in detail.







represented as a pair (c1 , c2 ) on the two-dimensional unit 2 circle S 1 (e.g., h = c1 e1 + c2 e2 and c2 1 + c2 = 1). To simplify 1 notation, we use a point (c1 , c2 )  S and its corresponding unit vector c1 e1 + c2 e2 interchangeably. In this setting, it is easy to see that for any x  span{e1 , e2 } sign x, h  = sign x, h  . (2)







At round m + 1, in order to find xm+1 (see line 3 of Algorithm 1), DC 2 first finds a line that passes through the centre of S 1 and cuts S 1 into two "halves" which have the same measure with respect to pm . Note that finding such a line can be done in O(m) steps because pm has the piecewise constant property. Once such a line is found, it is then easy to see that xm+1 can be any of the two points orthogonal to the line. As a result, DC 2 at round m + 1 can find xm+1 in O(m) operations. We denote the half-circle containing xm+1 by R+ and the other half by R- . We refer to Figure 1 for a schematic illustration. The key step in Algorithm 1 is the Bayesian update (line 5). Once a noisy response to the query sign xm+1 , h  is obtained (line 4)), the probability distribution pm will be updated to pm+1 in the following way. First, consider the event that the outcome of sign xm+1 , h  is +1. We have pm (sign xm+1 , h  = +1) = (1 - ) pm (R+ ) +  pm (R- ) = 1 2, and similarly pm (sign xm+1 , h  = -1) = 1 2. Therefore, by Bayes theorem we obtain the following update rules for pm+1 . If we observe that sign xm+1 , h  = +1, then for h  R+ we have and for h  R we have



-







Figure 1: Upon the completion of round m (left figure), the distribution (pdf of) pm is constant over each of the sectors [ui , ui+1 ). In the next round (right figure), in order to find xm+1 , DC2 first finds a diagonal line (red line) which separates two half-circles (R+ and R- ) that each has measure 1 2 w.r.t pm . The vector xm+1 will then be one of the two points on the unit circle that are orthogonal to this line. For updating pm to pm+1 , we note that all the points inside R+ get the same factor (either 2 or 2(1 - ) depending on the outcome of the query). The same is true for R- . Thus, pm+1 is again a piecewise constant pdf but now on 2(m + 1) sectors.







pm+1 (h) = 2(1 - )pm (h) pm+1 (h) = (2)pm (h).







Also, if we observe that sign xm+1 , h  = -1, then for h  R+ we have pm+1 (h) = (2)pm (h) and for h  R- we have pm+1 (h) = 2(1 - )pm (h).







Note that the factor of 2 here is due to the normalization. It is easy to verify that pm+1 is also a piecewise constant distribution (now on 2(m + 1) sectors; see Figure 1). 1 Theorem 1 shows that after T, = O(log 1 + log  )  2 rounds, with probability at least 1 -  , DC outputs a unit vector e   span{e1 , e2 } such that e  - h < . Also, as discussed above, the computational complexity of DC 2 is 2 + log 1 )2 ). O(T, ), i.e., O((log 1   1 1  M + max{T0 , T1 , T2 , T3 } = O(log + log ) (3)  







h . We present a detailed discussion with examples in Appendix B. A few comments are in order: The above guarantee for DC2 holds with probability one and thus the parameter  is irrelevant in the noiseless setting. Furthermore, during each round of DC 2 , the distribution pm can be represented by only two numbers (the starting and ending points of the sector Rm ), and the vector xm can be computed efficiently (it is the orthogonal vector to the midpoint of Rm ). Therefore, assuming one unit of complexity for performing the queries, DC 2 can be implemented with complexity O(T, ), i.e., O(log(1 )).







3







Dimension Coupling Based Framework







Theorem 1. (Proof in Appendix A) When the independent flip probability is , having T,







In Section 2, we devise an algorithm, called DC 2 (e1 , e2 , ,  ), that takes as input two orthonormal vectors e1 , e2 , uses noisy responses to queries of the form sign x, h , and outputs with probability at least 1 -  a vector e  with the following three properties: e   span{e1 , e2 }, e  = 1, e -



h ,e1 e1 +h ,e2 e2 h ,e1 e1 +h ,e2 e2







< .







is sufficient to guarantee that DC 2 outputs with probability at least 1 -  a vector that is within a distance  of h . 2 2 log 2 8 log   Here, we have M =  - log(4(1 , T0 = log(2(1- , T1 = -)) ))



1 8 log 8 8 4 , T2 = log(2( log(2M ) + log( log(2( ) log(2(1-)) 1-)) 1-)) 2 1- 24 log  and T3 = log2 (2(1-)) log(M ) + log( 4 ) . 







We would like to remark that when the independent flip probability  is 0 (i.e., in the noiseless case), the algorithm , DC 2 reduces to the binary search. If we let T, = log2   then DC2 outputs a vector that is within a distance  of







In other words, the unit vector e  is within a distance  to the (normalized) projection of h onto the subspace span{e1 , e2 }. In the current section, we explain a framework DC that estimates h using at most d - 1 calls to DC2 (a formal description will be given in Algorithm 2 later). Let us begin our discussion with a motivating example. Let {e1 , e2 , . . . , ed } be an orthonormal basis of Rd . Suppose d that h has the form h = d i=1 ci ei , where {ei }i=1 is an arbid trarily chosen orthonormal basis for R . We assume w.l.o.g. 2 that h is normalized (i.e., d i=1 ci = 1). Our objective is d then to learn the coefficients {ci }i=1 within a given precision by using the noisy responses to the selected sign queries. The key insight here is that this task can be partitioned in







 = DC2 (e h 12 , e 34 ) e 12 = DC (e1 , e2 )



2







tion h onto span{e1 , e2 , e3 }; finally call DC 2 (e 123 , e4 ) and   obtain an estimate for h which we denote by h.



2







e 34 = DC (e3 , e4 ) e3 e4







e1







e2







(a) Scheme 1: a balanced full binary tree







 = DC 2 (e h 123 , e4 ) e 123 = DC (e 12 , e3 )



2







Examples 1 and 2 show two possibilities of divide-andconquer schemes for a 4-dimensional problem. In fact, each scheme corresponds to a full binary tree of 4 leaf nodes. For general d, the idea is similar: We break the problem into at most d - 1 "two-dimensional" problems that each can be solved efficiently. Again, each divide-and-conquer scheme corresponds to a full binary tree of d leaf nodes. Consider the decomposition h = d i=1 ci ei . Without loss of generality, suppose that the first two leaf nodes to be combined are e1 and e2 . We can write h =



d







e4







ci e i = c 12



i=1







c1 e 1 + c2 e 2



2 c2 1 + c2







+







d







ci e i ,



i=3







(4)







e 12 = DC 2 (e1 , e2 ) e1 e2







e3







(b) Scheme 2: an unbalanced full binary tree







Figure 2: Two possible divide-and-conquer schemes for a 4dimensional problem. Each scheme can be represented by a full binary tree of 4 leaf nodes. a divide-and-conquer fashion into many smaller tasks, each involving a few dimensions. The final answer (the values of { ci } d i=1 ) will then be obtained by aggregating the answers of these subproblems. Example 1. Assume h = c1 e1 + c2 e2 + c3 e3 + c4 e4 , where ei 's are the standard basis vectors for R4 . Define c3 e 3 + c4 e 4 c1 e 1 + c2 e 2 , e34 = e12 = 2 2 2 c1 + c2 c2 3 + c4











of h onto span{e1 , e2 }. Hence, by using DC 2 (e1 , e2 , ,  ) we can obtain, with probability at least 1 -  , a good approximation e 12 (within a distance ) of this projection. Therefore, for small enough  we have h  c 12 e 12 + d i=3 ci ei . Since  h is now expressed (approximately) in terms of d - 1 orthonormal vectors {e 12 , e3 , e4 , . . . , ed }, we have effectively reduced the dimensionality of problem from d to d - 1. The idea is then to repeat the same procedure as in (4) to the newly obtained representation of h . Hence, by repeating this procedure d - 1 times in total we will reach a vector which is the final approximation of h . We present this general method in Algorithm 2.











2 where in the last step we have taken c 12  c2 1 + c2 . Now, c1 e1 +c2 e2 note that is the normalized orthogonal projection 2 2 c1 +c2







Note here that e12 is the (normalized) orthogonal projection of h onto span{e1 , e2 } and e34 is the (normalized) orthogonal projection of h onto span{e3 , e4 }. Consider the following procedure to learn h : first find out what e12 and e34 are, 2 2 and then use the relation h = c2 c2 1 + c2 e12 + 3 + c4 e34  to find h based on the orthonormal vectors e12 , e34 . By this procedure, the original "four-dimensional" problem has been broken into three "two-dimensional" problems. This procedure is illustrated in Figure 2a. We first call DC 2 (e1 , e2 ) to obtain an estimate e 12 for e12 ; then we call DC 2 (e3 , e4 ) to obtain an estimate e 34 for e34 ; finally we call  for h . DC 2 (e 12 , e 34 ) to obtain an estimate h Example 2. For another example of the 4-dimensional problem discussed in Example 1, let us consider another scheme illustrated in Figure 2b: We call DC2 (e1 , e2 ) and 2 obtain e 12 , e3 ) and 12 as an estimate for e12 ; then call DC (e obtain e 123 that estimates the normalized orthogonal projec-







Input: an orthonormal basis E = {e1 , e2 , . . . , ed } of Rd . Output: a unit vector  h which is an estimate for h . 1: for j  1 to d - 1 do 2: Replace any two vectors e and e in E with the vector DC2 (e , e , ,  ). 3: end for  be the only remaining vector in E . 4: Let h  5: return h Theorem 2. (Proof in Appendix D) For DC (outlined in Algorithm 2) and any of its divide-and-conquer scheme represented by a full binary tree, we have: 1. DC will call the two-dimensional subroutine DC2 d - 1 times. 2. Provided that the output of DC 2 is with probability 1 -  within distance  of the true value and   5 18, DC ensures an estimation error of at most 5(d - 1) with probability at least 1 -  (d - 1).







Algorithm 2 Dimension Coupling (DC)







As a result of Theorem 2, if we desire the framework DC to estimate h within distance   and with probability at least , then it is enough to fix the corresponding parameters 1-    of DC 2 to  = 5(d and  = d- . -1) 1







Theorem 2 indicates that DC requires O(d(log 1 +log 1 ))   2 1 1 queries, since each call to DC needs O(log  + log  ) queries. Recall that the computational complexity of DC2 1 2 + log  ) ). Hence, DC has computational comis O((log 1  1 1 2 plexity O(d(log  + log  ) ). As a special case, if in absence of noise, both the query complexity and time complexity of DC are O(d log 1 ). 







4 Empirical Results



In this section, we extensively evaluate the performance of DC against the following baselines: R ANDOM -S AMPLING: Queries are generated by sampling uniformly at random from the unit sphere S d-1 . U NCERTAINTY-S AMPLING: Queries are sampled uniformly at random from the orthogonal complement of w, where w is the vector learned by linear SVM. Q UERY- BY-BAGGING: The bag size is set to 20 and 1000 queries are generated at each iteration. The query with the largest disagreement is picked (Abe and Mamitsuka 1998). S PECTRAL: The version space is approximated by the largest ellipsoid consistent with all previous query-label pairs. Then, at each iteration a query is selected to approximately halve the ellipsoid (Alabdulmohsin, Gao, and Zhang 2015). R EPETITIVE -DC: In the noisy setting, one easy way to apply DC is to query each point R times and use the majority rule to determine its label; i.e., the combination of repetitive querying (Section 2) and the DC framework (Section 3). Our metrics to compare different algorithms are: a) estimation error, b) query complexity, and c) execution time. In particular, as we increase the number of queries we measure the average estimation errors and execution times for all the baselines (with 90% confidence intervals). By nature, in active learning via query synthesis, all data points and queries are generated synthetically. For all the baselines, we used the fastest available implementations in MATLAB. Noiseless setting: Figures 3a and 3b (with dimension d = 25 and 50, respectively) show that in terms of estimation error, DC outperforms all other baselines, and significantly outperforms R ANDOM -S AMPLING, U NCERTAINTYS AMPLING and Q UERY- BY-BAGGING. Note that the estimation errors are plotted in log-scales. In terms of execution times, we see in Fig. 3c that DC runs three orders of magnitude faster than other baselines. Training an SVM at each iteration for R ANDOM -S AMPLING, U NCERTAINTYS AMPLING and Q UERY- BY-BAGGING comes with a huge computational cost. Similarly, S PECTRAL requires solving a convex optimization problem at each iteration; thus its performance drastically deteriorates as the dimension increases, which makes it infeasible for many practical problems. Noisy setting: We set the noise level to  = 0.1 and compare the performance of DC against R ANDOM S AMPLING, U NCERTAINTY-S AMPLING, Q UERY- BYBAGGING, and R EPETITIVE -DC. As mentioned in (Alabdulmohsin, Gao, and Zhang 2015), and we have also observed in our experiments, S PECTRAL does not work even for small amounts of noise as it incorrectly shrinks the version space and misses the true linear separator;







therefore it is excluded here. We see again in Figures 3d and 3e (for d = 25 and 50) that DC significantly outperforms all other methods in terms of estimation error. More precisely, using the same number of queries, the estimation error of DC is around two orders of magnitude smaller than other baselines. We can also observe from these two figures that DC still runs around 100 times faster than R ANDOM -S AMPLING, U NCERTAINTY-S AMPLING, and Q UERY- BY-BAGGING. Clearly, DC has a higher computational cost than R EPETITIVE -DC, as DC performs a Bayesian update after each query. Finally, as we increase the dimension to d = 1000, R ANDOM -S AMPLING, U NCERTAINTY-S AMPLING, and Q UERY- BY-BAGGING become significantly slower. Hence, in Figure 3f we only show how the estimation error (for noise levels  = 0.01, 0.1, 0.2) decreases for DC and R EPETITIVE -DC with more queries. It can be observed from Figure 3f that consuming the same number of queries, DC can achieve an estimation error from one order (when the noise intensity is very small) to three orders of magnitude (when the noise intensity is 0.2) smaller than that of R EPETITIVE -DC.







5 Related Work



The sample complexity of learning a hypothesis was traditionally studied in the context of probably approximately correct (PAC) learning (Valiant 1984). In PAC learning theory, one assumes that a set of hypotheses H along with a set of unlabeled data points X are given, where each data point x  X is drawn i.i.d. from some distribution D. Classical PAC bounds then yield the sample complexity (i.e., the number of required i.i.d. examples) from D to output a hypothesis h  H that will have estimation error at most  with probability at least 1 -  , for some fixed ,  > 0. Here, the estimation error is defined as  = PrxD [h(x)  h (x)], where h is the unknown true hypothesis. In the realizable case of learning a halfspace, i.e., when h  Rd perfectly separates the data points into positive and negative labels, it  (d )1 i.i.d. samples one can find a linis known that with O ear separator with an estimation error . The main advantage of using active learning methods, i.e., sequentially querying data points, is to reduce the sample complexity exponential  (d log(1 )). In fact, a simple counting arfast, ideally to O gument based on sphere packing shows that any algorithm needs (d log(1 )) examples to achieve an estimation error of  (Dasgupta, Kalai, and Monteleoni 2009). For d = 2 and when the distribution is uniform over the unit sphere S 1 it is very easy to see that the halving or bi (log(1 )). By using the same halving section leads to O method, one can in principle extend the result to any dimension d. To do so, we need to carefully construct the version space (i.e., the set of hypotheses consistent with the queries and outcomes) at each iteration and then find a query that halves the volume (in the uniform case) or the density (in the general case if the distribution is known) (Dasgupta 2004). Finding such a query in high dimension is very challenging.



1  notation to ignore terms that are logarithmic or We use the O dependent on  .







100







100







104 103 Execution Time (sec.)







Estimation Error







10







-1







Estimation Error







10







-1







102 101 100 10-1 10-2







Random Uncertainty Bagging Spectral DC







10-2



Random Uncertainty Bagging Spectral DC







10-2



Random Uncertainty Bagging Spectral DC







10







-3







50







100 150 Number of Queries







200







10







-3







100







150







200 250 300 350 Number of Queries







400







450







10-3







d = 25 Dimension







d = 50







(a) Noiseless (d = 25)



103 102



1







(b) Noiseless (d = 50)



104 103







(c) Execution time (noiseless)



101



DC (noise = 0.01) DC (noise = 0.1) DC (noise = 0.2) Rep. (noise = 0.01) Rep. (noise = 0.1) Rep. (noise = 0.2)







Execution Time (sec.)







Uncertainty Bagging







Execution Time (sec.)







DC







Uncertainty







10 Estimation Error



Bagging







0







102 101 100 10-1







DC







10







10-1 10-2 10-3







100 10-1 10-2 -4 10







Random







Random







Repetitive 10-3 10-2 Estimation Error 10-1 100







10-2 -4 10







Repetitive 10-3 10-2 Estimation Error 10-1 100







10-4







0







2







4 6 Number of Queries







8







10 # 104







(d) Noisy (d = 25)







(e) Noisy (d = 50)







(f) Noisy (d = 1000)







Figure 3: Figures 3a and 3b show the estimation error in the noiseless setting as we increase the number of queries, for d = 25 and 100, respectively. Figure 3c shows the corresponding execution times. Figure 3d and 3e show the scatter plots of the execution time and the estimation error of different methods for d = 25, 50 and the noise level  = 0.1. We allow each algorithm to use a budget of 800 and 1800 queries in Figure 3d and 3e, respectively. Figure 3f presents the estimation error of DC and R EPETITIVE -DC as we increase the number of queries for d = 1000 and noise levels  = 0.01, 0.1, 0.2. One very successful approach that does not suffer from the aforementioned computational challenge is pool-based active learning (Settles 2010), where instead of ideally halving the space, effective approaximations are performed. Notable algorithms are uncertainty sampling (Lewis and Gale 1994) and query-by-committee (QBC) (Freund et al. 1997). In fact, our problem is closely related to learning homogeneous linear separators under the uniform distribution in the pool-based setting. This problem is very well understood and there exist efficient pool-based algorithms (Balcan, Broder, and Zhang 2007; Dasgupta, Kalai, and Monteleoni 2005; Dasgupta and Hsu 2008). In particular, Dasgupta et al. (Dasgupta, Kalai, and Monteleoni 2009) presented an efficient perceptron-based algorithm that achieve a near-optimal query complexity. Similar results can be obtained under log-concave distributions (Balcan and Long 2013). Most of the pool-based meth (1 ) number of unods require to have access to O labeled samples in each iteration or otherwise they perform very poorly (Balcan, Broder, and Zhang 2007; Dasgupta, Kalai, and Monteleoni 2009). This means that in order to have exponential guarantee in terms of sample complexity, we need to grow the pool size exponentially fast (note that there is no need to store all of these points). Moreover, with a few exceptions (Awasthi, Balcan, and Long 2014; Balcan, Beygelzimer, and Langford 2006) pool-based learning of linear separators in the noisy setting has been much less studied and the dependency of sample complexity on noise is not very well understood. An attractive alternative to the pool-based framework is query synthesis where we have access to membership queries (Angluin 1988)): a learner can request for any unlabeled data instance from the input space, including queries that the learner synthesizes from scratch. This way the pool size limitation is entirely eliminated. In many recent applications, ranging from automated science (King 2009), to robotics (Cohn, Ghahramani, and Jordan 1996), and to adversarial reverse engineering (Lowd and Meek 2005), query synthesis is the appropriate model. For instance, in securitysensitive applications (e.g., spam filters and intrusion detection systems) that routinely use machine learning tools, a growing concern is the ability of adversarial attacks to identify the blind spots of the learning algorithms. Concretely, classifiers are commonly deployed to detect miscreant activities. However, they are attacked by adversaries who generate exploratory queries to elicit information that in return allows them to evade detection (Nelson et al. 2012). In this work, we show how an adversary can use active learning methods by making synthetically de novo queries and thus identify the linear separator used for classification. We should emphasize that in active learning via synthesized queries the learning algorithm can query the label of any points in order to explore the hypothesis space. In the noiseless setting (if we ignore the dependency of the pool size on  (log(1 ))), one can potentially use the pool-based algoO rithms (under the uniform distribution). Our main contribution in this paper is to develop a noise resilient active learning algorithm that has access to noisy membership queries.







To the best of our knowledge, we are the first to show a near optimal algorithm that outperforms in theory and practice the naive repetition mechanism and the recent spectral heuristic methods (Alabdulmohsin, Gao, and Zhang 2015).







References



[Abe and Mamitsuka 1998] Abe, N., and Mamitsuka, H. 1998. Query learning strategies using boosting and bagging. In ICML, 1. Morgan Kaufmann Pub. [Alabdulmohsin, Gao, and Zhang 2015] Alabdulmohsin, I.; Gao, X.; and Zhang, X. 2015. Efficient active learning of halfspaces via query synthesis. In AAAI 2015. [Angluin 1988] Angluin, D. 1988. Queries and concept learning. Machine learning. [Awasthi, Balcan, and Long 2014] Awasthi, P.; Balcan, M. F.; and Long, P. M. 2014. The power of localization for efficiently learning linear separators with noise. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, 449-458. ACM. [Balcan and Long 2013] Balcan, M.-F., and Long, P. M. 2013. Active and passive learning of linear separators under log-concave distributions. In COLT, 288-316. [Balcan, Beygelzimer, and Langford 2006] Balcan, M.-F.; Beygelzimer, A.; and Langford, J. 2006. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, 65-72. ACM. [Balcan, Broder, and Zhang 2007] Balcan, M.-F.; Broder, A.; and Zhang, T. 2007. Margin based active learning. In Learning Theory. Springer. 35-50. [Cohn, Ghahramani, and Jordan 1996] Cohn, D. A.; Ghahramani, Z.; and Jordan, M. I. 1996. Active learning with statistical models. JAIR. [Dasgupta and Hsu 2008] Dasgupta, S., and Hsu, D. 2008. Hierarchical sampling for active learning. In Proceedings of the 25th international conference on Machine learning, 208-215. ACM. [Dasgupta, Kalai, and Monteleoni 2005] Dasgupta, S.; Kalai, A. T.; and Monteleoni, C. 2005. Analysis of perceptron-based active learning. In International Conference on Computational Learning Theory, 249-263. Springer. [Dasgupta, Kalai, and Monteleoni 2009] Dasgupta, S.; Kalai, A. T.; and Monteleoni, C. 2009. Analysis of perceptron-based active learning. Journal of Machine Learning Research 10(Feb):281-299. [Dasgupta 2004] Dasgupta, S. 2004. Analysis of a greedy active learning strategy. In Advances in neural information processing systems, 337-344. [Freund et al. 1997] Freund, Y.; Seung, H. S.; Shamir, E.; and Tishby, N. 1997. Selective sampling using the query by committee algorithm. Machine learning. [Hakkani-Tur, Riccardi, and Gorin 2002] Hakkani-Tur, D.; Riccardi, G.; and Gorin, A. 2002. Active learning for automatic speech recognition. In ICASSP), volume 4, IV-3904. IEEE. [Javdani et al. 2014] Javdani, S.; Chen, Y.; Karbasi, A.; Krause, A.; Bagnell, J. A.; and Srinivasa, S. 2014. Near optimal bayesian active learning for decision making. AISTAT.







[Kaariainen 2006] Kaariainen, M. 2006. Active learning in the non-realizable case. In International Conference on Algorithmic Learning Theory, 63-77. Springer. [Karbasi, Ioannidis, and Massoulie 2012] Karbasi, A.; Ioannidis, S.; and Massoulie, L. 2012. Comparison-based learning with rank nets. ICML. [Karp and Kleinberg 2007] Karp, R. M., and Kleinberg, R. 2007. Noisy binary search and its applications. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 881-890. Society for Industrial and Applied Mathematics. [King 2009] King, R. D. e. a. 2009. The automation of science. Science. [Lewis and Gale 1994] Lewis, D. D., and Gale, W. A. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval. [Lowd and Meek 2005] Lowd, D., and Meek, C. 2005. Adversarial learning. In KDD, 641-647. ACM. [Nelson et al. 2012] Nelson, B.; Rubinstein, B. I.; Huang, L.; Joseph, A. D.; Lee, S. J.; Rao, S.; and Tygar, J. 2012. Query strategies for evading convex-inducing classifiers. JMLR. [Nowak 2011] Nowak, R. D. 2011. The geometry of generalized binary search. IEEE Transactions on Information Theory 57(12):7893-7906. [Settles 2010] Settles, B. 2010. Active learning literature survey. University of Wisconsin, Madison 52(55-66):11. [Shawe-Taylor and Cristianini 2004] Shawe-Taylor, J., and Cristianini, N. 2004. Kernel methods for pattern analysis cambridge univ. Cambridge, UK. [Tong and Chang 2001] Tong, S., and Chang, E. 2001. Support vector machine active learning for image retrieval. In Proceedings of the 9th ACM international conference on Multimedia, 107-118. ACM. [Tong and Koller 2002] Tong, S., and Koller, D. 2002. Support vector machine active learning with applications to text classification. JMLR 2:45-66. [Valiant 1984] Valiant, L. G. 1984. A theory of the learnable. Communications of the ACM 27(11):1134-1142.







Appendix



Let {n , n  1} be a sequence of independent and identically distributed (iid) Bernoulli() random variables. Denote by (F , , Pr) the probability space generated by this sequence. At the m-th round of DC 2 , if m = 1 (which takes place with independent probability ) then we observe a flipped version of signxm , h . Also, if m = 0 we observe the correct version of signxm , h . Consider a query of the form signx, h . This query divides the unit circle into two parts (half-circles) depending on the sign of x, h  (see Figure 4). The two parts are: (i) Preferred part: all h such that signx, h = signx, h , and (ii) Unpreferred part: all h such that signx, h = -signx, h . The two parts can be separated by a line x that passes through the origin. We refer to Figure 4 for a schematic explanation.







A







Proof of Theorem 1







conduct the query signxm , h . As the result of the query is noisy, we have two different update rules depending on each of the following cases: (i) m = 0, i.e., we observe the correct value signxm , h . In this case, the measure pm is updated as follows pm+1 (h) = 2(1 - )pm (h) if h  Fxm , (2)pm (h) if h  Uxm .







(ii) m = 1, i.e., we observe the flipped value -signxm , h . In this case, the measure pm is updated as follows pm+1 (h) = (2)pm (h) if h  Fxm , 2(1 - )pm (h) if h  Uxm .







Consider the number T, given in (3). Our goal is to show that Pr y  S 1  d(y, h ) >  and pT, (y )  pT, (h ) < . (5) Clearly, the result of the theorem follows from (5). For better illustration, we assume w.l.o.g that h = (0, 1). Consider a point y on the right-hand side of the unit circle such  that d(y, h ) > 2 . Also, Consider points z0 , zK such that d(z0 , h ) =  4 and d(h , zK ) =  2. We now divide the sector starting with z0 and ending with zK into K = T, + 1 pints. That is, for i = 1, 2, , K we denote by zi the point   that d(h , zi ) = 4 + i 4(T, (see Figure 5). Also, for i  1, +1)







Figure 4: For any point z above the line x we have z, h  = x, h . Once we perform the query x, h , it is more likely that the (noisy) response is indeed the true value x, h . Therefore, the region above the line x is in general preferred by the query. In the figure, the sector (y, z ) is cut by the line x and the sector (z, x) is not. Also, (z, x) lies in the preferred part of the query x, h .







In this setting, we say that the query signx, h  prefers a point z if z belongs to the preferred part of the query. Otherwise, we say that the query does not prefer z. Also, we frequently use the line x rather than the query signx, h  when it causes no ambiguity. Finally, for a region A on the unit circle say that the query signx, h  cuts the region A if and only if the line x passes through region A. Otherwise, we say that the query does not cut A. If x does not cut A, then x prefers A if A is in the preferred part and does not prefer A otherwise (see Figure 4). Finally, for two points x, y we define the distance d(x, y ) to be the length of the (smaller) sector between them (see Figure 4). Clearly, we have d(x, y )  x - y 2 . At round m of DC2 a vector xm is chosen and the (noisy) outcome of signxm , h  is observed. As explained in Section 2, xm is chosen in a way that the preferred and unpreferred parts have equal measures under pm-1 , i.e., 1 pm-1 (Fxm ) = pm-1 (Uxm ) = 2 . Let us see what happens to pm (the posterior belief about h at round m) after we







Figure 5: Different regions for the proof of Theorem 1. we let the sector starting with zi-1 and ending with zi be denoted by Ai . Note that in the very beginning of the algorithm when we have uniform measure on the unit circle, each of  the regions Ai has p0 (Ai ) = 8(T (as Ai = 4(T, ). +1) , +1)  log 4((1  and consider the following events: -))



2 log



2







DC2 has in total T, rounds and in each round m it conducts a query with an associated line xm . We let M =







* E1 : There is at least M lines which separate zK from h or equivalently, there is at least M lines that cut the region (h , zK ). * E2,j (1  j  K ): The region Aj is not cut by any of the lines 1 , 2 , . . . , T, . * E3 : y such that d(y, h ) >



 2







and pT, (y )  pT, (h ).







It is easy to see that Pr K j =1 E2,j = 1 as we have T, queries and hence by the pigeon-hole principle there is always a region Aj that is not cut by any of the lines. We can write: Pr [E3 ] c = Pr [E3  E1 ] + Pr [E3  E1 ]  Pr [E3 E1 ] +



T, +1 j =1







Proof. For i  [m], define the random variable Zi as Zi  p (x ) log pi . Using the update rules of pi that we explained i (y ) above, it is easy to see that for i  1: Zi = Zi-1 + (1 -  . Also, as p0 is uniform over S 1 we have Z0 = 0. 2i ) log 1- 



1- We thus have Zm = m i=1 (1 - 2i ) log  . Hence,







c Pr [E3  E1







 E2,j ] . (6)   . 2







Pr [Zm  0]



m







= Pr log = Pr







Now using Lemma 3 (stated below), we have Pr [E3 E1 ]  Pr [E3 E1 ]  (4(1 - )) Let us now bound



c Pr [E3  E1



M 2







1- m (1 - 2i )  0  i=1 1 2



m







i 



i=1







(7)







and using the fact that E2,j = Lemma 4 that







c c Pr [E3  E1  E2,j ]  Pr [E2,j  E1 ],  4(T, +1)







 E2,j ] . We have







where the last step follows directly from the so called Chernoff bound. We note that the vector h is always a member of the preferred part of any test. As a result, at any round of DC 2 we have that h  m i=1 Fxi . Lemma 4. Consider a region A on the unit circle which does not contain h . Assume we are at round m of DC2 where a sequence of queries with associated lines x1 , x2 , . . . , xm have been conducted. We define events E1 and E2 as * E1  None of the lines xi cuts A; * E2  At most k of the lines do not prefer A, where k is an an integer. We have Pr [E1  E2 ]  k (1 + 2 ),







 (4(1 - )) 2 ,







we obtain from







and thus







c Pr [E2,j  E1 ]  (M - 1)(1 + 2 ), c Pr [E2,j  E1 ]  (T, + 1)M (1 + 2 ),







T, +1 j =1







(8)







where 1 and 2 are given in Lemma 4 with m  T, and k  M . Now, we show that the above expression is upper bounded by  2, and hence by using relations (6) and (7), we get the proof of the main theorem. The value of T0 is chosen in such a way that we have 2 log(T, + 1) log(2(1 - ))  . T, - M 4 (9)







T1 ensures that







2 8 log(2(1 - )) log  . T, - M  4 2M log(1 - 2) log(2)  . T, - M 2







(10)







T2 and ensures that







(11)







2  2 2      m - k  log(2(1 - )) - m-k log A )   , 1 = exp - 1 -    6    log ( )        and 2  2k      m - k  log(2(1 - )) + m-k log(2)   . 2 = exp - 1-  6     log(  )       Proof. We have







where







Finally, T3 ensures that 2        T - M  log(2(1 - ))    . (T, + 1)M exp - 1-   6 4   2  log        (12) Now, by plugging in (9)-(12) into the values of 1 and 2 in  . (8) we conclude that the right side of (8) is bounded by 2 Lemma 3. Let x1 , x2 , , xm be the vectors chosen by DC up to round m with Fxi and Uxi being their associated preferred and unpreferred parts (i.e. pi-1 (Fxi ) = pi-1 (Uxi ) = 1 2). Consider two points h1 , h2 such that h1  m i=1 Fxi and h2  m . We have for  > 0 that U x i i=1



2







Pr [E1  E2 ]  Pr [E2 E1 ]  where we define







k j =1







Pr [E2,j E1 ] ,







(13)







Pr [pm (x) < pm (y )]  (4(1 - )) .



m







We will now calculate Pr [E2,j E1 ] . In the beginning, p0 A . Let us puts a uniform measure on A and hence p0 (A) = 2  first investigate the dynamics of pi-1 (A) when we conduct the i-th query and condition on event E1 (i.e. given that none of the lines cut A). In this setting, we define the random variables Zi = log pi (A). At time i, assuming that the line xi does not cut A, Zi has different update rules depending on the two cases whether the line xi prefers A or does not







E2,j  Exactly j lines do not prefer A.







prefer A. (i) first case: if the line xi prefers A, then we know that either with probability 1 -  (if i = 0) we have pi (A) = 2(1 - )pi-1 (A) and with probability  (if i = 1) we have pi (A) = (2)pi-1 (A). Thus, we can write Zi = Zi-1 + Fi , where Fi  i log(2) + (1 - i ) log(2(1 - )). (ii) second case: if xi does not prefer A, then using a similar argument we obtain Zi = Zi-1 + Ui , where Ui  i log(2(1 - )) + (1 - i ) log(2). Now, in order to find an upper bound on Pr [E2,j E1 ], we assume without loss of generality that in the first m - j rounds we the lines are as in the first case and in the last j rounds the lines are as in the second case (note that any other given order of the lines is statistically equivalent to this simple order that we consider). Zm = Z0 + = log2



m-j i=1







Figure 6: An example to illustrate DC2 in the noiseless setting. In



the first round, x1 is arbitrarily chosen on S 1 . For the choice in the figure, we have sign x1 , h  = sign x1 , h  = -1. For any point h above the red line we have that sign x, h = -1 and for the points outside this half-circle the result is +1. Therefore, the distribution (pdf of) p1 is uniform on the region above the red line and is zero below it. For round m = 2 it is easy to see that the direction of x2 should be along the red line. For x2 chosen as in the figure, we have sign x2 , h  = +1 and hence at the end of the second round DC2 concludes that the vector h could uniformly be any point inside R2 . In a generic round m, any vector orthogonal to the mid-point of sector Rm-1 can be considered as a candidate for xm . For the choice in the figure, we have sign xm , h  = -1. Thus, at the end of round m, DC2 concludes that h can uniformly be any point inside Rm .







Fi +







m







Ui



i=m-j +1







Now, noting that pm (A)  1 and hence log pm (A)  0, we obtain Pr [E2,j E1 ] m-j   m    Pr log2 p0 (A) + Ui  0 Fi +   i=m-j +1 i=1    m-j m 2    F + = Pr  U  log2  i=1 i i=m-j +1 i A  



-j   m2  2   F  log 1 = Pr  i  A   i=1  







m A m-j Fi + + Ui . 2 i=1 i=m-j +1







Let us now define







and







DC2 (outlined in Algorithm 1) in the noiseless case reduces to the binary search. In this section, we explain DC 2 in the Pr [E2,j E1 ]  1 + 2 . (14) noiseless case (the binary search) with the help of a running Now, to bound 1 we obtain after some simplifications that example given in Figure 6. As we will see, after each round m - j of DC 2 the possible region that h can belong to will be 2   2 log log(2(1 - )) - m2   m - j -j A  "halved". 1 = Pr  i   x x  , 1- 2 We first note that as the initial distribution p0 is assumed  i=1   log    to be the uniform distribution on S 1 , the vector x1 (see step and by using the Chernoff bound we get 2-(a) of Algorithm 1) can indeed be any point on the unit 2   circle S 1 . Thus, DC2 chooses x1 arbitrarily on S 1 . By (2), 2 2   log ) log ( 2 ( 1 -  )) -     m - j   m-j A using the query sign x1 , h  will also give us the value of . 1  exp - 1 -   sign x1 , h . Depending on this value, it is easy to verify 6     log(  )       that only half of S 1 can possibly contain h (see Figure 6). (15) Let us denote this region by R1 . Hence, the probability disTo bound 2 we can similarly write after some simple steps tribution p1 (h) (which is our current belief about h ) is upthat dated as follows: for h  R1 we have that p1 (h) = 0, and  as all the points inside the half-circle  m-j 2j R1 are equiprobable,   log ( 2 ( 1 -  )) + log ( 2  ) m-j m-j    , we have for h  R1 that p1 (h) = 1  . In other words, at i   x 2  Pr  1 -    2  log  -j  time m = 0 the vector h could have been anywhere on the i=1+ m2  







 m-j    m   2 = Pr  Ui  0 Fi +   -j i=m-j +1 i=1+ m2    Using the union bound, we have







and using the Chernoff bound we get 2  2j      m - j  log(2(1 - )) + m-j log(2)   . 2  exp - 1-  6     log(  )       (16) We further note that both of the upper bounds on 1 and 2 decrease when we increase j . Hence, the proof of the theorem follows by letting j = k in (15) and (16), and also plugging these bounds into (13).







B







DC2 in the Noiseless Case







unit circle, but, after round m = 1 it can only belong to the half-circle R1 . Thus, after the first round, DC 2 "halves" the admissible region of h . Continuing in this theme, it is not hard to verify that (see Figure 6) at round m = 2 the value of p2 (h) is non-zero and uniform only on a region R2 which is a quarter-circle. In an inductive manner, letting Rm-1 denote the admissible region (sector) at round m - 1 (see Figure 6) and assuming that pm-1 is only non-zero and uniform on the sector Rm-1 , then xm at round m is precisely the vector that is orthogonal to the midpoint of the sector Rm-1 . Therefore, after observing the value of sign xm , h , the admissible region Rm is the better half of Rm-1 that is compatible with the observation (i.e., it contains h ). Also, Rm is again a sector and pm will be uniform on Rm and zero outside. It is also easy to see that the circular angle for the sector Rm is  . The following statement is now immediate. 2m Theorem 5. Consider DC in the absence of noise ( = 0). If , then it outputs a vector that is within we let T, = log2   a distance  of h . A few comments are in order: The above guarantee for DC 2 holds with probability one and thus the parameter  is irrelevant in the noiseless setting. Furthermore, during each round of DC2 , the distribution pm can be represented by only two numbers (the starting and ending points of the sector Rm ), and the vector xm can be computed efficiently (it is the orthogonal vector to the midpoint of Rm ). Therefore, assuming one unit of complexity for performing the queries, DC 2 can be implemented with complexity O(T, ). Finally, by using Theorem 2, we conclude that DC requires O(d log 1 ) queries with computational complexity  ) . O(d log 1 







Recall that n0 = O(log(1 )) (see Theorem 5). Plugging this into the expression of nR0 , we obtain that the query complexity of repetitive querying is O(log(1 )(log log(1 ) + log(1  )).







D Proof of Theorem 2



At each round, we replace two vectors in E , say e1 and e2 , with the output of DC 2 (e1 , e2 , ,  ); then the cardinality of E decreases by 1. Therefore, each call to DC2 will result in the cardinality of E decreasing by 1. Initially, there are d elements in E ; when the algorithm terminates, there is only one element (i.e., the final output of the algorithm) in E . Thus throughout the entire process of the algorithm, the cardinality of E decreases by d - 1; therefore, there are d - 1 calls to DC2 . If the probability of success for DC 2 is at least 1 -  , then by the union bound the probability of success of DC is at least 1 - (d - 1) . For the second part of the theorem, we prove a more general statement: Assume that we run DC with an input being an orthonormal set {e1 , e2 , . . . , et } where ei , h  Rd and t  d. We should note that the underlying space remains the d-dimensional Euclidean space Rd . We will prove that DC outputs a vector that is close to the normalized orthogonal projection of h onto span{e1 , e2 , . . . , et }. More precisely, we define t  1 ei , h ei h = i=t . i=1 ei , h 2







C







Analysis of Repetitive Querying







Firstly, we would like to compute the probability that the majority vote gives us the correct outcome. Let Ii (1  i  R) is the indicator random variable of the event that the i-th query gives us the right outcome. We know that {Ii  1  i  R} are i.i.d. Bernoulli random variables with success probability 1 - . Let SR = R i=1 Ii . By Hoeffding's inequality, we have Pr[SR  R 2] = Pr[SR - E [SR ]  R 2 - E [SR ]] = Pr[SR - E [SR ]  -R(1 2 - )]



2-)2 R







Then DC runs in d - 1 rounds, calls DC 2 d - 1 times, and out for which puts with probability at least 1 - (d - 1) a vector h   h - h  5d. In exactly similarly way as discussed above, we can conclude that DC runs in d - 1 time and uses DC 2 d - 1 times. Also, again by the union bound, with probability at least 1 - (d - 1) , all outputs of DC2 are a close estimate (within distance ) of their corresponding objective. Thus, by assuming that all the calls of DC 2 have been successful (which happens with probability at least 1 - (d - 1) ), we  - h  5(d - 1). use an inductive argument to prove that h We use induction on t. For t = 2 the result is clear. We now prove the result when t =  assuming that it holds for all t <  . Without loss of generality, assume that the algorithm calls DC 2 (e1 , e2 , ,  ) and the vectors e1 and e2 in E willl be replaced by the output of DC2 (e1 , e2 , ,  ) which we denote by e 1 . We can write h =



 i=1







 e-2(1







.







Suppose that the binary search queries n0 (distinct) points in total throughout the entire procedure. By the union bound, the probability that all n0 majority votes give us the right 2 outcome is greater than or equal to 1 - n0 e-2(1 2-) R . In order to ensure that this probability is at least 1 -  , we need log(n0  ) R . 2(1 2 - )2 Therefore the total number of queries is at least n0 R  n0 log(n0  ) . 2(1 2 - )2







ci e i = c 1 h 1+















ci e i ,



i=3 c1 e1 +c2 e2



2 c2 1 +c2







where ci = h , ei , c 1 = that h 1 =



c1 e1 +c2 e2



2 c2 1 +c2 







 2 c2 1 + c2 and h1 =







. Note







is precisely the normalized orthogonal







projection of h (and also h ) onto span{e1 , e2 }. Using the above notation, we have e 1 - h 1  .







Recall that after obtaining e 1 (the output of DC2 (e1 , e2 , ,  )), the algorithm will recursively call







DC (e 1 , e3 , e4 , . . . , e ). Suppose that the output of this   . By the assumption of the induccall is denoted by h   tion, the output h will be within the distance 5( - 2) of the normalized orthogonal projection of h onto span{e 1 , e3 , e4 , . . . , e }, which we denote by h . That is, We know that   - h  5( - 2). h







Thirdly, similarly we have h - h , e 1 e 1 +



 







ci e i



i=3 







=



i=1







h =







h , e 1 e 1 +  i=3 ci ei



2 h , e 1 2 +  i=3 ci







.







Now we will show that







h - h  5. If this is true, we will have   - h + h - h  5( - 1) h -  h  h







= c1 e1 + c2 e2 - h , e 1 e 1   = h , h  h -  h , e   e  1 1 1 1     h , h  h -  h , h 1 + h , h 1 - h , e 1 e 1 1 1 1 e 1 e      = h , h1 (h1 - e 1 ) + h , h1 - e 1 e 1 1  h 1 + h 1 -e 1 -e  2. Therefore h - (h , e 1 e 1 +  i=3 ci ei )  2   2   1 - 2  3,







ci ei - h , e 1 e 1 +







ci e i



i=3







and this completes the proof. Therefore, it suffices to show that h - h  5. 2 1 2 +  We define  = h , e i=3 ci . Firstly, we have h - h h -











= = = 







h , e 1 e 1 +  i=3 ci ei



2 h , e 1 2 +  i=3 ci 







(19)







h - (h







,e 1 e 1 +  i=3 ci ei ) 







where the last step follows from   5 18. Now, by plugging (18) and (19) into (17) we get h - h  2 + 3 = 5. Hence we have   h   - h + h - h  5( - 2) + 5 = 5( - 1). h - h







h - (h 1- + 











( - 1)h + h - (h , e 1 e 1 +  i=3 ci ei ) 











,e 1 e 1 +  i=3 ci ei ) 







. (17)







Secondly, we have 2 - 1 = = = h , e 1 2 + h , e 1 2 +



 i=3  i=3







c2 i -1 2 c2 1+ i - c



 i=3







c2 i







2 = h , e 1 2 - h , h 1  = h , e 1  - h , h 1  + h , h 1   h , e 1     1  + h , h = h , e 1 - h1   h , e 1   h  e 1 - h  e 1 + h  h 1 ( h 1 )  2,







h , e 1 2 - c 2 1







where the last step follows from h = e 1 = h 1 = 1 and e 1 - h   . Since   5 18 < 1 2 , we obtain 1    1 - 2, 1 + 2 and 1 1 1-  max  - 1, 1 -   2.  1 - 2 1 + 2 (18)







Deploying CommunityCommands: A Software Command Recommender System Case Study



Wei Li Justin Matejka Tovi Grossman George Fitzmaurice



Autodesk Research, Toronto, Canada. firstname.lastname@autodesk.com







Abstract



In 2009 we presented the idea of using collaborative filtering within a complex software application to help users learn new and relevant commands (Matejka et al. 2009). This project continued to evolve and we explored the design space of a contextual software command recommender system and completed a four-week user study (Li et al. 2011). We then expanded the scope of our project by implementing CommunityCommands, a fully functional and deployable recommender system. CommunityCommands was made available as a publically available plug-in download for Autodesks flagship software application AutoCAD. During a one-year period, the recommender system was used by more than 1100 AutoCAD users. In this paper, we present our system usage data and payoff. We also provide an in-depth discussion of the challenges and design issues associated with developing and deploying the front end AutoCAD plug-in and its back end system. This includes a detailed description of the issues surrounding cold start and privacy. We also discuss how our practical system architecture was designed to leverage Autodesks existing Customer Involvement Program (CIP) data to deliver in-product contextual recommendations to endusers. Our work sets important groundwork for the future development of recommender systems within the domain of end-user software learning assistance.







Introduction



Modern computer programs can have thousands of commands available to the user, with a general tendency to increase year after year (Baecher et al. 2000). For example, AutoCAD is a widely used software application for both 2D and 3D drafting and design. The number of commands in AutoCAD has been growing linearly and consistently over time. While the growth of commands increases a systems capabilities, the quantity can make learning the system a challenge. In particular, a users lack of



Copyright (c) 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







awareness of relevant functionality can act as a barrier to their efficiency with the system (Grossman et al. 2009, Shneiderman 1983). In a "best case scenario", a user would work with an expert next to them who could recommend commands when appropriate (Grossman et al. 2009). Indeed, this type of "over the shoulder" learning has been shown to be valuable in the workplace (Twidale 2005), yet it is obviously impractical to assume such assistance would be readily available. One promising way to address this challenge is to provide users with in-product command recommendations. Existing techniques, such as "tip-of-day" and "did you know", can expose new features, but they may be irrelevant to the user current task (Fischer 2001, Norman et al. 1986). An alternative is to provide personalized command recommendations, based on the users own history of usage. While some research has been initiated in this area (Linton and Schaefer 2000, Matejka et al. 2009), working implementations which deliver these recommendations have never been embedded within a target application. We contribute a recommender system that was released as a plug-in for AutoCAD, and has been used in real usage scenarios. During a one year period of time, over a thousand AutoCAD users downloaded and installed our CommunityCommands plugin from the official Autodesk1 website as a technology preview. In this paper, we provide an in-depth discussion of the important issues and challenges we have encountered during the development and deployment of this system. This includes many of the technical details of the recommender system itself, as well as the system architecture and implementation details required to make a real-time command recommender system hosted on a users local machine, work in practice. In particular, we



1







http://www.autodesk.com







discuss the key challenges associated with the domain of software functionality recommendations that required us to diverge from the traditional treatment of recommender system problems. This includes: cold start issue; contextual in-product real-time recommendations; and the system architecture to deliver the personal recommendations to end-users, while also protecting user privacy. In our software command recommender design, we leverage an existing Customer Involvement Program (CIP), which provides a mechanism to collect user command sequence logs anonymously. We also propose a novel architecture that pushes the item-by-item similarity matrix to each users computer. For users who have privacy and data security concerns, this push model can enable a download-only recommender being deployed to their systems. In addition to privacy concerns, CIP also provides valuable data source for solving the cold start problem. Our hope is that the presentation of these important details will set the groundwork for the future development of recommender systems within the domain of end-user software.







are not relevant to the individuals workflow will be avoided. In our previous work (Matejka et al. 2009, Li et al. 2011) we performed several offline evaluations and an online evaluation with a limited number of study participants. In this paper, we describe our deployment of Community-Commands, made available for public download and usage. We provide a detailed description of the system architecture, and report and reflect on the data which was collected from our deployment used by over 1,000 actual AutoCAD users resulting in over 55,000 command recommendations issued over a one year period of time.







Challenges of Building and Deploying a Software Command Recommender System



In this section, we describe a number of challenges that we encountered while preparing our system for public deployment. Privacy The issue of user privacy has been explored by recommender system users and researchers (Frankowski et al. 2006, Ramakrishnan et al. 2001). In many recommender systems, a central server has access to all user profiles and generates personal recommendations. This type of architecture may reveal details about the user, gained through examining their user-item relations. Some privacy research has focused on using a decentralized server architecture combined with strong algorithms to secure users data (Ahmad and Khokhar 2007, Berkovsky et al. 2007, Shokri et al. 2009), but this still requires user data to be sent to a network server. The issue of privacy is a significant concern for CommunityCommands. Customers often worry that their usage behaviors and data is being logged. For design software, such as AutoCAD, customer-generated data can be extremely sensitive. In an ideal usage situation, software users should have options and be able to control when to upload their software usage data. Cold Start The "Cold-Start" problem is a well-known issue in recommendation systems (Schein et al. 2001). For our implementation, we would have no previous data related to the individual users behavior, and thus, no information to base the recommendations on. It is also difficult to generate the required user-by-user or item-by-item similarity matrices without an existing software usage data set, which results in an inability to draw inferences to recommend items to users. Due to concerns surrounding privacy, it can be difficult to collect the usage data necessary to provide useful recommendations.







Prior Work



Collaborative filtering based recommender systems have become an important tool to help users deal with information overload and provide personalized suggestions (Hill et al. 1995, Shardanand and Maes 1995). Examples include recommending movies (Miller et al. 2003), news (Resnick et al. 1994), and books (Linden et al. 2003). However, little research has been conducted to help users learn and explore a complicated software package using a recommender system. We are aware of two such systems that have been proposed in the literature: OWL for Microsoft Office (Linton and Schaefer 2000) and CommunityCommands for Autodesk AutoCAD (Li et al. 2011). The OWL System compares a target users command frequencies to the average command frequencies of an entire user population. Based on the difference between these frequencies, OWL recommends commands that the target user should use either more or less often. OWL was designed to run within an organization, so it assumes that all users in the community should share the same command usage distribution, and in turn, use the software system in the same way. Across a broad user community, this assumption is unlikely to hold true. Users have different tasks, and preferences, so recommendations should be personalized (Mitchell and Shneiderman 1989). In contrast, CommunityCommands uses personalized collaborative filtering to produce recommendations tailored to an individual (Li et al. 2011). This adds a significant benefit over the OWL system; commands that







In-product Recommendation Another design challenge of our system is that it provides the recommendations within the product, and they are updated in real-time. This requires the recommendations to be available immediately, unlike previous software recommendation systems in which users receive periodic email updates (Linton and Schaefer 2000). Because of the in-product design and the possibility that the users might not have internet connections, the computations of recommendations must occur locally.







Customer Involvement Program



Many software applications have Customer Experience Improvement Programs2 (CEIP) or Customer Involvement Programs3 (CIP) to help collect users feedback (called CIP in the rest of paper). CIP lets users choose to send usage data to the software designers and developers, so they can get anonymous information about how their programs are being used. CIP usually gathers product usage and system configuration information, such as system memory, video card, screen resolution, and operating system details at regular intervals. This type of data is not particularly sensitive. However, in the aggregate, data items such as these give software developers a great deal of insight into what features customers are using, how well they're working, and where they could be improved.







CIP, when they execute a command, this action is recorded in the form of a (userID, commandID, timestamp) tuple in a centralized database. The voluntary nature of CIP also provides options to software users to either upload their command log to a central CIP server or keep the log on their computers. AutoCAD users can also turn off CIP anytime by clicking a menu item. Our system leverages CIP to generate command recommendations while users have the option of not revealing their personal information. Before the deployment of CommunityCommands, we used existing CIP data to solve the cold-start problem and implicitly define a rating scheme and generate item-by-item correlations.







Application Description



System architecture Based on the encouraging results of our one month user study (Li et al. 2011), we developed our recommender prototype into a plug-in for AutoCAD and released it to the public. The system runs as a palette embedded in the AutoCAD workspace, providing within-application and real-time recommendations while a user goes about their normal usage of the software (Figure 2).







Figure 2. System architecture.







Figure 1. Customer Involvement Program (CIP) Enrollment Interface in AutoCAD 2012.







In AutoCAD, command usage histories are collected as a part of the CIP data. A CIP participation window is presented to AutoCAD users during the software installation process (Figure 1). Sample data and generated reports are also presented to explain that the users privacy is still being protected. If the user agrees to participate in



2 3







http://www.microsoft.com/products/ceip/EN-US/default.mspx http://www.autodesk.com/acip/CIP_Privacy_eng.html







Here we describe our architecture design of this fully functional system for software command recommendations. The system architecture is composed of three components: the user's local machine, the CommunityCommands server, and the main AutoCAD CIP server (Figure 2). When the plug-in is installed and connected to the Internet, a 1.8 MB item-by-item similarity matrix is downloaded (pushed) to the users local machine, which is used for the item-based recommendation algorithm. The local machine collects the users command sequence, and computes the recommendations locally each time a new command is issued (using an item-based algorithm). In addition, the CommunityCommands server continuously receives command sequence logs from AutoCADs main CIP server. This allows us to generate







recommendations based on usage profiles of AutoCAD users that may not be running our plug-in. On a monthly basis, the server computes a new item-by-item similarity matrix and each users local machine downloads and replaces their existing matrix. This system architecture provides two important and unique design properties: preserving privacy and in-product recommendations. Push based recommendations For traditional recommender systems, there is no easy way to generate personalized recommendations, without some central system first receiving a user's data. In CommunityCommands, instead of uploading the users  data to the central server, the server pushes the similarity matrix to the user's local computer. Thus, we can still generate a personalized recommendation command list without ever receiving data from that user. The recommendations are still based on the personal data at the local computer, and the aggregated CIP data. In-product recommendations Recommended commands are placed in a list within the AutoCAD plug-in palette (Figure 3).







AutoCAD has a scripting language that can issue multiple commands without user input, we defer processing the recommendations and updating the UI until our threshold idle time of 0.5 seconds has been satisfied.







Figure 4. Recommended and recently used commands. Tooltip appears when mouse is hovered over the command.







Figure 3. Recommender plug-in palette is opened in AutoCAD.







Clicking on the command button executes the command. If a command in the recommendation list is used, it is immediately removed from the list and displayed in a "most recently used commands" list. Hovering over the command button causes the standard AutoCAD tooltip to appear, and dwelling longer reveals an extended tooltip with additional usage information (Figure 4). During our development process, we found it critical to be minimally disruptive to the computational resources needed by the main application. Under normal usage, computation of recommendations is unnoticeable to the user, so we compute the recommendations after an individual command has been executed. However, we have to delay the recommender computation if we observe a rapid succession of command usage. In addition, since







Training before recommending To further address the cold start problem, the plug-in begins in a training period, where commands are logged, but no recommendations are presented. Determining the right length of this training period is difficult - we wanted the recommendations to start as soon as possible, but only after we reliably know what commands the user is already aware of. To minimize the time needed for training, we ran a pilot test by analyzing data from 27 users (Li et al. 2011). On a daily interval, we measured the rate at which new commands were used (had not been previously observed for that user), across a period of 4 weeks (Figure 5). The data showed that the rate of using "new commands" levels off quickly. For example, after 8 days, 50% of users had less than 3 new commands per day. However, because users will have different daily usage rates, this public released recommender exits the training phase when the user performs less than 3 new commands on two consecutive days. To ensure enough data has indeed been collected, it also requires that the training phase was active for at least 10 usage days, or, until at least 200 commands have been captured.







Figure 5. New command adoption rates based on 27 users.







During this training phase, we display a message to the user, and use the pallet to provide access to recently used







commands. This gives the users some value, while waiting for the recommendations to begin (see Figure 6).







The inverse user frequency (iuf), a measure of the general importance of the command, is based on the percentage of total users that use it: | | |{ where: | |: total number of users in the community |* +|: Number of users who use ci. - A high rating in cf-iuf is obtained when a command is used frequently by a particular user, but is used by a relatively small portion of the overall population. For each user uj, we populate the command vector Vj such that each cell, Vj(i), contains the cf-iuf value for each command ci, and use these vectors to compute user similarity. Rather than matching users based on their command usage, our item-based collaborative filtering algorithm matches the active users commands to similar commands. Similar to user-based approach, each cell, Vi(j), contains the cf-iuf value for each user uj. In our released recommender, we applied item-based approach and customized our suggested commands based on active users short term preference (session-based command history) to generate contextual in-product real-time recommendations (Li et al. 2011). Novelty evaluation metrics Command recommendation is a top-N recommendation problem, which identifies a set of N commands that will be of interest to a user (Karypis 2001, Herlocker et al. 2004). We consider good recommendations to be those where the user was not previously familiar with the command, but after seeing the suggestion, will use it. As such, we were required a metric that would indicate usefulness and novelty. To do so, we developed a k-tail evaluation which dynamically measures the usefulness of an algorithm based on the sequential information in a users command log (Matejka et al. 2009). Here, we propose to approximate a command recommendations novelty factor using its binomial probability. We call this the binomial novelty indicator (BNI). To evaluate the novelty of the recommendations, we compute the probability that a command, which was correctly predicted by the recommender, would appear in the testing set by random chance. We do this by using the binomial probability formula, based on a commands overall frequency across the entire user community: ( ) ( ) ( ) With those two metrics we can compute the cf-iuf as }|







Figure 6. Recommender training phase UI







Use of AI Technology



As alluded to in our review of the related work, there are a number of unique considerations to address in developing a collaborative filtering system for software commands. Ratings Standard collaborative filtering algorithms work by viewing a dataset as a rating matrix. These ratings are either captured implicitly, for example, through purchase records and browsing histories, or explicitly, by asking users to rate the items. We need to map users command history onto a rating matrix. One approach is to allow a user to give explicit ratings for each command. This approach would not utilize the users historical data and would thus suffer from the cold start problem (Schein et al. 2001). Moreover, an explicit rating system would be impractical, since software application users will be focused on their primary task, not on rating the functions which they use. In addition, research has shown that users may be reluctant to provide explicit ratings (Shardanand and Maes 1995). As such, the implicit acquisition of user preferences of software commands is more favorable in practice. Our method uses the command frequency to imply the rating for the user (Li at el 2011). To model how important a command is to a particular user within a community, and to suppress the overriding influence of commands that are being used frequently and by many users, we have adapted tf-idf (Jones 1972) into a command frequency, inverse user frequency (cf-iuf) rating function. We first take the command frequency (cf) to give a measure of the importance of the command ci to the particular user uj.











where nij is the number of occurrences of the considered command for user uj, and the denominator is the number of occurrences of all commands for user uj.







where P(k) is the probability of a specific command C executed exactly k times in a commands sequence of length l, and p is the overall probability of C being executed in the dataset. The cumulative distribution function for k can be expressed as: ( ) ( ) ( )







F(l,l,p) represents the chance of seeing command C at least once. So we define the binomial novelty indicator (BNI) as: ( ) ( ) ( )







recommendations into their regular workflows. Figure 7 shows the number of recommended commands being used by the users who have moved past the training phase. The figure contains the recommended commands being used at least once, three times, ten times and twenty times. We call those commands adopted recommendations or useful recommendations. On average, 21.4 recommendations were used by users at least once. 14 new recommendations were used by users more than three times, 9.6 for 10 times and 7.3 for 20 times.







This gives us an explicit measurement as to the likelihood a recommended command would have appeared in the sequence by chance. For example, consider a command A that has a frequency of 0.036 and a command B that has a frequency of 0.002, across all users, and a testing set with 13000 commands. We compute that there is a 95% chance that A appears in the testing set once or more, and a 3% chance that B appears once or more. If the recommender predicts both A and B correctly, we can be reasonably certain that the user more likely knew A than B. Comparing this across all correctly recommended commands, we can get a measurement of how novel, overall, the commands that a recommender algorithm generates are. Thus, we combine BNI with k-tail offline evaluation by computing the mean of BNI for every unique command in RT, where l is the length of T. Our deployed recommender uses both k-tail and BNI to select collaborative filtering algorithms and tuning parameters.







Figure 7. Recommended command adoption







Figure 8. Total adopted useful recommendations over deployed time. The horizontal axis shows the percentage of time passed.







Application Use and Payoff



Overall Usage We report how long the CommunityCommands plugin was deployed on the users system. This deployment time was calculated using the time stamps of the first and last time the user ran the recommender. During the one year period after we released this recommender system, approximately 1100 AutoCAD users downloaded and installed the plugin. 983 users used the plug-in for at least one day. 709 users used the plug-in for more than 30 days. On average, the plug-in was installed at the users computer for more than two months (69.8 days). We also observed that most users who have very short usage times did not pass their training phase before they uninstalled or disabled the plugin. Recommendation adoption Our hope is that users of the recommender system would start using the recommended commands. We hope they not only try the command a few times, but adopt the







Figure 8 shows the distribution of the total adopted recommendations over time. Here we assume all users start at the same time and spend the same amount time using the system. This figure shows 50% recommendation adoptions happened during the first 19% of the entire period of system usage time. CommunityCommands only recommend commands that had never been executed in the users command history. But there may be commands used by the user before the installation of the plug-in. As such, some of these adopted commands may have already been known to the user. CIP Enrollment CIP is a key component for solving the users privacy concerns and cold-start problem. A large group of users (71.3%) who downloaded the CommunityCommands plugin enrolled in CIP. This of course means that 28.7% of users did not enroll into CIP, mostly due to privacy and technical concerns. As such, our system needs to work for both user groups.







Command usage visualization To help visualize the data which was collected during our deployment, we developed Personal Software Usage DNA diagrams for the users of our plug-in. These diagrams are generated by looking at the command usage patterns of each individual user. By ordering the commands based on the communitys overall usage, and coloring them based on the individuals usage, we can see commands that an individual is using more (or less) often than the community as a whole. By looking at how densely the individual row is filled in, we can also see if the individual uses a lot, or relatively few commands.







Conclusion and Future Work



Based on our experiences, we believe that recommender systems have a rich future for use within software applications. We have provided a detailed treatment of the issues surrounding the development of a command recommender system and the architecture used for its deployment. Our hope is that this research will serve as groundwork and inspiration for future efforts in this area. We have shown that collaborative filtering algorithms can identify commands that will be useful to a user. This leads us to believe that such systems could also be used to recommend higher-level task flows and relevant tutorial materials. The item-based collaborative filtering provides relevant and novel recommendations. It aggregates user-item relations into item-item relations. When combined with the system architecture we proposed here, the item-based algorithm can also preserve user's privacy, which is a desirable feature for many business applications. Certain software applications, including AutoCAD, have a main version, but also "parallel" customized versions for specific user groups. By using collaborative filtering technology, we will be able to recommend customized software features to the appropriate user groups. For example, AutoCAD has vertical versions for mechanical engineers, electric engineers, civil engineers and architects. Recommending commands commonly used by civil engineering to architects, when those commands fit the current workflow, could increase the diversity and novelty of current recommendations. Another issue is related to software upgrades. In ecommerce situations, when new products or services emerge, the interest of customers and the temporal feature of the ratings in collaborative filtering may change. Previous work (Ding and Li 2005) has used a time weighted item-by-item correlation to track concept drifting. It would be interesting to apply this same idea to help introduce new commands in each release of a software package to the users and allow the newer and







Figure 9. Legend of software usage DNA diagram







Figure 9 presents the information included in each DNA diagram. A red command name means that the command was recommended but was removed by the user from the recommendation list. A green command name means that it was normally showed in the recommendation list. The brightness of the command background represents the usage frequency of that command. So a green command on a bright background is a strongly adopted recommendation. Figure 10 shows the 17 most active users DNA diagram, with the top user enlarged. In the future, it could be interesting to present these Personal Software Usage DNA diagram to the end users, to encourage usage reflection and further command adoption.







Figure 10. Software usage DNA diagrams from the 17 most active users.







potentially more efficient commands to be recommended. In summary, the novel contribution of our work is the description of system architecture that has allowed us to embed a software command recommender system within a target application, during real usage situations. Software command/feature recommendation opens a new domain for recommender system research. Many interesting problems arise which open up areas for future work.







international conference management. 247-254.







on







Information







and







knowledge







Linden, G., Smith, B. and York, J. (2003). Amazon.com Recommendations: Item-to-Item Collaborative Filtering. IEEE Internet Computing. 7(1):76-80. Linton, F. and Schaefer, H.-P. (2000). Recommender Systems for Learning: Building User and Expert Models through Long-Term Observation of Application Use. User Modeling and UserAdapted Interaction. 10(2-3):181-208. Matejka, J., Li, W., Grossman, T. and Fitzmaurice, G. (2009). CommunityCommands: Command Recommendations for Software Applications. Proceedings of the 22nd Symposium on User Interface Software and Technology.193-202. Miller, B. N., Albert, I., Lam, S. K., Konstan, J. A. and Riedl, J. (2003). MovieLens unplugged: experiences with an occasionally connected recommender system. Proceedings of the 8th international conference on Intelligent user interfaces. 263-266. Mitchell, J. and Shneiderman, B. (1989). Dynamic versus static menus: an exploratory comparison. SIGCHI Bull. 20(4):33-37. Norman, D. A. and Draper, S. W., User Centered System Design; New Perspectives on Human-Computer Interaction. 1986: L. Erlbaum Associates Inc. 526. Schein, A., Popescul, A., Ungar, L., Pennock, D. (2001). Generative Models for Cold-Start Recommendations. the 2001 SIGIR Workshop on Recommender Systems. Ramakrishnan, N., Keller, B. J., Mirza, B. J., Grama, A. Y. and Karypis, G. (2001). Privacy Risks in Recommender Systems. IEEE Internet Computing. 5(6):54-62. Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P. and Riedl, J. (1994). GroupLens: an open architecture for collaborative filtering of netnews. Proceedings of the 1994 ACM conference on Computer supported cooperative work. 175-186. Shardanand, U. and Maes, P. (1995). Social information filtering: algorithms for automating "word of mouth". Proceedings of the SIGCHI conference on Human factors in computing systems. 210-217. Shneiderman, B. (1983). Direct Manipulation: A Step Beyond Programming Languages. Computer. 16(8):57-69. Shokri, R., Pedarsani, P., Theodorakopoulos, G. and Hubaux, J.P. (2009). Preserving privacy in collaborative filtering through distributed aggregation of offline profiles. Proceedings of the third ACM conference on Recommender systems. 157-164. Twidale, M. B. (2005). Over the Shoulder Learning: Supporting Brief Informal Learning. Comput. Supported Coop. Work. 14(6):505-547. Li, W., Matejka, J., Grossman, T., Konstan, J. A. and Fitzmaurice, G. (2011) Design and Evaluation of a Command Recommendation System for Software Applications. ACM Trans. Comput.-Hum. Interact.,18(2):6:1-6:35







Acknowledgement



We would like to thank Joseph A. Konstan for providing valuable suggestions and participating discussions during this project development.







References



Ahmad, W. and Khokhar, A. (2007). An Architecture for Privacy Preserving Collaborative Filtering on Web Portals. Proceedings of the Third International Symposium on Information Assurance and Security. 273-278. Baecker, R., Booth, K., Jovicic, S., McGrenere, J. and Moore, G. (2000). Reducing the gap between what users know and what they need to know. Universal Usability-2000. 17-23. Berkovsky, S., Eytani, Y., Kuflik, T. and Ricci, F. (2007). Enhancing privacy and preserving accuracy of a distributed collaborative filtering. Proceedings of the 2007 ACM conference on Recommender systems. 9-16. Ding, Y. and Li, X. (2005). Time weight collaborative filtering. Proceedings of the 14th ACM international conference on Information and knowledge management. 485-492. Fischer, G. (2001). User Modeling in Human\&ndash;Computer Interaction. User Modeling and User-Adapted Interaction. 11(12):65-86. Frankowski, D., Cosley, D., Sen, S., Terveen, L. and Riedl, J. (2006). You are what you say: privacy risks of public mentions. Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 565-572. Grossman, T., Fitzmaurice, G. and Attar, R. (2009). A Survey of Software Learnability: Metrics, Methodologies and Guidelines. ACM CHI conference on Human Factors in Computing Systems. 10 pages. Herlocker, J. L., Konstan, J. A., Terveen, L. G. and Riedl, J. T. (2004). Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22(1):5-53. Hill, W., Stead, L., Rosenstein, M. and Furnas, G. (1995). Recommending and evaluating choices in a virtual community of use. Proceedings of the SIGCHI conference on Human factors in computing systems. 194-201. Jones, K. S. (1972). A statistical interpretation of specificity and its application in retrieval. Journal of Documentation. 60(5):10. Karypis, G. (2001). Evaluation of Item-Based Top-N Recommendation Algorithms. Proceedings of the tenth







Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)







Fast Nonsmooth Regularized Risk Minimization with Continuation



Shuai Zheng, Ruiliang Zhang, James T. Kwok



Department of Computer Science and Engineering Hong Kong University of Science and Technology Hong Kong {szhengac, rzhangaf, jamesk}@cse.ust.hk  with the much faster O(1/ T ) and O(1/T ) rates, respectively (Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2013). Recently, Shamir and Zhang (2013) recovered these rates by using a polynomial-decay averaging scheme on the SGD iterates. However, a major drawback is that it does not exploit properties of the regularizer. For example, when used with a sparsity-inducing regularizer, its solution obtained may not be sparse (Duchi and Singer 2009). Nesterov (2005b) proposed to smooth the nonsmooth objective so that it can then be efficiently optimized. This smoothing approach is now popularly used for nonsmooth optimization. However, the optimal smoothness parameter needs to be known in advance. This restriction is later avoided by the (batch) excessive gap algorithm (Nesterov 2005a). In the stochastic setting, Ouyang and Gray (2012) combined Nesterov's smoothing with SGD. Though these methods achieve the fastest known convergence rates in the batch and stochastic settings respectively, they assume a Lipschitz-smooth regularizer, and nonsmooth regularizers (such as the sparsity-inducing regularizers) cannot be used. Recently, based on the observation that the training set is indeed finite, a number of fast stochastic algorithms are proposed for both smooth and composite optimization problems (Schmidt, Roux, and Bach 2013; Johnson and Zhang 2013; Xiao and Zhang 2014; Mairal 2013; Defazio, Bach, and Lacoste-Julien 2014). They are based on the idea of variance reduction, and attain comparable convergence rates as their batch counterparts. However, they are not applicable when both the loss and regularizer are nonsmooth. To alleviate this, Shalev-Shwartz and Zhang (2014) suggested running these algorithms on the smoothed approximation obtained by Nesterov's smoothing. However, as in (Nesterov 2005b), it requires a careful setting of the smoothness parameter. Over-smoothing deteriorates solution quality, while under-smoothing slows down convergence. The problem of setting the smoothness parameter can be alleviated by continuation (Becker, Bobin, and Cand es 2011). It solves a sequence of smoothed problems, in which the smoothing parameter is gradually reduced from a large value (and the corresponding smoothed problem is easy to solve) to a small value (which leads to a solution closer to that of the original nonsmooth problem). Moreover, solution of the intermediate problem is used to warm-start the next smoothed problem. This approach is also similar to that







Abstract



In regularized risk minimization, the associated optimization problem becomes particularly difficult when both the loss and regularizer are nonsmooth. Existing approaches either have slow or unclear convergence properties, are restricted to limited problem subclasses, or require careful setting of a smoothing parameter. In this paper, we propose a continuation algorithm that is applicable to a large class of nonsmooth regularized risk minimization problems, can be flexibly used with a number of existing solvers for the underlying smoothed subproblem, and with convergence results on the whole algorithm rather than just one of its subproblems. In particular, when accelerated solvers are used, the proposed algorithm achieves the fastest known rates of O(1/T 2 ) on strongly convex problems, and O(1/T ) on general convex problems. Experiments on nonsmooth classification and regression tasks demonstrate that the proposed algorithm outperforms the state-of-the-art.







Introduction



In regularized risk minimization, one has to minimize the sum of an empirical loss and a regularizer. When both are smooth, it can be easily optimized by a variety of solvers (Nesterov 2004). In particular, a popular choice for big data applications is stochastic gradient descent (SGD), which is easy to implement and highly scalable (Kushner and Yin 2003). For many nonsmooth regularizers (such as the 1 and nuclear norm regularizers), the corresponding regularized risks can still be efficiently minimized by the proximal gradient algorithm and its accelerated variants (Nesterov 2013). However, when the regularizer is smooth but the loss is nonsmooth (e.g., the hinge loss and absolute loss), or when both the loss and regularizer are nonsmooth, proximal gradient algorithms are not directly applicable. On nonsmooth problems, SGD can still be used, by simply replacing the gradient with subgradient. However, the information contained in the subgradient is much less informative (Nemirovski and Yudin 1983), and convergence is hindered. On general  convex problems, SGD converges at a rate of O(log T / T ), where T is the number of iterations; whereas on strongly convex problems, the rate is O(log T /T ). In contrast, its smooth counterparts converge



Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







2393







of gradually changing the regularization parameter in (Hale, Yin, and Zhang 2007; Wen et al. 2010; Mazumder, Hastie, and Tibshirani 2010). Empirically, continuation converges much faster than the use of a fixed smoothing parameter (Becker, Bobin, and Cand es 2011). However, the theoretical convergence rate obtained in (Becker, Bobin, and Cand es 2011) is only for one stage of the continuation algorithm (i.e., on the smoothed problem with a particular smoothing parameter), while the convergence properties for the whole algorithm are not clear. Recently, Xiao and Zhang (2012) obtained a linear convergence rate for their continuation algorithm, though only for the special case of 1 -regularized least squares regression. In this paper, we consider the general nonsmooth optimization setting, in which both the loss and regularizer may be nonsmooth. The proposed continuation algorithm can be flexibly used with a variety of existing batch/stochastic solvers in each stage. Theoretical analysis shows that the proposed algorithm, with this wide class of solvers, achieves the rate of O(1/T 2 ) on strongly convex problems, and O(1/T ) on general convex problems. These are the fastest known rates for nonsmooth optimization. Note that these rates are for the whole algorithm, not just one of its stages as in (Becker, Bobin, and Cand es 2011). Experiments on nonsmooth classification and regression models demonstrate that the proposed algorithm outperforms the state-of-the-art. Notation. For x, y  Rd , x



d 2







Other examples in machine learning include popular regularizers such as the 1 , total variation (Becker, Bobin, and Cand es 2011), overlapping group lasso, and graph-guided fused lasso (Chen et al. 2012). Minimization of the smooth (and convex) g  can be performed efficiently using first-order methods, including the so-called "optimal method" and its variants (Nesterov 2005b) that achieve the optimal convergence rate.







Nesterov Smoothing with Continuation



Consider the following nonsmooth minimization problem min P (x)  f (x) + r(x),



x







(5)







where both f and r are convex and nonsmooth. In machine learning, x usually corresponds to the model parameter, f is the loss, and r the regularizer. We assume that the loss f on a set of n training samples can be decomposed as n 1 f (x) = n i=1 fi (x), where fi is the loss value on the ith sample. Moreover, each fi can be written as in (1), i.e.,  fi (x) = f i (x) + maxuU [ Ai x, u - Q(u)]. One can then apply Nesterov's smoothing, and P (x) in (5) is smoothed to   (x) = f P  (x) + r (x),  where f  (x) =



1 n n i=1 uU







(6)







 f i (x) and (7)







=







d i=1







x2 i is its







2 -norm,







  f i (x) = fi (x) + max [ Ai x, u - Q(u) -  (u)] .







x 1 = i=1 |xi | is its 1 -norm, and x, y is the dot product between x, y . Moreover, f denotes the subdifferential of a nonsmooth function f , if f is differentiable, then f denotes its gradient. I is the identity matrix.







As for r, we assume that it is "simple", namely that its prox1 x - * 2 + r(x) for imal operator, proxr (*)  arg minx 2 any  > 0, can be easily computed (Parikh and Boyd 2014).







Related Work



Consider nonsmooth functions of the form g (x) = g (x) + max[ Ax, u - Q(u)],



uU







Strongly Convex Objectives



In this section, we assume that P is -strongly convex. This strong convexity may come from f (e.g., 2 -regularized hinge loss) or r (e.g., elastic-net regularizer) or both. Assumption 1. P is -strongly convex, i.e., there exists  > 2 0 such that P (y )  P (x) +  T (y - x) +  2 y - x 2 ,   d P (x) and x, y  R . The proposed algorithm is based on continuation. It proceeds in stages, and a smoothed problem is solved in each stage (Becker, Bobin, and Cand es 2011). The smoothness parameter is gradually reduced across stages, so that the smoothed problem becomes closer and closer to the original one. In each stage, an iterative solver M is used to solve the smoothed problem. It returns an approximate solution, which is then used to warm-start the next stage. In stage s, let the smoothness parameter be s , the s (x), x = arg minx P s (x), smoothed objective in (6) be P s and x s be the solution returned by M. As M is warms ( started by x s-1 , the error before running M is P xs-1 ) -   Ps (xs ). At the end of stage s, we assume that the error is reduced by a factor of s . The expectation E below is over the stochastic choice of training samples for a stochastic solver. For a deterministic solver, this expectation can be dropped. s ( s (x )  s (P s ( Assumption 2. EP xs ) - P xs-1 ) - s   Ps (x )), where s  (0, 1).



s







(1)







where g  is convex, continuously differentiable with L p Lipschitz-continuous gradient, U  R is convex, A  Rpxd , and Q is a continuous convex function. Nesterov (2005b) proposed the following smooth approximation: (x) + max [ Ax, u - Q(u) -  (u)] , g  (x) = g



uU







(2)







where  is a smoothness parameter, and  is a nonnegative  -strongly convex function. For example, consider the hinge loss g (x) = max(0, 1 - T x), where x is the linear model parameter, and (zi , yi ) yi zi is the ith training sample with yi  {1}. Using  (u) = 1 2 2 u 2 , g can be smoothed to (Ouyang and Gray 2012)  T x1 yi zi  0  T T 1 - yi zi x - 2 y i zi x < 1 -  . (3) g  (x) =  1 (1 - y z T x)2 otherwise i i 2



T x| can be smoothed to Similarly, the 1 loss g (x) = |yi - zi   T T yi - zi x  yi - zi x - 2  T T -(yi - zi x) - 2 yi - zi x < - . (4) g  (x) =  1 (y - z T x)2 otherwise i i 2







2394







Table 1: Examples of non-accelerated and accelerated solvers. Note that Prox-GD and APG are batch solvers while the others are stochastic solvers. Here,  and p are parameters related to the stepsize, and are fixed across stages. In particular,   (0, 0.25) and  p) satisfies (1 - 4)s - 4 > 0, and p  (0, 1) and satisfies s > p(2+ 1-p . Accelerated Prox-SVRG has Ts = O ( s log(1/s )) only when a sufficiently large mini-batch is used.



Prox-GD (Nesterov 2013) Prox-SVRG (Xiao and Zhang 2014) SAGA (Defazio, Bach, and Lacoste-Julien 2014) MISO (Mairal 2013) APG (Schmidt, Roux, and Bach 2011) Accelerated Prox-SVRG (Nitanda 2014) Ts 4s log(1/s )  (1-4 )s -4 (s + 4) 3n 3s s n +1  s log(2/s )  1 s (1-2 s p) log -



2+p







non-accelerated















ns s







accelerated







p 1-p







log(2/s ) 1 log s - p



2+p







1 (1-4 )s -4 1 s 1 s







(s ) log(1/s )







1-p







 2 (1-p)







a 4  9 n 1







b 0 4 3n 0 0 0







c 0 0 0 0 0 0







We consider two types of solvers, which differ in the number of iterations (Ts ) it takes to satisfy Assumption 2. 1. Non-accelerated solvers: Ts = as (s ) + b(s ) + c;  2. Accelerated solvers: Ts = a s (s ) + b(s ) + c. Here, s is the condition number of the objective, a, b, c  0 are constants not related to s and (s ). Moreover,  satisfies (i) (s ) > 0 and non-increasing for s  (0, 1); (ii) (s ) is not related to s . Note that when s is large (as is typical when the smoothed problem approaches the original problem), non-accelerated solvers need a larger Ts than accelerated solvers. Table 1 shows some non-accelerated and accelerated solvers popularly used in machine learning. Algorithm 1 shows the proposed procedure, which will be called CNS (Continuation for NonSmooth optimization). It is similar to that in (Becker, Bobin, and Cand es 2011), which however does not have convergence results. Moreover, a small but important difference is that Algorithm 1 specifies how Ts should be updated across stages, and this is essential for proving convergence. Note the different update options for non-accelerated and accelerated solvers. Algorithm 1 CNS algorithm for strongly convex problems. 1: Input: number of iterations T1 and smoothness parameter 1 for stage 1, and shrinking parameter  > 1. 2: Initialize: x 0 . 3: for s = 1, 2, . . . do s  smooth P with smoothing parameter s ; 4: P s (x) by running M for Ts itera5: x s  minimize P tions; 6: s+1 = s / ; 7: Option I (non-accelerated solvers): Ts+1 =  Ts ; 8: Option II (accelerated solvers): Ts+1 =  Ts ; 9: end for 10: Output: x s . The following Lemma shows that when T1 is large enough, error reduction can be guaranteed across all stages. Lemma 1. For both non-accelerated and accelerated solvers, if T1 is large enough such that 1  1/ 2 , then s  1/ 2 for all s > 1.







If 1 is known, a sufficiently large T1 can be obtained from Table 1; otherwise, we can obtain T1 by ensur1 ( 1 ( 1 ( ing P x1 )  P x0 )/ 2 , which then implies P x1 ) -   2 1 ( 1 (x ))/ . 1 (x )  (P x0 ) - P P



1 1







Convergence when Non-Accelerated Solver is used Let x = arg minx P (x), and Du = maxuU  (u). The following Lemma shows that if x is an -accurate solution of s (i.e., P s (x) - P s (x )  ), it is also an ( + s Du )the P s accurate solution of the original objective P . s (x ) - s Du  P (x) - P (x )  s (x) - P Lemma 2. P s    Ps (x) - Ps (x ) + s Du .



s







Since Lemma 2 holds for any x, it also holds in expectas (x ) - s Du  EP ( s ( xs ) - P xs ) - P (x )  tion, i.e., EP s    x s ) - Ps ( x s ) +  s D u . EPs ( Theorem 1. Assume that T1 in Algorithm 1 is large enough so that 1  1/ 2 . When non-accelerated solvers are used, EP ( xS )-P (x ) 



S s=1 S







s (P ( x0 )-P (x ))+O







1 Du , (8) T







where S is the number of stages, T = s=1 Ts , and S 2  = O (1 /T ) . s s=1 The first term on the RHS of (8) reflects the cumulative decrease of the objective after S stages, while the second term is due to smoothing. The condition 1  1/ 2 is used to obtain the O(1/T ) rate in the last term of (8). If we instead require that 1  1/ , it can be shown that the rate will be  slowed further to  to O(log T /T ); if 1  1/  , it degrades c O(1/ T ). On the other hand, if 1  1/ with c > 2, the rate will not be improved. Corollary 1. Together with Lemma 1, we have EP ( xS ) - P (x )  P ( x0 ) - P ( x ) +O  2S 1 Du T , (9)







where 1/ 2S = O(1/T 2 ). Existing stochastic algorithms such as SGD, FOBOS and RDA have a convergence rate of O(log T /T ) (Rakhlin, Shamir, and Sridharan 2012; Duchi and Singer 2009; Xiao 2009), while here we have the faster O(1/T ) rate. Recent







2395







works in (Shamir and Zhang 2013; Ouyang and Gray 2012) also achieve a O(1/T ) rate. However, Shamir and Zhang (2013) use stochastic subgradient, and do not exploit properties of the regularizer (such as sparsity). This can lead to inferior performance (Duchi and Singer 2009; Xiao 2009; Mazumder, Hastie, and Tibshirani 2010). On the other hand, (Ouyang and Gray 2012) is restricted to r  0 in (5). Next, we compare with the case where continuation is not used (i.e., s is a constant). Equivalently, this corresponds to setting  = 1 in Algorithm 1. Proposition 1. When continuation is not used, let   (0, 1) be the error reduction factor at each stage, and  > 0 be the fixed smoothing parameter. When either an accelerated or non-accelerated solver is used, EP ( xS ) - P (x )S (P ( x0 ) - P (x ))+(1+ S )Du .(10) Proposition 2. Assume that the two terms on the RHS of (9) and (10) are equal to  and (1 - ) , respectively, where  > 0 and > 0. Let 1 =  = 1/ 2 in (8) and (10). Assume that Algorithm 1 needs a total of T iterations to obtain an -accurate solution, while its fixed-s variant takes T iterations. Then,



T  S - 1  -1 a a1  1 2 + b  1 2 +c , 1 2 + b 1 2



1 2







Proposition 3. With the same assumptions in Proposition 2,  S  1 1  -1 + b +c , a 1  T  2  2  -1   2S + 1 1 1  T  S a + b + c , 1 + C  S +1 +  S 2 2 where S, C are as defined in Proposition 2.







General Convex Objectives



When P is not strongly convex, we add to it a small 2 term (with weight s ). We then gradually decrease s and s simultaneously to approach the original problem. The revised procedure is shown in Algorithm 2. Algorithm 2 CNS algorithm for general convex problems. 1: Input: number of iterations T1 , smoothness parameter 1 and strong convexity parameter 1 for stage 1, and shrinking parameter  > 1. 2: Initialize: x 0 . 3: for s = 1, 2, . . . do s  smooth P with smoothing parameter s ; 4: P s (x) + s x 2 by running M for 5: x s  minimize P 2 2 Ts iterations; 6: s+1 = s / ; s+1 = s / ; 7: Option I (non-accelerated solvers): Ts+1 =  2 Ts ; 8: Option II (accelerated solvers): Ts+1 =  Ts ; 9: end for 10: Output: x s . We assume that there exists R > 0 such that x 2  R, s 2  and x s 2  R for all s. Define Hs (x) = Ps (x) + 2 x 2 , and let x = arg min H ( x ) . The following assumption is x s s similar to that for strongly convex problems. Assumption 3. EHs ( xs ) - Hs ( x xs-1 ) - s )  s (Hs (  Hs (xs )), where s  (0, 1). Theorem 3. Assume that T1 in Algorithm 2 is large enough so that 1  1/ 2 . When non-accelerated solvers are used,



EP ( xS )-P (x ) 



S







T S







 2S + 1 1 + C  S +1 +  S







+c ,







where S 1-



2S















log



K ,







 (P ( x0 )-P (x ))







/ log







, C







=







 +1  S +1 + S







and K is a constant,







T and T are usually dominated by the a1 (1/ 2 ) term, and T is roughly S times that of T . This is also consistent with empirical observations that continuation is much faster than fixed smoothing (Becker, Bobin, and Cand es 2011). Convergence when Accelerated Solver is used Theorem 2. Assume that T1 in Algorithm 1 is large enough so that 1  1/ 2 . When accelerated solvers are used, EP ( xS )-P (x) 



S s=1 S s=1 S







s (P ( x0 )-P (x ))+O







1 Du . T2







Ts , and s=1 s = O(1/T 4 ). where T = As the s 's for non-accelerated and accelerated solvers S are different, the s=1 s term here is different from that in Theorem 1. Moreover, the last term is improved from O(1/T ) in Theorem 1 to O(1/T 2 ) with accelerated solvers. This is also better than the rates of existing stochastic algorithms (O(log T /T ) in (Duchi and Singer 2009; Xiao 2009) and O(1/T ) in (Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2013; Ouyang and Gray 2012)). Besides, the black-box lower bound of O(1/T ) for strongly convex problems (Agarwal et al. 2009) does not apply here, as we have additional assumptions that the objective is of the form in (1) and the number of training samples is finite. Though the (batch) excessive gap algorithm (Nesterov 2005a) also has a O(1/T 2 ) rate, it is limited to r  0 in (5). As in Proposition 2, the following shows that if continuation is not used, the algorithm is roughly S times slower.







s



s=1







P ( x0 )-P (x )+ +O







1 x 0 2 ,







2 2







+O







1 R2 /2  T







1 D u  T







(11)







where







S s=1







1 s = O( T ). For accelerated solvers,



S







EP ( xS )-P (x ) 







s



s=1







P ( x0 )-P (x )+ +O







1 x 0 2 ,







2 2







+O







1 R2 /2 T







1 D u T







(12)







where







 For non-accelerated solvers, the O(1/ T ) convergence rate in (11) is only as good as that obtained in (Xiao 2009; Duchi and Singer 2009; Ouyang and Gray 2012; Shamir and







S s=1







s = O( T12 ).







2396







Zhang 2013). Hence, they will not be studied further in the sequel. However, for accelerated solvers, the O (1/T ) con vergence rate in (12) is faster than the O(1/ T ) rate in (Xiao 2009; Duchi and Singer 2009; Ouyang and Gray 2012; T Shamir and Zhang 2013) and the O( T12 + log T ) rate in (Orabona, Argyriou, and Srebro 2012). The O(1/T ) convergence rate is also obtained in (Nesterov 2005a; 2005b), but again only for r  0 in (5). When continuation is not used, the following results are analogous to those obtained in the previous section. Proposition 4. Let x 0 = 0. When continuation is not used, let  be the error reduction factor at each stage. When either an accelerated or non-accelerated solver is used, EP ( xS ) - P (x )











archive, and is a subset of the Million Song data set. We use the hinge loss for classification, and 1 loss for regression. Both can be smoothed using Nesterov's smoothing (to (3) and (4), respectively). As for the regularizer, we use the 2 2 1. elastic-net regularizer r(x) = 1 x 1 +  2 x 2 (Zou and Hastie 2005), and problem (5) is strongly convex; and 2. 1 regularizer r(x) = 1 x 1 , and (5) is (general) convex. Here, 1 , 2 are tuned by 5-fold cross-validation. Obviously, all losses and regularizers are convex but nonsmooth. We use mini-batch for all methods. The mini-batch size b is 50 for rcv1, and 100 for YearPredictionMSD. Table 3: Data sets used in the experiments. rcv1 YearPredictionMSD #train 20,242 463,715 #test 677,399 51,630 #features 47,236 90















 (P ( x0 ) - P (x ))  +(1 + S )Du + R2 . (13) 2



S















Proposition 5. Let x 0 = 0. Suppose that the three terms on the RHS of (13) are equal to  ,  and  , respectively, where , ,  > 0 and  +  +  = 1. Let 1 =  = 1/ 2 in (12) and (13). Assume that Algorithm 2 (with accelerated solver) needs a total of T iterations to obtain an -accurate solution, while its fixed-s variant takes T iterations. Then, T  S - 1  -1 a  a 1  1 2 + b 1 2 1 2 +c , 1 2



1 2







T S where S 1-







 2S + 1 1 + C ( + 1)2  + log



 1+



S







+ b / log







+c , , C =







 (P ( x0 )-P (x ))







 +1 ( +1)2







2S







-







 +1 ( +1)2







2S







K 1







and K is a constant.







A summary of the convergence results is shown in Table 2. As can be seen, the convergence rates of the proposed CNS algorithm match the fastest known rates in nonsmooth optimization, but CNS is less restrictive and can exploit the composite structure of the optimization problem. Table 2: Comparison with the fastest known convergence rates for nonsmooth optimization problem (1). The fastest known batch solver is restricted to r  0, while the fastest known stochastic solver does not exploit properties of r. strongly convex yes no batch solver 1/T 2 1/T stochastic solver 1/T  1/ T CNS (batch/stochastic) non-accel. accel. 1/T 1/T 2  1/ T 1/T







The following stochastic algorithms are compared: 1. Forward-backward splitting (FOBOS) (Duchi and Singer 2009), a standard baseline for nonsmooth stochastic composite optimization. 2. SGD with polynomial-decay averaging (Poly-SGD) (Shamir and Zhang 2013), the state-of-art for nonsmooth optimization. 3. Regularized dual averaging (RDA) (Xiao 2009): This is another state-of-the-art for sparse learning problems. 4. The proposed CNS algorithm: We use proximal SVRG (PSVRG) (Xiao and Zhang 2014) as the underlying non-accelerated solver, and accelerated proximal SVRG (ACC-PSVRG) (Nitanda 2014) as the accelerated solver. The resultant procedures are denoted CNS-NA and CNSA, respectively. We set 1 = 0.01,  = 2, and T1 = n/b . Empirically, this ensures 1  1/ 2 (in Theorems 1 and 3) on the two data sets. Note that FOBOS, RDA and the proposed CNS can effectively make use of the composite structure of the problem, while Poly-SGD cannot. For each method, the stepsize is tuned by running on a subset containing 20% training data for a few epochs (for the proposed method, we tune 1 ). All algorithms are implemented in Matlab.







Strongly Convex Objectives



Figure 1 shows convergence of the objective and testing performance (classification error for rcv1 and 1 -loss for YearPredictionMSD). The trends are consistent with Theorem 1. CNS-A is the fastest (with a of O(1/T 2 )). This is followed by CNS-NA and Poly-SGD, both with O(1/T ) rate (from Theorem 1 and (Shamir and Zhang 2013)). The slowest are FOBOS and RDA, which converge at a rate of O(log T /T ) (Duchi and Singer 2009; Xiao 2009). Figure 2 compares with the case where continuation is not used. Two fixed smoothness settings,  = 10-2 and  = 10-3 , are used. As can be seen, they are much slower (Propositions 2 and 3). Moreover, a smaller  leads to slower convergence but better solution, while a larger  leads to







Experiments



Because of the lack of space, we only report results on two data sets (Table 3) from the LIBSVM archive: (i) the popularly used classification data set rcv1; and (ii) YearPredictionMSD, the largest regression data in the LIBSVM







2397







10 -2







10 0







10 -1







10 0







10 -3







10 -1







10 -2







10 -1







objective minus best







objective minus best







10 -4







10 -2







objective minus best







objective minus best







10 -2







10 -3







10 -5







10 -3







10 -3







10 -6







10 -4







10 -4







10 -4







10 -7







0







10







20







30







40







50







10 -5







0







10







20







30







40







50







10 -5







0







10







20







30







40







50







10 -5







0







10







20







30







40







50







CPU time (s)







CPU time (s)







CPU time (s)







CPU time (s)







3.9 3.895 3.89 3.885







0.22 0.2 0.18







4.6







0.24 0.22







4.55







0.2 0.18







test error (%)







test loss







test loss







3.88 3.875 3.87 3.865 3.86 3.855 3.85 0 10 20 30 40 50







test error (%)







0.16 0.14 0.12 0.1 0.08 0.06







4.5







0.16 0.14 0.12







4.45







4.4







0.1 0.08







0







10







20







30







40







50







4.35







0







10







20







30







40







50







0.06







0







10







20







30







40







50







CPU time (s)







CPU time (s)







CPU time (s)







CPU time (s)







rcv1.







YearPredictionMSD.







rcv1.







YearPredictionMSD.







Figure 1: Objective (top) and testing performance (bottom) vs CPU time (in seconds) on a strongly convex problem.







Figure 3: Objective (top) and testing performance (bottom) vs CPU time (in seconds) on a general convex problem.



10 -2 10 0







faster convergence but worse solution. This is also consistent with Proposition 1, as using a fixed  only allows convergence to the optimal solution with a tolerance of (1 + S )Du . Moreover, a smaller  leads to a larger condition number, and convergence becomes slower.



10 -2 10 0







10 -1







objective minus best







10 -3







objective minus best



0 10 20 30 40 50







10 -2







10 -3







10 -4







10 -4







10 -5







10 -5







0







10







20







30







40







50







CPU time (s)







CPU time (s)







(a) rcv1.







(b) YearPredictionMSD.







10 -3







10 -1







objective minus best







10 -4







objective minus best







10 -2







Figure 4: Effect of continuation (general convex problem).







10 -5







10 -3







10 -6







10 -4







10 -7







0







10







20







30







40







50







10 -5







Conclusion



0 10 20 30 40 50







CPU time (s)







CPU time (s)







(a) rcv1.







(b) YearPredictionMSD.







Figure 2: Effect of continuation (strongly convex problem).







General Convex Objectives



We set 1 in Algorithm 2 to 10-5 for rcv1, and 10-7 for YearPredictionMSD. As can be seen from Figure 3, the trends are again consistent with Theorem 3. CNS-A is the fastest (O(1/T ) convergence rate), while the others all have  a rate of O(1/ T ) (Duchi and Singer 2009; Xiao 2009; Shamir and Zhang 2013). Also, RDA shows better performance than FOBOS and Poly-SGD. Recall that Poly-SGD outperforms FOBOS and RDA on strongly convex problems. However, on general convex problems, Poly-SGD is the worst as its rate is only as good as others, and it does not exploit the composite structure of the problem. Figure 4 compares with the case where continuation is not used. As in the previous section, CNS-NA and CNS-A show faster convergence than its fixed-smoothing counterparts.







In this paper, we proposed a continuation algorithm (CNS) for regularized risk minimization problems, in which both the loss and regularizer may be nonsmooth. In each of its stages, the smoothed subproblem can be easily solved by either existing accelerated or non-accelerated solvers. Theoretical analysis establishes convergence results on the whole continuation algorithm, not just one of its stages. In particular, when accelerated solvers are used, the proposed CNS algorithm achieves the rate of O(1/T 2 ) on strongly convex problems, and O(1/T ) on general convex problems. These are the fastest known rates for nonsmooth optimization. However, CNS is advantageous in that it allows the use of a regularizer (unlike the fastest batch algorithm) and can exploit the composite structure of the optimization problem (unlike the fastest stochastic algorithm). Experiments on nonsmooth classification and regression models demonstrate that CNS outperforms the state-of-the-art.







Acknowledgments



This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614513).







2398







References



Agarwal, A.; Wainwright, M. J.; Bartlett, P. L.; and Ravikumar, P. K. 2009. Information-theoretic lower bounds on the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems, 1-9. Becker, S.; Bobin, J.; and Cand es, E. J. 2011. NESTA: A fast and accurate first-order method for sparse recovery. SIAM Journal on Imaging Sciences 4(1):1-39. Chen, X.; Lin, Q.; Kim, S.; Carbonell, J. G.; Xing, E. P.; et al. 2012. Smoothing proximal gradient method for general structured sparse regression. Annals of Applied Statistics 6(2):719-752. Defazio, A.; Bach, F.; and Lacoste-Julien, S. 2014. SAGA: A fast incremental gradient method with support for nonstrongly convex composite objectives. In Advances in Neural Information Processing Systems, 2116-2124. Duchi, J., and Singer, Y. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research 10:2899-2934. Hale, E.; Yin, W.; and Zhang, Y. 2007. A fixed-point continuation method for 1 -regularized minimization with applications to compressed sensing. Technical Report CAAM TR07-07, Rice University. Johnson, R., and Zhang, T. 2013. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, 315-323. Kushner, H. J., and Yin, G. 2003. Stochastic Approximation and Recursive Algorithms and Applications, volume 35. Springer Science & Business Media. Mairal, J. 2013. Optimization with first-order surrogate functions. In Proceedings of the 30th International Conference on Machine Learning. Mazumder, R.; Hastie, T.; and Tibshirani, R. 2010. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research 11:2287- 2322. Nemirovski, A., and Yudin, D. 1983. Problem Complexity and Method Efficiency in Optimization. Wiley. Nesterov, Y. 2004. Introductory Lectures on Convex Optimization, volume 87. Springer. Nesterov, Y. 2005a. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization 16(1):235-249. Nesterov, Y. 2005b. Smooth minimization of non-smooth functions. Mathematical Programming 103(1):127-152. Nesterov, Y. 2013. Gradient methods for minimizing composite functions. Mathematical Programming 140(1):125- 161. Nitanda, A. 2014. Stochastic proximal gradient descent with acceleration techniques. In Advances in Neural Information Processing Systems, 1574-1582. Orabona, F.; Argyriou, A.; and Srebro, N. 2012. PRISMA: Proximal iterative smoothing algorithm. Preprint arXiv:1206.2372.







Ouyang, H., and Gray, A. G. 2012. Stochastic smoothing for nonsmooth minimizations: Accelerating SGD by exploiting structure. In Proceedings of the 29th International Conference on Machine Learning, 33-40. Parikh, N., and Boyd, S. 2014. Proximal algorithms. Foundations and Trends in Optimization 1(3):127-239. Rakhlin, A.; Shamir, O.; and Sridharan, K. 2012. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, 449-456. Schmidt, M.; Roux, N. L.; and Bach, F. R. 2011. Convergence rates of inexact proximal-gradient methods for convex optimization. In Advances in Neural Information Processing Systems, 1458-1466. Schmidt, M.; Roux, N. L.; and Bach, F. 2013. Minimizing finite sums with the stochastic average gradient. Preprint arXiv:1309.2388. Shalev-Shwartz, S., and Zhang, T. 2014. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In Proceedings of the 31st International Conference on Machine Learning, 64-72. Shamir, O., and Zhang, T. 2013. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Proceedings of the 30th International Conference on Machine Learning, 71-79. Wen, Z.; Yin, W.; Goldfarb, D.; and Zhang, Y. 2010. A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation. SIAM Journal on Scientific Computing 32(4):1832-1857. Xiao, L., and Zhang, T. 2012. A proximal-gradient homotopy method for the 1 -regularized least-squares problem. In Proceedings of the 29th International Conference on Machine Learning, 839-846. Xiao, L., and Zhang, T. 2014. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization 24(4):2057-2075. Xiao, L. 2009. Dual averaging method for regularized stochastic learning and online optimization. In Advances in Neural Information Processing Systems, 2116-2124. Zou, H., and Hastie, T. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B 67(2):301-320.







2399







SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions



Han Xiao, Minlie Huang, Xiaoyan Zhu State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China bookman@vip.163.com; {aihuang,zxy-dcs}@tsinghua.edu.cn







Abstract







arXiv:1604.04835v1 [cs.CL] 17 Apr 2016







Knowledge graph embedding represents the entities and relations as numerical vectors, and then knowledge analysis could be promoted as a numerical method. So far, most methods merely concentrate on the fact triples that are composed by the symbolic entities and relations, while the textual information which is supposed to be most critical in NLP could hardly play a reasonable role. For this end, this paper proposes the method SSP which jointly learns from the symbolic triples and textual descriptions. Our model could interact both two information sources by characterizing the correlations, by which means, the textual descriptions could make effects to discover semantic relevance and offer precise semantic embedding. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines on the tasks of knowledge graph completion and entity classification.







methods have been proposed, such as TransE (Bordes et al., 2013), PTransE (Lin et al., 2015a), KG2E (He et al., 2015) etc. The translation-based methods as a key branch of embedding models, adopt the principle of translating the head entity to the tail one by a relation-specific vector, mathematically speaking h + r = t. As Fig.1 shows, in the knowledge graph, the entities such as h, t have textual descriptions, which contain much extra semantic information about knowledge triples.







Figure 1: Example of entity descriptions. Despite the success of triple embedding, there are still two reasons why textual descriptions would be necessary in this task: discovering semantic relevance and offering precise semantic expression. Firstly, the semantic relevance between entities is capable to recognize the true triples, which are difficult to be inferred only with fact triples. For example, the triple (Anna Roosevelt, Parents, Franklin Roosevelt), indicates "Franklin Roosevelt" is the parent of "Anna Roosevelt". There is almost no hint to infer this fact from other symbolic triples. In contrast, there are many keywords such as "Roosevelt" and "Daughter of the President" in the textual description of the head entity, which match the keywords "Roosevelt" and "President" in that of the tail one. These matched keywords imply this fact triple by offering extra semantic relevance. Specifically, we measure the possibility of a







1







Introduction







Knowledge graph provides an effective basis for NLP in many tasks such as question answering, web search and semantic analysis. In order to provide a numerical computation framework for knowledge graph, which could be leveraged by the numerical deep learning, knowledge graph embedding represents the entities and relations in a continuous low-dimensional vector space. More specifically, a basic fact in knowledge graph is usually represented as a symbolic triple (h, r, t), where h, r, t are the representation vectors of the head entity, the relation and the tail entity, respectively. To this end, a lot of embedding







triple by projecting the loss onto a hyperplane that represents the semantic relevance between entities. Thus, it is always possible to accept a fact triple so long as the projection of the loss vector onto the semantic hyperplane is sufficiently small. Secondly, precise semantic expression could promote the discriminative ability between two indistinguishable triples. For an instance, when we query about the profession of "Daniel Sturgeon", there are two possible options that "politician" and "lawyer". It's hard to distinguish if only focusing on the symbolic triples. However, the textual description of "Daniel Sturgeon" is full of politics-related keywords such as "Democratic Party", "State Legislature" etc. and even the word "Politician" also appears in this description. The textual descriptions help to refine the topic of "Daniel Sturgeon" in a more precise way from the social celebrities to the government officers, which makes the true answer "politician" more preferable. Formally, even though the loss vectors of the two facts are almost equal-length, after respectively projected onto the "politician" and "lawyer" related semantic hyperplanes, the losses are distinguished reasonably. In this way, precise semantic expression refines the embedding.







Figure 2: Simple illustration of TransE and SSP where h + r - t is the loss vector. The existing embedding methods with textual semantics such as DKRL (Xie et al., 2016) and "Jointly" (Zhong et al., 2015), have achieved much success. But there are still one important disadvantage, that is the weak-correlation modeling issue, indicating current models could hardly characterize the strong correlations between texts and triples. In the DKRL, for a triple, the embedding vector of the head entity is translated to that of the tail one as well as possible, where the encoded texts and entities are concatenated as the embedding vectors. Besides, "Jointly" model generally attempts to make the embeddings of the corresponding entity and text proximal. Both are







first-order constraints. It's noteworthy that triple embedding is always the main procedure and textual descriptions must interact with triples for better embedding. Only in this way, the semantic effects could make more senses. Actually, the strong correlation, that the high-order restriction, would interact texts and triples to complement each other in a more semantics-specific way than simple constraints. For the above example of "Daniel Sturgeon", the textual descriptions imply two candidate answers "Banker" and "Politician". Thus, only by considering both the triples and texts, we could come out with the true answer. Therefore, we focus on the stronger semantic interaction by projecting triple embedding onto a semantic subspace such as hyperplane, as shown in Fig.2. Mathematically, the quadratic constraint is adopted to model the strong correlation, thus the embedding topologies are sufficiently semanticsspecific. We evaluate the effectiveness of our model Semantic Subspace Projection (SSP) on two tasks that are knowledge graph completion and entity classification, for three benchmark datasets that are the subsets of Wordnet (Miller, 1995) and Freebase (Bollacker et al., 2008). Experimental results on real-world datasets show that our model consistently outperforms the other baselines with an extensive improvement. Contributions. We propose a knowledge graph embedding method SSP which models the strong correlations between the symbolic triples and the textual descriptions by performing the embedding in a semantic subspace as hyperplane. Besides, our method outperforms all the baselines on the tasks of knowledge graph completion and entity classification, which justifies our effectiveness.







2







Related Work







We have surveyed the related literature and categorized the embedding methods into two branches: Translation-Based Embedding that is only triplespecific and "Text-Aware" Embedding that involves textual descriptions. 2.1 Triple-Specific Embedding







TransE (Bordes et al., 2013) is a seminal work for this branch, which translates the head entity to the tail one by the relation vector, or h + r = t. Naturally, the scale of the loss vector is the score function, which measures the plausibility of triples







and a smaller score is better. The following variants transform entities into different subspaces to play different roles. TransH (Wang et al., 2014b) utilizes the relation-specific hyperplane to lay the entities. TransR (Lin et al., 2015b) applies the relation-related matrix to rotate the embedding space. Similar researches also contain TransA (Xiao et al., 2015), TransD (Ji et al., 2015) and TransM (Fan et al., 2014). Further researches take extra structural information into embedding. PTransE (Lin et al., 2015a) is a path-based model, simultaneously considering the information and confidence level of the path in the knowledge graph. (Wang et al., 2015) incorporates the rules to restrict the embeddings for the complex relation types such as 1-N, N-1 and N-N. SSE (Guo et al., 2015) aims at discovering the geometric structure of embedding topologies then based on these assumptions, designs a semantically smooth score function. Also, KG2E (He et al., 2015) involves probabilistic analysis to characterize the uncertain concepts of knowledge graph. There are also some pioneering work such as SE (Bordes et al., 2011), LFM (Jenatton et al., 2012), NTN (Socher et al., 2013) and RESCAL (Nickel et al., 2011) etc. 2.2 Embedding with Textual Information







3







Methodology







In this section, we first introduce our model and discuss the details. Furthermore, we provide two different insights to address the ability of our model. We also list some notations: all the symbols h, t indicate the head and tail entity, respectively. h (or t) is the embedding of the entity from the triples, sh (or st ) is the semantic vector generated from the texts, and d is the dimension of embedding. The data involved in our model are the knowledge triples and the textual descriptions of entities. In experiments, we adopt the "entity descriptions" of Freebase and the textual definitions of Wordnet as textual information. Actually, any related textual data could be involved in our method. 3.1 Model Description







Previous analysis in the introduction suggests to characterize the strong correlation between triples and texts. For the purpose of interacting the symbolic triples and textual descriptions, this paper attempts to restrict the embedding procedure of a specific triple in the semantic subspace. Specifically, we leverage a hyperplane with normal vector . s = S (sh , st ) as the subspace, where S : R2d  Rd is the semantic composition function discussed in  3.2 , and sh , st is the head-specific and tailspecific semantic vectors. The score function in the translation-based methods is ||h + r - t||2 2 , which means the triple embedding focuses on the loss vector . e = h + r - t . According to our motivation, assuming e is length-fixed, the target is to maximize the component inside the hyperplane, which is ||e - s es||2 2 . In detail, the component of the loss in the normal vector direction is (s es), then the other orthogonal one, that is inside the hyperplane, is (e - s es). Naturally, the total loss vector should also be punished. For this end, we introduce a factor  to balance the two parts, stated as:



2 fr (h, t) = -||e - s es||2 2 + ||e||2







"Text-Aware" Embedding, that indicates representing knowledge graph with textual information, generally dates back to NTN (Socher et al., 2013). NTN makes use of entity name and embeds an entity as the average word embedding vectors of the name. (Wang et al., 2014a) attempts to align the knowledge graph with the corpus then jointly conduct knowledge embedding and word embedding. However, the necessity of the alignment information limits this method both in performance and practical application. Thus, (Zhong et al., 2015) proposes "Jointly" method that only aligns the freebase entity to the corresponding wiki-page. DKRL (Xie et al., 2016) extends the translation-based embedding methods from the triple-specific one to the "Text-Aware" model. More importantly, DKRL adopts a CNNstructure to represent words, which promotes the expressive ability of word semantics. Generally speaking, by further jointing knowledge and texts, DKRL keeps the state-of-the-art performance of this branch.







where  is a suitable hyper-parameter. Moreover, a smaller score means more plausible. For clarity, the definitions of the symbols are boxed. Notably, the projection part in our score function is negative, so more projection means less loss.







3.2







Semantic Vector Generation







There are at least two methods that could be leveraged to generate the semantic vectors: topic model (Blei, 2012) such as LSA, LDA, NMF (Stevens et al., 2012) and word embedding such as CBOW (Mikolov et al., 2013b), Skip-Gram (Mikolov et al., 2013a). More concretely, this paper adopts the topic model, treating each entity description as a document and then obtains the topic distribution of document as the semantic vector of entity. The entities are usually organized by the topic in knowledge base, for an example of the "entity type" in Freebase. Therefore, we conjecture that the topic model could be more competent. Notably, the word embedding would also work well though maybe not better. Given the pre-trained semantic vectors, our model fixes them and then optimizes the other parameters. We call this setting Standard (short as Std.). The reason why we could not adapt all the parameters, is the training procedure would refill the semantic vectors and flush the semantics out. For the purpose of jointly learning the semantics and the embeddings, we conduct the topic model and the embedding model, simultaneously. In this way, the symbolic triples also pose a positive effect on the textual semantics and we call this setting Joint. As each component of semantic vector indicates the relevant level to a topic, we suggest the semantic composition should take the addition form: S (sh , st ) = sh + st ||sh + st ||2 2







hyperplane, implying the head and tail entity also lay on it, as the beginning and ending point. Thus, there exists the important restriction, that the entities co-occur in a triple should be embedded in the semantic space composed by the associated textual semantics. This restriction is implemented as a quadric form to characterize the strong correlation between texts and triples, in other words, to interact both the information sources. A strong interaction between the textual descriptions and symbolic triples complements each other in a more semantics-specific form, which guarantees the semantic effects. More concretely, the embeddings are decided in the training procedure not only by triples but also by semantics, based on which, our embedding topologies are semantically different from the other methods. Comparatively, the firstorder constraints could hardly reach so far, making an unsatisfactory usage of textual semantics. 3.4 Semantic Perspective







where the normalization is applied to make a normal vector. Since the largest components represent the topics, the addition corresponds to the topic union, making the composition indicate the whole semantics. For example, when sh = (0.1, 0.9, 0.0) and st = (0.8, 0.0, 0.2), the topic of the head entity is #2 and that of the tail is #1, while the composition is s = (0.45, 0.45, 0.10), corresponding to the topic of #1,#2, or we say the topic union of both entities. 3.3 Correlation Perspective







Specifically, our model attempts to lay the loss . h - t onto the hyperplane, where h = h + r is the translated head entity. Mathematically, if a line lies on a hyperplane, so do all the points of this line. Correspondingly, the loss lays on the







There are two semantic effects for textual descriptions: discovering semantic relevance and offering precise semantic expression. Our model characterizes the strong correlations with a semantic hyperplane, which is capable of taking the advantages of both two semantic effects. Firstly, according to the correlation perspective, the entities which are semantically relevant, approximately lay on a consistent hyperplane. Therefore, the loss vector between them (h - t) is also around the hyperplane. Based on this geometric insight, when a head entity matches a negative tail, the triple is far from the hyperplane, making a large loss to be classified. Conversely, even if a correct triple makes much loss, the score function after projected onto the hyperplane could be relatively smaller (or better). By this mean, the semantic relevance achieved from the texts, promotes embedding. For instance, the fact triple (Portsmouth Football Club, Locate, Portsmouth) could hardly be inferred only within the triple embedding. It ranks 11,549 out of 14,951 by TransE in link prediction, which means a totally impossible fact. But the keywords "Portsmouth", "England", and "Football" occur many times in both the textual descriptions, making the two entities semantically relevant. Unsurprisingly, after the semantic projection, the case ranks 65 out of 14,951 in our model, which is regarded as nearly true.







Secondly, all the equal-length loss vectors in TransE are equivalent in term of discrimination, which leads to the weak distinction. However, with textual semantics, the distinguishing ability could be strengthened in our model. Specifically, the equal-length loss vectors are measured with the projection onto the corresponding semantic hyperplanes, which makes a reasonable division of the losses. For an instance of the query about which film "John Powell" contributes to, there are two candidate entities, that the true answer "Kung Fu Panda" and the negative one "Terminator Salvation". Without textual semantics, it's difficult to discriminate, thus the losses calculated by TransE are 8.1 and 8.0, respectively, leading to a bad classification. Diving into the textural semantics, we discover, "John Powell" is much relevant to the topic of "Animated Films", which matches that of "Kung Fu Panda" and does not for the other. Based on this fact, both the query and the true answer lie in the "Animated Films"directed hyperplane, whereas the query and the negative one do not co-occur in the corresponding associated semantic hyperplane. Thus, the projected loss of the true answer could be much less than that of the false one. Concretely, the losses in our model are 8.5 and 10.8, respectively, which are sufficient for discrimination. Notably, the scales of scores in TransE and our model are different as usual and only relative scores make sense. 3.5 Objectives & Training







max( * , 0) is the hinge loss. The false triples are sampled with "Bernoulli Sampling Method" as introduced in (Wang et al., 2014b) and the method selects the negative samples from the set of {(h , r, t)|h  E }  {(h, r, t )|t  E }  {(h, r , t)|r  R} We initialize the embedding vectors by the similar methods used in the deep neural network (Glorot and Bengio, 2010) and pre-train the topic model with Non-negative Matrix Factorization (NMF) (Stevens et al., 2012). The stochastic gradient descent algorithm (SGD) is adopted in the optimization. For the topic-related objective, we take the advantage of the NMF Topic Model (Stevens et al., 2012), which is both simple and effective. Then we re-write the target as an inner-product form with the L2 -loss, stated as: Ltopic =



eE, wDe







(Ce,w - se w)2 s e  0, w  0







(2) (3)







There are two parts in the objective function, which are respectively embedding-specific and topic-specific. To balance both the two parts, a hyper-parameter  is introduced. Overall, the total loss is: L = Lembed + Ltopic (1)







Notably, there is only the first part in the Standard setting where  = 0 in fact. In term of the embedding-related objective, the rank-based hinge loss is applied, which means to maximize the discriminative margin between the golden triples and the negative ones:



embed







where E is the set of entities, and De is the set of words in the description of entity e. Ce,w is the times of the word w occurring in the description e. se is the semantic vector of entity e and w is the topic distribution of word w. Similarly, SGD is applied in the optimization. Theoretically, our computation complexity is comparable to TransE, as O( x O(T ransE )), and the small constance  is caused by the projection operation and topic calculation. In practice, TransE costs 0.28s for one round in Link Prediction and our model costs 0.36s in the same setting. Generally, TransE is most efficient among all the translation-based methods, while our method could be comparable to TransE in running time, justifying the efficiency of our model.







4



4.1







Experiments



Datasets & General Settings







L







=



(h, r, t)   (h , r , t )  







[fr (h , t ) - fr (h, t) +  ]+







where  is the set of golden triples and  is that of the negative ones.  is the margin, and [ * ] =







Our experiments are conducted on three public benchmark datasets that are the subsets of Wordnet and Freebase. About the statistics of these datasets, we strongly suggest the readers to refer to (Xie et al., 2016) and (Lin et al., 2015b). The entity descriptions of FB15K and FB20K are the same as DKRL (Xie et al., 2016), each of which is a small part of the corresponding wiki-page. The







textual information of WN18 is the definitions that we extract from the Wordnet. Notably, for the zero-shot learning, FB20K is involved, which is also built by the authors of DKRL. 4.2 Knowledge Graph Completion







This task is a benchmark task, a.k.a "Link Prediction", which concerns the performance to infer the missing element of the triple (entity/relation) when given the other two elements. Many NLP tasks could benefit from Link Prediction, such as relation extraction (Hoffmann et al., 2011). Evaluation Protocol. The same protocol used in previous studies, is adopted. First, for each testing triple (h, r, t), we replace the tail t (or the head h) with every entity e in the knowledge graph. Then, a probabilistic score of this corrupted triple is calculated with the score function fr (h, t). By ranking these scores in the ascending order, we then get the rank of the original triple. The evaluation metrics are the average of the ranks as Mean Rank and the proportion of testing triple whose rank is not larger than 10 (as HITS@10). This is called "Raw" setting. When we filter out the corrupted triples that exist in the training, validation, or test datasets, this is the"Filter" setting. If a corrupted triple exists in the knowledge graph, ranking it ahead the original triple is also correct. To eliminate this effect, the "Filter" setting is more preferred. In both settings, a higher HITS@10 and a lower Mean Rank mean better performance. Table 1: Evaluation results of Knowledge Graph Completion (Entity) on FB15K and WN18. FB15K Mean Rank HITS@10 TransE TransH Jointly DKRL(BOW) DKRL(ALL) SSP (Std.) SSP (Joint) WN18 TransE TransH SSP (Std.) SSP (Joint) 210 212 167 1 200 181 154 163 263 401 204 168 119 87 39 1 113 91 77 82 251 338 193 156 48.5 45.7 51.7 1 44.3 49.6 57.1 57.2 75.4 73.0 81.3 81.2 66.1 64.4 77.3 1 57.6 67.4 78.6 79.0 89.2 82.3 91.4 93.2







Implementation. As the datasets are the same, we directly report the experimental results of several baselines from the literature. We have attempted several settings on the validation dataset to get the best configuration. Under the "bern." sampling strategy, the optimal configurations of our model SSP are as follows. For WN18, embedding dimension d = 100, learning rate  = 0.001, margin  = 6.0, balance factor  = 0.2 and for SSP(Joint)  = 0.1. For FB15K, embedding dimension d = 100, learning rate  = 0.001, margin  = 1.8, balance factor  = 0.2 and for SSP(Joint)  = 0.1. We train the model until convergence. Table 2: Evaluation results of Knowledge Graph Completion (Relation) on FB15K. FB15K Mean Rank HITS@10 TransE TransH DKRL(BOW) DKRL(ALL) SSP (Std.) SSP (Joint) 2.91 8.25 2.85 2.41 1.58 1.87 2.53 7.91 2.51 2.03 1.22 1.47 69.5 60.3 65.3 69.8 69.9 70.0 90.2 72.5 82.7 90.8 89.2 90.9







Results Evaluation results are reported in Tab.1 and Tab.2. Note that "Jointly" refers to (Zhong et al., 2015). We observe that: 1. SSP outperforms all the baselines in all the tasks, yielding the effectiveness of our models and the correctness of our theoretical analysis. Specifically, SSP(Joint) improves much more than SSP(Std.) for jointly learning the textual semantics and symbolic triples. 2. DKRL and "Jointly" model only consider the first-order constraints, which interact the textual and symbolic information, unsatisfactorily. By focusing on the strong correlation, SSP outperforms them. Notice that the "Jointly" model involves much more extra corpus to produce the result, but SSP also has an extensive advantage against it. Though TransH is also a hyperplanebased method, SSP adopts the hyperplane in a semantics-specific way rather than a simple relation-specific form. Thus, SSP outperforms TransH extensively. 3. TransE could be treated as missing textual descriptions, and DKRL(BOW) could approx-







Mean Rank







HITS@10







1 This method involves much more extra text corpus, thus it's unfair to directly compare with others.







imately be conjectured as missing symbolic triples. SSP (Joint) improves 12.4% against TransE while 20.9% against DKRL(BOW), illustrating the triple embedding is always the key point and the interactions between both the information sources play a critical role. 4.3 Entity Classification This task is essentially a multi-label classification, focusing on predicting entity types, which is crucial and widely used in many NLP & IR tasks (Neelakantan and Chang, 2015). The entity in Freebase always has types, for instance, the entity types of "Scots" are Human Language, Rosetta Languoid. We adopt the same datasets as DKRL, for the details of which, we suggest the readers to refer to (Xie et al., 2016). Overall, this is a multi-label classification task with 50 classes, which means for each entity, the method should provide a set of types rather one specific type. Evaluation Protocol In the training, we use the concatenation of semantic vector and embedding vector (se , e) as entity representation, which is the feature for the front-end classifier. For a fair comparison, our front-end classifier is also the Logistic Regression as DKRL in a one-versus-rest setting for multi-label classification. The evaluation is following (Neelakantan and Chang, 2015), which applies the mean average precision (MAP) that is commonly used in multi-label classification. Specifically, for an entity, if the methods predict a rank list of types and there are three correct types that lie in #1, #2, #4, the MAP is calculated as 1/1+2/2+3/4 . For FB20K, the methods could only 3 make use of the descriptions. Obviously, FB20K is a zero-shot scenario which is really hard. Table 3: Evaluation results of Entity Classification Metrics FB15K FB20K TransE BOW DKRL(BOW) DKRL(ALL) NMF SSP (Std.) SSP (Joint) 87.8 86.3 89.3 90.1 86.1 93.2 94.4 57.5 52.0 61.9 59.6 67.4







of several baselines from the literature. We have attempted several settings on the validation dataset to get the best configuration. Under the "bern." sampling strategy, the optimal configurations of our model SSP are as follows. For FB15K,embedding dimension d = 100, learning rate  = 0.001, margin  = 1.8, balance factor  = 0.2 and for SSP(Joint)  = 0.1. For FB20K, embedding dimension d = 100, learning rate  = 0.001, margin  = 1.8, balance factor  = 0.2 and for SSP(Joint)  = 0.1. We train the model until convergence. Results The evaluation results are listed in Tab.3. Notably, TransE are only triple-specific and SSP (Std.) performs as well as the NMF in the zero-shot learning. thus they are out of the ability for FB20K. The observations are as following: 1. Overall, our method SSP yields the best accuracy, justifying the effectiveness of our method and the correctness of our theories. 2. Compared to NMF and TransE, the results prove the critical declaration that the interactions between triples and texts make more sense and neither of them could hardly produce an effective method. The promotion of SSP (Joint) in FB20K demonstrates the triple embedding also has a positive effect on textual semantics, and on the other side, illustrates our effectiveness in the zero-shot learning. 3. Compared to TransE, the improvements illustrate the effectiveness of semantic integration. Compared to DKRL, the promotion demonstrates the important role of strong correlation. 4.4 Semantic Relevance Analysis







Implementation. As the datasets are the same, we directly report the experimental results







One of the actual effects of the semantic relevance is to recognize the true triples that could not be inferred by just focusing on the symbolic triples. Hence, we make a statistic analysis of the results in Link Prediction, reported in the Tab.4. The cell in the table is under the condition of the column and the row. For example, the cell in the second column and the second row means there are 601 triples, which are ranked inside the top 100 in SSP, yet outside the top 500 in TransE. Note that SSP(S.) indicates the standard setting, SSP(J.) means the joint setting and E is short for TransE. The statistic matrix addresses there







are many triples could benefit from the semantic relevance offered by textual descriptions. The experiments also justify the theoretical analysis about the semantic relevance and demonstrate the effectiveness of our models. We also study some specific cases. The triple (Luis Miguel Gallego Basteri, Music Artist Genre, Ballad) even transforms from the rank 1,602 in TransE to the rank 8 in SSP. Diving into the textual details, we discover the two entities share the relevant topics of "Cross-Country" and "Music". For another triple that (Laura Elizabeth Dern, Film Performance, Alice Doesn't Live Here Anymore), it ranks from 1,137 in TransE to 77 in SSP, because both the entities share the relevant semantics as "Best Actress" and "Academy Award". Table 4: Statistics of the results of Link Prediction SSP(S.)#100 SSP(J.)#100 E#500 E#1000 E#2000 E#3000 E#5000 4.5 601 275 80 32 3 672 298 89 39 3







Figure 3: Precise semantic expression analysis for SSP (Std.). The x-axis indicates the score difference, where a bigger value means better. The y-axis means the proportion of the corresponding triple pairs. The gray part indicates that both the TransE and SSP make mistakes while the colored part means that SSP makes it correct but TransE fails. Notably, it is similar for Joint setting. likely to be semantically similar. To conduct the case study, we obtain the top three entities closest to a given one. For the entity Peoria that is a city, SSP (Std.) comes up with Jolie, Newark and Springfield, all of which are cities, while TransE comes up with Blue Ridge Mountain, U.S.A and Grammy Award, all of which are city-irrelevant. For another entity of Indian National Football Team, SSP (Std.) comes up with Atalanta B.C., C.C& Football Club and A.E.K. Athens F.C., all of which are football clubs, but TransE comes up with Brendan Coyle, Dayton and Akiva D. Kiv, which are respectively an actor, a city and a writer. Comparatively, the embedding topologies in our model are obviously semantics-specific, which justifies that our model fuses the symbolic triples with the textual semantics, effectively.







Precise Semantic Expression Analysis







As discussed previously, precise semantic expression leads to better discrimination. To justify this argument, we have collected the negative triples by link prediction, which are scored sightly better than the golden ones by TransE, and then plotted the SSP score difference between each corresponding pair of the negative and golden triples as Fig.3 shows. All these triples are mistaken by TransE, but with precise semantic expression, our model correctly distinguishes 82.0% (Std.) and 83.2% (Joint) of them. In the histogram, the right part indicates that SSP makes it correct and the left means both the SSP and TransE mistakes. Bigger x-axis direction means involving stronger semantics. The experiments prove the theoretical analysis about the precise semantic expression and demonstrate the effectiveness of our models. 4.6 Embedding Topologies Analysis







5







Conclusion







As the "Correlation Perspective" suggests, our embedding topologies are semantics-specific. Thus the adjacent entities, which are more probably to co-occur in the same subspace, are







In this paper, we propose the knowledge graph embedding model SSP, which jointly learns from the symbolic triples and textual descriptions. SSP could interact the triples and texts by characterizing the strong correlations, by which means, the textual descriptions could make more effects to discover semantic relevance and offer precise semantic expression. Extensive experiments show our method achieves the substantial improvements against the state-of-the-art baselines.







References



[Blei2012] David M Blei. 2012. Probabilistic topic models. Communications of the ACM, 55(4):77-84. [Bollacker et al.2008] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247- 1250. ACM. [Bordes et al.2011] Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of knowledge bases. In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence. [Bordes et al.2013] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pages 2787-2795. [Fan et al.2014] Miao Fan, Qiang Zhou, Emily Chang, and Thomas Fang Zheng. 2014. Transitionbased knowledge graph embedding with relational mapping properties. In Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, pages 328-337. [Glorot and Bengio2010] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics, pages 249-256. [Guo et al.2015] Shu Guo, Quan Wang, Bin Wang, Lihong Wang, and Li Guo. 2015. Semantically smooth knowledge graph embedding. In Proceedings of ACL. [He et al.2015] Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015. Learning to represent knowledge graphs with gaussian embedding. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 623-632. ACM. [Hoffmann et al.2011] Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541-550. Association for Computational Linguistics. [Jenatton et al.2012] Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, and Guillaume R Obozinski. 2012. A latent factor model for highly multirelational data. In Advances in Neural Information Processing Systems, pages 3167-3175.







[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge graph embedding via dynamic mapping matrix. [Lin et al.2015a] Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015a. Modeling relation paths for representation learning of knowledge bases. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. [Lin et al.2015b] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015b. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. [Mikolov et al.2013a] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013a. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119. [Mikolov et al.2013b] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In HLTNAACL, pages 746-751. [Miller1995] George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41. [Neelakantan and Chang2015] Arvind Neelakantan and Ming-Wei Chang. 2015. Inferring missing entity type instances for knowledge base completion: New dataset and methods. arXiv preprint arXiv:1504.06658. [Nickel et al.2011] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 809-816. [Socher et al.2013] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926-934. [Stevens et al.2012] Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Buttler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 952-961. Association for Computational Linguistics. [Wang et al.2014a] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014a. Knowledge graph and text jointly embedding. In EMNLP, pages 1591-1601. Citeseer.







[Wang et al.2014b] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014b. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1112-1119. [Wang et al.2015] Quan Wang, Bin Wang, and Li Guo. 2015. Knowledge base completion using embeddings and rules. In Proceedings of the 24th International Joint Conference on Artificial Intelligence. [Xiao et al.2015] Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. 2015. TransA: An adaptive approach for knowledge graph embedding. arXiv preprint arXiv:1509.05490. [Xie et al.2016] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge graphs with entity descriptions. [Zhong et al.2015] Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. 2015. Aligning knowledge and text embeddings by entity descriptions. In Proceedings of EMNLP, pages 267- 272.







Online Bandit Learning for a Special Class of Non-convex Losses



National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China 2 Department of Computer Science, the University of Iowa, Iowa City, IA 52242, USA 3 Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA {zhanglj, zhouzh}@lamda.nju.edu.cn, tianbao-yang@uiowa.edu, rongjin@cse.msu.edu



1







Lijun Zhang1 and Tianbao Yang2 and Rong Jin3 and Zhi-Hua Zhou1







Abstract



In online bandit learning, the learner aims to minimize a sequence of losses, while only observing the value of each loss at a single point. Although various algorithms and theories have been developed for online bandit learning, most of them are limited to convex losses. In this paper, we investigate the problem of online bandit learning with non-convex losses, and develop an efficient algorithm with formal theoretical guarantees. To be specific, we consider a class of losses which is a composition of a non-increasing scalar function and a linear function. This setting models a wide range of supervised learning applications such as online classification with a non-convex loss. Theoretical analysis shows e (poly (d)T 2/3 ) regret that our algorithm achieves an O bound when the variation of the loss function is small. To the best of our knowledge, this is the first work in online bandit learning that does not rely on convexity.







Introduction



Online decision-making has become a popular learning paradigm in many disciplines such as Artificial Intelligence, Economics and Control Theory (Saha and Tewari 2011). At each round of online learning, the learner chooses a decision from a given set, and an adversary responds with a loss function that decides the cost of decisions. The performance of an online learning algorithm is measured by the regret, which is the difference between the total cost of the decisions it chooses, and the cost of the optimal decision chosen in hindsight. According to the amount of information revealed to the learner, online learning can be classified into two categories (Cesa-Bianchi and Lugosi 2006): i) full information setting where the learner observes the entire cost function, and ii) bandit setting where only the cost of the selected action is available. In the past decades, there have been tremendous progresses made in online bandit learning, ranging from multiarmed bandit (Robbins 1952; Auer et al. 2003), online linear optimization with bandit feedback (Awerbuch and Kleinberg 2004; Dani, Kakade, and Hayes 2008; Abernethy, Hazan, and Rakhlin 2008), to online convex optimization with bandit feedback (Flaxman, Kalai, and McMahan 2005;



Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







Saha and Tewari 2011; Agarwal, Dekel, and Xiao 2010). A major limitation of the previous work is that most of them are restricted to convex losses. The drawback of using convex losses has been revealed by several recent studies. In (Calauz enes, Usunier, and Gallinari 2012), the authors show it is impossible to find any convex loss that is calibrated with the standard evaluation metrics for ranking. Similarly, for multi-label learning, no convex loss is consistent with the popular ranking loss (Gao and Zhou 2011). The success of deep learning also indicates the significance of using non-convex losses (Hinton, Osindero, and Teh 2006). The resurge of non-convex losses in machine learning motives us to investigate online bandit learning with nonconvex loss functions. In particular, we consider the following learning protocol. * At the t-th round, the learner submits a point xt 2 Rd with kxt k2  1, and simultaneously an oblivious adversary selects a vector ut and a non-increasing scalar function ft : R 7! R that assesses the consistency between the ground truth ut and the answer xt by ft (x> t ut ). * Instead of revealing the loss function ft (hut , *i) directly, the adversary only provides the learner ct 2 R whose expectation is the cost ft (u> t xt ), i.e., Et 1 [ c t ] = ft ( u > (1) t xt ), where Et 1 [*] is the expectation conditioned on the randomness until round t 1. Notice that the above protocol generalizes many machine learning tasks. Taking the online classification as an example, we can set ut = yt zt , where zt 2 Rd is an instance, yt 2 {1} is the assigned class label, and ft (*) can be any loss function such as the ramp loss (Ertekin, Bottou, and Giles 2011). We emphasize that in our setting both ut and ft (*) are unknown to the learner. More importantly, unlike most online learning that assume the loss function to be convex, in this study, ft (*) can be non-convex. This relaxation makes our problem significantly more challenging than most online learning problems, including the traditional online bandit learning. Following the convention in online learning, our goal is to generate a sequence of answer vectors x1 , . . . , xT , that leads to a small regret defined below T T X X regret = ft ( u > x ) min ft ( u > t t t x).



t=1 kx k2  1 t=1







We present a simple algorithm for online bandit learning with non-convex losses. It is computationally efficient and achieves a non-trivial regret bound under appropriate conditions. Our approach follows the standard explorationexploitation framework for bandit learning (Awerbuch and Kleinberg 2004; McMahan and Blum 2004). In an exploration round, the algorithm submits a random vector in order to obtain an unbiased estimate of ut , and updates the current solution based on the bandit feedback. In an exploitation round, it submits the current solution in order to incur a small loss. Under the assumption that ft (*) is non-increasing and Lipschitz continuous, we are able to bound the regret by the number of iterations and the variation of the target vectors {ut }T specific, the regret bound takes the form t=1 . To be p e (poly (d)T 2/3 + T VT ),1 where VT is the variation of vecO tors u1 , . . . , uT . Thus, the proposed algorithm achieves an e ((poly (d)T 2/3 ) regret bound if VT  O(T 1/3 ). O







and Proutiere 2014), the regret could be further improved. The online linear optimization problem with bandit feedback was first introduced by Awerbuch and Kleinberg (2004) who obtained a O(d3/5 T 2/3 ) regret bound against an oblivious adversary. Later, McMahan and Blum (2004) achieved an O(poly (d)T 3/4 ) regret bound against an adaptive adversary. In (Dani, Kakade, and Hayes 2008; Abernethy, Hazan, and Rakhlin 2008), the regret bound was improved p to O(poly (d) T ), where the dependence on T is optimal (Bubeck, Cesa-Bianchi, and Kakade 2012).







Online Non-convex Optimization



Several heuristic approaches have been developed for online learning with non-convex loss in the full information setting, such as the online version of the concave-convex procedure (Ertekin, Bottou, and Giles 2011; Gasso et al. 2011). However, none of them are equipped with a formal regret bound. One exception is the online submodular p minimization (Hazan and Kale 2012) that achieves O( dT ) and O(dT 2/3 ) regret bounds in the full information and bandit settings, respectively. But these algorithms rely on the specific property of submodular function (i.e., the Lov asz extension is convex), and thus cannot be applied to the problem considered here.







Related Work







In this section, we briefly review the related work in online convex and non-convex optimizations.







Online Convex Optimization



In the full information setting, online convex optimization has been extensively studied (Kivinen, Smola, and Williamson 2002; Zhang et al. 2013). Zinkevich (2003) shows that a p simple online gradient descent algorithm achieves an O( T ) regret bound for convex and Lipschitz continuous functions. When the loss function is strongly convex, the regret bound can be improved to O(log T ) (Hazp an, Agarwal, and Kale 2007). Both the O( T ) and O(log T ) regrets bounds, for convex and strongly convex loss functions respectively, are known to be minimax optimal (Abernethy et al. 2009). Compared to the full information setting, the regret bound for the bandit setting is usually worse and has an explicit dependence on the dimensionality d. The current best-known e ( d 2 / 3 T 2 / 3 ) , O ( d 2 / 3 T 2 /3 ) , regret bounds are O(dT 3/4 ), O p and O(d T ) for convex, convex-and-smooth, strongly convex, and strongly-convex-and-smooth functions, respectively (Flaxman, Kalai, and McMahan 2005; Saha and Tewari 2011; Agarwal, Dekel, and Xiao 2010). Notice that when the learner is allowed to query the loss function at multiple points, the regret can be improved to match its counterpart in the full information setting (Agarwal, Dekel, and Xiao 2010). In the bandit setting, there are two special cases that are well-studied: multiarmed bandit and online linear optimization with bandit feedback. In the first problem, we assume there are K arms, and a gambler pulls one of them to receive a reward in each round. Auer et al. (2003) p prove that e the gambler's regret can be bounded by O( KT ), which is optimal up to logarithmic factors (Audibert and Bubeck 2009). Furthermore, if the reward function has some structural properties, such as Lipschitz (Magureanu, Combes,



1 e notation to hide constant factors as well as polyWe use the O logarithmic factors in d and T .







An Efficient Algorithm for Online Bandit Learning



We first describe the proposed algorithm for online bandit learning with non-convex losses, and then state its theoretical guarantees.







The Algorithm



Algorithm 1 summarizes the key steps of the proposed algorithm. We maintain two sequences of vectors during the learning process: the answer vectors xt and the auxiliary vector wt . We initialize the answer vector x1 to be a random normalized vector, and the auxiliary vector w1 to be 0. At each iteration t, we generate a Bernoulli random variable Zt with Pr(Zt = 1) =  to determine whether to explore or exploit. When Zt = 0, we will simply submit the answer vector xt as the solution, and make no update. When Zt = 1, we will first compute a normalized Gaussian random vector vt /kvt k2 and submit it as the answer. Based on the received feedback ct , we update the auxiliary vector and the answer vector by wt+1 = wt ct wt+1 vt and xt+1 = . kvt k2 kwt+1 k2







We note that for the sake of simplicity, Algorithm 1 follows the early studies of online bandit learning that separate the exploration steps from the exploitation steps (Awerbuch and Kleinberg 2004; McMahan and Blum 2004). This is different from the more recent strategy for explorationexploitation (Flaxman, Kalai, and McMahan 2005; Abernethy, Hazan, and Rakhlin 2008) that usually combines exploration and exploitation into a single step by adding random perturbation to the submitted solutions. We will exam-







Algorithm 1 An Efficient Algorithm for Online Bandit Learning Input: step size  and number of trials T 1: Set  = T 1/3 2: Initialize x1 as any random normalized vector and w1 = 0 3: for t = 1, 2, . . . , T do 4: Sample binary random variable Zt with Pr(Zt = 1) =  . 5: Sample a random vector vt from an Gaussian distribution N (0, Id ) vt 6: Submit the solution x0 Zt ) x t t = Zt kvt k2 + (1 7: Receive ct from the adversary t ct 8: Update the auxiliary vector wt+1 = wt kZ v t k2 v t 9: Update the answer vector xt+1 = wt+1 /kwt+1 k2 10: end for ine in the future the second strategy for online bandit learning with non-convex losses. Finally, we would like to point out that following the idea of discretizing the decision space (Dani, Kakade, and Hayes 2008), our problem can be reduced to the multiarmed bandit and solved by existing methods (Auer et al. 2003). However, this strategy is inefficient because the number of arms is exponential in the dimensionality d, and the regret bound may also have a high dependence on d. In contrast, our algorithm is very efficient and the regret bound only has a polynomial dependence on d.







where (*) is the Dirac delta function. When d 2, we have (Cho 2009) ( d 3 (d 2) p (1 z 2 ) 2 , for 1 < z < 1 h( z ) = (3) ( d2 1 )  0, otherwise where (*) is the Gamma function. The following proposition, which is inspired by the recent developments in one-bit compressive sensing (Plan and Vershynin 2013; Zhang, Yi, and Jin 2014), provides the key observation for our analysis. Proposition 1. We have  Zt ct Et 1 vt =  u, t = 1, . . . , T kvt k2 where = Z



1







(4)







f (z )h(z )z d z.



1







(5)







From our assumption that f (*) is non-increasing, it is easy to verify that 0. We note that will have a strong dependence on d. Generally speaking, we have = poly (d 1 ). For instance, when f (z ) = z , we have = E[z 2 ] = d 1 (Cho 2009). Proposition 1 shows that in the exploration step, our algorithm is able to find an unbiased estimate of u, up to a scaling factor. Based on this observation, we obtain the following regret bound. Theorem 1. Assume 8 !3 9 r < = T (d + 1) T max e, log log T . : ; regret    1 4B log + 1 + 4B 







The Main Results



Besides the basic assumption in (1), we further make the following assumptions in our analysis. * ft (*) is non-increasing and L-Lipschitz continuous. * Both ct and ft (*) are upper bounded by a constant B . That is,



t2 [T ]







(6)







Set  = T 1/3 in Algorithm 1. Then, with a probability 1  , we have 3L r T (d + 1) !







sup |ct |  B, and







sup



t 2 [ T ] ,kx k2  1







|ft ( u > t x)|  B.







(2)







log







+ 1 T 3.







2







* The target vectors ut 's are of unit length, i.e., kut k2 = 1, t = 1, . . . T . * The adversary is oblivious, meaning that both ut 's and ft 's are fixed. As a starting point, we first analyze the regret bound for the simplest case when all the target vectors are the same, and then move to the general case. Regret Bound for a Single Target Vector We first consider the simplest case when u1 = u2 = * * * = uT = u and f1 = f2 = * * * = fT = f . Define h(z ) as the probability density function (PDF) of the inner product of random unit vectors in Rd . When d = 1, it is easy to verify h( z ) = 1 ( (z 2 1) + (z + 1)),







To simplify the presentation, we assume the horizon T is known so that we can choose  = T 1/3 in the algorithm. This limitation could be addressed by the well-known "doubling trick" (Cesa-Bianchi and Lugosi 2006). Theorem 1 ime ( 1 T 2/3 ) regret bound, plies our algorithm achieves an O which is even better than the regret bound in the general online convex optimization with bandit feedback (Flaxman, Kalai, and McMahan 2005). From the discussion in (Dani and Hayes 2006; Abernethy, Hazan, and Rakhlin 2008), we also know that (T 2/3 ) regret bound is unavoidable if any algorithm ignores the feedback received during exploitation. We finally note that although the regret bound in Theorem 1 does not have an explicit dependence on d, its dependence on d comes from . We now extend the simple case to a slightly more complicated scenario where a different loss function is used. In this case, we have the following proposition.







Proposition 2. We have  Zt ct Et 1 vt = kvt k2 where



t







t  u,







t = 1, . . . , T







=







Following almost the same analysis for Theorem 1, we obtain the following theorem to bound the regret. Theorem 2. Suppose T and  satisfy the conditions in Theorem 1. With a probability 1  , we have regret    1 4B log + 1 + 4B  where  = min



t 2 [T ] t.







Z







1







ft (z )h(z )z d z.



1







3L 







r







log







T (d + 1)







+1 T3







!







2







When VT  O(T 1/3 ), the additional p the total variation p term T (VT + 12 VT ) is on the order of T 2/3 . Furthermore, if we assume a small variation for each iteration, that  t k2 will be lower is, Vt  O(t1/3 ) for all t 2 [T ], each ku bounded by some constant,2 and thus 1/T is upper bounded e ( 1 T 2/3 ) by some constant. As a result, we still have an O regret bound. On the other hand, the regret bound becomes trivial when VT = (T ), which is significantly worse than the previous results on variation based regret bound (Hazan and Kale 2010). This is because we are dealing with nonconvex optimizations and therefore the gradient does not provide an universal lower bound for the entire function. Thus, we cannot utilize the analysis for convex functions, but only rely on the assumption that ft (*) is non-increasing and Lipschitz continuous. Finally, it is straightforward to extend the above result to the case when a different loss function is used, by introducing  defined in Theorem 2.







The only difference between Theorems 2 and 1 is that in Theorem 1 is replaced with  , the smallest one among { t }T t=1 . Regret Bound for the General Case We now consider the t more general case where each ut is a different vector. Let u be the average of vectors u1 , . . . , ut , i.e., t = u 1 t



t X i=1







Analysis



We here present the proofs of main theorems. The omitted proofs are provided in the supplementary material.







Proof of Theorem 1



Under the assumption u1 = u2 = * * * = uT = u, we have regret



T X t=1







ut . =







Similar to Theorem 1, we have the following regret bound for the general case when a single loss function is used. Theorem 3. Suppose T and  satisfy the conditions in Theorem 1. Then, with a probability 1  , we have   T X 1  t 1 k2 regret 4B log + 1 + 2L kut u  t=2 ! r 2 3L T (d + 1) + 4B log + 1 T 3, T where  t k2 . is defined in (5) and T = min ku



t 2 [T ]







f (u> x0 t)







T min f (u> x)



kxk1







(2)







  >   T X u vt > = Zt f f (u xt ) kvt k2 t=1  T  X + f (u> xt ) min f (u> x)



t=1 T X t=1







(8)







2B







Zt +



1







To bound the second term in the regret bound, we need the following inequality (Hazan and Kale 2010) v u T T T uX X X 2 t  t 1 k2   T k2 kut u  k u u k +12 ku t u t T 2 2 2.



t=2 t=1 t=1







Then, we discuss how to bound 1 and 2 . According to the Multiplicative Chernoff Bound (Angluin and Valiant 1979) provided in the supplementary, we have with a probability at least 1 



1







| {z }







T  X







kxk1







f (u> xt )







|







t=1







{z







kxk1







 min f (u> x) . }







2







From (7) and Theorem 3, we have   q p 1 regret 4B log + 1 + 2L T (VT + 12 VT )  ! r 2 3L T (d + 1) + 4B log +1 T3 T where VT =



T X t=1







(7)







 2E[







1]







+ 2 log







1 1 = 2 T + 2 log .  







(9)







To bound 2 , we introduce the vector-valued martingaledifference sequence



i







=







The following lemma follows immediately from the Freedman's inequality for matrix martingales (Tropp 2011).



2  t k2  c, then We can prove it by contradiction. Suppose ku we must have Vt (1 c)2 t = (t).







Zi ci vi kvi k2







 u, i = 1, . . . , T.







(10)







kut







 T k2 u 2.







Lemma 1. With a probability 1



t X i=1 i 2







, we have, for any t 2 [T ],  t ( ) r







Proof of Lemma 1



We first state the Freedman's inequality for matrix martingales below. Theorem 4. (Tropp 2011, Corollary 1.3) Let k * k be the spectral norm of a matrix, which returns its largest singular value. Consider a matrix martingale {Yi : i = 0, 1, 2, . . . } whose values are matrices with dimension d1  d2 . Let {Xi : i = 1, 2, 3, . . .} be the difference sequence, and assume that the difference sequence is uniformly bounded: kXi k  R almost surely i = 1, 2, 3, . . . . Define two predictable quadratic variation processes for this martingale: Wcol,t := Wrow,t := Then, for all Pr kYt k



t X i=1 t X i=1







where t ( ) =







4B T (d + 1) T (d + 1) log + B 2 t log . 3 From the assumption that f (*) is non-increasing and LLipschitz continuous, we have the following lemma. Lemma 2. We have f (x> min f (x> u) t u) kxk1 ( 2B, if t = 1; Pt 1  2L , otherwise. i=1 i  (t 1)



2







Based on Lemmas 1 and 2, we have with a probability at least 1 , T 2 L X t 1 ( )  2 B + 2  t=2 t 1 =2B +



T 1 8LB T (d + 1) X 1 log 3  t t=1 s T 1 2LB 2 T (d + 1) X 1 p + log  t t=1







Ei Ei







> 1 [ Xi Xi ] ,







> 1 [ Xi Xi ] ,







t = 1, 2, 3, . . . .







0 and







2







> 0,







(11)







(6)







 2B +







6LB T (d + 1) log log T  s 4LB 2T T (d + 1) + log , 







where we use the following inequalities Z T T X 1 1 1 + dt = 1 + log t|T 1 = log T + 1, and t t t =1 t=1 Z T T X p p 1 1 p 1+ p dt = 1 + 2 t|T 1. 1 =2 T t t t=1 t=1







and max{kWcol,t k, kWrow,t k}  2   2 /2  (d1 + d2 ) exp . 2 + R /3   2 /2 By setting = exp 2 +R /3 , Theorem 4 implies that with a probability at most , r 2R d1 + d2 d1 + d2 kY t k log + 2 2 log and 3 max{kWcol,t k, kWrow,t k}  2 . We then introduce several facts that will be used in our analysis. Let  be a random vector. Based on Jensen's inequality, we have kE[ ]k2  E[k k]2 . (12)







Combining (8), (9) and (11), we have with a probability at least 1  , regret   1 6LB T (d + 1) 4B  T + log + 1 + log log T   s 4LB 2T T (d + 1) + log    2 1 6LB 1 T (d + 1) 3 =4B T + log + 1 + T 3 log log T  r 6LB T (d + 1) 2 + log T3 r   (6) 2 1 12LB T (d + 1) 2 3 4B T + log + 1 + log T 3. 







From the property of positive semidefinite (PSD) matrices, we have   E ( E [ ])( E [ ])> = E[ > ] E[ ]E[ ]> => E[ > ] E[ ]E[ ]>  (13) > E[ > ]  kE[ > ]k, where  is the largest eigenvector of the PSD matrix E[ > ] E[ ]E[ ]> . Furthermore, it is easy to verify that   E ( E [ ])> ( E [ ]) (14) =E[ >  ] E[ ]> E[ ]  E[ >  ]. Notice that the spectral norm of a vector is its 2 -norm.







We bound the 2 -norm of the k i k2 = Zi c i  vi kvi k2 (4), (12) Zi ci  vi kvi k2



(2)







i







as follows u



2







Zi ci vi kvi k2



2







+ k  uk2  Zi ci + Ei 1 vi kvi k2 2







According to the procedure in Algorithm 1, we have t 1 t 1 X X Zi ci (10) wt = vi =  (t 1)u + i. kv i k2 i=1 i=1 Then, we have kwt



2







 (t







1)uk2  u



2







We then bound the two predictable quadratic variation processes as follows



t X







2B, i = 1, . . . , T.







,







wt  (t 1)















Ei Ei Ei







> 1[ i i ]







= 







Zi ci v i + u kv i k2 i=1  t (4), (13) X Zi c2 i >  Ei 1 2 vi vi k v k i 2 i=1



1 t X







t X







i=1 t X i=1







1







" "







Zi ci v i + u kv i k2







 







Zi ci v i + u kvi k2 Zi ci v i + u kvi k2







> # > #







Following a simple geometric argument, we have t 1 X wt 2 u  i kw t k  (t 1) i=1 2



2 (15)







t 1 X 1  (t 1) i=1







t 1 X i=1







i 2 i 2







.







) f (x> t u)







kxk1







min f (x> u) 







Proof of Theorem 2







t 1 X 2L  (t 1) i=1







i 2







.







(2)







  B 2 t,







Ei Ei



t X i=1







> 1[ i i]







=







(4), (14)







i=1 t X i=1







1







"



1







Zi ci vi +  u kvi k2 



(2)







> 







Zi ci vi +  u kvi k2







#















Ei







2 Zi c2 i   B t.







In this case, we define the vector-valued martingaledifference sequence as Zi ci vi i = i  u, i = 1, . . . , T. kvi k2 It is easy to verify that Lemma 1 still holds and Lemma 2 become the following one. Lemma 3. We have ft ( x > min ft (x> u)  t u) kxk1 ( 2B, if t = 1; Pt 1 2L , otherwise. i=1 i   (t 1)



2







The rest proof is the same as that for Theorem 1.







Then, based on Theorem 4, for each t 2 [T ], we have with a probability at least 1 r t X 4B d+1 d+1  log + B 2 t log . i 3 i=1



2







We complete the proof by taking the union bound over t = 1, . . . , T .







Proof of Lemma 2



When t = 1, it is clear that f (x> 1 u)



kxk1







min f (x> u)  2B.







(2)







In the following, we discuss the case when t 2. From our assumption that f (*) is a non-increasing function, we have f (x> t u)



kxk1







In this paper, we study the problem of online bandit learning with non-convex losses, and assume the loss function is a composition of a non-increasing scalar function and a linear function. Following the idea of exploration and exploitation, we develop an efficient algorithm which achieves e (poly (d)T 2/3 ) regret bound under appropriate conditions. O One limitation of the current work is that the regret bound only holds against an oblivious adversary. In the future, we will investigate how to extend our results to the adaptive adversary. There are also many open problems for bandit learning with non-convex losses, such as under what condition there exists a Hannan-consistent algorithm and what is the lower bound. We will leave these questions for future investigations. This research was supported by NSFC (61333014, 61321491), the Collaborative Innovation Center of Novel Software Technology and Industrialization, NSF (IIS1251031), ARO (W911NF-11-1-0383), and Baidu Fund (181315PO5760).







Conclusion and Future Work







min f (x> u) = f (x> t u)







f (u> u).







Since f (*) is L-Lipschitz continuous, we further have f (x> t u) f ( u > u )  L| x > t u uk2 = L  Lkxt wt kwt k u> u| u



2







Acknowledgments







.







(15)







Abernethy, J.; Agarwal, A.; Bartlett, P. L.; and Rakhlin, A. 2009. A stochastic view of optimal regret through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory. Abernethy, J.; Hazan, E.; and Rakhlin, A. 2008. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning, 263-274. Agarwal, A.; Dekel, O.; and Xiao, L. 2010. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning, 28-40. Angluin, D., and Valiant, L. 1979. Fast probabilistic algorithms for hamiltonian circuits and matchings. Journal of Computer and System Sciences 18(2):155-193. Audibert, J.-Y., and Bubeck, S. 2009. Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory. Auer, P.; Cesa-Bianchi, N.; Freund, Y.; and Schapire, R. E. 2003. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing 32(1):48-77. Awerbuch, B., and Kleinberg, R. D. 2004. Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches. In Proceedings of the 36th Annual ACM Symposium on Theory of Computing, 45-53. Bubeck, S.; Cesa-Bianchi, N.; and Kakade, S. M. 2012. Towards minimax policies for online linear optimization with bandit feedback. In Proceedings of the 25th Annual Conference on Learning Theory. Calauz enes, C.; Usunier, N.; and Gallinari, P. 2012. On the (non-)existence of convex, calibrated surrogate losses for ranking. In Advances in Neural Information Processing Systems 25, 197-205. Cesa-Bianchi, N., and Lugosi, G. 2006. Prediction, Learning, and Games. Cambridge University Press. Cho, E. 2009. Inner product of random vectors. International Journal of Pure and Applied Mathematics 56(2):217-221. Dani, V., and Hayes, T. P. 2006. Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary. In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithm, 937-943. Dani, V.; Kakade, S. M.; and Hayes, T. P. 2008. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems 20, 345-352. Ertekin, S.; Bottou, L.; and Giles, C. L. 2011. Nonconvex online support vector machines. IEEE Transactions on Pattern Analysis and Machine Intelligence 33(2):368-381. Flaxman, A. D.; Kalai, A. T.; and McMahan, H. B. 2005. Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms, 385- 394. Gao, W., and Zhou, Z.-H. 2011. On the consistency of multilabel learning. In Proceedings of the 24th Annual Conference on Learning Theory, 341-358.







References







Gasso, G.; Pappaioannou, A.; Spivak, M.; and Bottou, L. 2011. Batch and online learning algorithms for nonconvex neyman-pearson classification. ACM Transactions on Intelligent Systems and Technology 2(3):28:1-28:19. Hazan, E.; Agarwal, A.; and Kale, S. 2007. Logarithmic regret algorithms for online convex optimization. Machine Learning 69(2-3):169-192. Hazan, E., and Kale, S. 2010. Extracting certainty from uncertainty: regret bounded by variation in costs. Machine Learning 80(2-3):165-188. Hazan, E., and Kale, S. 2012. Online submodular minimization. In Journal of Machine Learning Research, volume 13, 2903-2922. Hinton, G. E.; Osindero, S.; and Teh, Y.-W. 2006. A fast learning algorithm for deep belief nets. Neural Computation 18(7):1527-1554. Kivinen, J.; Smola, A. J.; and Williamson, R. C. 2002. Online learning with kernels. In Advances in Neural Information Processing Systems 14, 785-792. Magureanu, S.; Combes, R.; and Proutiere, A. 2014. Lipschitz bandits: Regret lower bounds and optimal algorithms. In Proceedings of the 27th Conference on Learning Theory, 975-999. McMahan, H. B., and Blum, A. 2004. Online geometric optimization in the bandit setting against an adaptive adversary. In Proceedings of the 17th Annual Conference on Learning Theory, 109-123. Plan, Y., and Vershynin, R. 2013. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory 59(1):482-494. Robbins, H. 1952. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society 58(5):527-535. Saha, A., and Tewari, A. 2011. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 636-642. Tropp, J. A. 2011. Freedman's inequality for matrix martingales. Electronic Communications in Probability 16:262- 270. Zhang, L.; Yi, J.; Jin, R.; Lin, M.; and He, X. 2013. Online kernel learning with a near optimal sparsity bound. In Proceedings of the 30th International Conference on Machine Learning (ICML). Zhang, L.; Yi, J.; and Jin, R. 2014. Efficient algorithms for robust one-bit compressive sensing. In Proceedings of the 31st International Conference on Machine Learning (ICML). Zinkevich, M. 2003. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, 928- 936.







Stochastic Optimization for Kernel PCA



Lijun Zhang1,2 and Tianbao Yang3 and Jinfeng Yi4 and Rong Jin5 and Zhi-Hua Zhou1,2



National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, China 3 Department of Computer Science, the University of Iowa, Iowa City, USA 4 IBM Thomas J. Watson Research Center, Yorktown Heights, USA 5 Alibaba Group, Seattle, USA {zhanglj, zhouzh}@lamda.nju.edu.cn, tianbao-yang@uiowa.edu, jinfengy@us.ibm.com, jinrong.jr@alibaba-inc.com



2 1







Abstract



Kernel Principal Component Analysis (PCA) is a popular extension of PCA which is able to find nonlinear patterns from data. However, the application of kernel PCA to largescale problems remains a big challenge, due to its quadratic space complexity and cubic time complexity in the number of examples. To address this limitation, we utilize techniques from stochastic optimization to solve kernel PCA with linear space and time complexities per iteration. Specifically, we formulate it as a stochastic composite optimization problem, where a nuclear norm regularizer is introduced to promote low-rankness, and then develop a simple algorithm based on stochastic proximal gradient descent. During the optimization process, the proposed algorithm always maintains a low-rank factorization of iterates that can be conveniently held in memory. Compared to previous iterative approaches, a remarkable property of our algorithm is that it is equipped with an explicit rate of convergence. Theoretical analysis shows that the solution of our algorithm converges to the optimal one at an O(1/T ) rate, where T is the number of iterations.







the low-rank structure, the approximator can be easily stored and manipulated. The major limitation of approximate approaches is that there always exists a non-vanishing gap between their solution and that found by eigendecomposing K directly. Iterative approaches (Kim, Franz, and Sch olkopf 2005) use partial information of K in each round to estimate the top eigenvectors, and thus do not need to keep the entire matrix in memory. With appropriate initialization, the solution of iterative approaches will converge to the groundtruth asymptotically. However, there is no guarantee of the convergence rate or the global convergence property for general initial conditions. Inspired by the recent progresses in stochastic optimization (Avron et al. 2012; Rakhlin, Shamir, and Sridharan 2012), we develop a novel iterative algorithm for kernel PCA that has a solid convergence guarantee. The staring point is the following observation: Since only the top eigensystems of K are used in kernel PCA, it is sufficient to find a low-rank matrix K from which we can recover the top eigensystems of K . In this paper, we choose K as the low-rank matrix obtained by applying Singular Value Shrinkage (SVS) operator (Cai, Cand es, and Shen 2010) to K . Thus, the problem becomes how to estimate K without constructing K explicitly. To this end, we formulate the SVS operation as a stochastic composite optimization problem, and develop an efficient algorithm based on Stochastic Proximal Gradient Descent (SPGD). The advantage of the stochastic formulation is that only a low-rank estimate of K is needed during the optimization process. Since the SVS operation is applied in each iteration, all the iterates are prone to be low-rank. Furthermore, the low-rankness of iterates in turn makes the SVS operation very efficient. As a result, in each iteration, both space and time complexities are linear in n. By exploiting the strong convexity of the objective, we prove that the last iterate of SPGD converges to K at an O(1/T ) rate, where T is the number of iterations. It implies we can simply take the last iterate as the final solution, and thus avoid the averaging operation in the traditional algorithms (Hazan and Kale 2011; Rakhlin, Shamir, and Sridharan 2012). Finally, we examine the empirical performance of the proposed algorithm on two benchmark data sets.







Introduction



Principal Component Analysis (PCA) is a powerful dimensionality reduction method that has been widely used in various applications including data mining, information retrieval, and pattern recognition (Duda, Hart, and Stork 2000). While the classical PCA is limited to identifying linear structures, kernel PCA, a non-linear extension of PCA, has been proposed for extracting non-linear patterns from data (Sch olkopf, Smola, and M uller 1998). The key idea is to map the data into a kernel-induced Hilbert space, where dot product between points can be computed efficiently through the kernel evaluation. Given a set of n training examples, kernel PCA needs to perform eigendecomposition of the n x n kernel matrix K . As it takes O(n2 ) space to store K and O(n3 ) time to eigendecompose it, kernel PCA is prohibitively expensive for big data, where n is very large. Existing studies for reducing the computational cost of kernel PCA can be classified into two categories: approximate and iterative. Approximate approaches (Lopez-Paz et al. 2014) construct a low-rank approximator of the kernel matrix, and use its eigensystems as an alternative. Due to



Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







Related Work



In this section, we briefly review the related work on kernel PCA and stochastic optimization.







Kernel PCA



The basic idea of kernel PCA is to map the input data into a Reproducing Kernel Hilbert Space (RKHS) induced by a kernel function, and perform PCA in that space (Sch olkopf, Smola, and M uller 1998). Let H be a RKHS with a kernel function (x, y) = (x) (y), x, y  Rd , where  : Rd  H is a possibly nonlinear feature mapping. For the sake of simplicity, we assume the data are centered, i.e., n i=1 (xi ) = 0. The covariance matrix in H is given by n 1 C= n i=1 (xi )(xi ) . We now have to find the eigenvalues   0 and eigenvectors v  H \ {0} satisfying C v = v. Since all solutions v with  = 0 lie within the span of (x1 ), . . . , (xn ), we can represent v as v = u, where  = [(x1 ), . . . , (xn )] and u  Rn . As a result, it is equivalent to consider the following problem 1  u = u. (1) n Let K  Rnxn be the kernel matrix with Kij = (xi , xj ) for i, j = 1 . . . , n. Multiplying both sides of (1) by  , we obtain K 2 u = nK u, which can be simplified to the eigenvalue problem K u = nu (Sch olkopf, Smola, and M uller 1998, Appendix A). Let (ui , i ) be the i-th eigenvector and eigenvalue pair of K , with normalization i ui 2 2 = 1. Then, the i-th eigenvector of C is given by vi = ui , which has unit length as indicated below vi vi = ui  ui = ui K ui = i ui



2 2 2







that aim to reduce its cost in testing. In particular, sparse kernel PCA (Tipping 2001) has been proposed to express each eigenvector vi in terms of a small number of training examples. It was later extended to online setting (Honeine 2012), where training examples arrive sequentially. Finally, we note that it is always possible to cast the problem of kernel PCA as a special case of linear PCA, which can be solved efficiently by online algorithms designed for linear PCA. To do this, we simply treat columns of K as feature vectors, evaluate them sequentially, and pass them to online algorithms for linear PCA. In this way, we can find the top eigensystems of K 2 , from which we can derive the top eigensystems of K . However, this kind of approaches suffers from one of the following limitations. 1. Some online PCA algorithms, such as the generalized Hebbian algorithm, are only able to find top eigenvectors. But for kernel PCA, we need both top eigenvectors and eigenvalues. 2. Many online algorithms for linear PCA, such as capped MSG (Arora, Cotter, and Srebro 2013) and incremental SVD (Brand 2006), lack formal theoretical guarantees. 3. Although certain online algorithms are equipped with regret bounds (Warmuth and Kuzmin 2008), the difference between the eigenvectors returned by online algorithms and the ground-truth remains unclear.







Stochastic Optimization



Stochastic optimization refers to the setting where we can only access to the stochastic gradient of the objective function (Hazan and Kale 2011; Zhang, Mahdavi, and Jin 2013). For general Lipschitz continuous convex functions, Stochastic Gradient Descent (SGD) exhibits the unimprovable O(1/ T ) rate of convergence (Nemirovski and Yudin 1983). For strongly convex functions, some variants of SGD (Hazan and Kale 2011; Rakhlin, Shamir, and Sridharan 2012; Zhang et al. 2013) achieve the optimal O(1/T ) rate (Agarwal et al. 2012). Recently, a special case of stochastic optimization, namely Stochastic Composite Optimization (SCO), has received significant interest in optimization and learning communities (Ghadimi and Lan 2012; Lin, Chen, and Pe na 2014; Zhang et al. 2014). In SCO, the objective function is given by the summation of non-smooth and smooth stochastic components (Lan 2012). The most popular non-smooth components are the 1 -norm regularizer for vectors and the nuclear norm regularizer for matrices, which enforce sparseness and low-rankness, respectively. Although the generic algorithms designed for stochastic optimization can also be applied to SCO, by replacing gradient with subgradient, they can not utilize the structure of the objective function to generate sparse or low-rank intermediate solutions. The specialized algorithms for SCO are all built upon Stochastic Proximal Gradient Descent (SPGD), where the power of the non-smooth term is preserved (Lan 2012; Ghadimi and Lan 2012; Chen, Lin, and Pe na 2012; Lin, Chen, and Pe na 2014). A major limitation of existing algorithms for SCO is that they did not treat memory as a limited resource. If we apply them to the SCO problem considered in this paper, the







= 1.







Generally speaking, it takes O(dn ) time to calculate K , O(n2 ) space to store, and O(n3 ) time to eigendecompose it. Thus, the vanilla procedure described above becomes computationally expensive when n is large. Approximate approaches for kernel PCA (Achlioptas, Mcsherry, and Sch olkopf 2002; Ouimet and Bengio 2005; Zhang, Tsang, and Kwok 2008; Lopez-Paz et al. 2014) adopt matrix approximation techniques, such as the Nystr om method (Williams and Seeger 2001; Drineas and Mahoney 2005), to construct a low-rank approximator of K , and then perform eigendecomposition of this low-rank matrix. For approximate approaches, there is a dilemma between the space complexity and the accuracy of their solution. The smaller the memory, the larger the approximation error, and vice visa. On the other hand, iterative approaches can find an accurate solution with a small memory, at the cost of a longer time. The most popular iterative approach for kernel PCA is the Kernel Hebbian Algorithm (KHA) (Kim, Franz, and Sch olkopf 2005; G unter, Schraudolph, and Vishwanathan 2007), which is a kernelized version of the generalized Hebbian algorithm designed for linear PCA (Sanger 1989). Similar to the algorithm proposed here, KHA is also a stochastic approximation algorithm. However, due to the non-convexity of its formulation, there is no global convergence guarantee for KHA. While the work referenced above focus on reducing the cost of kernel PCA during training, there are some studies







memory complexity is still O(n2 ). We do find a heuristic algorithm (Avron et al. 2012) in the literature which combines truncated SVD with SGD to control the space complexity. But it relies on the assumption that the objective value can be evaluated easily, which unfortunately does not hold in our case. That is the reason why we choose the basic SPGD instead of more advanced methods to optimize our problem and establish a novel convergence guarantee for SPGD.







is an unbiased estimate of K with rank at most k . If a symmetric matrix is desired, we can set   k k n  = Kij eij + eij Kij  2k j =1 j =1 which is an unbiased estimate of K with rank at most 2k . 2. When the kernel matrix K is generated by a shiftinvariant kernel, such as the Gaussian kernel and the Laplacian kernel. We can construct  by the random Fourier features (Rahimi and Recht 2008). Let (x, y ) be the shift-invariant kernel with Fourier representation (x, y) = p(w) exp(j w (x - y))dw







Algorithm



We first formulate kernel PCA as a SCO problem, then develop the optimization algorithm, next discuss implementation issues, and finally present the theoretical guarantee.







Reformulation of Kernel PCA



Denote the eigendecomposition of the kernel matrix K by U U , where U = [u1 , . . . , un ],  = diag[1 , . . . , n ], and 1  2  * * * n . To train kernel PCA, it is sufficient to find a low-rank matrix K from which the top eigensystems of K can be recovered. The ideal low-rank matrix would be k the truncated SVD of K , i.e., i=1 i ui ui for some integer k > 0. However, the truncated SVD operation is nonconvex, making it difficult to design a principled algorithm. Instead, we consider the low-rank matrix K obtained by applying the Singular Value Shrinkage (SVS) operator to K with threshold  (Cai, Cand es, and Shen 2010), 1 i.e., K = D [K ] = (i - )ui ui .



i:i >







where p(w) is a density function. Let w be a Fourier component randomly sampled from p(w), and let a(w) and b(w) be the feature vectors generated by w, i.e., a(w) =[cos(w x1 ), . . . , cos(w xn )] , b(w) =[sin(w x1 ), . . . , sin(w xn )] . By drawn k independent samples from p(w), denoted by w1 , . . . , wk , we construct  as = 1 k



k







a(wi )a(wi ) + b(wi )b(wi )



i=1







From the above expression, we observe that eigenvectors of K with nonzero eigenvalues are the top eigenvectors of K . Furthermore, nonzero eigenvalues of K are the top eigenvalues of K minus . As a result, we can recover the top eigensystems of K (with eigenvalues larger than ) from the eigendecomposition of K . In the following, we formulate the SVS operation as a SCO problem. First, it is well-known K is the optimal solution to the following convex composite optimization problem 1 min Z -K 2 (2) F + Z  n x n 2 Z R where *  is the nuclear norm of matrices. Let  be a lowrank random matrix which is an unbiased estimate of K , i.e., K = E[ ]. We list examples of such random matrices below. 1. For general kernel matrix K , we can construct  by sampling its rows or columns randomly. Let {i1 , . . . , ik } be a set of random indices sampled from [n] uniformly, Kij be the ij -th column of K , eij be the ij -th canonical base. Then, k n Kij eij = k j =1



For a matrix X  Rmxn with singular value decomposition U V , where  = diag[1 , . . . , min(m,n) ], D [X ] is given by D [X ] = U D []V and D [] = diag max(0, 1 - ), . . . , max(0, min(m,n) - ) .



1







which is an unbiased estimate of K with rank at most 2k . 3. For dot product kernels such as the polynomial kernel, we can generate the random matrix  in a similar way (Kar and Karnick 2012). Then, we rewrite Z - K 2 F in (2) as Z -K = Z =E



2 F 2 F







= Z



2 F







2 F







- 2 tr(Z K ) + K



2 F







2 F 2 F







- 2 tr(Z E[ ]) + E[  + K



2 F]







Z -







- E[







2 F] +  2 F]







K







- E[ 







2 F]







Since K 2 F - E[  (2) is equivalent to min nxn







is a constant term with respect to Z ,







1 E Z - 2 (3) F + Z  2 Z R a standard SCO problem with a nuclear norm regularizer.







Optimization by Stochastic Proximal Gradient Descent (SPGD)



At this point, one may consider applying existing algorithms for stochastic optimization to the problem in (3). Unfortunately, we find that all the previous algorithms can not be applied directly due to the high space complexity or unrealistic assumptions, as explained below. 1. The generic algorithms for stochastic optimization (Nemirovski et al. 2009; Hazan and Kale 2011; Rakhlin, Shamir, and Sridharan 2012; Shamir and Zhang 2012) are built up SGD, and thus cannot utilize the structure of (3) to enforce low-rankness. Furthermore, those algorithms return the average of iterates as the final solution, which could be full-rank.







2. Although the specialized algorithms for SCO can generate low-rank iterates based on SPGD, they need to keep track of the averaged iterates as an auxiliary variable (Chen, Lin, and Pe na 2012; Lin, Chen, and Pe na 2014) or as the final solution (Lan 2012; Ghadimi and Lan 2012). Thus, the space complexity is still O(n2 ). 3. Although the heuristic algorithm in (Avron et al. 2012) is able to make the space complexity linear in n, it needs to evaluate the objective value in each iteration, which is impossible for the SCO problem in (3). Furthermore, it is designed for general SCO problems and thus cannot exploit the strong convexity of (3). Due to the above reasons, we develop a new algorithm to optimize (3), which is purely based on SPGD and takes its last iterate as the final solution. Denote by Zt the solution at the t-th iteration. In this iteration, we first sample a random matrix t  Rnxn , and it is easy to verify that Zt - t is an unbiased estimate of the gradient of 1 Z - 2 F . 2E Then, we update the current solution by the SPGD, which is essentially a stochastic variant of composite gradient mapping (Nesterov 2013) Zt+1 1 Z - Zt 2 F + t Z - Zt , Zt - t + t  Z 2 Z Rnxn 1 2 = argmin Z - [(1 - t )Zt + t t ] F + t  Z  2 n x n Z R = argmin =Dt  [(1 - t )Zt + t t ] where t > 0 is the step size. Let Zt+1 = (1 - t )Zt + t t . The SVS operation applies a soft-thresholding rule to the singular values of Zt+1 , effectively shrinking them toward zero. In particular, singular values of Zt+1 that are below the threshold t  vanish, and thus Zt+1 tends to be a lowrank matrix. Let ZT +1 be the final solution obtained after T iterations. If ZT +1 is symmetric, we will eigendecompose ZT +1 and obtain its eigensystems {(ui , i )}k i=1 with nonzero eigenvalues. Otherwise, we will use the eigensystems of (ZT +1 + ZT +1 )/2 instead of ZT +1 . Note that (ZT +1 + ZT +1 )/2 is symmetric and always more close to K than ZT +1 , since 1 (ZT +1 + ZT +1 ) - K 2 F 1 1  ZT +1 - K F + ZT +1 - K 2 2 = ZT +1 - K F .











Algorithm 1 A Stochastic algorithm for Kernel PCA Input: The number of trials T , and the regularization parameter  1: Initialize Z1 = 0 2: for t = 1, 2, . . . , T do 3: Sample a random matrix t 4: t = 2/t 5: Zt+1 = Dt  [(1 - t )Zt + t t ] 6: end for 1 7: Calculate the nonzero eigensystems of 2 (ZT +1 + k ZT +1 ): {(ui , i )}i=1 8: return {(ui , i + )}k i=1







and 1n is a n-dimensional vector of all ones. If  is an unbiased estimate of K , then it is easy to verify  +  where 1 1 1 (1n  1n )1n 1n - 1n 1n  -  1n 1n 2 n n n is an unbiased estimate of K + . To find the top eigensystems of K + , we just need to replace the random matrix  in our algorithm with  +  and all the rest is the same. =







Implementation Issues



In this section, we discuss how to ensure all the iterates are represented in low-rank factorization form and how to accelerate the SVS operation by utilizing this fact. First, the random matrices t can always be represented by t = t t , where t , t  Rnxat are two rectangular matrices with at n. Now, suppose Zt is also represented by Zt = Ut Vt , where Ut , Vt  Rnxbt are two rectangular matrices with bt n.2 Then, Zt+1 = Dt  (1 - t )Ut Vt + t t t can be solved efficiently according to Lemma 3.4 of (Avron et al. 2012). Specifically, we introduce two matrices Xt , Yt  Rnx(at +bt ) such that  Xt =[ 1 - t Ut , t t ],  Yt =[ 1 - t Vt , t t ], and Zt+1 = Dt  [Xt Yt ]. Next, we perform a reduced QR decomposition (Golub and Van Loan 1996) of Xt = QX RX and Yt = QY RY , and find the SVD of RX RY = U V . Define Ut = QX U and Vt = QY V . It is easy to verify that Ut Vt is the SVD of Xt Yt , from which Zt+1 can be calculated trivially, and represented in the from of Zt+1 = Ut+1 Vt+1 . From the above discussion, it is clear that the space complexity is O(n(at + bt )) in each iteration. The running time is dominated by calculating t , which takes O(ndat ) time, and QR decompositions, which take O(n(at + bt )2 ) time. In summary, the time complexity is O(n[dat + (at + bt )2 ]). Thus, both space and time complexities are linear in n.







F







Finally, we return {(ui , i + )}k i=1 as the top eigensystems of K . The above procedure is summarized in Algorithm 1. Although we assume that data are centered in RKHS, our algorithm can be immediately extend to the general case. If data are uncentered, kernel PCA (Sch olkopf, Smola, and M uller 1998) needs the top eigensystems of K + , where = 1 1 1 (1 K 1n )1n 1n - 1n 1n K - K 1n 1n n2 n n n







Theoretical Guarantee



The following theorem shows that with a high probability, ZT +1 converges to K , the optimal solution to (3), at an O(1/T ) rate.



2







At least, we can represent Z1 in this form since Z1 = 0.







Theorem 1 Assume the Frobenius norm of the random matrix  is upper bounded by some constant C > 0. By setting t = 2/t, with a probability at least 1 -  , we have ZT +1 - K 



2 F







 8 2 log2 T C max rt + C 2 8 + 6 log T  t[T ] log log T =O T







The random matrix in SKPCA is constructed by random Fourier features (Rahimi and Recht 2008). The experiments are done on two benchmark data sets: Mushrooms (Chang and Lin 2011) and Magic (Frank and Asuncion 2010), which contain 8, 124 and 19, 020 examples, respectively. We choose those two medium-size data sets, because they can be handled by Baseline and thus allow us to compare different methods quantitatively. For all the experiments, we repeat them 10 times and report the averaged result.







where rt is the rank of Zt . Note that the O(1/T ) convergence rate matches the lowerbound of stochastic optimization of strongly convex functions (Agarwal et al. 2012). Our result differs from previous studies of SPGD (Rosasco, Villa, and V u 2014) in the sense that we prove a high probability bound instead of an expectation bound. Although a similar result has been proved for SGD (Rakhlin, Shamir, and Sridharan 2012), this is the first time such a guarantee is established for SPGD. The proof of this theorem relies on the recent analysis of SGD (Rakhlin, Shamir, and Sridharan 2012) and concentration inequalities (Bartlett, Bousquet, and Mendelson 2005; Cesa-Bianchi and Lugosi 2006). Due to space limitations, details are provided in the supplementary material.







Experimental Results



We first examine the convergence rate of SKPCA. We run SKPCA with four different combinations of the parameter  and the number of random Fourier components k . In Fig. 1(a), we report the normalized recover error Zt - 2 K 2 F /n with respect to the number of iterations t on the Mushrooms data set. For comparison, we also plot the curve of 0.03/t. From the similarity among those curves, we believe the proposed algorithm achieves the O(1/T ) rate. As can be seen, the two curves of k = 5 (or k = 50) almost overlap with each other. That is probably because on this data set  is not the dominating term in the upper bound given in Theorem 1. On the other hand, the convergence rate highly depends on the number of Fourier components k . The curves of k = 50 converge significantly faster than those of k = 5. The reason is that the larger k is, the closer  and K are, and the smaller the constant C in Theorem 1 is. Then, we check the rank of the intermediate iterate Zt , denoted by rank(Zt ), which determines the computational complexity of the t-th round. Fig. 1(b) plots rank(Zt ) as a function of t, which first increases and then converges to certain constant. The rank of the target matrix K is 158 when  = 1 and 55 when  = 10. As can be seen, rank(Zt ) is just a constant factor larger than rank(K ). To compare different methods, we use the top 50 eigensystems returned by each algorithm to construct a rank-50 approximator of K , denoted by K 50 , and report the approximation error K 50 - K F /n in Fig. 1(c). In order to fit the figure, the training time of Baseline was divided by 2. The result returned by Baseline is optimal, but it takes a longer time and a much larger memory. Although Nystr om is able to find a good solution, it cannot further reduce the approximation error. In comparison, SKPCA is able to refine its solution continuously and outperforms Nystr om after 10 seconds. Finally, we note that SKPCA is much faster than KHA. Experimental results on the Magic data set are provided in Fig. 2, which exhibits similar behaviors. On this data set, The rank of the K is 89 when  = 10 and 17 when  = 100. The training time of Baseline was divided by 20 in Fig. 2(c).







Experiments



In this section, we perform several experiments to examine the performance of our method.







Experimental Setting



We compare our stochastic algorithm for kernel PCA (SKPCA) with the following methods. 1. Baseline (Sch olkopf, Smola, and M uller 1998), which calculates the kernel matrix K explicitly and eigendecomposes it. 2. Approximation based on the Nystr om method (Drineas and Mahoney 2005; Zhang, Tsang, and Kwok 2008), which uses the Nystr om method to find a low-rank approximator of K , and eigendecomposes it. 3. Kernel Hebbian Algorithm (KHA) (Kim, Franz, and Sch olkopf 2005), which is an iterative approach for kernel PCA. In order to run SKPCA, we need to decide the value of the parameter  in (3), which in turn determines the number of eigenvectors used in kernel PCA. To minimize the generalization error, we would like to find a  such that eigenvalues of K that are smaller than it fall quickly (Shawe-Taylor et al. 2005). However, it is infeasible to calculate eigenvalues of K for large n, so we will use eigenvalues of a small kernel matrix K of m examples to estimate . Note that eigenvalues of K/n and K/m both converges to those of the integral operator (Braun 2006). Although the optimal step size of KHA in theory is 1/t, we found it led to very slow convergence, and thus set it to be 0.05 as suggested by (Kim, Franz, and Sch olkopf 2005). We choose the Gaussian kernel (xi , xj ) = exp( xi - xj 2 /(2 2 )), and set the kernel width  to the 20-th percentile of the pairwise distances (Mallapragada et al. 2009).







Conclusions



In this paper, we have formulated kernel PCA as a stochastic composite optimization problem with a nuclear norm regularizer, and then develop an iterative algorithm based on the stochastic proximal gradient descent algorithm. The main advantages of our method are i) both space and time complexes are linear in the number of samples; and ii) it is guaranteed to converge at an O(1/T ) rate, where T is the num-







4







x 10







-3







Zt - K







2







Rank(Zt )







200 150 100 50







= = = =







1, k = 5 1, k = 50 10, k = 5 10, k = 50







Approximation Error







3



2 2 F /n







= = = =



0 .03 t







1, k = 5 1, k = 50 10, k = 5 10, k = 50







0.06







300



0.05







250







0.04 0.03 0.02 0.01 0 0 10







Basline (1/2) Nystrom KHA SKPCA







1







0







50







100 t



2 2 F /n







150







200







50







100 t







150







200







20 30 40 50 Training Time (s)







60







70







(a) Zt - K







versus t







(b) rank(Zt ) versus t







(c) Approximation error for K







Figure 1: Experimental Results on the Mushrooms data set. To fit the figure, the training time of Baseline was divided by 2 in (c).



4 x 10



-3







2 2 F /n







150



= = = = 10, k = 5 10, k = 50 100, k = 5 100, k = 50







Approximation Error







3







= = = =



0 .05 t







10, k = 5 10, k = 50 100, k = 5 100, k = 50







200



0.05 0.04 0.03 0.02 0.01 0







Zt - K







2







Rank(Zt )







100







Basline (1/20) Nystrom KHA SKPCA







1







50







0







50







100 t



2 2 F /n







150







200







50







100 t







150







200







0







20







40 60 Training Time (s)







80







100







(a) Zt - K







versus t







(b) rank(Zt ) versus t







(c) Approximation error for K







Figure 2: Experimental Results on the Magic data set. To fit the figure, the training time of Baseline was divided by 20 in (c). ber of iterations. Experiments on two benchmark data sets illustrate the efficiency and effectiveness of the proposed method. scent for nuclear norm regularization. In Proceedings of the 29th International Conference on Machine Learning, 1231- 1238. Bartlett, P. L.; Bousquet, O.; and Mendelson, S. 2005. Local rademacher complexities. The Annals of Statistics 33(4):1497-1537. Brand, M. 2006. Fast low-rank modifications of the thin singular value decomposition. Linear Algebra and its Applications 415(1):20-30. Braun, M. L. 2006. Accurate error bounds for the eigenvalues of the kernel matrix. Journal of Machine Learning Research 7:2303-2328. Cai, J.-F.; Cand es, E. J.; and Shen, Z. 2010. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization 20(4):1956-1982. Cesa-Bianchi, N., and Lugosi, G. 2006. Prediction, Learning, and Games. Cambridge University Press. Chang, C.-C., and Lin, C.-J. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology 2(3):27:1-27:27. Chen, X.; Lin, Q.; and Pe na, J. 2012. Optimal regularized dual averaging methods for stochastic optimization. In Ad-







Acknowledgments



This research was supported by NSFC (61333014, 61321491), NSF (IIS-1463988, IIS-1545995), and MSRA collaborative research project (FY14-RES-OPP-110).







References



Achlioptas, D.; Mcsherry, F.; and Sch olkopf, B. 2002. Sampling techniques for kernel methods. In Advances in Neural Information Processing Systems 14, 335-342. Agarwal, A.; Bartlett, P. L.; Ravikumar, P.; and Wainwright, M. J. 2012. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory 58(5):3235-3249. Arora, R.; Cotter, A.; and Srebro, N. 2013. Stochastic optimization of pca with capped msg. In Advances in Neural Information Processing Systems 26, 1815-1823. Avron, H.; Kale, S.; Kasiviswanathan, S.; and Sindhwani, V. 2012. Efficient and practical stochastic subgradient de-







vances in Neural Information Processing Systems 25, 404- 412. Drineas, P., and Mahoney, M. W. 2005. On the nystr om method for approximating a gram matrix for improved kernel-based learning. Journal of Machine Learning Research 6:2153-2175. Duda, R. O.; Hart, P. E.; and Stork, D. G. 2000. Pattern Classification. Wiley-Interscience Publication. Frank, A., and Asuncion, A. 2010. UCI machine learning repository. Ghadimi, S., and Lan, G. 2012. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization 22(4):1469-1492. Golub, G. H., and Van Loan, C. F. 1996. Matrix computations, 3rd Edition. Johns Hopkins University Press. G unter, S.; Schraudolph, N. N.; and Vishwanathan, S. V. N. 2007. Fast iterative kernel principal component analysis. Journal of Machine Learning Research 8:1893-1918. Hazan, E., and Kale, S. 2011. Beyond the regret minimization barrier: an optimal algorithm for stochastic stronglyconvex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, 421-436. Honeine, P. 2012. Online kernel principal component analysis: A reduced-order model. IEEE Transactions on Pattern Analysis and Machine Intelligence 34(9):1814-1826. Kar, P., and Karnick, H. 2012. Random feature maps for dot product kernels. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics, 583-591. Kim, K. I.; Franz, M. O.; and Sch olkopf, B. 2005. Iterative kernel principal component analysis for image modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(9):1351-1366. Lan, G. 2012. An optimal method for stochastic composite optimization. Mathematical Programming 133:365-397. Lin, Q.; Chen, X.; and Pe na, J. 2014. A sparsity preserving stochastic gradient methods for sparse regression. Computational Optimization and Applications 58(2):455-482. Lopez-Paz, D.; Sra, S.; Smola, A. J.; Ghahramani, Z.; and Sch olkopf, B. 2014. Randomized nonlinear component analysis. In Proceedings of the 31st International Conference on Machine Learning. Mallapragada, P. K.; Jin, R.; Jain, A. K.; and Liu, Y. 2009. Semiboost: Boosting for semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 31(11):2000-2014. Nemirovski, A., and Yudin, D. B. 1983. Problem complexity and method efficiency in optimization. John Wiley & Sons Ltd. Nemirovski, A.; Juditsky, A.; Lan, G.; and Shapiro, A. 2009. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization 19(4):1574- 1609.







Nesterov, Y. 2013. Gradient methods for minimizing composite functions. Mathematical Programming 140(1):125- 161. Ouimet, M., and Bengio, Y. 2005. Greedy spectral embedding. In Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics, 253-260. Rahimi, A., and Recht, B. 2008. Random features for largescale kernel machines. In Advances in Neural Information Processing Systems 20, 1177-1184. Rakhlin, A.; Shamir, O.; and Sridharan, K. 2012. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, 449-456. Rosasco, L.; Villa, S.; and V u, B. C. 2014. Convergence of stochastic proximal gradient algorithm. ArXiv e-prints arXiv:1403.5074. Sanger, T. D. 1989. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks 2(6):459-473. Sch olkopf, B.; Smola, A.; and M uller, K.-R. 1998. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation 10(5):1299-1319. Shamir, O., and Zhang, T. 2012. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. ArXiv e-prints arXiv:1212.1824. Shawe-Taylor, J.; Williams, C. K. I.; Cristianini, N.; and Kandola, J. 2005. On the eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory 51(7):2510-2522. Tipping, M. E. 2001. Sparse kernel principal component analysis. In Advances in Neural Information Processing Systems 13, 633-639. Warmuth, M. K., and Kuzmin, D. 2008. Randomized online pca algorithms with regret bounds that are logarithmic in the dimension. Journal of Machine Learning Research 9:2287- 2320. Williams, C., and Seeger, M. 2001. Using the nystr om method to speed up kernel machines. In Advances in Neural Information Processing Systems 13, 682-688. Zhang, L.; Yang, T.; Jin, R.; and He, X. 2013. O(log T ) projections for stochastic optimization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning. Zhang, W.; Zhang, L.; Hu, Y.; Jin, R.; Cai, D.; and He, X. 2014. Sparse learning for stochastic composite optimization. In Proceedings of the 28th AAAI Conference on Artificial Intelligence, 893-899. Zhang, L.; Mahdavi, M.; and Jin, R. 2013. Linear convergence with condition number independent access of full gradients. In Advance in Neural Information Processing Systems 26, 980-988. Zhang, K.; Tsang, I. W.; and Kwok, J. T. 2008. Improved nystr om low-rank approximation and error analysis. In Proceedings of the 25th International Conference on Machine Learning, 1232-1239.







Deep Contextual Networks for Neuronal Structure Segmentation



Hao Chen ,  , Xiaojuan Qi , , Jie-Zhi Cheng , Pheng-Ann Heng , 











Department of Computer Science and Engineering, The Chinese University of Hong Kong  School of Medicine, Shenzhen University, China  Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China







Abstract



The goal of connectomics is to manifest the interconnections of neural system with the Electron Microscopy (EM) images. However, the formidable size of EM image data renders human annotation impractical, as it may take decades to fulfill the whole job. An alternative way to reconstruct the connectome can be attained with the computerized scheme that can automatically segment the neuronal structures. The segmentation of EM images is very challenging as the depicted structures can be very diverse. To address this difficult problem, a deep contextual network is proposed here by leveraging multi-level contextual information from the deep hierarchical structure to achieve better segmentation performance. To further improve the robustness against the vanishing gradients and strengthen the capability of the back-propagation of gradient flow, auxiliary classifiers are incorporated in the architecture of our deep neural network. It will be shown that our method can effectively parse the semantic meaning from the images with the underlying neural network and accurately delineate the structural boundaries with the reference of low-level contextual cues. Experimental results on the benchmark dataset of 2012 ISBI segmentation challenge of neuronal structures suggest that the proposed method can outperform the state-of-the-art methods by a large margin with respect to different evaluation measurements. Our method can potentially facilitate the automatic connectome analysis from EM images with less human intervention effort.







Introduction



In neuroscience, the neuronal circuit reconstruction, also termed as connectome, from biological images can manifest the interconnections of neurons for more insightful functional analysis of the brain and other nervous systems (Sporns, Tononi, and K otter 2005; Laptev et al. 2012). For instance, the 2D serial high resolution Electron Microscopy (EM) imaging is commonly used for the visualization of micro neural circuits and hence is a very informative imaging tool for the connectome analysis. In this paper, we focus



Authors contributed equally. Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.











on the widely used serial section Transmission Electron Microscopy (ssTEM) images for neuronal structure segmentation (Cardona et al. 2010). To illustrate the image complexity, a 2D example of original ssTEM image and corresponding segmentation by expert are illustrated in Figure 1. It can be found that the neuronal structures depicted in the ssTEM images are very complex and hence require the further segmentation of each structure to elucidate the interconnection relation. However, this is a non-trivial task. The ssTEM images can depict more than tens of thousands of neurons where each neuron may have thousands of synaptic connections. Thus, the size of ssTEM images is usually formidably large in a terabyte scale. Accordingly, the extremely complicated interconnections of neuronal structures and sheer image volume are far beyond the human capability for annotation, as the manual labeling of all neuronal structures may take decades to finish (White et al. 1986; Bock et al. 2011; Wu 2015). In this case, automatic segmentation methods are highly demanded to assist the parsing of the ssTEM images into concrete neurological structures for further analysis (Seung 2011). However, as can be observed in Figure 1, the segmentation problem for the neuronal structures can be very challenging in threefold. First, the image deformation during the acquisition may blur the membrane boundaries between neighboring neurons as shown in Figure 1 (left). Second, the variation of neuron membrane in terms of image contrast and membranal thickness can be very large. Particularly for the thickness, it can range from solid dark curves to grazed grey swaths (Jurrus et al. 2010). Third, the presence of intracellular structures makes edge detection and region growing based methods ineffective for the identification of neuron membrane. Some confounding micro-structures may also mislead the merging of regions or incorrect splitting of one region into several sections. Meanwhile, the imaging artifacts and image alignment errors can impose difficulties on the design of effective segmentation algorithm as well.







Related Work



Because of the anisotropic nature of ssTEM data, most previous methods were devised under the framework of initial 2D membrane detection and latter 3D linking process (Jurrus et al. 2010). Although considerable progress has been witnessed over the last decade, earlier studies achieved an in-







Figure 1: Left: the orginal ssTEM image. Right: the corresponding segmentation annotation (individual components are denoted by different colors).







ferior accuracy of segmentation and often failed to suppress the intracellular structures effectively by utilizing the handcrafted features, e.g., radon and ray-like features (Kumar, V azquez-Reina, and Pfister 2010; Mishchenko 2009; Laptev et al. 2012; Kaynig, Fuchs, and Buhmann 2010). Recently, deep neural networks with hierarchical feature representations have achieved promising results in various applications, including image classification (Krizhevsky, Sutskever, and Hinton 2012), object detection (Simonyan and Zisserman 2014; Chen et al. 2015) and segmentation (Long, Shelhamer, and Darrell 2014). In terms of EM segmentation, Ciresan et al. (2012) employed the deep convolutional neural network as a pixel-wise classifier by taking a square window centered on the pixel itself as input, which contains contextual appearance information. This method achieved the best performance in 2012 ISBI neuronal structure segmentation challenge. A variant version with iterative refining process has been proposed to withstand the noise and recover the boundaries (Wu 2015). Besides, several methods worked on the probability maps produced by deep convolutional neural networks as a post-processing step, such as learning based adaptive watershed (Uzunbas , Chen, and Metaxsas 2014), hierarchical merge tree with consistent constraints (Liu et al. 2014) and active learning approach for hierarchical agglomerative segmentation (Nunez-Iglesias et al. 2013), to further improve the performance. These methods refined the segmentation results with respect to the measurements of rand error and warping error (Jain et al. 2010) with significant performance boost in comparison to the results of (Ciresan et al. 2012). However, the performance gap between the computerized results and human neuroanatomist annotations can be still perceivable. There are two main drawbacks of previous deep learning based studies on this task. First, the operation of sliding window scanning imposes a heavy burden on the computational efficiency. This must be taken into consideration seriously regarding the large scale neuronal structure reconstruction. Second, the size of neuronal structure can be very diverse in EM images. Although, classification with single size sub-window can achieve good performance, it may produce unsatisfactory results in some regions where the size of contextual window is set inappropriately.







In order to tackle the aforementioned challenges, we propose a novel deep contextual segmentation network to demarcate the neuronal structure in EM stacks. This approach incorporates the multi-level contextual information with different receptive fields, thus it can remove the ambiguities of membranal boundaries in essence that previous studies may fail. Inspired by previous studies (Ciresan et al. 2012; Lee et al. 2014), we further make the model deeper than (Ciresan et al. 2012) and add auxiliary supervised classifiers to encourage the back-propagation flow. This augmented network can further unleash the power of deep neural networks for neuronal structure segmentation. Quantitative evaluation was extensively conducted on the public dataset of 2012 ISBI EM Segmentation Challenge (Ignacio et al. 2012), with rich baseline results for comparison in terms of pixel- and object-level evaluation. Our method set the state-of-the-art record, which outperformed those of other methods on all evaluation measurements. It is also worth noting that our results surpassed the annotation by neuroanatomists on the measurement of warping error.







Method



Deeply Supervised Contextual Network



In this section, we present a deeply supervised contextual network for neuronal structure segmentation. Inspired by recent studies of fully convolutional networks (FCN) (Long, Shelhamer, and Darrell 2014; Chen et al. 2014), which replace the fully connected layers with all convolutional kernels, the proposed network is a variant and takes full advantage of convolutional kernels for efficient and effective image segmentation. The architecture of the proposed method is illustrated in Figure 2. It basically contains two modules, i.e., down-sampling path with convolutional and maxpooling layers and upsampling path with convolutional and deconvolutional layers. Noting that we upsampled the feature maps with the backwards strided convolution in the upsampling path, thus we call them as deconvolutional layers. The downsampling path aims at classifying the semantical meanings based on the high level abstract information, while the upsampling path reconstructing the fine details such as boundaries. The upsampling layers are designed by taking full advantage of the different feature maps in hierarchical layers. The intuition behind this is that global or abstract information from higher layers helps to resolve the problem of what (i.e., classification capability) and local information from lower layers helps to resolve the problem of where (i.e., localization accuracy). Finally, these multi-level contextual information are fused together with a summing operation. The probability maps are generated by inputting the fused map into a softmax classification layer. Specifically, the architecture of neural network contains 16 convolutional layers, 3 max-pooling layers for downsampling and 3 deconvolutional layers for upsampling. The convolutional layers along with convolutional kernels (3 x 3 or 1 x 1) perform linear mapping with shared parameters. The max-pooling layers downsample the size of feature maps by the max-pooling operation (kernel size 2 x 2 with a







3







64







64 max-pool 2x2 conv 3x3 64 128 128 128 256 256 256 256 512 512 512 up-conv classifier conv 1x1 fusion







Input







1202







1202







1202







1202







602







602







602







480x480







480x480







480x480







2402







2402







4802







2402







4802







4802



4802 4802







4802



4802 C2







4802



4802 C3



Softmax







C1







Figure 2: The architecture of the proposed deep contextual network. stride 2). The deconvolutional layers upsample the size of feature maps by the backwards strided convolution (Long, Shelhamer, and Darrell 2014) (2k x 2k kernel with a stride k , k = 2, 4 and 8 for upsampling layers, respectively). A non-linear mapping layer (element-wise rectified linear activations) is followed for each layer that contains parameters to be trained (Krizhevsky, Sutskever, and Hinton 2012). In order to alleviate the problem of vanishing gradients and encourage the back-propagation of gradient flow in deep neural networks, the auxiliary classifiers C are injected for training the network. Furthermore, they can serve as regularization for reducing the overfitting and improve the discriminative capability of features in intermediate layers (Bengio et al. 2007; Lee et al. 2014; Wang et al. 2015). The classification layer after fusing multi-level contextual information produces the EM image segmentation results by leveraging the hierarchical feature representations. Finally, the training of whole network is formulated as a per-pixel classification problem with respect to the ground-truth segmentation masks, as shown following: L(X ; ) =  ( 2



2 ||Wc ||2 2 + ||W ||2 )- c







  







4802







weight. Finally, the parameters  = {W, Wc } of deep contextual network are jointly optimized in an end-to-end way by minimizing the total loss function L. For the testing data of EM images, the results are produced with an overlap-tile strategy to improve the robustness.







Importance of Receptive Field



In the task of EM image segmentation, there is a large variation on the size of neuronal structures. Therefore, the size of receptive field plays a key role in the pixel-wise classification given the corresponding contextual information. It's approximated as the size of object region with surrounding context, which is reflected as the intensity values within the window. As shown in Figure 3, different regions may depend on a different window size. For example, the cluttered neurons need a small window size for clearly separating the membranes between neighboring neurons, while a large size is required for neurons containing intracellular structures so as to suppress the false predictions. In the hierarchical structure of deep contextual networks, these upsampling layers have different receptive fields. With the depth increasing, the size of receptive field is becoming larger. Therefore, it can handle the variations of reception field size properly that different regions demand for correct segmentation while taking advantage of the hierarchical feature representations.







wc c (x, (x)) -



c xX xX







 (x, (x))







(1)







where the first part is the regularization term and latter one including target and auxiliary classifiers is the data loss term. The tradeoff of these two terms is controlled by the hyperparameter . Specifically, W denotes the parameters for inferring the target output p(x; W ),  (x, (x)) denotes the cross entropy loss regarding the true label (x) for pixel x in image space X , similarly c (x, (x)) is the loss from cth auxiliary classifier with parameters Wc for inferring the output, the parameter wc denotes the corresponding discount







Morphological Boundary Refinement



Although the probability maps output from the deep contextual network are visually very good, we observe that the membrane of ambiguous regions can sometimes be discontinued. This arises partially from the averaging effect of probability maps, which are generated by several trained models. Therefore, we utilized an off-the-shelf watershed algorithm (Beucher and Lantuejoul 1979) to complete the







602







2 2 2







2 2 2







2 2 2







dataset took about three hours using a standard PC with a 2.50 GHz Intel(R) Xeon(R) E5-1620 CPU and a NVIDIA GeForce GTX Titan X GPU.







Qualitative Evaluation



Two examples of qualitative segmentation results without morphological boundary refinement can be seen in Figure 4. We can see that our method can generate visually smooth and accurate segmentation results. As the red arrows shown in the figure, it can successfully suppress the intracellular structures and produce good probability maps that classify the membrane and non-membrane correctly. Furthermore, by utilizing multi-level representations of contextual information, our method can also close gaps (contour completion as the blue arrows shown in Figure 4) in places where the contrast of membrane is low. Although there still exist ambiguous regions which are even hard for human experts, the results of our method are more accurate in comparison to those generated from previous deep learning studies (Stollenga et al. 2015; Ciresan et al. 2012). This evidenced the efficacy of our proposed method qualitatively.







Figure 3: Illustration of contextual window size. Left: the original ssTEM image. Right: manual segmentation result by an expert human neuroanatomist (black and white pixels denote the membrane and non-membrane, respectively). contour. The final fusion result pf (x) was produced by fusing the binary contour pw (x) and original probability map p(x) with linear combination: pf (x) = wf p(x) + (1 - wf )pw (x) (2)







The parameter wf is determined by obtaining the optimal result of rand error on the training data in our experiments.







Quantitative Evaluation and Comparison



In the 2012 ISBI EM Segmentation Challenge, the performance of different competing methods is ranked based on their pixel and object classification accuracy. Specifically, the 2D topology-based segmentation evaluation metrics include rand error, warping error and pixel error (Ignacio et al. 2012; Jain et al. 2010), which are defined as following: Rand error: 1 - the maximal F-score of the foregroundrestricted rand index (Rand 1971), a measure of similarity between two clusters or segmentations. For the EM segmentation evaluation, the zero component of the original labels (background pixels of the ground truth) is excluded. Warping error: a segmentation metric that penalizes the topological disagreements (object splits and mergers). Pixel error: 1 - the maximal F-score of pixel similarity, or squared Euclidean distance between the original and the result labels. The evaluation system thresholds the probability maps with 9 different values (0.1-0.9 with an interval 0.1) separately and return the minimum error for each segmentation metric. The quantitative comparison of different methods can be seen in Table 1. Noting that the results show the best performance for each measurement across all submissions by each team individually. More details and results are available at the leader board1 . We compared our method with the state-of-the-art methods with or without post-processing separately. Furthermore, we conducted extensive experiments with ablation studies to probe the performance gain in our method and detail as following. Results Comparison without Post-Processing Preliminary encouraging results were achieved by IDSIA team (Ciresan et al. 2012), which utilized a deep convolutional neural network as a pixel-wise classifier in a sliding



Please refer to the leader board for more details: http://brainiac2.mit.edu/isbi_challenge/ leaders-board



1







Experiments and Results



Data and Preprocessing



We evaluated our method on the public dataset of 2012 ISBI EM Segmentation Challenge (Ignacio et al. 2012), which is still open for submissions. The training dataset contains a stack of 30 slices from a ssTEM dataset of the Drosophila first instar larva ventral nerve cord (VNC), which measures approximately 2x2x1.5 microns with a resolution of 4x4x50 nm/voxel. The images were manually annotated in the pixellevel by a human neuroanatomist using the software tool TrakEm2 (Cardona et al. 2012). The ground truth masks of training data were provided while those of testing data with 30 slices were held out by the organizers for evaluation. We evaluated the performance of our method by submitting results to the online testing system. In order to improve the robustness of neural network, we utilized the strategy of data augmentation to enlarge the training dataset (about 10 times larger). The transformations of data augmentation include scaling, rotation, flipping, mirroring and elastic distortion.







Details of Training



The proposed method was implemented with the mixed programming technology of Matlab and C++ under the opensource framework of Caffe library (Jia et al. 2014). We randomly cropped a region (size 480 x 480) from the original image as the input into the network and trained it with standard back-propagation using stochastic gradient descent (momentum = 0.9, weight decay = 0.0005, the learning rate was set as 0.01 initially and decreased by a factor of 10 every two thousand iterations). The parameter of corresponding discount weight wc was set as 1 initially and decreased by a factor of 10 every ten thousand iterations till a negligible value 0.01. The training time on the augmentation







Slice #06







Slice #13







Figure 4: Examples of original EM images and segmentation results by our method (the darker color of pixels denotes the higher probability of being membrane in neuronal structure). Table 1: Results of 2012 ISBI Segmentation Challenge on Neuronal Structures



Group name ** human values ** CUMedVision (Our) DIVE-SCI IDSIA-SCI optree-idsia (Uzunbas , Chen, and Metaxsas 2014) motif (Wu 2015) SCI (Liu et al. 2014) Image Analysis Lab Freiburg (Ronneberger, Fischer, and Brox 2015) Connectome PyraMiD-LSTM (Stollenga et al. 2015) DIVE IDSIA (Ciresan et al. 2012) INI MLL-ETH (Laptev et al. 2012) CUMedVision-4(C3) CUMedVision-4(C2) CUMedVision-4(C1) CUMedVision-4(with C) CUMedVision-4(w/o C) CUMedVision-6(with C) CUMedVision-4(with fusion) Rand Error 0.002109173 0.017334163 0.017841947 0.018919792 0.022777620 0.026326384 0.028054308 0.038225781 0.045905709 0.046704591 0.047680695 0.048314096 0.060110507 0.063919883 Warping Error 0.000005341 0.000000000 0.000307083 0.000616837 0.000807953 0.000426483 0.000515747 0.000352859 0.000478999 0.000462341 0.000374222 0.000434367 0.000495529 0.000581741 Pixel Error 0.001041591 0.057953485 0.058436986 0.102692786 0.110460288 0.062739851 0.063349324 0.061141279 0.062029263 0.061624006 0.058205303 0.060298549 0.068537199 0.079403258 0.060940140 0.061248112 0.102325669 0.058372960 0.062864362 0.059902422 0.057953485 Rank 1 2 3 4 5 6 7 8 9 10 11 12 13







0.043419035 0.000342178 0.046058434 0.000421524 0.258966855 0.001080322 0.035134666 0.000334167 0.040492503 0.000330353 0.040406591 0.000000000 0.017334163 0.000188446 There are total 38 teams participating this challenge till Sep 2015.







window way. The best results were obtained by averaging the outputs from 4 deep neural network models. Different from this method by training the neural network with different window sizes (65 and 95) separately, our approach integrates multi-size windows (i.e., different receptive fields in upsampling layers) into one unified framework. This can help to generate more accurate probability maps by leveraging multi-level contextual information. The Image Analysis Lab Freiburg team (Ronneberger, Fischer, and Brox 2015) designed a deep U-shaped network by concatenating features from lower layers and improved the results than those of (Ciresan et al. 2012). This further demonstrated the effectiveness of contextual information for accurate segmentation. However, with such a deep network (i.e., 23 convolutional layers), the back-propagation of gradient flow may be a potential issue and training took a long time (about 10







hours). Instead of using the convolutional neural network, the PyraMiD-LSTM team employed a novel parallel multidimensional long short-term memory model for fast volumetric segmentation (Stollenga et al. 2015). Unfortunately, a relatively inferior performance was achieved by this method. From Table 1, we can see that our deep segmentation network (with 6 model averaging results, i.e., CUMedVision6(with C)) without watershed fusion achieved the best performance in terms of warping error, which outperformed other methods by a large margin. Notably it's the only result that surpasses the performance of expert neuroanatomist annotation. Our submitted entry CUMedVision-4(with C) on averaging 4 models (the same number of models as (Ciresan et al. 2012)) achieved much smaller rand and warping errors than the results of other teams also employing deep learning methods without sophisticated post-processing steps, such







as DIVE, IDSIA, and Image Analysis Lab Freiburg. This corroborates the superiority of our approach by exploring multilevel contextual information with auxiliary supervision. Results Comparison with Post-Processing In order to further reduce the errors, we fused the results from watershed method as illustrated in the method section, which can reduce the rand error dramatically while increasing the warping error unfortunately. This is reasonable since these two errors consider the segmentation evaluation metric from different aspects. The former one could penalize even slightly misplaced boundaries while the latter one disregards non-topological errors. Different from our simple postprocessing step, the SCI team post-processed the probability maps generated by the team DIVE and IDSIA with a sophisticated post-processing strategy (Liu et al. 2014). The post-processed results were evaluated under the team name of DIVE-SCI and IDSIA-SCI, respectively. Although it utilized a supervised way with hierarchical merge tree to achieve structure consistency, the performance is relatively inferior compared to ours, in which only an unsupervised watershed method was used for post-processing. In addition, our method also outperformed other methods with sophisticated post-processing techniques including optreeidsia and motif by a large margin. This further highlights the advantages of our method by exploring multi-level contextual information to generate probability maps with better likelihood. We released the probability maps including training and testing data of our method for enlightening further sophisticated post-processing strategies2 . Ablation Studies of Our Method In order to probe the performance gain of our proposed method, extensive ablation studies were conducted to investigate the role of each component. As illustrated in Table 1, compared with methods using single contextual information including CUMedVision-4(C3/C2/C1), the deep contextual model harnessing the multi-level contextual cues achieved significantly better performance on all the measurements. Furthermore, we compared the performance with (CUMedVision-4(with C)) and without (CUMedVision-4(w/o C)) the injection of auxiliary classifiers C , the rand error and pixel error from method with C were much smaller while the warping error with C is competitive compared to the method without C . This validated the efficacy of auxiliary classifiers with deep supervision for encouraging back-propagation of gradient flow. By fusing the results from the watershed method, we achieved the result with rand error 0.017334, warping error 0.000188, and pixel error 0.057953, which outperforms those from other teams by a large margin. To sum up, our method achieved the best performance on different evaluation measurements, which demonstrates the promising possibility for read-world applications. Although there is a tradeoff with respect to different evaluation metrics, the neuroanatomists can choose the desirable results based on the specific neurological requirements.



2 Results: http://appsrv.cse.cuhk.edu.hk\ %7Ehchen/research/2012isbi_seg.html







Computation Time Generally, it took about 0.4 seconds to process one test image with size 512 x 512 using the same configuration of training. Taking advantage of fully convolutional networks, the computation time is much less than previous studies (Ciresan et al. 2012; Wu 2015) utilizing a sliding window way, which caused a large number of redundant computations on neighboring pixels. With new imaging techniques producing much larger volumes (terabyte scale) that contain thousands of neurons and millions of synapses, the automatic methods with accurate and fast segmentation capabilities are of paramount importance. The fast speed and better accuracy of our method make it possible for large scale image analysis.







Conclusion



In this paper we have presented a deeply supervised contextual neural network for neuronal structure segmentation. By harnessing the multi-level contextual information from the deep hierarchical feature representations, it can have better discrimination and localization abilities, which are key to image segmentation related tasks. The injected auxiliary classifiers can help to encourage the back-propagation of gradient flow in training the deep neural network, thus further improve the segmentation performance. Extensive experiments on the public dataset of 2012 ISBI EM Segmentation Challenge corroborated the effectiveness of our method. We believe the promising results are a significant step towards automated reconstruction of the connectome. In addition, our approach is general and can be easily extended to other biomedical applications. Future work will include further refining the segmentation results with other sophisticated post-processing techniques (Uzunbas , Chen, and Metaxsas 2014; Liu et al. 2014; Nunez-Iglesias et al. 2013) and investigating on more biomedical applications. Acknowledgements This work is supported by National Basic Research Program of China, 973 Program (No. 2015CB351706) and a grant from Ministry of Science and Technology of the People's Republic of China under the Singapore-China 9th Joint Research Program (No. 2013DFG12900). The authors also gratefully thank the challenge organizers for helping the evaluation.







References



Bengio, Y.; Lamblin, P.; Popovici, D.; Larochelle, H.; et al. 2007. Greedy layer-wise training of deep networks. Advances in neural information processing systems 19:153. Beucher, S., and Lantuejoul, C. 1979. Use of watersheds in contour detection. In International Conference on Image Processing. Bock, D. D.; Lee, W.-C. A.; Kerlin, A. M.; Andermann, M. L.; Hood, G.; Wetzel, A. W.; Yurgenson, S.; Soucy, E. R.; Kim, H. S.; and Reid, R. C. 2011. Network anatomy and in vivo physiology of visual cortical neurons. Nature 471(7337):177-182. Cardona, A.; Saalfeld, S.; Preibisch, S.; Schmid, B.; Cheng, A.; Pulokas, J.; Tomancak, P.; and Hartenstein, V. 2010. An integrated micro-and macroarchitectural analysis of the







drosophila brain by computer-assisted serial section electron microscopy. PLoS biology 8(10):2564. Cardona, A.; Saalfeld, S.; Schindelin, J.; Arganda-Carreras, I.; Preibisch, S.; Longair, M.; Tomancak, P.; Hartenstein, V.; and Douglas, R. J. 2012. Trakem2 software for neural circuit reconstruction. PloS one 7(6):e38011. Chen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and Yuille, A. L. 2014. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062. Chen, H.; Shen, C.; Qin, J.; Ni, D.; Shi, L.; Cheng, J. C.; and Heng, P.-A. 2015. Automatic localization and identification of vertebrae in spine ct via a joint learning model with deep neural networks. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015. Springer. 515-522. Ciresan, D.; Giusti, A.; Gambardella, L. M.; and Schmidhuber, J. 2012. Deep neural networks segment neuronal membranes in electron microscopy images. In Advances in neural information processing systems, 2843-2851. Ignacio, A.-C.; Sebastian, S.; Albert, C.; and Johannes, S. 2012. 2012 ISBI Challenge: Segmentation of neuronal structures in EM stacks. http://brainiac2.mit.edu/ isbi_challenge/. Jain, V.; Bollmann, B.; Richardson, M.; Berger, D. R.; Helmstaedter, M. N.; Briggman, K. L.; Denk, W.; Bowden, J. B.; Mendenhall, J. M.; Abraham, W. C.; et al. 2010. Boundary learning by optimization with topological constraints. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, 2488-2495. IEEE. Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.; Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093. Jurrus, E.; Paiva, A. R.; Watanabe, S.; Anderson, J. R.; Jones, B. W.; Whitaker, R. T.; Jorgensen, E. M.; Marc, R. E.; and Tasdizen, T. 2010. Detection of neuron membranes in electron microscopy images using a serial neural network architecture. Medical image analysis 14(6):770-783. Kaynig, V.; Fuchs, T. J.; and Buhmann, J. M. 2010. Geometrical consistent 3d tracing of neuronal processes in sstem data. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2010. Springer. 209-216. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097-1105. Kumar, R.; V azquez-Reina, A.; and Pfister, H. 2010. Radonlike features and their application to connectomics. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on, 186-193. IEEE. Laptev, D.; Vezhnevets, A.; Dwivedi, S.; and Buhmann, J. M. 2012. Anisotropic sstem image segmentation using dense correspondence across sections. In Medical Image







Computing and Computer-Assisted Intervention-MICCAI 2012. Springer. 323-330. Lee, C.-Y.; Xie, S.; Gallagher, P.; Zhang, Z.; and Tu, Z. 2014. Deeply-supervised nets. arXiv preprint arXiv:1409.5185. Liu, T.; Jones, C.; Seyedhosseini, M.; and Tasdizen, T. 2014. A modular hierarchical approach to 3d electron microscopy image segmentation. Journal of neuroscience methods 226:88-102. Long, J.; Shelhamer, E.; and Darrell, T. 2014. Fully convolutional networks for semantic segmentation. arXiv preprint arXiv:1411.4038. Mishchenko, Y. 2009. Automation of 3d reconstruction of neural tissue from large volume of conventional serial section transmission electron micrographs. Journal of neuroscience methods 176(2):276-289. Nunez-Iglesias, J.; Kennedy, R.; Parag, T.; Shi, J.; Chklovskii, D. B.; and Zuo, X.-N. 2013. Machine learning of hierarchical clustering to segment 2d and 3d images. PloS one 8(8):08. Rand, W. M. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association 66(336):846-850. Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Convolutional networks for biomedical image segmentation. arXiv preprint arXiv:1505.04597. Seung, H. S. 2011. Neuroscience: towards functional connectomics. Nature 471(7337):170-172. Simonyan, K., and Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Sporns, O.; Tononi, G.; and K otter, R. 2005. The human connectome: a structural description of the human brain. PLoS Comput Biol 1(4):e42. Stollenga, M. F.; Byeon, W.; Liwicki, M.; and Schmidhuber, J. 2015. Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation. arXiv preprint arXiv:1506.07452. Uzunbas , M. G.; Chen, C.; and Metaxsas, D. 2014. Optree: a learning-based adaptive watershed algorithm for neuron segmentation. In Medical Image Computing and ComputerAssisted Intervention-MICCAI 2014. Springer. 97-105. Wang, L.; Lee, C.-Y.; Tu, Z.; and Lazebnik, S. 2015. Training deeper convolutional networks with deep supervision. arXiv preprint arXiv:1505.02496. White, J.; Southgate, E.; Thomson, J.; and Brenner, S. 1986. The structure of the nervous system of the nematode caenorhabditis elegans: the mind of a worm. Phil. Trans. R. Soc. Lond 314:1-340. Wu, X. 2015. An iterative convolutional neural network algorithm improves electron microscopy image segmentation. arXiv preprint arXiv:1506.05849.







Journal of Biogeography (J. Biogeogr.) (2006) 33, 2120-2135







ORIGINAL ARTICLE







Recent advance of white spruce (Picea glauca) in the coastal tundra of the  bec, eastern shore of Hudson Bay (Que Canada)



Marco Caccianiga* and Serge Payette







NSERC Northern Research Chair, Centre tudes nordiques, Universite  Laval, Que bec d'e City, Canada G1K 7P4







ABSTRACT







Aim The species-specific response of tree-line species to climatic forcing is a crucial topic in modelling climate-driven ecosystem dynamics. In northern  bec, Canada, black spruce (Picea mariana) is the dominant species at the tree Que line, but white spruce (Picea glauca) also occurs along the maritime coast of Hudson Bay, and is expanding along the coast and on lands that have recently emerged because of isostatic uplift. Here we outline the present distribution, structure, dynamics and recent spread of white spruce from the tree line up to its northernmost position in the shrub tundra along the Hudson Bay coast. We aimed to obtain a minimum date of the arrival of the species in the area and to evaluate its dynamics relative to recent climate changes. Location White spruce populations and individuals were sampled along a latitudinal transect from the tree line to the northernmost individual in the shrub tundra along the Hudson Bay coast and in the Nastapoka archipelago in northern  bec and Nunavut, Canada (5606-5632 N). Que Methods White spruce populations were mapped, and the position, dimension, growth form and origin (seed or layering) of every individual recorded. Tree-ring analyses of living and dead trees allowed an estimation of the population structure, past recruitment, growth trends and growth rate of the species. A macrofossil analysis was performed of the organic horizon of the northernmost white spruce stands and individuals. Radiocarbon dates of white spruce remains and organic matter were obtained. The rate of isostatic uplift was assessed by radiocarbon dating of drifted wood fragments. Results The first recorded establishment of white spruce was almost synchronous at all sites and occurred around ad 1660. Spruce recruitment was rather continuous at the tree line, while it showed a gap in the northern shrub tundra during the first decades of the 19th century. A vigorous, recent establishment of seedlings was observed in the shrub tundra; only wind-exposed, low krummholz (stunted individuals) did not show any sexual regeneration. A period of suppressed growth occurred from the 1810s to the 1850s in most sites. A growth increase was evident from the second half of the 19th century and peaked in the 1880s and the 20th century. A shift from stunted to tree growth form has occurred since the mid19th century. No sample associated with white spruce remains gave a date older than 300 14C years bp [calibrated age (cal.) ad 1430-1690]. Main conclusions White spruce probably arrived recently in the coastal tundra of Hudson Bay due to a delayed post-glacial spread. The arrival of the species probably occurred during the Little Ice Age. The established individuals survived by layering during unfavourable periods, but acted as nuclei for sexual recruitment almost continuously, except in the northernmost and most exposed sites. Warmer periods were marked by strong seedling recruitment and







*Correspondence: Marco Caccianiga, Dipartimento di Biologia, Sezione Botanica  di Milano, Via Celoria Sistematica, Universita 26, 20133 Milano, Italy. E-mail: marco.caccianiga@cen.ulaval.ca Present address: Dipartimento di Biologia,  di Sezione Botanica Sistematica, Universita Milano, Via Celoria 26, 20133 Milano, Italy.







2120







www.blackwellpublishing.com/jbi doi:10.1111/j.1365-2699.2006.01563.x







 2006 The Authors Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra







a shift to tree growth form. Unlike white spruce, black spruce showed no evidence of an ongoing change in growth form and sexual recruitment. Ecological requirements and recent history of tree-line species should be taken into account in order to understand the present dynamics of high-latitude ecosystems. Keywords Forest tundra, isostatic uplift, macrofossil analysis, Picea glauca, shrub tundra,  bec, tree colonization, tree line, tree-ring analysis, white spruce. subarctic Que







INTRODUCTION The position of the arctic tree line and its ecological context has always been considered a major indicator of past and present climatic conditions (Lamb, 1985; Cwynar & Spear, 1991; Payette & Lavoie, 1994; Gamache & Payette, 2005; Lloyd, 2005). The species-specific response of tree-line species to climatic forcing is a crucial topic in modelling the pathways of ecosystem dynamics linked to changing climatic conditions, and has sometimes been overlooked. Different ecological requirements, life-history traits and recent history may induce divergent responses in closely related species living in the same ecological context, such as the two dominant boreal spruce species in North America, white spruce, Picea glauca (Moench) Voss, and black spruce, Picea mariana (Mill.) B.S.P. In eastern Canada, white spruce is particularly abundant along the maritime coasts, while black spruce is the dominant species inland and reaches the northernmost latitudes (Payette, 1993). This pattern may be due to a higher tolerance by white spruce to humid weather conditions, and to a lower recurrence  gin, 1999). of fire on the coast (Payette, 1993; Ricard & Be White spruce reaches the tree line (the limit of normal spruce trees generally  2.5 m tall) in two different areas, along the Hudson Bay coast and in northern Labrador. This distribution is probably associated with post-glacial spreading around the slowly decaying ice sheet, which may still be in progress (Richard et al., 1982; Ritchie & MacDonald, 1986; Dyke & Prest, 1987; Ritchie, 1987). In northern Labrador, white spruce reached its northern limit between 4500 and 3800 bp (Payette, 1993) with a dispersal route located mainly along the coast (Ritchie & MacDonald, 1986; Payette, 1993). No data are available about the date of arrival of the species along the eastern Hudson Bay coast: this population is probably the result of an independent dispersal route of problematic origin (Gajewski & Garralla, 1992; Payette, 1993; Gajewski et al., 1996). In this area, white spruce responded to recent climate warming and fire occurrence with an increase in population density, rather than a shift in the altitudinal tree limit (Payette & Filion, 1985). A similar pattern has been described in central and western Canada (Scott et al., 1987; Szeicz & MacDonald, 1995); in Alaska this pattern was often accompanied by a forest-to-tundra shift in both altitude and latitude (Suarez et al., 1999; Lloyd & Fastie, 2002, 2003; Lloyd, 2005).







Contrasting patterns of white spruce dynamics have been observed in interior and coastal northern Labrador (S.P., unpublished data). White spruce is expanding along the maritime coasts (Payette, 1993), but no progression of the species inland has been observed so far. The species is currently colonizing lands  gin et al., 1993; recently emerged because of isostatic uplift (Be  gin, 1999), although with several local regeneration Ricard & Be failures due to unfavourable soil conditions (Marr, 1948). In many forest-tundra sites, white spruce shows normal tree growth and is surrounded by stunted black spruce clones (Payette & Filion, 1985). At its northern limit of distribution across North America, white spruce shows a contrasting response to warmer temperatures, as its establishment may be limited by both the negative influence of lichens on seed regeneration (Scott et al., 1987; Scott & Hansell, 2002); and drought stress induced by warmer temperatures, as reported in Alaska (Barber et al., 2000; Lloyd & Fastie, 2003). Whereas it has been shown that white spruce density increased markedly south of the tree line during the 20th century (Payette & Filion, 1985), it is not known when the species arrived and expanded in the region corresponding to the area between its current position at the tree line and the northernmost position of the species in the coastal tundra. The main goals of this paper are to outline the present distribution, structure, dynamics and recent spread of white spruce from the tree line up to its northernmost position in the shrub tundra along the Hudson Bay coast. In doing so, it is possible to obtain a minimum date of the arrival of the species in the area, and to evaluate its dynamics relative to recent changes in climate. To fulfil our goals we have used a retrospective approach based on tree-ring analysis of living, dead and subfossil trees, supplemented by a macrofossil analysis of the organic horizon of the northernmost white spruce stands and individuals, both at the tree line and in the shrub tundra. METHODS Study area The study was carried out along the east shore of Hudson Bay  langer islands, two islands belonging to the and on Ross and Be  bec and Nunavut Nastapoka Archipelago, in northern Que 2121







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







M. Caccianiga and S. Payette







Inukjuak



58N







TUNDRA







T+FT



HUDSON BAY



Umiujaq



56







FOREST-TUNDRA







Kuujjuarapik







LICHEN WOODLAND



JAMES BAY







CLOSED FOREST







Figure 1 Location of the study area and the main ecological domains. Dashed line, northern range limit of white spruce; dotted line, northern range limit of black spruce (both from Payette, 1993).







(Canada), between 5606 and 5632 N (Fig. 1). The area is part of the Canadian Precambrian Shield, with uniform, lowelevated morphology. Hudson Bay coast is subjected to a fast isostatic rebound of c. 1-1.3 m per century (Allard & Tremblay, 1983). The study area corresponds to the northern limit of the forest-tundra and the adjacent treeless tundra populated by scattered white spruce individuals and black spruce krummholz. The forest-tundra consists of small forest stands in the lowlands and on protected slopes, and extensive tundra on the hills. The Hudson Bay coast belongs to the oceanic domain of the forest-tundra characterized by the occurrence of white spruce (Payette, 1983, 1993). The other tree species occurring in the study area is black spruce, which becomes dominant in the interior, where extensive forests are distributed in the lowlands. The tree limit dips southwards approaching the Hudson Bay and runs almost parallel to the coast because of the negative influence of the cold water body of Hudson Bay (Lescop-Sinclair & Payette, 1995). Site description Sampling was carried out during summer 2003 and 2004, along a latitudinal transect from forests and open groups of trees at the tree line in the forest-tundra to isolated krummholz 2122







(deformed spruce trees generally  2.5 m tall) in the tundra, up to the northernmost white spruce individual. The distribution limits were assessed by an extensive helicopter survey. Sixteen sites were sampled (Fig. 2; Table 1), each consisting of a white spruce population with dimensions ranging from a single spruce clone to a few tens of individuals. A black spruce clone (site EN1) was also sampled on Ross Island. Some isolated individuals and small populations within the range limit were not sampled. Black spruce individuals were sampled when present within a given area occupied by a white spruce population. At each site, a detailed description of every tree was given. Stem height, basal diameter, growth form, occurrence of cones, scars, dead branches and frost damage on needles were recorded. The snowpack level was inferred based on the position of reddish and yellowish needles, stem anomalies (e.g. base of branchless stems above a dense mass of living foliage) and mean height of shrub cover (mostly dwarf birch, Betula glandulosa Michx.). This evidence usually indicated the multiyear thickness of snow cover (Lavoie & Payette, 1992). The diameter and circumference of spruce clones and the number of supranival (above snow) stems were also recorded. For the largest populations consisting of several individuals (sites EB1, EB9 and EB21), the position of each tree, each







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra







5630' N







Umiujaq EB20 EB19 EB10-13 EB2 EB3-4 EB5







5630' N







Hudson Bay







EB1







5615' Ross Island







EB7 EB8-EN1







5615'







Lac Guillaume-Delisle



EB9







Belanger Island







EB 21 0 10 km 7700' W 7630' 7600'







Figure 2 Sampling sites along the Hudson Bay coast and on the Nastapoka Islands.







Table 1 Details of sites sampled



Maximum spruce height (cm) 400 350 300 160 230 220 650 150 100 90 150 120 500 100 Snow thickness (cm) 40-250 40-110 40-120 50 < 40 70-110 70-110 90-230 < 50 < 50 < 50 < 50 70 70







Site EB1 EB2 EB3 EB4 EB5 EB7 EB8 EB9 EB10 EB11 EB12 EB13 EB19 EB20 EB21 EN1







Latitude (N) 5623 5630 5630 5630 5628 5614 5614 5609 5631 5632 5633 5634 5631 5632 5607 5614







Longitude (W) 7631 7632 7632 7632 7631 7645 7645 7644 7632 7632 7632 7632 7632 7632 7646 7645







Altitude (m a.s.l.) 74 4.8 7 7 5 20 24 60 10 8 9 9 5.2 6.5 3.5 12







Life form High krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Tree-high krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Krummholz Tree Krummholz







Cones X X X







Seedlings X X X







Tree establishment 1661 1693 1663 1763 1771 1756 1959 1666 Not dated 1780 1751 1719 1660 1778 1897 Not sampled







X







X







X X X X







X







seedling and each dead stump was mapped with an infrared theodolite (0.001-m precision; Leica T1010). In site EB1, the largest white spruce stand, mapping was performed along a 10m-wide belt transect across the whole population. At site EB21,  langer Island, the rate of isostatic uplift was located on Be assessed by radiocarbon dating of drifted wood fragments distributed along a linear altitudinal gradient from the sea level up to 10.75 m a.s.l. We assumed the influence of storm surge, waves and sea ice as constant through the investigated time







span. Dating was performed at the 14C laboratory of the Centre  tudes nordiques (Universite  Laval, Que  bec City, Canada). d'e The measured radiocarbon age was calibrated using calib 4.4 (Stuiver et al., 2003). Age structure and tree-ring analysis At each site a core was taken from every living tree, using a Pressler increment borer at the base of the trunk. 2123







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







M. Caccianiga and S. Payette Cross-sections were taken from dead trees at the lowest position possible along the trunk. In the whole area, 195 trees were sampled. Trees that showed rotten wood were discarded and sometimes more than one sample was taken from a tree. Samples showing reaction wood were not measured, but were taken into account in the evaluation of population dynamics. Samples were air-dried and finely sanded. Annual tree-ring width was measured with a Velmex micrometer at 40* with a precision of 0.01 mm. Two radii were measured for each crosssection. Cross-dating of samples was performed using light rings (rings with exceptionally few latewood cells; Filion et al., 1986). High-frequency (> 75%) light-ring years occurred in 1686, 1784, 1816, 1817, 1853, 1912, 1956, 1965 and 1969, respectively. A total of 177 seedlings and saplings too small to be cored were dated by counting the number of internodes (whorls) along the stem. A tree-ring width chronology was built for each site. In two sites (sites EB1 and EB9) where a large number of individuals were available, highly correlated individual curves (r > 0.6) were retained and the subsample signal strength (SSS; Wigley et al., 1984) was calculated. This test defines the minimum number of samples representative of the whole series and consequently the significant length of the chronology. Only the part of the series with a SSS value > 0.85 was considered representative. In the remaining sites, consisting of isolated clones (EB5, EB7, EB8, EB11-13, EB19 and EB20) or very small populations (EB2, EB3 and EB21), the mean tree-ring width for each site was calculated after discarding samples with rotten and reaction wood and poor correlation values (r < 0.4); the SSS was not calculated for these sites. Tree-ring width curves were considered instead of index chronologies to allow better comparison between growth rates at the different sites. Statistics and correlation values were calculated using the program cofecha (Holmes, 1983). Long-term growth trends were evaluated by linear regression, calculated on 50-year periods: 1659-1700, 1701-50, 1751-1800, 1801-50, 1851- 1900, 1901-50, 1951-2003 (Lloyd & Fastie, 2002). Macrofossil analysis Fourteen soil monoliths were recovered from 10 sites. The soil samples consisted of the topmost organic (O and A) horizons accumulated over coarse, well drained mineral sediments. These soil monoliths were used for their strategic position beneath spruce clones and individuals and the centre of spruce populations, and because of the absence of peat deposits in the study area. The soil samples were kept in the freezer before analysis. Soil subsamples were taken at every centimetre, treated with 5% KOH, and searched for spruce macrofossils. Spruce needles from the lowest level were extracted using 850- and 425-lm mesh sieves and identified based on resin duct morphology, as suggested by Weng & Jackson (2000). Resin ducts were observed longitudinally where possible. When this observation was not possible (as for charred needles), three cross-sections were performed to assess the continuity of the resin ducts. If two continuous resin ducts 2124 with constant diameter were observed but the needle was not complete, the identification referred to `Picea mariana uncertain'. If no resin ducts could be observed and the needle was not complete, the identification referred to `Picea glauca uncertain'. If no evidence could be observed, the sample was classified as unidentifiable. Where available, 100 needles were identified for each 1-cm slice; otherwise all available needles were analysed. The identification was performed upwards along each soil monolith until recent, well preserved needles corresponding to the present species composition at the soil surface. For site EB3, the identification of spruce needles was performed on the whole soil profile, as both spruce species occurred at the same time, and as a charcoal layer with identifiable charred needles could be observed. The whole plant macrofossil assemblage was analysed in the soil monolith of EB3. Plant macrofossils were extracted using 850-, 425-, 180- and 125-lm mesh sieves and identified under a stereomicroscope. The lowest needles and charcoal fragments (when present) were sampled and dated by 14C accelerator mass spectrometry (AMS) dating at Beta Analytic, Inc. (Miami, FL, USA). Charred needles were chosen for the dating of charcoal layers in order to date spruce that were probably alive when the fire occurred. Where no charcoal or needles were available for AMS dating, organic matter was taken from the base of the profile and dated by conventional radiocarbon  tudes nordiques. dating at the 14C laboratory of the Centre d'e RESULTS Site description The northernmost white spruce was recorded near the village of Umiujaq (site EB20, 5632 N; Fig. 2). Along the coast, the tree line occurred between 5620 N and 5623 N (EB1). High krummholz were observed until 5624 N. In the Nastapoka  langer Island (site EB9, Islands, the tree line is located on Be 5609 N), whereas only scattered, stunted spruce were observed on Ross Island (EB7 and EB8). Seedlings were recorded until the latitude of 5630 N (EB2), but mature cones were observed on the ground under the canopy of the northernmost individual at Umiujaq.







Hudson Bay coast



Site EB1 was located at the tree line and corresponded to an open spruce population comprising 150-200 spruce individuals. The largest individuals were c. 4 m tall. All showed supranival stems with damaged foliage (red needles, dead twigs) and, in a few cases, dead stems above the snowpack. The inferred snow level ranged from 40 to c. 250 cm at the centre of the tree population, and was strongly influenced by the presence of spruce. A vigorous establishment of seedlings was observed on one side of the stand on moist ground (Fig. 3). The directional establishment could be also due to the predominantly westerly wind blowing from the sea (LescopSinclair & Payette, 1995). All living trees were < 100 years old.







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra







Figure 3 Map of the white spruce population of site EB1 (Hudson Bay coast). Data on stem establishment in 50-year classes, except after 1950. The average snow level, estimated from tree growth forms and patterns of foliage damage, is also indicated.







Some trees that died in the 1980s and 1990s were c. 100 years old; two others were much older: one of these could not be dated because of rotten wood, the other was established in 1661 and died in the 1920s or early 1930s. North of the tree line, a small clone with no regeneration and established in the 18th century was sampled in an open, wind-exposed tundra site (EB5). Located 3.5 km north of EB5, site EB3 was occupied by a large clone and three dead stumps. The older stem of the clone, still alive, was established in 1727; two of the dead stumps were established in 1663 and 1693, respectively, and died after 1785 and 1825, respectively. Eleven seedlings and saplings were recorded, including eight spruce < 10 years old, one spruce > 20 years old, and two spruce c. 30 years old. A dead clone established in the 18th century, which died in the 1930s, was also sampled nearby (EB4). The northernmost limit of seed regeneration was located in site EB2. The site was occupied by a clone 14 m in diameter and three smaller clones nearby. Three of the 11 stems of the main clone were cut by Inuit. The older stem started to grow in 1693, but a bigger and probably older stem could not be dated because of rotten wood. Ten seedlings and saplings < 20 years old, originated by sexual reproduction, were also located close to the main clone. About 500 m north of site EB2, sites EB10-13 were composed of stunted clones growing in wind-exposed tundra conditions. The EB10 clone was dead, whereas the others were in a regressive stage with dead wood in the central part of the clone; the sampled stems died in the 1880s and 1920s. Two clones bore mature and open cones, but no seedling was found. Clones EB11, EB12 and EB13 were all established during the 18th century, while EB10 could not be dated. Sites EB19 and EB20 corresponded to the northernmost sites. EB19 was occupied by a living individual established in







1936 and two dead stumps established in 1660 and before 1666, respectively. EB20, the northernmost white spruce individual along the Hudson Bay coast, was a clone consisting of four stems, two alive and developed in the 20th century, two dead and established in the 18th century.







Nastapoka Islands



 langer Island and corresponded to Site EB9 was located on Be the tree line on the Nastapoka Islands. It was composed of a large white spruce population located in a flat area with longlasting snow cover, and surrounded by stunted black spruce clones. The white spruce population consisted of about 15 living tree individuals, 22 clones, 71 seedlings and 65 dead stumps (Fig. 4). Tree individuals were single-stemmed trees, originated from seeds, with no evidence of vegetative reproduction (layering), 3-6.5 m tall. Clones consisted of groups of stems originated by layering; they were up to 7.5 m long and 6 m wide, more than 20 m in circumference and including up to 20 stems. The inferred snow level in the site varied from 130 to 200 cm within the clones, and from 30 to 110 cm among the sparse tree individuals. The overall age structure (Fig. 5) showed that living trees were all < 100 years old, whereas most of the stems from the clones were slightly older (established at the end of the 19th century), except for one established in 1711, which died in 1992. Most dead individuals showed a stunted growth form: they were established in the 18th century and died during the 20th century. Four were established in the 17th century, with the oldest established in 1666. The oldest individuals were mostly located at the centre of the stand (Fig. 4). The changing growth forms among the different spruce generations are shown through the relationship between age and diameter (Fig. 6). Dead stems showed a 2125







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







M. Caccianiga and S. Payette







Figure 4 Map of the white spruce popula langer Island). Open tion of site EB9 (Be circles indicate the position and dimension of living individuals and clones (seedlings excluded). Symbols for date of stem establishment and average snow level as in Fig. 3.







45 40 35







lndividuals (n)







30 25 20 15 10 5 0



0 0 0 0 0 93 19 0 60 -1 97 19 0 80 -1 99 20 0 00 -2 01 0 0 0 0 0 0 0 0 75 77 79 87 0 67 69 71 73 81 83 85 -1 -1 -1 -1 89 -1 -1 -1 -1 -1 -1 -1 -1 -1 91 40 60 80 60







Years







Figure 5 Age structure of the white spruce  langer Island) population at site EB9 (Be (establishment dates in 10-year classes). White bars, dead spruce; black bars, living spruce.







60







80







00







20







00







20







40







80







00







17







17







17







18







16







16







17







17







18







18







18







18







19







19







20







-1







markedly lower growth rate than living trees and clones. Only two individuals established in 1899 and 1929, respectively, and thus belonging to a younger generation, showed a growth rate close to that of living individuals. Stunted black spruce clones nearby were up to 60 m in circumference, corresponding to an inferred age of c. 1000 years (Laberge et al., 2000).  langer Island and is Site EB21 is situated along the shores of Be occupied by a young spruce population colonizing the emergent coast due to rapid isostatic uplift; the population consisted of 45 tree individuals and 35 seedlings (Fig. 7). The oldest spruce was 106 years old. The rate of isostatic uplift assessed by the radiocarbon dates of drifted wood (Fig. 8) indicates that the area 2126







occupied by the oldest spruce trees emerged from the sea between cal. ad 1800-1940, thus suggesting a fast colonization by white spruce on newly emerged substrates. Moreover, seedlings and saplings almost reached the present tide line (Fig. 7). Spruce could be observed only on the recently emerged substrates, while the uppermost terrains representing older shorelines were devoid of white spruce individuals. Only scattered black spruce krummholz occupied the surrounding hills. The position of white spruce seedlings indicated that the main spruce colonization of the area was occurring seaward (Fig. 7). Only one single stunted black spruce seedling was observed, at 7.5 m above the tide line.







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra



50 45 40 35 30 25 20 15 10 5 0 0 50 100 150 200







12 10



Altitude (m)



Living stems Dead stems







Diameter (cm)







8 6 4 2 0







250







300







0







100







200







Age (years)







300 400 500 600 Age (cal years BP)







700







800







Figure 6 Age-diameter relationship among different spruce  langer Island). Dead tree individuals generations in site EB9 (Be indicated by arrows were established in the late 19th-20th century.







 langer Island (site Figure 8 Curve of isostatic rebound on Be EB21) as assessed by radiocarbon dating of drifted wood fragments. Error bars indicate 2r calibrated age (age intervals including the 95% probability distribution).







 langer Island, was occupied by open Ross Island, north of Be tundra with scattered spruce clones. Sites EB7 and EB8 corresponded to two white spruce clones established around 1750 and in the 20th century, respectively. The black spruce clone of site EN1 could not be dated, but its circumference reached 15 m, corresponding to an inferred age of c. 250- 350 years (Laberge et al., 2000). Age structure and tree-ring analysis The oldest living stem was 276 years old (site EB3). The oldest  langer individual was a 281-year-old tree established on Be Island (EB9) in 1711, which died in 1992. No tree was present before ad 1660, and the first establishment of white spruce was almost synchronous at all sites (Table 1). The number of individuals established per decade (Fig. 9) was almost continuous in the southern part of the study area, while it showed







a marked gap in the northern part during the first decades of the 19th century. Spruce establishment occurred throughout the whole area up to the northernmost latitude during the 18th century (except the 1740s and 1750s), particularly in the 1780s and 1790s, and in the 1660s, when the first occurrence of spruce was recorded. An overall increase of spruce establishment occurred during the 20th century, followed by a decrease in the 1970s and another increase in the 1980s and 1990s. However, the survival rate of seedlings and saplings should be taken into account to make a realistic comparison with the recruitment of the older cohorts (Fig. 9). Only one black spruce seedling has been observed in the whole study area, at  langer Island. A high mortality rate was site EB21 on Be observed at the end of the 20th century in the southernmost sites; at the species limit, mortality occurred all through the 20th century (Fig. 9).







Figure 7 Map of white spruce population of  langer Island). Symbols for date site EB21 (Be of establishment as in Fig. 3. Dashed line, uppermost limit of present drifted wood; solid line, present tide line. The position of ancient drifted wood fragments and their respective 14C dates and altitude above sea level are also reported.



Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







2127







M. Caccianiga and S. Payette



(a)



60 >5630' 5625'-5630' 50 5620'-5625' 5610'-5620' 40 <5610'







lndividuals (n)







30







20







10







0







(b) lndividuals (n)







1660 1680 1700 1720 1740 1760 1780 1800 1820 1840 1860 1880 1900 1920 1940 1960 1980 2000







0 10 20







Figure 9 Number of spruce established (a) and dead (b) per decade along the latitudinal gradient in the study area. Individuals established by seed and by layering are both included.







2 (mm) 1 0 1 (mm) 0







EB20



1 (mm) 0







EB19







EB13



1







EB12



1 (mm) 0 1 (mm) 0 2 (mm) 1 0







(mm)







EB11 EB2







1 (mm) 0







EB3 1 EB5



0 (mm)







EB1



0 2







EB9



2 (mm) 1 0







1 (mm) 0







EB21







1655







1705







1755







1805







1855







1905







1955







2005







Figure 10 Ring-width series. Curves are arranged along the latitudinal gradient, from tree and high krummholz stands (EB21-EB9 and EB1 respectively, bottom) to the northernmost individual (EB20, top). The interrupted period of the curve of sites EB5 and EB19 is not covered by spruce samples. Grey-shaded areas in the chronologies of sites EB1 and EB9 indicate the part with SSS < 0.85, where samples are not sufficiently abundant to be representative of the whole population.







Tree-ring width series from all sites were ordered along a latitudinal gradient (Fig. 10), except for sites EB4, EB7 and EB8, which were excluded from the analysis. Mean annual tree-ring widths were calculated for sites EB2, EB3, EB5, EB11, EB12, EB13, EB19, EB20 and EB21. The two radii from site EB11 were retained, even if poorly correlated (r 1/4 0.27), as the 2128







low r value was probably due to their short period of overlapping. Mean tree-ring width ranged from 0.14 mm (EB11) to 1.04 mm (EB21) (Table 2) with an overall decreasing trend with latitude, except for the low values of the exposed krummholz of sites EB5 and EB11-13 and the high value of the northernmost individual from site EB20, consisting of







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra



Table 2 Statistics of the tree-ring width chronologies. Sites are arranged along the latitudinal gradient.



Trees (n) 3 30 28 2 3 4 1 2 1 2 3 Radii (n) 5 47 34 3 6 6 2 4 2 4 4 Mean ring width (mm) 1.04 0.70 0.64 0.25 0.33 0.31 0.14 0.20 0.25 0.32 0.54 Mean r 0.39 0.60 0.66 0.72 0.57 0.46 0.27 0.51 0.66 0.55 0.45 Time series 1899-2003 1669-2002 1685-2002 1770-2002 (gap 1859-90) 1795-1986 1663-2002 1791-1888 1753-1925 1713-1885 1659-2003 (gap 1915-36) 1778-2003 Significant time series (SSS > 0.85)







Site EB21 EB9 EB1 EB5 EB2 EB3 EB11 EB12 EB13 EB19 EB20







SD 0.483 0.320 0.285 0.123 0.117 0.137 0.054 0.109 0.134 0.199 0.238







Sensitivity 0.27 0.19 0.29 0.25 0.20 0.21 0.21 0.24 0.23 0.25 0.24







Autocorrelation 0.59 0.80 0.75 0.85 0.69 0.80 0.61 0.79 0.82 0.84 0.74







1713-2002 1885-2002







Table 3 Growth trends from 1659 to present







Site EB21 EB9 EB1 EB5 EB3 EB2 EB11 EB12 EB13 EB19 EB20







1659-1700







1701-50







1751-1800







1801-50







1851-1900







1901-50 )0.113 0.091 0.124 )0.141 )0.675 )0.534 0.233 )0.727 0.051







1951-present )0.300 )0.060 )0.754 )0.200 )0.724 )0.160







0.505 )0.355 0.307







)0.699 )0.621 )0.155







0.244







0.081







0.440 0.461 )0.452 0.096 0.418 )0.580 )0.447 0.042 )0.481 )0.067







)0.215 )0.776 )0.308 )0.011 )0.618 )0.320 )0.780 )0.143 0.591 0.284







0.842 0.726 0.77 0.036 0.391 )0.192 0.474 0.859 0.740 )0.507







0.813 0.470







Standardized regression coefficients (b) of 50-year time periods are reported for each site. Significant values (P < 0.05) are in bold type.







relatively young stems in a more protected site. Site EB21 was occupied by young individuals showing fast growth. The overall growth trend of this site was close to that of the adjacent site EB9. Fast growth was observed in the late 17th and the late 18th centuries, when an overall positive growth trend could be observed (Table 3). Periods of suppressed growth occurred in the early 18th century and from the 1810s (starting with an abrupt decrease in 1816) to the 1850s in almost every site. In site EB20 recovery had already started in the late 1820s, and the overall growth trend of the period 1801-50 is positive (Table 3). During this period, spruce growth of the most exposed krummholz (sites EB5 and EB11-13) was reduced almost to zero, with slow or no recovery. A ring-width increase was evident everywhere in the second half of the 19th century, except in site EB20 where the maximum growth was reached in the 1850s. Ring width peaked in the 1880s in site EB3 and in the 20th century in sites EB9, EB1 and EB19, the latter showing a severe growth suppression in the 1940s and 1950s. The period after 1951 was marked by a positive growth trend in the northernmost sites (EB19 and EB20); elsewhere a stable or







even significant downward trend (EB1 and EB3) was apparent (Table 3). Macrofossil analysis Spruce needles were recorded throughout the soil profiles in seven out of 14 monoliths (sites EB1, EB2, EB3, EB7, EB9, EB20, EN1). Their identification reflected the present species composition (see Appendix S1 in Supplementary Material) except for site EB3, where both black spruce and white spruce needles were observed (Fig. 11). Black spruce needles were slightly more frequent in the lowest layers. Charcoal fragments were found only in site EB3 and consisted of charred wood and spruce needles. Charred needles were mostly white spruce needles, although a few black spruce needles were recorded (Fig. 11). Macrofossils from site EB3 (Fig. 11) showed the alternation of levels rich in spruce remains with levels of plant remains from open tundra conditions such as B. glandulosa, Empetrum nigrum L. subsp. hermaphroditum (Lange ex  cher, and Vaccinium spp. Remains of Cornus Hagerup) Bo spp. (possibly Cornus suecica L.) occur in the lowest layers. 2129







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







2130







M. Caccianiga and S. Payette







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Figure 11 Macrofossil concentration diagram of site EB3. Black bars, number of macrofossils per cm3; open bars, percentage of Picea mariana needles calculated on the total number of needles examined in each 1-cm slice. Criteria for assessing uncertain attribution are given under Methods.







Advance of white spruce in the coastal tundra



Table 4 Radiocarbon dates from the sampled sites along the Hudson Bay coast and on the Nastapoka Islands



Laboratory number UL-2813 UL-2825 Beta-190595 Beta-190597 Beta-190599 Beta-190598 Beta-190594 UL-2777 UL-2778 UL-2779 Beta-199326 Beta-190596 UL-2849 UL-2850 UL-2851 UL-2852 UL-2855 UL-2859 UL-2860 UL-2864 UL-2865 UL 2866 UL 2868 Sample number EB1(43)-21-20 EB2 EB3 Bottom EB3 Charcoal EB3-10cm EB7-9-8cm EB9-2-8-7cm EB938-45-12-11 EB938-45-7-6 EB938-45-6-5 EB20-10-9 EN1-11-10cm Bel-2 Bel-9 Bel-5 Bel-6 Bel-3 Bel-4 Bel-1a Bel-1b Bel-7 Bel-10 Bel-8 Depth (cm) 20-21 22-23 23-24 17-18 10-11 8-9 7-8 11-12 6-7 5-6 9-10 10-11 Age (14C years bp) 280  80 300  90 110  40 140  40 90  40 104.1 124.1 1060 110 10 103.1 400 300 580 400 370 150 270 110 140 460 730 380                   0.5 pMC* 0.6 pMC* 90 90 90 0.4 pMC* 40 60 60 60 60 40 50 50 40 40 40 40 Age (calibrated years) 2r ad 1440-1690; ad 1720-1810; 1840-80; 1920-50 ad 1430-1690; ad 1720-1810; 1840-80; 1920-50 ad 1670-1780; ad 1800-1950 ad 1660-1950 ad 1680-1770; ad 1800-1940; ad 1950-60 Modern Modern ad 770-1190 ad 1660-1950 ad 1670-1780; ad 1800-1940; ad 1950 Modern ad 1430-1530; ad 1560-1630 ad 1450-1670 ad 1300-1430 ad 1430-1640 ad 1440-1640 ad 1665-1784 ad 1480-1680 ad 1800-1940 ad 1668-1781; ad 1796-1895 ad 1401-94 ad 1216-1305 ad 1440-1530







Site EB1 EB2 EB3 EB3 EB3 EB7 EB9 EB9 EB9 EB9 EB20 EN1 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21 EB21







Location Hudson Bay coast Hudson Bay coast Hudson Bay coast Hudson Bay coast Hudson Bay coast Ross Island  langer Island Be  langer Island Be  langer Island Be  langer Island Be Umiujaq Ross Island  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be  langer Island Be







Material Organic matter Organic matter Needles (unidentified) Charcoal (Picea glauca) Needles (P. glauca) Needles (P. glauca) Needles (P. glauca) Organic matter Organic matter Organic matter Needles (P. glauca) Needles (unidentified) Wood Wood Wood Wood Wood Wood Wood Wood Wood Wood Wood







Age (14C years bp) 1/4 conventional radiocarbon age  SE. 2r calibrated age: age intervals including the 95% probability distribution. *pMC, percentage of modern carbon: the result indicates that the sample was living after ad 1950.







No sample associated with white spruce remains gave a date older than 300 14C years bp (cal. ad 1430-1690; site EB2); charred white spruce needles gave a date of 140 14C years bp (cal. ad 1660-1950; site EB3) (Table 4). A date of 400 14C years bp (cal. ad 1430-1530) was obtained for unidentifiable needles found under a black spruce clone (site EN1, Ross Island) where only black spruce needles were recovered from the soil sample (Appendix S1). No spruce needles or other spruce remains were present at the level dated 1060 14C years bp (cal. ad 770- 1190) at site EB9. Radiocarbon dating of the lowest white spruce needles at this site gave a modern age (Table 4). DISCUSSION White spruce dynamics White spruce expanded throughout the study area over the past 400-500 years. In particular, all tree-line populations increased significantly during the past 100 years. Seedlings have been found up to c. 4 km south of the northernmost position of the species in the coastal tundra. A recently established spruce (established 1936) was found close to the species limit, and the northernmost individual was bearing







apparently fertile cones. The establishment of new individuals has been almost continuous during the past 300 years at the tree line, but no regeneration occurred during the 19th century in the northern part of the study area dominated by tundra communities. The downward colonization of the newly emerged shorelines was fast and effective on favourable substrates, as already observed along the Hudson Bay coast  gin et al., 1993). However, the absence of white spruce (Be  langer Island suggested the from the ancient shorelines on Be past occurrence of unfavourable periods, or the recent arrival of the species at the site. The first establishment of white spruce was almost synchronous from the current position of the tree line to the species limit. Further establishment has been effective mostly in sites already colonized by single spruce, with few recently created populations all in the southern part of the study area:  langer Island (established at the end of the 19th EB21 on Be century), and the clone EB8 on Ross Island. Even recently expanding populations (e.g. site EB1) showed the remains of one or few older `founder' individuals, usually located in the inner part of the stand. The existing populations thus seem to have originated from single, scattered individuals, with a progressive recruitment of seeds towards the periphery of the 2131







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







M. Caccianiga and S. Payette stand (sometimes asymmetrically due to prevalent wind, e.g. EB1) during favourable periods. Even during such periods, seedlings could establish only on moist, protected sites, with a positive feedback effect on further recruitment (EB1, EB9, EB21). This pattern is consistent with the observations of Scott et al. (1987) in forest-tundra sites at Churchill, Manitoba. However, the possibility should be taken into account that isolated young populations, not originating from a single individual, have been overlooked. The process of expansion at the present tree line was accompanied by a marked change of growth forms. Trees established in the second half of the 19th century replaced stunted individuals already present since the 17th century. Most of those ancient individuals died in the 19th or the 20th century. The expansion trend of white spruce along the Hudson Bay coast has been linked to recent climate warming, effects of which could also be observed through the effectiveness of postfire recruitment (Payette & Filion, 1985). The present study focused mostly on the northernmost sites rather than on the forest stands where such recruitment was apparent. Only one site showed charcoal, suggesting a low fire frequency in the scattered spruce populations along the coast. The fire affected a pre-existing white spruce clone, but the occurrence of black spruce remains below the charcoal layer may indicate a black spruce stand surrounding the white spruce clone: a post-fire decline of black spruce clones could be assumed. Tree-ring data confirmed the increase in radial growth affecting the tree and high-krummholz populations in the late 1800s, after a marked period of suppressed growth. These trends were already documented for white spruce in eastern Canada (Parker et al., 1981; Jacoby, 1983; Jacoby et al., 1988) and could be linked to global climate events such as the last peak of the Little Ice Age (Grove, 1988) and the subsequent warming (Tett et al., 1999; Houghton et al., 2001; Parmesan & Yohe, 2003). However, the spruce population at the tree line showed a significant downward trend in the late 20th century (EB9 and EB1) (Table 3). White spruce krummholz have registered such changes in a contrasting way. More exposed individuals from sites EB11-13 hardly survived the unfavourable period of the 1820-50s, and died shortly after. We can presume the same story for the clone at site EB10, which could not be dated. A significant upward growth trend in the late 19th century (Table 3) failed to result in ring widths comparable with those of the other sites. The individual from EB12 showed an ephemeral recovery in the early 20th century, but without a long-term trend, and this was rapidly followed by death. Larger krummholz in sites EB2 and EB3 showed a slight recovery after the 1860s, a peak in the 1880s but with no real increase in ring width, and a downward trend in the 20th century. EB19 and EB20, although the northernmost individuals, showed overall faster growth and were less affected by the cold conditions of the early 1800s. The contrasting growth pattern in the past 50 years between the northernmost sites and those situated at lower latitude is noteworthy. In our sites this pattern was probably affected by the young age of the stems covering the last part of the series in the northernmost sites, and by the characteristics of each site. The old clone of site EB3, situated well north of the tree line, showed a downward trend and was already declining at the beginning of the 20th century (Table 3). The response to climate by scattered tree islands appeared to be influenced strongly by exposure to wind and subsequent snow cover, which can overrule the influence of the latitudinal gradient (Fig. 12). A positive feedback between tree island dimension and snow cover has been described for white spruce islands (Payette & Filion, 1985; Scott & Hansell, 2002). However, the declining pattern of the young population of site EB1 seemed to reflect a long-term trend, also indicated by the high mortality of the 1980s and 1990s (Fig. 9). Species-specific response to climate White spruce responded to recent, favourable climatic conditions through active seed recruitment of new individuals, particularly in the northernmost forest sites and in foresttundra (Payette & Filion, 1985; Scott et al., 1987; Szeicz & MacDonald, 1995; MacDonald et al., 1998; Lloyd et al., 2005). Present data show the same behaviour at the tree limit and in shrub-tundra sites, and almost to the species limit. A similar pattern, with continuous although low levels of seedling establishment, was observed for white spruce in the shrub tundra at Tuktoyatuk (north-western Canada), the northernmost conifer population in North America (McLeod & Henry, 2002).







7







1*2 1 0*8 0*6 0*4 0*2 0



EB21 South Snow height EB9 EB1 EB5 EB3 EB2 EB11 EB12 EB13 EB19 EB20 North Ring width







Tree/snow height (m)







6 5 4 3 2 1 0







Ring width (mm)







Sites



Tree height







Figure 12 Trends of maximum tree height, inferred snow level and mean ring width of study sites positioned according to latitudinal gradient.







2132







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra In contrast, black spruce seed recruitment in the same area was almost absent. Black spruce tree-line displacements across  bec were driven by seedling establishment only in northern Que the southern forest-tundra (Gamache & Payette, 2005) and by layering and change of growth forms in the northern foresttundra and in shrub-tundra (Lescop-Sinclair & Payette, 1995; Gamache & Payette, 2004, 2005). The different behaviour of the two species may be due to their reproductive traits. In particular, semi-serotinous cones of black spruce retain seeds for a certain number of years in the absence of fire (Viereck & Johnston, 1990), thus delaying the response to favourable climatic conditions (Gamache & Payette, 2005). Furthermore, the phenotypic plasticity of black spruce, although rendering it able to withstand harsh climatic conditions, causes the establishment of stunted growth forms with reduced cone  gin & Filion, production and effective post-fire recruitment (Be 1999; Sirois, 2000; Gamache & Payette, 2004; Lloyd et al., 2005). On the other hand, the more constant seed release of white spruce cones (Nienstaedt & Zasada, 1990) renders this species more able to respond readily to favourable climatic conditions, despite its lower morphological plasticity. Also, white spruce was probably favoured by its recent arrival after a constant decline of pre-existing black spruce stands due to the failure of post-fire regeneration (Payette & Gagnon, 1985; Asselin & Payette, 2005). In the absence of changes in the fire regime, white spruce will probably continue its process of expansion through seedling establishment, increase of population density and a northward shift of the tree growth form. Warmer climate could enhance the inception of arborescent growth form and seedling establishment in the more exposed sites where the magnitude of climate warming has been insufficient until now. The latter process could also involve black spruce clones in a northward and coastward process (Lescop-Sinclair & Payette, 1995; Gamache & Payette, 2005). Higher fire frequency could cause a further decline of black spruce if it occurs before the inception of sexual reproduction. With both species fully able to reproduce sexually, fire would probably favour black spruce regeneration, or at least limit the inception of a long-term dominance of white spruce (Lloyd et al., 2005). Arrival of white spruce on the eastern coast of Hudson Bay None of the tree-ring and macrofossil analyses performed showed any evidence of the occurrence of white spruce along the Hudson Bay coast before the 17th century: tree-ring data and 14C dates gave converging results. Black spruce was certainly present in the area long before that date, as shown by 14 C dates, clone dimension and extensive literature data  guin, 1987; Gajewski & Garralla, 1992; Payette, (Allard & Se 1993 and references therein; Gajewski et al., 1996; Asselin & Payette, 2005). The peculiarities of the Hudsonian white spruce population, and the uncertainties about its origin and date of arrival, have already been emphasized (Payette, 1993). The present data suggest a recent and ongoing arrival of the species. Its establishment probably derived from scattered individuals already present on the sites, traces of which are difficult to find in the absence of surviving or recently dead individuals. Given the position and altitude of all study sites, most white spruce individuals are established on soil surfaces generally < 2000 years old, assuming a general rate of land  langer emergence of c. 1-1.3 m per century according to our Be Island curve and Allard & Tremblay's (1983) curve. All treeline stands and tundra tree islands have in common the same overall structure of living and dead spruce, whatever their location and altitude along the coast. With these facts in mind, it seems likely that the arrival of white spruce occurred during the Little Ice Age, a period well defined in eastern Canada between the end of the 1500s and the late 1800s. The arrival of the species to its northern limit during a climatically unfavourable period is noteworthy, as well as its failure of further expansion during the subsequent warmer periods. Even if the colonizing event probably took place during a favourable period of the Little Ice Age, the observed pattern of establishment is probably the outcome of a migrational lag rather than the result of climatic limitation. Also, we must emphasize the fact that the lack of evidence for white spruce presence before 1660 does not prove that the species was not present in the study area earlier. It is noteworthy that spruce needles from 420-520 cal. years bp preserved in well drained soil conditions, and organic matter from 760-1180 cal. years bp, could be found not associated with white spruce remains. It is also possible that some evidence could have been lost by the rapid decomposition of dead wood at the soil surface and needles in the soil in the maritime climate along the coast of Hudson Bay. However, the preservation of wood remains depends on the date of death, rather than the date of establishment. We found that well preserved individuals died in the late 18th to early 19th centuries (Fig. 9); given the longevity of the species, trees established 200-250 years before could have been found, if present. CONCLUSIONS The dynamic behaviour of white spruce on the eastern coast of Hudson Bay appears to be the result of a complex ecological history. The species probably arrived recently in the area due to a delayed post-glacial spread along the Hudsonian pathway (Payette, 1993). A colonizing event brought white spruce to its latitudinal limit in tundra areas during a favourable climatic moment during the Little Ice Age. The established individuals survived by layering during unfavourable periods, but acted as nuclei for sexual recruitment almost continuously, except in the northernmost and more exposed sites. Warmer periods were marked by strong seedling recruitment and consequently increased density of populations, with no or minor latitudinal shift. A shift of the tree limit driven by change of growth form was observed, but even small krummholz were able to reproduce sexually. The recovery to tree growth form of white spruce was faster than that of black spruce. Small black spruce krummholz surround arborescent populations of white spruce 2133







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







M. Caccianiga and S. Payette at the tree line with no evidence of an ongoing change of growth forms. The response of white spruce to the present climatic conditions is different from that of black spruce in similar ecological contexts. Species-specific life-history traits appear critical in understanding the differential responses of both species to climate change, and these responses should be taken into account in modelling the behaviour of tree-line ecosystems to climatic forcing. ACKNOWLEDGEMENTS This study has been supported financially by the Natural Sciences and Engineering Research Council of Canada  bastien Cyr, Ann(NSERC). We express our gratitude to Se  lisabeth Robert and Simon Thibault for , E Catherine Laliberte assistance in macrofossil analysis. The assistance of Ann Delwaide in the field and the laboratory was much appreciated. REFERENCES  guin, M.K. (1987) The Holocene evolution of Allard, M. & Se permafrost near the tree-line, on the eastern coast of  bec). Canadian Journal of Earth Hudson Bay (northern Que Sciences, 24, 2206-2222. Allard, M. & Tremblay, G. (1983) La dynamique littorale des   ne. Zeitschrift fu  r Geomoriles Manitounuk durant l'Holoce phologie. N.F., 47, 61-95. Asselin, H. & Payette, S. (2005) Late Holocene opening of the  bec, Canada. Global forest-tundra landscape in northern Que Ecology and Biogeography, 14, 307-313. Barber, V., Juday, G.P. & Finney, B.P. (2000) Reduced growth of Alaskan white spruce in the twentieth century from temperature-induced drought stress. Nature, 405, 668-673.  gin, C. & Filion, L. (1999) Black spruce (Picea mariana) Be architecture. Canadian Journal of Botany, 77, 664-672.  gin, Y., Be  rube  , D. & Gre  goire, M. (1993) Downward Be migration of coastal conifers as a response to recent land  bec. Quaternary emergence in eastern Hudson Bay, Que Research, 40, 81-88. Cwynar, L.C. & Spear, R.W. (1991) Reversion of forest to tundra in the central Yukon. Ecology, 72, 202-212. Dyke, A.S. & Prest, V.K. (1987) Late Wisconsinan and Holoographie physique cene history of the Laurentide ice sheet. Ge et Quaternaire, 41, 237-263. Filion, L., Payette, S., Gauthier, L. & Boutin, Y., (1986) Light rings in subarctic conifers as a dendrochronological tool. Quaternary Research, 26, 272-279. Gajewski, K. & Garralla, S. (1992) Holocene vegetation histories from three sites in the tundra of northwestern  bec, Canada. Arctic and Alpine Research, 24, 329-336. Que Gajewski, K., Garralla, S. & Milot-Roy, V. (1996) Postglacial vegetation at the northern limit of lichen woodland in  bec. Ge ographie physique et Quaternaire, northwestern Que 50, 341-350. Gamache, I. & Payette, S. (2004) Height growth response of tree line black spruce to recent climate warming across the 2134 forest-tundra of eastern Canada. Journal of Ecology, 92, 835- 845. Gamache, I. & Payette, S. (2005) Latitudinal response of subarctic tree lines to recent climate changes in eastern Canada. Journal of Biogeography, 32, 849-862. Grove, J.M. (1988) The Little Ice Age. Methuen, London. Holmes, R.L. (1983) Computer-assisted quality control in treering dating and measurement. Tree-Ring Bulletin, 43, 69-78. Houghton, J.T., Ding, Y., Griggs, D.J., Noguer, M., van der Linden, P.J. & Xiaosu, D. (2001) Climate change 2001: the scientific basis. Contribution of Working Group I to the third assessment report of the Intergovernmental Panel on Climate Change (IPCC). Cambridge University Press, Cambridge, UK. Jacoby, G.C. (1983) A dendroclimatic study in the foresttundra ecotone on the east shore of Hudson Bay. Nordicana, 47, 95-99. Jacoby, G.C., Ivanciu, I.S. & Ulan, L.D. (1988) A 263-year record of summer temperature for northern Quebec reconstructed from tree-ring data and evidence of a major climatic shift in the early 1800s. Palaeogeography, Palaeoclimatology, Palaeoecology, 64, 69-78. Laberge, M.-J., Payette, S. & Bousquet, J. (2000) Life span and biomass allocation of stunted black spruce clones in the subarctic environment. Journal of Ecology, 88, 584-593. Lamb, H.F. (1985) Palynological evidence for postglacial change in the position of tree limit in Labrador. Ecological Monographs, 55, 251-258. Lavoie, C. & Payette, S. (1992) Black spruce growth forms as a record of a changing winter environment at treeline, Quebec, Canada. Arctic and Alpine Research, 24, 40-49. Lescop-Sinclair, K. & Payette, S. (1995) Recent advance of the arctic treeline along the eastern coast of Hudson Bay. Journal of Ecology, 83, 929-936. Lloyd, A.H. (2005) Ecological histories from Alaskan tree lines provide insight into future change. Ecology, 86, 1687-1695. Lloyd, A.H. & Fastie, C.L. (2002) Spatial and temporal variability in the growth and climate response of treeline trees in Alaska. Climatic Change, 52, 481-509. Lloyd, A.H. & Fastie, C.L. (2003) Recent changes in treeline forest distribution and structure in interior Alaska. Ecoscience, 10, 176-185. Lloyd, A.H., Wilson, A.E., Fastie, C.L. & Landis, R.M. (2005) Population dynamics of black spruce and white spruce near the arctic tree line in the southern Brooks Range, Alaska. Canadian Journal of Forest Research, 35, 2073-2081. MacDonald, G.M., Szeicz, J.M., Claricoates, J. & Dale, K.A. (1998) Response of the central Canadian treeline to recent climatic changes. Annals of the Association of American Geographers, 88, 183-208. Marr, J.W. (1948) Ecology of the forest-tundra ecotone on the east coast of Hudson Bay. Ecological Monographs, 18, 117- 144. McLeod, T.K. & Henry, G.H.R. (2002) Site-specific patterns in establishment of Picea glauca (white spruce) at its northern range limits, NWT, Canada. 6th International Conference on







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







Advance of white spruce in the coastal tundra  tudes nordiques, Dendrochronology Abstracts, Centre d'e  Laval, Que  bec City, Canada. Universite Nienstaedt, H. & Zasada, J.H. (1990) Picea glauca. Silvics of North America. Volume 1. Conifers, Agriculture Handbook 654 (ed. by R.M. Burns and B.H. Honkala). pp. 204-226. US Department of Agriculture, Forest Service, Washington, DC, USA. Parker, M.L., Josza, L.A., Johnson, S.G. & Bramhal, P.A. (1981) Dendrochronological studies of the coasts of James Bay and Hudson Bay. Climatic Change in Canada. Parts 1 and 2. Syllogeus 33 (ed. by C.R. Harrington), pp. 129-188. National Museum of Canada, Ottawa, Canada. Parmesan, C. & Yohe, G. (2003) A globally coherent fingerprint of climate change impacts across natural systems. Nature, 421, 37-42. Payette, S. (1983) The forest-tundra and present tree-lines of  bec-Labrador peninsula. Tree-line ecology. the Northern Que bec Tree-line Conference. Proceedings of the Northern Que Nordicana, 47, 3-23. Payette, S. (1993) The range limit of boreal tree species in  bec-Labrador: an ecological and palaeoecological interQue pretation. Review of Palaeobotany and Palynology, 79, 7-30. Payette, S. & Filion, L. (1985) White spruce expansion at the tree line and recent climatic change. Canadian Journal of Forest Research, 15, 241-254. Payette, S. & Gagnon, R. (1985) Late Holocene deforestation and tree regeneration in the forest-tundra of Quebec. Nature, 313, 570-572. Payette, S. & Lavoie, C. (1994) The arctic tree line as a record of past and recent climatic changes. Environmental Reviews, 2, 78-90.  gin, Y. (1999) Le de  veloppement d'une pessie  re Ricard, B. & Be e  pinette blanche et a  lichens sur la co  te en e  mersion rapide a  bec subarctique. Ge ographie de la Baie d'Hudson au Que physique et Quaternaire, 53, 351-364.  ge de Richard, P.J.H., Larouche, A. & Bouchard, M.A. (1982) N  glaciation finale et histoire postglaciaire de la vegetation la de  bec. Ge ographie dans la partie centrale du Nouveau-Que physique et Quaternaire, 36, 63-90. Ritchie, J.C. (1987) Postglacial Vegetation of Canada. Cambridge University Press, Cambridge, UK. Ritchie, J.C. & MacDonald, G.M. (1986) The patterns of postglacial spread of white spruce. Journal of Biogeography, 13, 527-540. Scott, P.A. & Hansell, R.I.C. (2002) Development of white spruce tree islands in the shrub zone of the forest-tundra. Arctic, 55, 238-246. Scott, P.A., Hansell, R.I.C. & Fayle, D.C.F. (1987) Establishment of white spruce populations and responses to climatic change at the treeline, Churchill, Manitoba, Canada. Arctic and Alpine Research, 19, 45-51. Sirois, L. (2000) Spatiotemporal variation in black spruce cone and seed crops along a boreal forest-tree line transect. Canadian Journal of Forest Research, 30, 900-909. Stuiver, M., Reimer, P.J. & Reimer, R.W. (2003) CALIB 4.4. http://www.calib.org. Suarez, F., Binkley, D., Kaye, M.W. & Stottlemyer, R. (1999) Expansion of forest stands into tundra in the Noatak National preserve, northwest Alaska. Ecoscience, 6, 465- 470. Szeicz, J.M. & MacDonald, G.M. (1995) Recent white spruce dynamics at the subarctic alpine treeline of north-western Canada. Journal of Ecology, 83, 873-885. Tett, S.F.B., Stott, P.A., Allen, M.R., Ingram, W.J. & Mitchell, J.F.B. (1999) Causes of twentieth-century temperature change near the earth's surface. Nature, 399, 569-572. Viereck, L.A. & Johnston, W.F. (1990) Picea mariana. Silvics of North America. Volume 1. Conifers, Agriculture Handbook 654 (ed. by R.M. Burns and B.H. Honkala). pp. 227-237. US Department of Agriculture, Forest Service, Washington, DC, USA. Weng, C. & Jackson, S.T. (2000) Species differentiation of North American spruce (Picea) based on morphological and anatomical characteristics of needles. Canadian Journal of Botany, 78, 1367-1383. Wigley, T.M.L., Briffa, K.R. & Jones, P.D. (1984) On the average value of correlated time series, with applications in dendroclimatology and hydrometeorology. Journal of Climate and Applied Meteorology, 23, 201-213. SUPPLEMENTARY MATERIAL The following supplementary material is available for this article online from http://www.Blackwell-Synergy.com: Appendix S1 Identification of spruce needles found in the soil samples.







BIOSKETCHES  di Marco Caccianiga obtained a PhD at the Universita Milano, working on primary succession on alpine glacier forelands. This project was part of his postdoctoral research at  Laval concerning the recent dynamics of subarctic Universite white spruce. His research interests focus on vegetation dynamics in climatically limited environments. Serge Payette is a professor of plant ecology and palaeo partement de Biologie and Centre d'e  tudes ecology at the De  Laval. As chairman of the NSERC nordiques of Universite Northern Research Chair on subarctic forest ecology, he studies the relationships between boreal and subarctic ecosystems, climate and disturbances at various temporal and spatial scales.







Editor: Glen MacDonald







Journal of Biogeography 33, 2120-2135  2006 The Authors. Journal compilation  2006 Blackwell Publishing Ltd







2135







Loss-calibrated Monte Carlo Action Selection



Ehsan Abbasnejad Justin Domke Scott Sanner



NICTA & ANU scott.sanner@nicta.com.au







NICTA & ANU ANU & NICTA ehsan.abbasnejad@anu.edu.au justin.domke@nicta.com.au







Abstract



Bayesian decision-theory underpins robust decisionmaking in applications ranging from plant control to robotics where hedging action selection against state uncertainty is critical for minimizing low probability but potentially catastrophic outcomes (e.g, uncontrollable plant conditions or robots falling into stairwells). Unfortunately, belief state distributions in such settings are often complex and/or high dimensional, thus prohibiting the efficient application of analytical techniques for expected utility computation when real-time control is required. This leaves Monte Carlo evaluation as one of the few viable (and hence frequently used) techniques for online action selection. However, loss-insensitive Monte Carlo methods may require large numbers of samples to identify optimal actions with high certainty since they may sample from high probability regions that do not disambiguate action utilities. In this paper we remedy this problem by deriving an optimal proposal distribution for a loss-calibrated Monte Carlo importance sampler that bounds the regret of using an estimated optimal action. Empirically, we show that using our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by conventional loss-insensitive samplers.







a1 u a2







p







q  Figure 1: Motivation for loss-calibration in Monte Carlo action



selection. (Top) Utility u( ) for actions a1 and a2 as a function of state  . (Middle) A belief state distribution p() for which the optimal action arg maxa{a1 ,a2 } Ep [u( , a)] should be computed. (Bottom) A potential proposal distribution q ( ) for importance sampling to determine the optimal action to take in p( ).







Introduction



Bayesian decision-theory (Gelman et al. 1995; Robert 2001; Berger 2010) provides a formalization of robust decisionmaking in uncertain settings by maximizing expected utility. Formally, a utility function u( , a) quantifies the return of performing an action a  A = {a1 , . . . , ak } in a given state  . When the true state is uncertain and only a belief state distribution p( ) is known, Bayesian decision-theory posits that an optimal control action a should maximize the expected utility (EU)  U (a) = E[u( , a)] = u( , a)p( )d . (1) where by definition, the optimal action a is a = arg max U (a).



a







belief distribution p( ) may be complex (e.g., highly multimodal) and/or high-dimensional thus prohibiting the application of analytical methods to evaluate the EU integral of (1). Practitioners often resort to the use of Monte Carlo methods to compute an approximate (but unbiased) expectation using n samples from p( ):



1 n



n







[u( i , a)],



i=1







 i  p.







(3)







(2)







In real-world settings such as robotics (Thrun 2000), the



Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







Unfortunately, na ive application of Monte Carlo methods for optimal action selection often proves to be inefficient as we illustrate in Figure 1. At the top we show two utility functions for actions a1 and a2 as a function of univariate state  on the x-axis. Below this, in blue, we show the known state belief distribution p(). Here, it turns out that U (a1 ) > U (a2 ). Unfortunately, if we sample from p() to compute a Monte Carlo expected utility for each of a1 and a2 , we find ourselves sampling frequently in the region where u( , a1 ) and u( , a2 ) are very close, but where suboptimal a2 is marginally better than a1 . An intuitive remedy to this problem is provided by an im-







portance sampling approach (see e.g. (Geweke 1989)) where we sample more heavily in regions as indicated by distribution q () and then reweight the Monte Carlo expectation to provide an unbiased estimate of U (a). Formally, the theory of importance sampling tells us that since  u( , a)p( ) U (a) = q ( )d , (4) q ( ) we can draw samples from q to compute an (unbiased) estimate of U (a) as n (a) = 1 U n



n







i=1







u( i , a)p( i ) , q ( i )







 i  q.







(5)







This leaves us with one key question to answer in this paper: How can we automatically derive a q ( ) to increase the probability that the optimal action a is selected for a finite set of n samples? Answering this question is important for real-time applications of Bayesian decision-theoretic approaches where efficiency and optimality are two key operating criteria. To this end, in the subsequent section we derive an optimal proposal distribution for a loss-calibrated Monte Carlo importance sampler. To do this, we first show connections between regret and the probability of non-optimal action selection and then further connect an upper bound on the latter to the variance of our expected utility estimator. We are then able to derive an optimal proposal distribution q that minimizes this variance through the calculus of variations. We evaluate our loss-calibrated Monte Carlo method in two domains. We first examine synthetic plant control examples building on those of (Lacoste-Julien, Huszar, and Ghahramani 2011), who were also motivated by losscalibration in Bayesian decision theory, albeit not in the case of Monte Carlo methods as we focus on in this work. We also demonstrate results in a Bayesian decision-theoretic robotics setting with uncertain localization motivated by the work of (Thrun 2000). In both empirical settings and in states with up to 100 dimensions, we demonstrate that using our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by conventional loss-insensitive samplers to achieve the same level of accuracy. This suggests a new class of loss-calibrated Monte Carlo samplers for efficient online Bayesian decision-theoretic action selection.







Since the samples are drawn randomly from q , a n is a random variable and so is its expected utility U ( an ). As such, we use E, P and V henceforth to denote the expectation, probability and variance operators. We emphasize that all random variables are determined by q . In principle, we would like to select the distribution q to minimize regret, i.e. maximize the true EU of the estimated action a n . As this is challenging to do directly, we proceed in three steps: 1. We establish a connection between regret and the probability of non-optimal action selection in Theorem 1. 2. Since calculating the probability of selecting the nonoptimal action is intractable to be directly minimized, we derive an upper bound in Theorem 3, based on the variance of the difference of estimated utilities. 3. Theorem 5 shows how to calculate the distribution q to minimize this bound.







Minimizing regret



To find the optimal estimated action a n with fewer samples, we wish to select q that minimizes the regret. Formally we define this as



min



q







( an )







=







E [U (a ) - U ( an )] .







(7)







Direct minimization of Equation 7 is difficult, hence we bound it with the probability of selecting a non-optimal action instead. Tightening this bound with respect to q will lead to a practical strategy. It is detailed in the following theorem. Theorem 1 (Regret bounds). For the optimal action a and its estimate a n the regret as defined in Equation 7, is bounded as  P [a = a n ]  ( an )   P [a = a n ] , where  = U (a ) - maxa mina A U (a ). P [a = a n ] U (a ) +



aA\{a }  A\{a } 







(8)







U (a ) and  = U (a ) -







Proof. We know E [U ( an )] is equal to P [a = a n ] U (a) P [a = a n ] min U (a )



aA\{a } a A a A







P [a = a n ] U (a ) +







=P [a = a n ] U (a ) + P [a = a n ] min U (a ). This is equivalent to stating that ( an )   P [a = a n ] after some manipulation. Similarly, we have that



E [U ( an )]  P [a = a n ] U (a ) + P [a = a n ]



a A\{a }







Loss-calibrated Monte Carlo Importance Sampling



In many applications of decision theory, sampling is the most time-consuming step. Since we know these samples are ultimately used to estimate high-utility actions, we are interested in guiding the sampler to be more efficient for this task. Here, we will pick a distribution q to draw samples from, which are in turn used to select the action that maximizes the EU. The estimated optimal action a n from n samples is defined as n (a). a n = arg max U (6)



a







max







U (a )







which leads to ( an )  P [a = a n ] . The bound is very intuitive: minimizing the probability of the estimated optimal action a n being non-optimal will lead to a bound on the regret. Clearly, for two actions we have  = . Thus, in the two-action case, minimizing the probability of selecting a non-optimal action is equivalent to maximizing the expected utility of the selected action. With more actions, these objectives are not equivalent, but we can see that the difference is controlled in terms of  and .







Minimizing the probability of non-optimal action



We now turn to the problem of minimizing P [a = a n ]. In the following, Lemma 2 provides a bound on the probability of non-optimal action. Further details and another view of the same problem with slightly better bounds is provided in supplementary material. In the subsequent lemma, we upper bound the indicator function with a smooth and convex upper bound that will be easier to minimize. The use of surrogate function for minimizing indicator has also been used in similar problems (see e.g. (Bartlett, Jordan, and McAaliffe 2006)). Lemma 2. For an optimal action a and its estimate a n obtained from sampling, we have t > 0,



P [a = a n ] 



a=a a =a















E







n (a) - U n (a ) + 1 t U







2







.







The critical feature of Equation 9 is that all terms on the RHS other than the variance are constant with respect to the sampling distribution q . Thus, this theorem suggests that a reasonable surrogate to minimize the regret in Equation 7 and consequently maximize the expected utility of the estimated optimal action is to minimize the variance of the difference of the estimated utilities. This result is quite intuitive -- if we have a low-variance estimate of the differences of utilities, we will tend to select the best action. This is aligned with the importance sampling literature where it is well known that the optimal distribution to sample from is the one that minimizes the variance (Rubinstein 1981; Glasserman 2004). Our analysis shows the variance of the function that has to be minimized is of a particular form that depends on the difference of the utilities (rather than each utility independently).







Proof. Firstly, decompose P [a = a n ] as



P[a = a n ] 



a=a a=a a =a







Optimal q



We established that to find the optimal proposal distribution q  (i.e. optimal q ), we minimize the sum of variances obtained from Theorem 3. Since a is unknown, we sum over all actions in A, rather than just A\{a }. Since everything except variance in Equation 9 is independent of q , we formulate the objective    min V Un (a) - Un (a ) s.t. q ( )d = 1.



q aA a A\{a}







n (a) > U n (a )] P[U n (a) > U n (a ) E I U



a=a a =a







=







.







Applying the inequality that I[v > 0]  (tv + 1)2 gives the result.







The next theorem then relates the value inside the above expectation to the variance, thus bonding the probability of incorrect action selection by the sum of variances. Theorem 3 (Upper bound on the probability of non-optimal actions). We have the following upper bound on the probability of non-optimal action selection for k actions in set A, n (a) obtained true expected utility U (a) and its estimation U from finite samples:



P [a = a n ] 



a=a a =a







1 + 2t U (a) - U (a )



2







(10) Here, the constraint on q is to ensure the resulting solution is a proper probability distribution. The following theorem provides the solution to the optimization problem in Equation 10 that we are interested in. Theorem 5. Let A = {a1 , . . . , ak } with non-negative utilities. The optimal distribution q  ( ) is the solution to problem in Equation 10 and has the following form:



q  ( )  p( )



aA a A\{a}







(u( , a) - u( , a ))2 .







(11)







n (a) - U n (a ) + t2 U (a) - U (a ) + t2 V U







, (9)







Proof. Expand the quadratic in Lemma 2 and use E[X 2 ] = n (a) - U n (a )] = U (a) - U (a ). V[X ] + E[X ]2 and E[U In general, one would like to select the constant t to minimize this bound. As this depends on the variance, which is a function of q and the number of samples n, this is difficult to do analytically. However, we expect the variance to decrease as n increases, so we instead derive a value for t that leads to an asymptotically tight upper bound. Lemma 4. For the bounds detailed in Theorem 3, if the variance term is zero, the value of t that minimizes the upper bound is a=a a =a U (a ) - U (a) t = 2. a=a a =a (U (a) - U (a )) Proof. In general, the value of t minimizing 2ta + t b is t = -a/b.



2







Proof. Since we know V[X ] = E[X 2 ] - E[X ]2 , for computing the objective in Equation 10 the second expectation 2 becomes (U (a) - U (a )) and is independent of q , then we only need to minimize E



n (a) - U n (a ) U



2







. Consider this







value for a particular pair (a, a ). Denoting ( i , a, a ) = u( , a) - u( , a ), this is equal to  q ( 1,...,n )  1 = 2 n



n







1 n



n







n







i=1







( i , a, a )p( i ) q ( i )







2







d 1,...,n







( i , a, a )( j , a, a )p( i )p( j ) q ( i )q ( j ) i=1 j =1 x q ( 1 , . . . ,  n )d 1,...,n .







Since all the samples are independent, q ( 1 , . . . ,  n ) = q ( 1 ) . . . q ( n ). Now if i = j , it is easy to see that q vanishes and those terms become independent of q . If i = j however, only one of the terms in the denominator cancels out with the







joint. Also because the sum is over similar terms, we have n times the same expression, leading to the Lagrangian of



1 n 



aAa A\{a}







( , a, a )2 p( )2 d +  q ( )







 q ( )d  - 1 .







Taking the derivative with respect to q ( ), we have that



- 1 n ( , a, a )2 p( )2 +=0 q ( )2







aA a A\{a}







which concludes the theorem since n only induces a proportionality constant. This is quite intuitive - the samples  will be concentrated on regions where p( ) is large, and the difference of utilities between the actions is large, which is precisely the intuition that motivated our work in Figure 1. This will tend to lead to the empirically optimal action being the true one, i.e. that a n approaches a . In practice, the normalization constants for p and q are likely to be unknown, meaning that direct use of Eq. 5 is impossible. However, there are well-known self-normalized variants that can be used in practice with p()  p ( ) and q ( )  q ( ), namely p ( i ) i  q . q ( i ) i=1 i=1 (12) This simply means that for the case of unnormalized p  and 1 terms cancel, the summed utility values q  and letting the n have to now be reweighted by the slightly more complex p ( j ) n ( i ) value of p j =1 q q ( i ) ( j ) . Furthermore, as it is hard to directly sample q , we must resort to Markov Chain Monte Carlo (MCMC) methods (Neal 1993), e.g. Metropolis-Hastings (MH). This disregards an important aspect, namely that the samples we obtain for q are not truly independent. Rather, the number of effective samples are affected by the mixing rate of the Markov chain. Our derivation above does not account for these mixing rates, which could be important in many applications. For this reason, our experiments will distinguish between two settings: First, one can run an extremely long chain, and subsample from this, approximating nearly independent samples as in the derivation above, which we call Subsampled MC. Second, one can run a single Markov chain, as would be typical in practice, which we call Sequential MC. u( i , a) n (a) = 1 U n



n







each action. In case direct sampling is not possible we use Metropolis-Hastings MCMC by initializing the chain at a random point and using a Normal distribution centered at the current sample with isotropic covariance optimally tuned so that around 23% of samples are accepted (Roberts, Gelman, and Gilks 1997). In each experiment n samples are generated 200 times and the mean of the percentage of times the true optimal action is selected is reported. We include two diagnostics for MCMC samplers: in the first one (Subsampled MC) we have generated a large chain of 100000 samples and selected random subsamples to compute the best action using the empirical expected utilities n (a). Since samples drawn from Markov chains are typU ically correlated, this diagnosis will help ensure samples are independent. In the second diagnostic (Sequential MC) we draw samples sequentially from a single Markov Chain started at a random point to calculate the expected utilities for selecting the best action.







Power-plant Control



We consider a power plant where the temperature of the generator has to be maintained in a safe range following the example from (Lacoste-Julien, Huszar, and Ghahramani 2011); the only actions available to achieve this are to turn the generator on or off. Suboptimal action choices that keep the generator on in high temperatures or turn it off in unnecessary cases when the temperature is safe and no maintenance is required lead to financial loss for the power plant and should be avoided. For this problem, we can model the distribution of the temperature and use a high utility for cases where a safe action of turning the generator on or off is taken, formally,



u( , a = on/off) = Ha La ca,1 <  (d) < ca,2 otherwise



(d) (d)







p ( i ) q ( i )







1 n







n







(13)







where  (d) is the d-th dimension of the temperature, Ha , La (for action a  on/off) is the reward for the given ac(d) (d) tion a in the temperature intervals defined by ca,1 and ca,2 . We use three distinct one dimensional utilities for simu(1) (1) (1) lations: (i) c(1) on,1 = 15, con,2 = 20, coff,1 = 15, coff,2 =



21, Hon = 6.5, Hoff = 5, Lon = 1.5, Loff = 4; (ii) con,1 =



(1) 45, con,2 (1) (1)







=







(1) 50, coff,1







3, Lon = 1.5, Loff =







(1) = 35, coff,2 (1) 2.5; (iii) con,1







= 40, Hon = 6.5, Hoff = = 5, con,2 = +, coff,1 =



(1) (1)







-, coff,2 = +, Hon = 5, Hoff = 3, Lon = 2.5, Loff = 3.







Applications



As discussed earlier, many applications require optimal actions to be selected efficiently given known (but complex) p and u. In this section we provide applications and evaluate how well the samples drawn from p and q  compare. In these simulations we are interested in finding the optimal action, i.e., the one that maximizes the expected utility, with the minimum number of samples. As such we generate samples from the true distribution p and the proposed optimal distribution q  (obtained from Theorem 5 as per the application's specifications) and compute the expected utilities for







Corresponding to each utility, the following three distributions are used to demonstrate how the samples drawn from p and q  obtained from Theorem 5 perform in selecting the optimal action:



(i) p( ) = 0.7  N ( (1) ; 3, 7) + 0.3  N ( (1) ; 12, 2) where N ( (1) ; ,  2 ) is a normal distribution with mean  and variance  2 ; (ii) p( ) = 0.05  N ( (1) ; 3, 1) + 0.2  N ( (1) ; 6, 1) + 0.05  N ( (1) ; 10, 3) + 0.3  N ( (1) ; 15, 2) + 0.05  N ( (1) ; 20, 7) + 0.1N ( (1) ; 25, 2)+0.05N ( (1) ; 30, 3)+0.2N ( (1) ; 40, 5) (iii) a log-normal distribution p( ) = Log-N ( (1) ; 0, 1).







u( , a = on/off)



6 Utility Value 5 4 3 2 -10 0 10  20 30



0.035







distribution of p and q 



% Optimal Action Selected 100 90 80 70 60 0







Subsampled MC



% Optimal Action Selected 100 90 80 70 60 0







Sequential MC







u( , on) u( , off)



Probability







0.05 0.04 0.03 0.02 0.01 -10 0 10  20 30







p q* 500 1000 1500 Markov Chain Length 2000







500 1000 1500 No. Subsamples







2000







% Optimal Action Selected







0.03 Probability 0.025 0.02 0.015 0.01 0.005







Utility Value







5 4 3 2 0



5







80 60 40 20 0 100 80 60 40 20 0 500 1000 1500 No. Subsamples 2000







% Optimal Action Selected







6







u( , on) u( , off)







100







100 80 60 40 20 0 100 80 60 40 20 0 p q* 500 1000 1500 Markov Chain Length 2000 500 1000 1500 Markov Chain Length p q* 2000







20 







40







60



u( , on) u( , off) Probability 0.12 0.1 0.08 0.06 0.04 0.02







0







20 







40







60







% Optimal Action Selected







Utility Value







4.5 4 3.5 3 20 40 60  80 100







20







40







60 







80







100







500 1000 1500 No. Subsamples







2000







Figure 2: Power-plant simulations: the step-valued utility function (as in Equation 13) in the first column, the true distribution p (in blue) and q  (in red) in the second column and in the third and forth columns the result of performing Subsampled MC and Sequential MC (as described in the text) are shown. In the two right-hand columns, note that q  achieves the same percentage of optimal action selection performance as p in a mere fraction of the number of samples. In Figure 2, the utility functions are shown in the first column (with black indicating the utility of on as detailed in Equation 13) and the temperature distributions (p in blue as described above and q  in red) in the second column are shown. In the third and fourth columns the result of performing Subsampled MC and Sequential MC of the Metropolis-Hastings sampler for selecting the best action is shown such that the x-axis represents the number of samples and the y-axis shows the percentage of times the correct optimal action is selected. Here, in general, we observe that a significantly smaller number of samples from q  is needed to select the best action in comparison to the number of samples from p required to achieve the same performance. To investigate the performance of samples from p and q  in higher dimensions, we use a d-dimensional Gaussian mixture corresponding to temperatures at each point in the plant as p() = N (; 10, ) + N (; 20, ) where 10 and 20 are d-dimensional vectors with constant value 10 and 20 as the mean and i,j = 5 + I[i = j ] and i,j = 3 + 7I[i = j ] as d x d covariance matrix. In addition, the utility function in d) (d) (d) Equation 13 is specified with c( on,1 = 23, con,2 = 25, coff,1 =



20, coff,2 = 22, Hon = 50d, Hoff = 13, Lon = 1.1, Loff = 1.5 log(d). In Figure 3 for d  {2, 4, 10, 20, 50, 80, 100}, we



(d)







ficient. In fact, for a 100-dimensional bimodal Gaussian we are unable to find the optimal action using only 200 samples from p, which should be contrasted with the significantly improved performance given by sampling from q  .







100 % Optimal Action Selected 80 60 40 20 0 % Optimal Action Selected p q*







% Optimal Action Selected







100 80 60 40 20 0 p q*







0







20







40 60 Dimension







80







100







0







20







40 60 Dimension







80







100







(a) Subsampled MC







(b) Sequential MC







observe that in an average of 100 runs of the MCMC with 200 samples, as the dimensions increase using q  is more ef-







Figure 3: Performance of the decision maker in selecting the best action as the dimension of the problem increases in the power-plant. Note that at 100 dimensions, p is unable to select the optimal action whereas q still manages to select it a fraction of the time (and would do better if more samples were taken).







Assuming a deterministic transition dynamics model



 = T ( , a) and denoting (T ( x , a), T ( y , a)) as the loca-







tion of the robot after taking action a from state  (that is, moving from the current location in the direction of the the selected action heading) and Rr the set induced by region r, we use the following utility function:



  H u( , a) = L  M (a) Environment's Map



% Optimal Action Selected % Optimal Action Selected 100 90 80 70 60 0 500 1000 1500 No. Subsamples 2000 100 90 80 70 60 0 500 1000 1500 Markov Chain Length p q* 2000







(T ( x , a), T ( y , a))  Rcharger (T ( x , a), T ( y , a))  Rstair , otherwise







(14)







(b) Subsampled MC







(c) Sequential MC







where L < M < H (in our experiments: L = 1, M = 10, H = 400) and a  {forward, backward, right, left}. Using distribution q  from Theorem 5 as illustrated in Figure 4a, the samples from q  (in red) concentrated on the charger's location which has higher utility value compared to the samples from p (in blue) that are from the mode of the distribution. As shown in Figure 4b and 4c, using distribution q  and running the same diagnostics as the previous experiment we see significant improvement in selection of the optimal action, requiring only a fraction of the samples of p to achieve the same optimal action selection percentage.







Figure 4: A robot's internal map showing the samples taken from its true belief distribution p (two modes are shown in blue, the second one is slightly obfuscated by the robot) and the optimal sampling distribution q  derived by our loss calibrated Monte Carlo importance sampler in 4a. In 4b and 4c we see the performance (in terms of percentage of optimal action selected) of our loss-calibrated sampling method using q  leads to near immediate detection of the optimal action in only a few samples.







Conclusion and Future Work



We investigated the problem of loss-calibrated Monte Carlo importance sampling methods to improve the efficiency of optimal Bayesian decision-theoretic action selection in comparison to conventional loss-insensitive Monte Carlo methods. We derived an optimal importance sampling distribution to minimize the regret bounds on the expected utility for multiple actions. This, to the best of our knowledge, is the first result linking the utility function for actions and the optimal distribution for Monte Carlo importance sampling in Bayesian decision theory. We drew connections from regret to the probability of selecting non-optimal actions and from there to the variance. We showed using an alternative distribution as derived in Theorem 5 will sample more heavily from regions of significance as identified by their sum of utility differences. Empirically, we showed that our loss-calibrated Monte Carlo method yields high-accuracy optimal action selections in a fraction of the number of samples required by lossinsensitive samplers in synthetic examples of up to 100 dimensions and robotics-motivated applications. Future work should investigate the extension of the novel results in this work to the case of (a) continuously parameterized actions (Alessandro, Restelli, and Bonarini 2007), (b) imprecise utility functions (e.g, when the return of a state is not known precisely, but can be sampled) (Boutilier 2003), (c) uncontrollable sampling (where the utility partially depends on auxiliary variables that cannot be directly sampled from) and (d) applications in active learning and crowdsourcing (Beygelzimer, Dasgupta, and Langford 2009). Furthermore, the bounds obtained here are not tight in the multiaction setting and can be improved in future work. Altogether, this work and the many avenues of further research it enables suggest a new class of state-of-the-art loss-calibrated Monte Carlo samplers for efficient online Bayesian decision-theoretic action selection.







Robotics



Another application where sampling has been commonly used is localization in robotics (Thrun 2000). It is a risksensitive-decision making problem where the robot is rewarded for navigating to the charger in order to maintain its power but wrong actions may lead to catastrophic incidents like falling down the stairs or crashing into obstacles. Due to minimal resources on-board a robot and the nature of the real-time localization problem, it is crucial for the robot to be able to select the optimal action rapidly, yet safely. The state of the robot is the combination of its coordinates on a map and its heading direction. In our example for these experiments, we use a three dimensional Gaussian belief state distribution with two locations in a room intended to model that a robot's belief update has been confused by symmetries in the environment: one mode is at the robot's true location and the other at the opposite end of the room. In this experiment, we consider a map as shown in Figure 4a where there is a flat in-door environment that the robot can move by selecting one of the four actions forward, backward, right or left. This action will lead to a movement step in robot from the current point on map with the heading direction towards the selected action. In doing so however, the robot has to avoid the stairs (low utility region) and select the charging source (high utility region).







Acknowledgements



NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.







References



Alessandro, L.; Restelli, M.; and Bonarini, A. 2007. Reinforcement learning in continuous action spaces through sequential monte carlo methods. In Advances in Neural Information Processing Systems. Bartlett, P.; Jordan, I. M.; and McAaliffe, J. D. 2006. Convexity, classification, and risk bounds. Journal of the American Statistical Association 101(473):138-156. Berger, J. 2010. Statistical Decision Theory and Bayesian Analysis. Springer, 2nd edition. Beygelzimer, A.; Dasgupta, S.; and Langford, J. 2009. Importance weighted active learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, 49-56. New York, NY, USA: ACM. Boutilier, C. 2003. On the foundations of expected expected utility. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI'03, 285-290. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. Gelman, A.; Robert, C.; Chopin, N.; and Rousseau, J. 1995. Bayesian Data Analysis. CRC press. Geweke, J. 1989. Bayesian inference in econometric models using monte carlo integration. Econometrica 57(6):1317- 1339. Glasserman, P. 2004. Monte Carlo Methods in Financial Engineering. Applications of Mathematics. Springer, 1st edition. Lacoste-Julien, S.; Huszar, F.; and Ghahramani, Z. 2011. Approximate inference for the loss-calibrated bayesian. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), volume 15, 416-424. Neal, R. M. 1993. Probabilistic inference using markov chain monte carlo methods. Technical report, University of Toronto, University of Toronto. Robert, C. 2001. The Bayesian Choice. Springer Texts in Statistics. Springer, 2nd edition. Roberts, G. O.; Gelman, A.; and Gilks, W. R. 1997. Weak convergence and optimal scaling of random walk metropolis algorithms. The Annals of Applied Probability 7(1):110- 120. Rubinstein, R. Y. 1981. Simulation and the Monte Carlo Method. John Wiley & Sons, Inc., 1st edition. Thrun, S. 2000. Probabilistic algorithms in robotics. AI Magazine 21:93-109.







From: AAAI-88 Proceedings. Copyright (c)1988, AAAI (www.aaai.org). All rights reserved.







Robust Operative Diagnosis sthesis Space as Problem Solving in a



Kathy W. Abbott NASA Langley Research Center Hampton, Virginia 23665-5225







Abstract



The lack of robustness in current diagnostic systems is an important research issue because it has two major consequences: inability to diagnose novel faults and inability to diagnose more than one type of fault. This paper describes an approach that formulates diagnosis of physical systems in operation (operative diagnosis) as problem solving in a hypothesis space. Such a formulation increases robustness by: (1) incremental hypotheses construction via dynamic inputs, since the fault propagation results in changes in symptoms over time; (2) reasoning at a higher level of abstraction to construct hypotheses, albeit less specific ones, when specific knowledge is not available; and (3) partitioning the space by grouping fault hypotheses according to the type of physical system representation and problem solving techniques used in their construction. The approach was implemented for aircraft subsystems and evaluated on eight actual aircraft accident cases involving engine faults, with very promising results.







%







Introduction







The lack of robustness in current diagnostic systems is an important research issue because it has two major consequences: inability to diagnose novel faults and inability to diagnose more than one type of fault. For example, most current approaches to diagnosis depend on compiled, specific knowledge about the associations between symptoms and faults. However, when novel faults occur for which there is no specific associational knowledge, approaches that depend on such knowledge are inadequate. When the diagnosis is done for physical systems in operation (operuti-ue diagnosis), it is even more important to diagnose novel faults because the cost of inappropriate responses may be high. The purpose of operative diagnosis is to facilitate continued, safe operation, rather than identifying the part to repair. Moreover, identifying the eflects of the fault on the status of the physical system is equally as important as identifying the cause of the fault. In operative diagnosis, determining system status is often a dynamic process, as the effect of the fault propagates while the system continues to operate. Therefore, the operative nature of the diagnosis affects the reasoning in two ways: the need to reason about dynamic inputs and to generate system status. Another important consideration is that testing for







additional information is limited because of the need for safe, continued operation. Limited testing means that information available to discriminate hypotheses is less than sometimes desired. Moreover, sensed parameters are not available for every component in the system, and these sensor readings are sampled at (usually fixed) intervals. The set of symptoms may change because of fault propagation, and some changes may be undetected between samples. Much research has been done in diagnosis. Several of these approaches diagnose known faults where the effect of the fault propagates. For example, [Fagan et ad., 1984; Patil, 1987; Weiss et al., 1978; Pan, 19831 address diagnosis of known faults. Although these and other research efforts address the problem in much depth, they do not address novel faults. The fragility of these systems motivated several current approaches that use deep models in the diagnosis process [Fink and Lusth, 1987; Davis, 1985; Hamilton e$ ad., 19861. These model-based approaches generate hypotheses that identify the cause of the problem, (e.g., the faulty component), but not the system status. While this may be sufficient in cases where all the diagnostician needs to do is identify the part to replace, it assumes that no other parts need to be replaced or repaired as a result of the fault. Additionally, because they only use functional models, they cannot diagnose failures where one component damages another physically-adjacent component. Their capability to use multiple physical system representations is limited or nonexistent. Diagnosing some faults requires multiple representations [Davis, 19851, although even Davis' approach cannot generate system status or combine representations. This paper describes an approach that views diagnosis as problem solving in a hypothesis space. This view enables an improvement in robustness through incremental hypothesis updates, and the abstraction and partitioning of the hypotheses in the space. Incremental hypothesis updates enable diagnosis of dynamic fault behavior caused by fault propagation. Within the hypothesis space, the approach uses specific associational knowledge when available. However, when novel faults occur, the diagnostic problem solver uses abstraction of the individual hypotheses to provide a diagnosis, albeit a less specific diagnosis. The hypothesis space is partitioned into fault classes, grouping faults into different classes if their behavior requires different problem solving techniques or representations to diagnose them. Other diagnostic approaches can be viewed as diagnosing a subset of the classes included here. This approach was implemented in a computer program called Draphys (aagnostic Reasoning About Physical Systems) and demonstrated in the domain of aircraft sub-







Abbott







369







3.2







Diagnosis







of Known







Faults







Fall Compressor Combustor Turbines







Figure 1: Aircraft Turbofan Engine. systems; specifically, an aircraft turbofan engine and hydraulic subsystem. The approach was evaluated using actual aircraft accident cases involving engine faults, with very promising results.







Draphys uses compiled associational knowledge to diagnose known, commonly occurring faults. For the aircraft domain, the fault-symptom associations were obtained by interviewing domain experts (pilots and engine designers) and by examining actual fault cases. They were implemented in a rule-based system that permits the temporal functions defined by Allen [Allen, 19841 as part of the rules. These rules can adequately capture the sequence of symptoms as described by the experts, but have representational limitations as discussed in [Abbott et al., 19871. These include the awkwardness of expressing all the propagation behavior over time that could occur for any one fault, as well as temporal duration. The major question addressed below is what to do if the fault is one whose symptoms do not correspond to the associational knowledge. This question is of great interest, since novel faults appear to be very difficult for humans to diagnose.







3.3







Graceful egradation Abstraction







Via







2







Hypotheses Diagnosis







in Operative







In operative diagnosis, the diagnosis is done to assist in continued operation of the system under consideration. Each element of the diagnosis problem space is a hypothesis that describes the cause of the fault and the current system status. In Draphys, a hypothesis includes: the fault type, the cause or source of the problem, the propagation path, and the system status. The fault type is either single fault or multiple independent faults. The source of the fault is the physical component that is broken. The specific cause describes how that component is broken. The propagation path describes the order in which the fault affected the components. The system status describes the components affected by the fault and their operational status. The operational status of an affected component is either definitely aflected by the failure when symptom information justifies it, or possibly u$ected when there is reason to believe that the component might be affected but symptom information cannot confirm or refute it.







3



3.1







Diagnokis as Problem Solving in a Hypothesis Space



Aircraft Subsystem Diagnosis







Inflight diagnosis of aircraft subsystems is an example of operative diagnosis. The aircraft subsystems diagnosed by Draphys are two turbofan engines, two fuel subsystems, and a hydraulic subsystem. A schematic of the engine used in later examples is shown in figure 1. The input to the diagnosis system is a set of qualitative sensor values that identify which sensors are abnormal and how they are abnormal, e.g., fuel flow is high. A fault monitor generates these symptoms by comparing the sensor readings to expected values computed from a numerical simulation model of, for example, the engine. Schutte [Schutte and Abbott, 19861 describes the fault monitor.







Much of the related research views graceful degradation in the presence of novel faults as reasoning with deep models. In such a view, it is the eficiency of the reasoning process that degrades. The approach presented here does not view graceful degradation as an issue of degraded efficiency, but an issue of degraded specificity. If the diagnostic system cannot identify exactly what the fault is, it can still generate useful diagnostic information, even if that information is less specific than desired. Before presenting the approach, it is useful to explore what information should be abstracted and why. If the goal of the diagnostician is to select a remedial action to take in response to the fault, information should be generated to support that selection. During the interviews of experts, they described default actions that they would take if they did not recognize the fault or if there were multiple hypotheses. This action was generally a conservative response to the fault. For example, if the pilot knew he had a compressor failure, but did not know how the fan was broken, he would shut down the engine. However, if he knew it was an eroded compressor blade, he might reduce the throttle on that engine. Thus he had an action associated with the general class of compressor failures that was (potentially) d ifferent from the action associated with the specific compressor fault. Motivated by this and other examples, a structured way of forming general categories of faults with associated default actions was identified. In the aircraft domain, these categories are defined as the components in the physical system, as exemplified above. When novel faults occur, diagnostic reasoning takes place at a higher level of abstraction. Hypotheses are produced that identify what component is faulty, without identifying how the component is broken. The operational status of the component that is abstracted (e.g., abnormal rather than low pressure), so Draphys uses this abstraction is called status abstraction. two such levels, shown in figure 2. Since the diagnostic reasoning at the higher abstraction level is designed to identify the component that is faulty, the symptoms can be abstracted as well. Although it is







370







Common Sense Reasoning







HYPOTHESIS







1 OF 2







HYPOTHESIS







2 OF 2







Current Symptoms: N 1 Abnormal Fault Type: Single Fault Propagation Path And Component Status:







Current Symptoms: Nl Abnormal Fault Type: Single Fault Propagation Path And Component Status:







Propagation Type: Functional







Propagation Type: Functional







abstracts1







to







Responsible abstracts to abstracts to a







Component







Definitely Affected







Figure 3: Hypotheses Resulting From a Symptom in Ni. would be symptomatic to reflect this. The turbines would not be extracting energy, so the fan and compressor would not turn as fast since they derive their power from the turbines. Thus the faulty response is perpetuated. For this fault, suppose that the first symptom that Draphys detects is in Ni. Since Ni is an engine parameter, Draphys is able to localize the fault to the engine subsystem. Each component in the engine subsystem is then proposed as a candidate responsible component. For each proposed responsible component, Draphys generates a fault hypothesis by qualitatively simulating the fault propagation behavior. For example, when Draphys proposes the fan as the responsible component, it uses a model of the engine and its functional interconnections to determine that the high-pressure compressor and the Ni sensor functionally depend on the fan. Knowing these interconnections, Draphys then attempts to continue simulating the propagation of the failure to these functionally dependent components. In this example, it checks whether the fault' s effect has reached the high-pressure compressor by examining the symptoms to see if N2 is symptomatic. If it is, then Draphys assumes that the failure affects the high-pressure compressor, and continues the process from there. If N2 is not symptomatic, as in this example, simulated propagation halts on this path. Draphys then explores all remaining functional propagation paths. After exhausting all paths, the hypothesis is tested for validity. Draphys does the same process for each candidate component. In this example, two valid hypotheses are generated, shown in figure 3. The first is that the fan is the responsible component, and the second is that the Ni sensor failed. A fault in either component could result in the current symptoms. Extending this example illustrates the incremental updating of hypotheses. Assume that a short time after the Ni symptom was first detected and diagnosed, a symptom in N2 is detected. Draphys then tries to extend the propagation path of all the valid hypotheses to explain the new symptoms by continuing the qualitative simulation from the end of the propagation path in the old hypotheses. For instance, in one valid hypothesis propagation stopped at the fan, because the next component on this functional propagation path was the high-pressure compressor. Since earlier there was no symptom in N2, Draphys assumed that the compressor was unaffected. Now that there is a







Figure 2: The Two Levels of Status Abstraction in Drapb







necessary at the lower level to identify how the symptomatic sensor compares with its expected value (e.g., high or low), it is not necessary to make this distinction at the higher level. It is only necessary to identify that the value of the sensor is abnormal. This is also status abstraction, but it is the parameter value status that is abstracted. Figure 2 also illustrates the relationship between the specific fault hypotheses and the corresponding symptoms. The reasoning at the higher level of abstraction is a generate-and-test process. When symptoms first appear, the generator localizes the fault in a component hierarchy, resulting in a set of candidate components that might be the source of the problem. It then constructs fault hypotheses by simulating fault propagation from each of the candidates. Each resulting hypothesis is then tested to determine if it is valid; that is, if it accounts for all the current symptoms. Often this generate-and-test process results in multiple valid hypotheses. If new symptoms arrive as time progresses, the generator incrementally updates the old hypotheses to determine whether they can account for the new symptoms. If they can, the generator retains them. Otherwise, it prunes them. An example will clarify this process. Suppose the fault is a fan failure. In such a failure, the first sensor affected would be Ni. Since the fan would not compress air properly, the effect of that failure would propagate to the high-pressure compressor and thus to N2. It would then propagate to the combustor since the under-compressed air would not ignite as efficiently. Therefore, the expanding gases resulting from combustion would not turn the turbines as rapidly as it normally would. EGT and EPR







Abbott







371







HYPOTHF,SlS 1 OF 1



Current Symptoms: N2 Abnormal Fault Type: Single Fault Propaqation Path And Component Status:







kIYPOTHFSIS



Current Symptoms:







1 OF I.



:.:.:.X.X.&$+ Functional 4 Physical Propagation Propagation







Ni Abnormal N2 Abnormal Hydraulic Pressure Abnormal







Responsible Component Defintely Affected Possibfy Affected







Faulf Type: Single Fauft







Propagation







Type: Functional







Figure 4: Hypothesis Remaining After a Symptom in N2.



Propagation Type: Hybrid







symptom in N2, Draphys updates the system status for this hypothesis and continues the simulated propagation. The resulting hypothesis accounts for all symptoms. It is the only member of the set of old valid hypotheses that can do so, since a sensor failure in N1 could not result in functional propagation that would account for the symptom in Nz. Figure 4 shows this remaining hypothesis.



3.3.1 Using Multiple Representations Physical System







Figure 5: Composed Hypothesis Explaining a Fan Blade Separation.







The reasoning based on the functional model is sufficient for the faults that propagate along functional dependency links, but not all faults do. Suppose that the fan blade broke off and damaged a hydraulic line in the wing to which the engine was attached. The monitor detects symptoms in N1 and in the hydraulic pressure sensor. Draphys cannot explain these symptoms by simulating functional propagation, because there is no functional relation$hip between I these components. A physical proximity relationship does exist. Therefore, by knowing that the fan is physically adjacent to the wing containing the hydraulic line, Draphys can identify propagation from the engine to the wing. This represents another class of faults, since it requires a different representation (physical rather than the functional structure). This type of fault is analogous to Davis' bridge fault [Davis, 19851. The reasoning process used is the same as described with faults that propagate functionally, except that the models used in localization and simulation are based on physical structure rather that functional structure. The component hierarchy used for localization groups components according to physical location rather than functional relationships. The simulation model used is a specialized model of physical proximity. This specialized model includes directional information in representing these physical proximity relationships. For instance, it is possible for the fan blade to break off and damage the hydraulic line, but not vice versa. Unfortunately, reasoning with a single representation is not sufficient. Once a fan blade separation has caused damage in both the engine and in the hydraulic system,







the effect of the fault will propagate functionally in both subsystems. The initial propagation was physical, but subsequent propagation was functional. Therefore, explaining the current fault behavior requires models of both physical and functional structure. Draphys diagnoses hybrid fault propagation by composing the simple hypotheses that describe the single type of propagation, as illustrated in figure 5. Faults involving physical damage illustrate that some known faults are more appropriately represented at the higher level of abstraction. The reasoning described for diagnosing physical damage could be compiled into specific rules, but doing so may not improve ability to take remedial action. Moreover, physical damage can occur so many different ways that a large number of specific rules would result, possibly inhibiting their timely retrieval.



3.3.2 Partitioning the Hypothesis Space







So far, four fault classes were described that require different problem solving techniques or different physical system representations. Figure 6 includes these four fault classes, and shows the partitioning of the hypothesis space. The present implementation of Draphys diagnoses all fault classes shown except for multiple faults. The fault classes are examined in order of likelihood and correspond to a depth-first, left-to-right traversal of the space as shown.







4







Evaluation







Draphys was evaluated by reconst rutting actual civil transport aircraft accident cases and using their symptoms as input [a; b; c; d; e; f; g; h]. Each accident was an enginerelated failure that resulted in the loss of life and property. Four of the eight accident cases were used to guide the design and construction of Draphys. The remaining four were set aside for evaluation purposes. All eight were reconstructed by an objective party and presented as input







372







Common Sense Reasoning







Stage 1 Hvbothesis 1. Turbine Blade Separation l l . Turbine Blade Separation 2. Flamaout







Stage 2 Hypothesis 1. Fan 2. Compressor 3. Combustor 4. Turbine







l







2. Fan Failure







1. Turbine Blade Separation 1. Turbine Blade Separation







` l.Fan







3. Fan Failure 4. Foreign Object Ingestion







l







1. Fan Fan Compressor Combustor Turbine







* 1. 2. 3. 4. 1. Turbine Blade Separation 2. Flameout 1. Fuel System Failure 2. Flameout



l







5. Water







Ingestion







1. Combustor 2. Turbine







6. Engine Separation







` 1. Engine







- Fan







Figure 6: Hypothesis Space Partitioning.







7. Turbine







Disk







Separation







1. Turbine Blade Separation







l







1. Combustor 2. Turbine







Each level of abstraction was invoked for to Draphys' . each case to determine the diagnosis success of the associational rules at the specific level and the generate-andtest at the higher abstraction level. The physical system model used contained approximately 40 components and 100 interconnections. A brief summary of the resulting hypotheses (without system status) is shown in table 1. A successful diagnosis was defined as one in which the correct hypothesis was among the set of valid hypotheses. This definition was used because Draphys may generate several valid hypotheses for a particular set of symptoms. It may be impossible to isolate to one hypotheses with the sensor information available, even for a human expert. Moreover, since Draphys does not yet include any representation of uncertainty, the valid hypotheses cannot be ordered by likelihood. Using this criterion for success, seven of the eight accident cases were successfully diagnosed. Of the seven successes, two were diagnosed using the associational rules at the specific level of abstraction. All seven of the successes were diagnosed at the higher abstraction level. Of these seven cases, five involved physical damage. In each of the five cases, functional propagation resulted from the physical damage. No physical damage cases were diagnosed successfully by the associational rules at the specific abstraction level. The accident case that was not successfully diagnosed was not a structural fault. It involved massive water ingestion into the engine during a heavy rainstorm, leading to engine failure. Modifying Draphys to diagnose this failure would require modeling the inputs to a device as a potential source of the fault, which may be a desirable enhancement. Why did this approach work so well? The credit for success lies mainly with two aspects of the approach: the



` 1 am indebted to Paul Schutte for reconstructing the accident cases and doing the initial evaluation as described in [Schutte et aI., 19871.







8. Bearing Failure







1. Turbine Blade Separation 2. Flameout







l l . Compressor *correct diannosis







Table 1: Summary of Accident Case Diagnoses. symptoms detected and the models used. Symptoms provided by the monitor identify abnormal sensor readings as soon as they occur (or the first sample thereafter). This detects symptoms sooner than current operational systems, which alert the operator when a sensor exceeds its total normal operating range. The physical system models used must represent the behavior of the faulted system for the fault class being diagnosed. For example, the functional model represents a model of the normal system, but is at a high enough level of abstraction that it represents behavior under many fault conditions as well. In contrast, the model of physical structure only includes directional proximity information for possible physical damage, thus it does not model normal behavior. Including all nondirectional proximity relationships may be much less efficient and might not incorporate domain knowledge known from the device design about how internal physical damage might occur. In addition to appropriate representations, the ability to combine the physical and functional models was also important.







This paper presented an approach that views diagnosis as problem solving in a hypothesis space. With this view, robustness is improved through reasoning about fault propagation, permitting incremental hypothesis construction; status abstraction of the individual hypotheses, and partitioning of the hypothesis space to group fault hypotheses according to representation and problem solving technique. Incremental hypothesis construction based on fault propagation behavior can be used to discriminate hypotheses, particularly when symptoms change over time. How-







Abbott







373







ever, this requires that the detection process identify when sensor readings become abnormal, not just when they exceed the normal operating range. Abstraction of hypotheses is useful when actions are associated with the general fault categories represented by the abstract hypotheses. The approach of using different abstraction levels for diagnosing novel faults is appropriate when specific hypotheses are most desirable, but abstract hypotheses are better than nothing. Moreover, some known faults are more appropriately represented at the higher level of abstraction, such as, physical damage. This is the caSe when more specific hypotheses do not improve ability to take remedial action or the increase in number of specific hypotheses would inhibit their timely retrieval. Partitioning the fault space is appropriate when different problem solving techniques or representations are required to diagnose different classes of faults. In this approach, differentfault classes and their corresponding diagnostic techniques and representations were identified. One of these classes involved diagnosis of faults which propagate within multiple representations, which no other current approach can do. Evaluation of this approach revealed that diagnostic capability depends on the available physical system models and the fault propagation behavior that they can represent.







4lst Mechanical sium, Ott 1986.







Failure







Preventions







Group







Sympo-







[Pan, 19831 Y-C. Pan. Qualitative







Reasonings With DeepLevel Mechanism Models for Diagnosis of Dependent PhD thesis, University of Illinois, 1983. Failures.







[Patil, 19871 R. Patil. A case study on evolution of system building expertise: medical diagnosis. In Grimson and Patil, editors, AI in the 1980s and Beyond, MIT Press, 1987. [Schutte and Abbott, 19861 P. Schutte and K. Abbott. An artificial intelligence approach to onboard fault monitoring and diagnosis for aircraft applications. In



AIAA Guidance, ence, 1986. Navigation, and Control Confer-







[Schutte et al., 19871 P. Schutte, K. Abbott, M. Palmer, and W. Ricks. An evaluation of a real time fault diagnosis expert system for aircraft applications. In



Proceedings of the 26th IEEE and Control, 1987. Conference on Decision







[Weiss et al., 19781 S. M. Weiss, C. Kulikowski, S. Amarel, and A. Safir. A model-based method for computeraided medical decision making. Artificial Intelligence, 11:145-172, 1978.







Acknowledgements



The research described here is part of the author' s dissertation research at Rutgers University. I thank my advisor, Professor Lou Steinberg, and Professors Chris Tong, Don Smith, and Chuck Schmidt for guidance and suggestions. Peter Friedland, Paul Schutte, and George Steinmetz also commented on a draft of this paper.







bl







tional Transportation 9.







Aircraft Accident Report: United Airlines, Inc., Boeing 737-222, N9005U, Philadelphia International Airport, Philadelphia, Pennsylvania, July, 19, 1970. Na-







Safety Board. NTSB-AAR-72-







bl







Board. NTSB-AAR-75-2.







Aircraft Accident Report: National Airlines, Inc., DC-lo-lo, NGONA, Near AIbuquerque, New Mexico, November 3, 1973. National Transport ation Safety Aircraft Accident Report: Overseas National Airways, Inc., Douglas DC-10-30, Nl032F, John F. Kennedy International Airport, Jamaica, New York, November 12, 1975. National Transportation Safety Board.







References



[Abbott et al., 19871 K. Abbott, P. Schutte, M. Palmer, and W. Ricks. Faultfinder: a diagnostic expert system with graceful degradation for onboard aircraft applications. In 14th International Symposium on Aircraft Integrated Monitoring Systems, Friedrichshafen, West Germany, September 1987. [Allen, 19841 J. Allen. Towards a general theory of action and time. Artificial Intelligence, 23, 1984. [Davis, 19651 Randall Davis. Diagnostic reasoning based on structure and function. In Daniel G. Bobrow, editor, Qualitative Reasoning About Physical Systems, The MIT Press, 1985. [Fagan et al., 19841 L. Fagan, J. Kunz, E. Feigenbaum, and J. Osborn. Extensions to a rule-based formalism for a monitoring task. In B. Buchanan and E. Shortliffe, editors, Rule-Based Expert Systems, AddisonWesley, 1984. [Fink and Lusth, 19871 P. K. Fink and J. C. Lusth. Expert systems and diagnostic expertise in the mechanical and electrical domains. IEEE Transactions on Systems, Man, and Cybernetics, SMC-17(3), 1987. [Hamilton et al., 19861 T. Hamilton, D. Simmons, and R. Carlson. HELIX: an engine monitoring system. In







[cl







NTSB-AAR-76-19.







PI







Aircraft DC-9-31, 78-3.







Accident Nl335U,







Report: Southern Airways New Hope, Georgia, April4,







Inc., 1977.







National Transport ation Safety Board. NTSB-AAR-







kl







Aircrafl Accident Report: American Airlines Inc., DC-lo-lo, NllOAA, Chicago-O' Hare International Airport, Chicago, Illinois, May 25, 1979. National







Transportation Safety Board. NTSB-AAR-79-17.







M







Aircraft Incident Report: Northwest Airlines 79, McDonnell Douglas DC-10-40, Nl43US, Leesburg, Virginia, January 31, 1981. National Transportation







Safety Board. NTSB-AAR-81-10.







kl







portation Safety Board. NTSB-AAR-82-3.







Aircraft Accident Report: Air Florida Airlines, Inc., McDonnell-Douglas, Inc. DC-lo-JOCF, NlOl TV, Miami, Florida, September 22, 1981. National TransAircraft Accident Report: Eastern Airlines Flight 935, Lockheed L-1011-384, N309EA, Near Colts Neck, New Jersey, September 22, 198.2. National Transportation







bl







Safety Board. NTSB-AAR-82-5.







374







Common Sense Reasoning







Visibility Induction for Discretized Pursuit-Evasion Games



Ahmed Abdelkader and Hazem El-Alfy



Department of Engineering Mathematics and Physics Faculty of Engineering, Alexandria University Alexandria 21544, EGYPT {abdelkader,helalfy}@alexu.edu.eg







Abstract



We study a two-player pursuit-evasion game, in which an agent moving amongst obstacles is to be maintained within "sight" of a pursuing robot. Using a discretization of the environment, our main contribution is to design an efficient algorithm that decides, given initial positions of both pursuer and evader, if the evader can take any moving strategy to go out of sight of the pursuer at any time instant. If that happens, we say that the evader wins the game. We analyze the algorithm, present several optimizations and show results for different environments. For situations where the evader cannot win, we compute, in addition, a pursuit strategy that keeps the evader within sight, for every strategy the evader can take. Finally, if it is determined that the evader wins, we compute its optimal escape trajectory and the corresponding optimal pursuit trajectory.







Introduction



We consider the problem of target tracking, that is planning the motion of a mobile robot (pursuer) as it tracks a target (evader) moving amongst obstacles. We use the term target tracking to mean following that target or, more precisely, maintaining its visibility. This terminology is common within the robotic planning community (Hsu, Lee, and Rong 2008) as opposed to the broader notion of tracking, known in the computer vision literature, which refers to the identification of paths of different targets. Several applications of that problem are suggested in the literature (Hsu, Lee, and Rong 2008; Bhattacharya and Hutchinson 2008; Murrieta-Cid et al. 2007). Those cover surveillance and security in sensitive or restricted areas, providing home care by watching over children or elderly people and monitoring the performance of human workers. The studied problem is part of the more general visibilitybased pursuit-evasion problems (LaValle 2006). In those problems, the task of a pursuer is to compute a path which guarantees finding an evader that might be hiding in an environment. Obviously, the evader might "sneak" between different hiding places as the pursuer is traveling making a single pursuer unable to solve the problem. The natural extension becomes that of finding the minimum number of pursuers needed to eliminate any hiding places for one or



Copyright c 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







more evaders under different constraints of knowledge of the environment and information about other players' locations (Gerkey, Thrun, and Gordon 2004; Kolling and Carpin 2010; Klein and Suri 2011; Borie, Tovey, and Koenig 2011). Once the evader is found, the related problem that arises is that of maintaining its visibility which is known as target tracking. An early attempt at that problem is presented in (LaValle et al. 1997) for fully predictable targets (optimal tracking paths found offline) and partially predictable targets (best next step found online). Recently, there has been interest in a variant of that problem in which a pursuer, tracking an unpredictable evader, loses immediately if its view of the evader is obstructed by an obstacle (Bhattacharya and Hutchinson 2008; Murrieta-Cid et al. 2007). The problem of deciding which player wins for any pair of initial positions has been shown to be NP-complete (Murrieta-Cid et al. 2008). In this paper, we are interested in the latter problem of deciding which player wins the pursuit-evasion game described above. We develop an algorithm, using a mesh discretization approach, to decide the outcome of the game. For any input pair of players' positions, the algorithm decides whether the evader can win by moving in such a way to break the line of sight with the pursuer at any time instance or otherwise, the pursuer wins by maintaining the visibility of the evader throughout the game. Using a grid to discretize the surveyed environment has the advantages of being independent of the geometry and layout of the obstacles as well as being computationally feasible. Grid discretization is often used in Artificial Intelligence to solve problems in AI planning (Ishida and Korf 1995). In AI, the focus is on solving the problem of finding and (or) catching the evader using algorithms that search the discretized state space along with heuristics to speed up the process. The problem of deciding if a solution exists is only approached theoretically and is often intractable. Recently, a polynomial time algorithm that decides which player wins and in how many steps has been developed (Hahn and MacGillivray 2006). Mathematical results from graph theory are used to present bounds on the time complexity of the problem that can be generalized, in theory, to the case of more players. A realization of such algorithms in real world problems is still not practical. In contrast, our approach presents a grid-based solution that can be applied in practice to a robotics path planning problem. It enriches the







literature by linking to existing research in that area, modeling realistic constraints of bounded speeds, different players' speeds and limited sensor range. In the robotics motion planning area, two approaches are used. Using the terminology in (LaValle 2006), these are combinatorial approaches that find exact solutions through a continuous space and sampling-based approaches that divide the space (probabilistically or deterministically) into discrete regions and find paths through these regions. The simplest form of deterministically dividing the space (a.k.a cell decomposition) is with a grid of fixed resolution. The main advantage of this approach is its simple implementation in contrast to combinatorial methods, many of which are impractical to implement. However, cell decomposition methods are resolution complete (unlike combinatorial methods), which means that they find a solution when one exists only if the resolution of the sampling grid is fine enough. Another common drawback with grid methods is that their complexity depends on the grid size (Gonz alezBa nos, Hsu, and Latombe 2006). The main contribution of this paper is the development of an algorithm that enhances recent results in AI planning and visibility-based pursuit-evasion to tackle the computationally prohibitive task of deciding the outcome of the pursuitevasion game. This is a significant improvement over earlier depth-limited or heuristic-based approaches. The computed results are also used to find optimal player trajectories and optimize various objectives. The basic model (El-Alfy and Kabardy 2011) is formally studied to facilitate the derivation of several suggested optimizations. This, to our knowledge, provides first evidence of the feasibility of optimal decisions at such high grid-resolutions under varying speed ratios, different visibility constraints and regardless of obstacle geometries. In the rest of this paper, we proceed with a more detailed literature review followed by a formal problem definition. Our approaches to solve the decision problem and compute tracking strategies are then presented and the results follow. Finally, we conclude and suggest future work.







Related Work



A large amount of literature has been devoted to pursuitevasion games. In this section, we review closely related work, with a focus on attempts at deciding the outcome of the game. In the field of robotics, we survey recent work in the problem where one pursuer maintains the visibility of one evader, in an environment with obstacles. In artificial intelligence, we review the related problem of cops and robbers. In the area of robotics motion planning, the main approach used is to decompose the environment into noncritical regions with critical curve boundaries, across which critical changes in occlusion and collision occur, then use a combinatorial motion planning method. Murrieta-Cid et al. (2007) model the pursuit-evasion game as a motion planning problem of a rod of variable length, creating a partitioning of the environment that depends on the geometry of obstacles. Later (2008), they present a convex partitioning that







is modeled as a mutual visibility graph. Their method alternates between an evader assumed to take the shortest step to escape, countered by a pursuer that computes a preventionfrom-escape step, which produces a sequence of locally optimal paths. This leads to an interesting result: to decide which player wins, every feasible ordering of local paths has to be checked, concluding it is an NP-complete problem. Bhattacharya et al. address the problem of maintaining the visibility of an escaping evader and show that it is completely decidable around one corner with infinite edges (Bhattacharya, Candido, and Hutchinson 2007). The authors then extend their work in (2008) to deal with more general environments with convex obstacles. They split the environment into decidable and non-decidable regions and approximate bounds on these regions. They also provide a sufficient condition for escape of the evader. Recently, they used differential games theory to analyze that problem under complete information, suggesting a formulation in which the pursuer maximizes the time for which it can track the evader while the evader minimizes it (Bhattacharya, Hutchinson, and Bas ar 2009; Bhattacharya and Hutchinson 2010). Computing equilibrium strategies gives necessary and sufficient conditions for tracking. They present results around a point obstacle, a corner and a hexagonal obstacle. Within the AI planning community, the related problem of cops and robbers consists of one or more players (cops) trying to find and catch one or more evaders (robbers). The players perform alternating moves. This makes the game naturally discrete, often modeled as a graph with vertices representing the game's states. The mathematical foundations for solving these problems are surveyed in (Hahn 2007). A polynomial time optimal algorithm that determines whether the cops or the robbers win, and in how many steps, has only been recently given in (Hahn and MacGillivray 2006). Unfortunately, methods for computing optimal strategies have always been impractical to implement. For example, Moldenhauer and Sturtevant (2009a) compute optimal move policies offline in 2.5 hours per environment using an enhanced form of the algorithm in (Hahn and MacGillivray 2006). For that reason, several heuristics have been used in order to compute near optimal approximations in practical time. In (Moldenhauer and Sturtevant 2009b), different optimal strategies are studied on small maps while in (2009a), larger maps are used to evaluate less optimal strategies against optimal ones. One of the first practical implementations of the cops and robbers game is presented in (Ishida and Korf 1995) under the name of movingtarget search. Since, that problem has been extensively studied using a variety of heuristics such as incremental heuristic search (Koenig, Likhachev, and Sun 2007) and Cover heuristic (Isaza et al. 2008), to name a few recent references.







Problem Definition



This is a two-player game, with one pursuer and one evader modeled as points that can move in any planar direction (holonomic robots). Each player knows exactly both the position and the velocity of the other player. We consider two-dimensional environments containing obstacles that obstruct the view of the players. Obstacles have known ar-







bitrary geometries and locations. The assumption of complete information is used here to derive the outcome of the game, since if some player loses with complete information, it will always lose under other conditions. Both players have bounded speeds, move at different speeds and can maneuver to avoid obstacles. We will denote the maximum speed of the pursuer vp , that of the evader ve and their ratio r = ve /vp . Players are equipped with sensors that can "see" in all directions (ommnidirectional) and as far as the environment boundaries or obstacles, whichever is closer. We will see later that we can model minimum and maximum ranges for sensors with a simple variation in our algorithm. The pursuit-evasion game proceeds as follows. Initially, the pursuer and the evader are at positions from which they can see each other. It is common in the literature to define two players to be visible to one another if the line segment that joins them does not intersect any obstacle. The goal of the game is for the pursuer to maintain visibility of the evader at all times. The game ends immediately, if at any time, the pursuer loses sight of the evader. In that case, we say that the pursuer loses and the evader wins.







Deciding the Outcome of the Game



Deciding the game requires the construction of a binary function in two variables for the initial positions of both players. In order to study the progress of the game, we introduce a third parameter for the time index which is a discrete version of time in the continuous case. We call this function Bad(p, e, i). When Bad evaluates to 1, it means there exists a strategy for an evader starting at e to go out of sight of a pursuer at p by the ith time index. It is clear that Bad(p, e, 0) corresponds directly to visibility constraints and is straight forward to compute. We seek an algorithm to determine the value of Bad(p, e), with the time index dropped to indicate the end result of the game as time tends to infinity. In logical contexts, Bad is used as a predicate.







Equation (1) computes Bad(p, e, i + 1) inductively by evaluating the necessary escape conditions as of the ith step. For any given non-trivial initial configuration, the very first application of the inductive step would only yield a change for cases where the evader starts right next to an obstacle and is able to hide behind it immediately. That is because the Bad(p, e, 0) is only 1 for initially obstructed cells. After many iterations of the induction over all cells, more pairs farther and farther from obstacles get marked as bad. The expansion of the bad region only stops at cells which the pursuer in question is able to track and the function stabilizes with Bad(p, e) = 1 if and only if the evader can win. This bears a discrete resemblance to integrating the adjoint equations backward in time from the termination situations as presented in (Bhattacharya and Hutchinson 2010). Algorithm 1 performs backward visibility induction as defined in (1) to matrix M which is initialized with initial visibility constraints. To determine mutual visibility between cells, we use Bresenham's line algorithm (Bresenham 1965) to draw a line connecting every two cells and see if it passes through any obstacle. This approach works for any geometry and is easy to implement. More sophisticated algorithms could be used but are hardly justified. Restrictions on visibility can be easily incorporated by modifying the initialization part at line 5. For example, for a limited sensing range Rmax , we add or D(p, e) > Rmax , where D is the distance. If the pursuer is not to come any closer than a minimum distance to the evader, a lower bound Rmin can be added as well. Algorithm 1: Decides the game for a given map. Input : A map of the environment. Output: The Bad function encoded as a bit matrix. Data: Two N x N binary matrices M and M . 1 begin 2 Discretize the map into a uniform grid of N cells. // Visibility initialization 3 Initialize M and M to 0. 4 foreach (p, e)  grid x grid do 5 if e not visible to p then M [p, e] = 1 6 end // Induction loop 7 while M = M do 8 M =M 9 foreach (p, e)  grid x grid do 10 if e  N (e) s.t.p  N (p) M [p , e ] = 1 then M [p, e] = 1 11 end 12 end 13 return M 14 end







The Visibility Induction Algorithm



Fix a pursuer and evader at grid cells p and e and let i be the time index. If Bad(p, e, 0) = 1, then the evader is not initially visible to the pursuer and the game ends trivially with the pursuer losing. Now, consider the case where the evader manages to escape at step i + 1. To do so, the evader must move to a neighboring cell e where no corresponding move exists for the pursuer to maintain visibility. In other words, all neighbors p of the pursuer either cannot see the evader at e or, otherwise, were shown unable to keep an evader at e in sight up to step i i.e. Bad(p , e , i) = 1. This means that when Bad(p, e, i + 1) is updated to 1, the outcome of the game has been decided for the configuration in question as a losing one, i.e. there is a strategy for the evader to escape the sight of the pursuer at some step  i, but not any sooner. We use N (c) for the set of neighboring cells a player at c can move to. With that, we have the following recurrence: Bad(p, e, i + 1) = 1 (p, e)e  N (e) s.t. p  N (p) Bad(p , e , i) = 1 (1)







Proof of Correctness



We start by showing that the algorithm always terminates. At the end of each iteration, either M = M and the loop exits or more cells get marked as bad, which stops when







all cells are marked, leaving M = M . Next, we use this induction: 1. By line 6, M contains the decision at step 0 as enforced by the visibility constraints computed in line 5. 2. After the ith iteration of the loop at line 9, M [p, e] = 1 iff the evader has an escape strategy e where the pursuer has no corresponding strategy p with M [p , e ] = 0.







Level-0 Caching It is evident most of the computations are dedicated to evaluating the escape conditions as presented in (1). It is crucial to skip any unnecessary evaluations that yield no updates. Lemma 2. Bad(p, e, i) = Bad(p, e, j )  j > i Proof. For Bad(p, e, i + 1) in (1), put e = e. Corollary 3. Bad(p, e, i) = Bad(p, e, j )  j < i Lemma 2 allows skipping pairs that have already been decided by a simple addition to the condition in line 10. Level-1 Caching As Lemma 4 suggests, we need only reevaluate the conditions for those players who witnessed a change at the previous iteration. This can be applied independently to pursuers and evaders leading to a 45% average speedup. When applied to both we reached 50%. This comes at an additional O(N ) storage, which is negligible compared to the M matrix. Lemma 4. (Synchronized Neighborhoods) Bad(p, e, i)  Bad(p, e, i + 1) = (p , e )  N (p) x N (e) s.t. Bad(p , e , i - 1)  Bad(p , e , i) Proof. Bad(p, e, i) = Bad(p, e, i - 1)



 







Complexity Analysis



The algorithm uses O(N 2 ) storage for the output and temporary matrices M and M . Initializing the matrices takes O(N 2 N ) time if naive line drawing is used for each pair, which is linear in the length of the line. The inner loop at line 9 processes O(N 2 ) pairs each costing O(2 ) where  = max(|N (p)|, |N (e)|). Per the preceding discussion, at the ith iteration, the algorithm decides the game for all escape paths of length (i+1). Let L be the length of the longest minimal escape trajectory for the given environment. Obviously, the induction loop at line 7 is executed O(L) times. We can see that L depends on the largest open area in the environment and also the speed ratio r, with equal speeds being the worst case, where the distance between the players may not change, as the game ends earlier otherwise. A worst case scenario is an equally fast evader starting very close to the pursuer. For such an evader to win, it would need to move along with the pursuer to the closest obstacle where it can break visibility. We conclude that L = O(N ) and would typically be smaller in practice. With that, the visibility induction algorithm is O(2 N 3 ). Theorem 1. (Visibility Induction) Algorithm 1 decides the discretized game for a general environment in O(2 N 3 ). Proof. By the discussion above, the proof follows.







(by C.3) (2)







= e  N (e) p  N (p) s.t. Bad(p , e , i - 1) Bad(p , e , i)







Bad(p, e, i + 1) = e  N (e) s.t. p  N (p) By (2) and (3), the existence of (p , e ) is established.







(3)







Practicalities and Optimizations



We present several enhancements to the visibility induction algorithm and the speedups they yield. Our time measurements are performed using test maps of 100 x 100 cells for 4 2 1 1 1 , 3 , 2 , 3 , 5 ]. All run times are averaged speed ratios [1, 5 over 10 runs. Memory Savings As binary matrices, both M and M need only 1 bit per entry. It is also obvious we need only store bits for valid states, which reduces N to the number of free cells. This allows N to exceed 60, 000 using less than 1GB of memory, which enables processing at resolutions around 250 x 250 cells. Parallelization Observe that the loop at line 9 reads from matrix M and writes to M . This means that M updates are embarrassingly parallel. In our C++ implementation, we used the cross-platform OpenMP library to exploit this property. By adding a single line of code, we were able to harness the multiprocessing capabilities commonly available today. This allowed a 36% average speedup.







Level-2 Caching Strict application of Lemma 4 results in re-evaluating the conditions only for pairs who witness related changes. Keeping track of that comes at a higher storage cost of O(N 2 ), which is equivalent to the M matrix. Adding the level-2 cache resulted in a 52% average speedup. With that, we reach a new complexity result. Note that caching under parallelization is particularly tricky and requires careful update and invalidation mechanisms. Lemma 5. Level-2 caching makes the induction loop O(4 N 2 ). Proof. By only processing a pair (p, e) having a related update in both N (p) and N (e), no pair gets processed more than |N (p)| x |N (e)| = O(2 ) times. As the total number of pairs is O(N 2 ) and processing a single pair takes O(2 ), this amounts to O(4 N 2 ). More Memory Savings It is possible to do without the auxiliary M matrix. Instead of copying values before the inner loop and doing all checks on old values, we can use the M matrix for both checks and updates. If some entry M [p, e] is not updated, the behavior would be the same. On the other hand, if M [p, e] got updated, the algorithm would use a newer value instead of waiting for the next iteration which results in a minor speedup as a side-effect.







Applying all the enhancements discussed in this section led to a 64% average speedup on our test sample. We notice that for the medium sized square grids we consider, initialization of matrices is above quadratic by a small factor which is dominated by the number of iterations L. Furthermore, as  is typically limited (players have bounded speeds) and can be considered constant for a given realization, it may be ignored in comparison to N as the algorithm approaches O(N 2 ). For the typical case of a limited sensing region of size R, the algorithm need only consider that many evaders. This effectively replaces one N in all the above expressions and allows processing at much higher resolutions.







Optimal Trajectory Planning



If the pursuer can keep the evader in sight, there is not much the evader can do as far as we are concerned. On the other hand, if the evader can win the game, it is particularly important to minimize the time taken to break the line of sight to the pursuer. The losing pursuer must also maximize that time by not making suboptimal moves that allow the evader to escape faster. To compute these optimal trajectories, we modify Algorithm 1 to store the time index i, at which the game got decided, into M [p, e]. In lines 5 and 10, we use 0 and i, respectively, instead of just 1, and only make the assignment once for the smallest i. The matrices are initialized to  to indicate the absence of an escape strategy for the evader and the condition in line 10 is modified accordingly. We call the enhanced Bad function J (p, e) as it gives the time left for visibility, which corresponds to the value of the game as in (Bhattacharya and Hutchinson 2010). Theorem 6. (Time-Optimal Trajectories) J (p, e) gives the time left before visibility is broken, assuming both players move optimally. Proof. Trivially, J (p, e) =  = Bad(p, e) and the evader has no escape strategy. When Bad(p, e) is marked at the ith iteration for a given pair, an escape trajectory becomes available to the evader at e . No such escape trajectory could be found up to step i - 1. By definition of the escape cell e and J : p  N (p) Bad(p , e , i - 1) = J (p , e ) < J (p, e) By following e  e  * * *  e(k) , J (p(k) , e(k) ) is guaranteed to reach 0 at cell e(k) where visibility is broken. From Lemma 4, p  N (p) s.t. J (p, e) = J (p , e ) + 1. By repeatedly selecting p , the pursuer can force k to attain its maximum value i.e. J (p, e). At each step, the players will be moving to neighbors p and e , maximizing J (p , e) and minimizing J (p, e ), respectively. The obtained J (p, e) can be processed further to have these neighbors precomputed into separate functions Sp and Se , encoding the trajectories for each player. However, as storing the time index i increases the required storage, the algorithm can be modified to compute Sp and Se directly. This allows reducing the storage by describing the neighbor relative to the current position of the player. The neighborhoods can be indexed unambiguously which allows the precomputed S functions to store just as many bits as necessary per entry, which is as small as log2  . We omit the modified algorithms due to the limited space.







Discrete Tracking Strategies



Because the Bad function decides the game for all pairs including all combinations of the neighboring cells for both players, it can be used beyond determining the winner for trajectory planning. As a zero-sum game, there will only be one winner; and a valid trajectory must maintain this property. Further objectives can be defined as needed and an optimal trajectory can then be chosen. In particular, we are interested in optimal escape trajectories for a winning evader minimizing the visibility time. Other objectives relating to the distance between players, the distance traveled or speed of maneuvering can also be used.







Extensions to Guaranteed Tracking



The computed Bad function can be used directly in basic trajectory planning for the winning player. A valid trajectory must maintain the winning state by only moving via cells with guaranteed win regardless of the strategy followed by the opponent. A higher level plan may then choose any of the valid neighbors, which are guaranteed to exist for the winner. To unify the notation used below, we define: Lose(a, b) =(P ursuer(a)  Bad(a, b))  (Evader(a)  Bad(b, a)) (4)







Algorithm 2 first discards invalid neighbors a winner must not move to, then chooses one of the remaining neighbors. Typically, a distance function is used for tie breaking. For example, a pursuer would generally prefer to move closer to the evader and keep it away from bad cells. Algorithm 2: Generic trajectory planning for winners. Input: Bad(., .), current state (player, opponent). 1 begin 2 N  = {} 3 foreach n  N (player) do 4 if Lose(n, n ) n  N (opponent) then 5 N = N  n 6 end 7 end 8 Move to any neighbor in N  . 9 end







Experimental Results



We implemented our algorithms in C++ and performed experiments on an Intel Core i7 CPU running at 2.67GHz with 4GB of RAM. We experiment with manually created environments containing obstacles of various forms. Initial players positions are randomly selected satisfying certain visibility constraints and the evader's paths are automatically generated as discussed in the above section on tracking strategies. We discretized the maps of the used environments with







Figure 1: Decision map Figure 2: Decision map (evader) - 50x50 map. (evader) - 200x200 map.







Figure 5: Decision map Figure 6: Decision map (evader) for a circular ob- (evader) with restricted visibility. stacle.







Figure 4: Decision map Figure 3: Decision map (pursuer) for polygonal ob(pursuer) around a corner. stacles. regular grids of sizes ranging from 50x50 to 400x400 cells and used 4-connected and 8-connected neighborhoods. With such fine granularities, we anticipate moving to real world environments. The effect of varying the grid size on the resolution of decision maps is shown in figures 1 and 2 for speed ratios 2 1 1 1 [1, 4 5 , 3 , 2 , 3 , 5 ]. The boundaries of the nested convex regions are such that if the two players fall inside a region, the pursuer can track the evader indefinitely, while if one player is inside and the other outside, the evader can escape. The darker the gray shade, the smaller the speed ratio. Obstacles are in black and the player mentioned in each figure is the black dot roughly centered inside all nested regions. Our approach works independently of obstacle shapes and layouts as shown in figures 3 to 5. Modeling visibility constraints (e.g. limited sensor range) affects the decision regions as in figure 6. All previous results are computed for 4-connected neighborhoods. Figures 7 and 8 contrast 4-connected to 8-connected neighborhood maps for a speed ratio of 0.5. Finally, we show two tracking scenarios in figures 9 and 10. As we vary the grid size, runtime is affected as shown in table 1. It fits a quadratic model in N (for a fixed neighborhood size) as by the discussion following Lemma 5.







Figure 7: Decision map Figure 8: Decision (evader) for a 4-connected map (evader) for an 8neighborhood. connected neighborhood. particular, we developed an algorithm that decides the outcome of the game for any pair of initial positions of the players. By employing a space/time discretization approach, the solution becomes feasible in polynomial time, is independent of the geometry of the environment and does not require the use of heuristics. We give a detailed analysis for the correctness of the algorithm, derive its space and time complexities and present and verify several approaches to reduce its time and memory demands. We extended our algorithm to compute tracking or escape trajectories for both players in real time and verified their optimality. We tested our method on different tracking scenarios and environments. We are currently working on a realization of the proposed







Conclusions



We addressed the problem of maintaining an unobstructed view of an evader moving amongst obstacles by a pursuer. In







Table 1: Average runtime for Algorithm 1 vs. grid size Grid size Runtime (hh:mm:ss) 50 x 50 00:00:01 60 x 60 00:00:02 75 x 75 00:00:06 100 x 100 00:00:23 120 x 120 00:00:52 150 x 150 00:02:27 200 x 200 00:09:24 300 x 300 01:06:18 400 x 400 06:47:57







Figure 9: Example of a Figure 10: Example of a winning evader (blue; top). winning pursuer (red; top). method using real robots equipped with sensors. To model the real world more accurately, we consider hexagonal mesh discretizations, with several interesting properties, and study decision errors for a given resolution. We already implemented limited range sensors and an extension to a limited field of view is systematic. Other constraints on players motion can be considered such as restricted areas where the pursuer is not allowed to go into. Regaining lost visibility or, alternately, allowing for blind interruptions are interesting extensions with a slight relaxation to the hard visibility constraint. Finally, we envision efficient ways to extend this approach to the case of more players.







References



Bhattacharya, S., and Hutchinson, S. 2008. Approximation schemes for two-player pursuit evasion games with visibility constraints. In Proceedings of Robotics: Science and Systems IV. Bhattacharya, S., and Hutchinson, S. 2010. On the existence of nash equilibrium for a two player pursuit-evasion game with visibility constraints. The International Journal of Robotics Research 29(7):831-839. Bhattacharya, S.; Candido, S.; and Hutchinson, S. 2007. Motion strategies for surveillance. In Proceedings of Robotics: Science and Systems III. Bhattacharya, S.; Hutchinson, S.; and Bas ar, T. 2009. Gametheoretic analysis of a visibility based pursuit-evasion game in the presence of obstacles. In Proc. American Control Conference (ACC'09). Borie, R.; Tovey, C.; and Koenig, S. 2011. Algorithms and complexity results for graph-based pursuit evasion. Autonomous Robots 31(4):317-332. Bresenham, J. 1965. Algorithm for computer control of a digital plotter. IBM Systems Journal 4(1):25-30. El-Alfy, H., and Kabardy, A. 2011. A new approach for the two-player pursuit-evasion game. In Proc. 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI'11), 396-397. Incheon, Korea: IEEE. Gerkey, B. P.; Thrun, S.; and Gordon, G. 2004. Visibilitybased pursuit-evasion with limited field of view. 20-27. Gonz alez-Ba nos, H. H.; Hsu, D.; and Latombe, J.-C. 2006. Motion planning: Recent developments. Autonomous Mo-







bile Robots: Sensing, Control, Decision-Making and Applications (CRC Press) 373-416. Hahn, G., and MacGillivray, G. 2006. A note on k-cop, l-robber games on graphs. Discrete mathematics 306(1920):2492-2497. Hahn, G. 2007. Cops, robbers and graphs. Tatra Mountains Mathematical Publications 36:163-176. Hsu, D.; Lee, W. S.; and Rong, N. 2008. A point-based POMDP planner for target tracking. In Proc. IEEE International Conference on Robotics and Automation (ICRA'08), 2644-2650. Isaza, A.; Lu, J.; Bulitko, V.; and Greiner, R. 2008. A cover-based approach to multi-agent moving target pursuit. In Artificial Intelligence and Interactive Entertainment Conference (AIIDE). Ishida, T., and Korf, R. E. 1995. Moving-target search: a real-time search for changing goals. Pattern Analysis and Machine Intelligence, IEEE Transactions on (TPAMI) 17(6):609-619. Klein, K., and Suri, S. 2011. Complete information pursuit evasion in polygonal environments. In TwentyFifth AAAI Conference on Artificial Intelligence (AAAI'11), 1120-1125. Koenig, S.; Likhachev, M.; and Sun, X. 2007. Speeding up moving-target search. In Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems (AAMAS'07), 1-8. Honolulu, HI, USA: ACM. Kolling, A., and Carpin, S. 2010. Multi-robot pursuitevasion without maps. In Proc. IEEE International Conference on Robotics and Automation (ICRA'10), 3045-3051. LaValle, S. M.; Gonz alez-Ba nos, H. H.; Becker, C.; and Latombe, J.-C. 1997. Motion strategies for maintaining visibility of a moving target. In Proc. IEEE International Conference on Robotics and Automation (ICRA'97), volume 1, 731-736. LaValle, S. M. 2006. Planning Algorithms. New York, NY, USA: Cambridge University Press. Moldenhauer, C., and Sturtevant, N. R. 2009a. Evaluating strategies for running from the cops. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI'09), 584-589. Moldenhauer, C., and Sturtevant, N. R. 2009b. Optimal solutions for moving target search. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS'09) - Volume 2, 1249-1250. Murrieta-Cid, R.; Muppirala, T.; Sarmiento, A.; Bhattacharya, S.; and Hutchinson, S. 2007. Surveillance strategies for a pursuer with finite sensor range. The International Journal of Robotics Research 26(3):233-253. Murrieta-Cid, R.; Monroy, R.; Hutchinson, S.; and Laumond, J.-P. 2008. A complexity result for the pursuitevasion game of maintaining visibility of a moving evader. In Proc. IEEE International Conference on Robotics and Automation (ICRA'08), 2657-2664.







INSTITUT FUR INFORMATIK



Lehr- und Forschungseinheit fur Programmier- und Modellierungssprachen Oettingenstra e 67, D{80538 Munchen







Nurse Scheduling using Constraint Logic Programming



Slim Abdennadher, Hans Schlenker







http://www.pms.informatik.uni-muenchen.de/publikationen Forschungsbericht/Research Report PMS-FB-1999-7, July 1999







appeared in Eleventh Annual Conference on Innovative Applications of Arti cial Intelligence, IAAI-99, Orlando, Florida







Nurse Scheduling using Constraint Logic Programming



Computer Science Institute, University of Munich Oettingenstr. 67, 80538 Munich, Germany Slim.Abdennadher@informatik.uni-muenchen.de







Slim Abdennadher







Technical University of Berlin Franklinstr. 28/29, 10587 Berlin, Germany hans@cs.tu-berlin.de ming and constraint solving. In logic programming, problems are stated in a declarative way using rules to de ne relations (predicates). Problems are solved using chronological backtrack search to explore choices. In constraint solving, e cient special-purpose algorithms are employed to solve sub-problems involving distinguished relations referred to as constraints, which can be considered as pieces of partial information. The nurse scheduling problem can be elegantly formalized as a constraint satisfaction problem (Mac92) and implemented by means of specialized constraint solving techniques that are available in CLP languages. In this paper, the generation of duty rosters for hospitals is tackled using the CLP framework. The System is called INTERDIP and has been successfully tested on a real ward at the \Klinikum Innenstadt " hospital in Munich (AS97). INTERDIP has been implemented in collaboration with Siemens-Nixdorf-Informationssysteme AG using IF/Prolog (Sie96b) which includes a constraint package (Sie96a) based on CHIP (DVS+ 88). This package includes, among others linear equations, constraints over nite domains and boolean constraints. The nurse scheduling problem consists in assigning a working shift to each nurse on each day of a planning period (usually one month), whereby several requirements must be considered, such as minimal allocation of a ward, legal regulations and wishes of the personnel. Usually not all speci ed requirements can be ful lled. The nurse scheduling problem can be modelled as a partial constraint satisfaction problem (FW92). It requires the processing of hard and soft constraints to cope with. Hard constraints are conditions that must be satis ed, soft constraints may be violated, but should be satis ed as far as possible. Several approaches have been proposed to deal with soft constraints: Hierarchical constraint logic programming (HCLP) (BFW92) supports a hierarchical organiziaton of constraints, where a constraint on some level is more important than any set of constraints from lower levels. To avoid the so called inter-hierarchy comparison in HCLP, the soft constraints are encoded in a hierarchical constraint satisfaction problem (HCSP) (Mey97). The Conplan/SIEDAplan (Mey97) considers the representation of nurse scheduling as a HCSP, where legal







Hans Schlenker







The nurse scheduling problem consists of assigning working shifts to each nurse on each day of a certain period of time. A typical problem comprises 600 to 800 assignments that have to take into account several requirements such as minimal allocation of a station, legal regulations and wishes of the personnel. This planning is a di cult and time-consuming expert task and is still done manually. INTERDIP1 is an advanced industrial prototype that supports semi-automatic creation of such rosters. Using the arti cial intelligence approach, constraint reasoning and constraint programming, INTERDIP creates a roster interactively within some minutes instead of by hand some hours. Additionally, it mostly produces better results. INTERDIP was developed in collaboration with Siemens Nixdorf. It was presented at the Systems'98 Computer exhibition in Munich and several companies have inquired to market our system.







Abstract







Many real-life problems lead to combinatorial search, computationally a very intensive task. Unfortunately, no general method exists for solving this kind of problems e ciently. The automatic generation of duty rosters for hospital wards falls under this class of problems. Since the manually generated solution of the nurse scheduling problem usually requires several hours of work, a lot of research has been done to reduce the amount of time needed in the roster development. The most popular technique is based on mathematical programming (War76). The main disadvantage of this approach is the di culty of incorporating applicationspeci c constraints into the problem formulation. Other methods include goal programming (AR81) and heuristic models (SWB79). Recently, Constraint Logic Programming (JM94; FA97; MS98) (CLP) has become a promising approach for solving scheduling problems. CLP combines the advantages of two declarative paradigms: logic programINTERDIP is an acronym for the German \Interaktiver Dienstplaner". Copyright c 1999, American Association for Arti cial Intelligence (www.aaai.org). All rights reserved.



1







Introduction







regulations are hard constraints and wishes of nurses usually have the lowest priority level. The result is also not necessarily of a reasonable quality in respect to the nurse's wishes. However, in practice nurses' wishes should be considered in order to support the working climate. Furthermore, some wishes of nurses are sometimes more important than some legal regulations. To deal with these requirements, INTERDIP provides a solution technique based on a variant of branch-and-bound search instead of chronological backtracking. This approach starts with a solution and requires the next solution to be better. Quality is measured by a suitable cost function. The cost function depends on the set of satis ed soft constraints. To improve on the theoretical complexity of the problem, our system is based on an imitation of the human way of solving the problem: A roster is generated with INTERDIP through several phases. Additionally, several days in the roster are assigned simultaneously through user de ned patterns. A pattern describes a preferred sequence of working days. With INTERDIP, a user who is to some extent familiar with nurse scheduling can interactively generate a roster within minutes. The paper is organized as follows. The next section introduces the nurse scheduling problem. Then we show how the problem can be modelled as a partial constraint satisfaction problem. In Section 4 and Section 5 we describe the implementation and the user interface. Finally, we conclude with an evaluation of our tool. Portions of this paper were taken from (AS99). In a hospital, a new duty roster must be generated for each ward monthly. A hospital ward is an organizational unit that has to ful l some concrete tasks, and has both rooms and personnel, the nurses, at its disposal. Usually, the wards of a hospital are completely distinct: each has its own rooms and its own personnel. Therefore all rosters of a hospital can be scheduled separately. We consider in the following the scheduling problem for one ward. A roster of one month is an assignment of the personnel of the ward to the shifts for all the days of the month. A shift is a working unit: in a common working model, each day has the units morning shift (e.g. 06:00 to 15:00), evening shift (14:00 to 23:00), and night shift (22:00 to 07:00) and possibly others. To each shift of every day, personnel has to be assigned. For the generation of a roster, di erent kinds of constraints must be taken into account: Legal regulations, e.g. the maximum working time of a person per day or week, or time o in lieu, or maternity leave. In Germany for example the statutory monthly core working hours for a hospital with a 37.5 hour week is about 160 hours depending on the month. So, with an average shift length of 8 hours,







Description of the problem







each nurse has to work on average 20 shifts. Another law says that between two (working-) shifts, each nurse has to have a break of at least 11 hours (\11 hours rule"). If a nurse works one day in the night shift, she must therefore not be assigned the morning or evening shift the next day. Also a morning shift must not follow an evening shift. Organizational rules are those that apply speci cally to one particular hospital, a part of a hospital or even only one ward. They are given by the respective management. Those are mainly the number and kind of the shifts and { within statutory limits { the minimum personnel allocation of each ward. In the following we consider a model with three shifts: morning, evening and night shift. To morning shift and evening shift at least three nurses must be assigned, and the night shift requires at least two nurses. Personnel data de ne the individual frame for each person. These are mainly the contractually established monthly core working time, pending vacation and accrued hours of overtime. If, for example, a nurse has 16 hours overtime, she might be scheduled two shifts less than average. Finally, wishes are requirements given by the personnel. These are mostly wishes to have some days o , for example at weekends, holidays, birthdays, or for a vacation period. Often, there is no duty roster that ful lls all the constraints. Therefore we distinguish two kinds of constraints. Hard constraints must always be satis ed, soft constraints may be violated. Roughly speaking, legal regulations, organizational rules and personnel data determine hard constraints, wishes may be hard or soft constraints. So for example the vacation scheduling might be done for a longer term (some months) apart from the actual roster planning. Then a wish for one day of vacation would be a hard constraint, because it was planned externally. Other wishes are mostly soft constraints. Often the nurses have the opportunity to classify their wishes into some \priority levels". If possible, the wishes in one of those levels will then be regarded as hard constraints. A roster is correct, i all hard constraints hold. The quality of a roster results from the number of the fullled soft constraints and their priorities. Constraint Satisfaction Problems (CSPs) have been a subject of research in arti cial intelligence for many years. A CSP is a pair (V; C ), where V is a nite set of variables, each associated with a nite domain, and C is a nite set of constraints. A solution of a CSP maps each variable to a value of its domain such that all the constraints are satis ed. A partial constraint satisfaction problem (PCSP) (FW92) is a triple (V; C; !), where (V; C ) is a CSP and ! maps constraints







Modeling the problem as PCSP







to weights. A constraint's weight expresses the importance of its ful llment, allowing to distinguish hard constraints, which must not be violated, from soft constraints, which should not be violated, but may be violated in case this is unavoidable. Hard constraints have an in nite weight. The nite weights of soft constraints allow for the speci cation of priorities among constraints. A solution of a PCSP maps each variable to a value of its domain such that all hard constraints are satis ed and the total weight of the violated soft constraints is minimal. In the representation of nurse scheduling as a PCSP, there is a constraint variable for each nurse on each day. The domains of the variables consist of possible shifts (also comprising vacations, recuperation of a worked public holiday, special leaves, maternity protection, unpaid leave etc.), so they usually consist of 10 values. (HW96) proposed a reduction of variable domains, based on elimination of interchangeable values introduced by Freuder (Fre91). The values of the above mentioned free shifts, e.g. vacations, can be reduced to only one value and each variable takes its values now in f0; 1; 2; 3g. For a nurse i and a day j a variable V may have one of the following values: V = 0: The nurse i is o -duty the day j . V = 1: The nurse i is assigned to the \morning" shift on the day j . V = 2: The nurse i is assigned to the \evening" shift on the day j . V = 3: The nurse i is assigned to the \night" shift on the day j . Reducing the variable domains from 10 values to 4 considerably improves the e ciency of the solution research. Figure 1 shows a complete schedule for 10 nurses and 14 days. Each row comprises the shifts of a certain nurse. The columns contain the shifts performed on a certain day. So, each square of the chart speci es for each nurse the working days and shifts, and days o . E.g. on the 4th day the second nurse Hilde is scheduled in shift 1, i.e. morning shift. Now we describe how to express the most important requirements of our application in terms of IF/PrologConstraints (Sie96b). In the following, we use a Prologlike notation with meta-variables. We denote the total number of nurses to be scheduled by s, the total number of days by t and a variable by Vij , where i denotes the number of the nurse or the row in the roster, respectively, and j denotes the number of the day, i.e. the column in the roster. With this notation, we can write down all the variables of this modeling in a list: V11,V12,...,Vst]. One requirement for a correct roster is the minimum personal allocation, i.e. the minimal number of nurses, the ward must be allocated each shift. Actually, the allocation is limited downward and upward. Let Min1 be the lower and Max1 be the upper allocation limit for the morning shift and Min2, Max2, Min3 and Max3 the



ij ij ij ij ij







Figure 1: A nurse schedule for 10 nurses over a period of 14 days lower and upper limits for the evening and night shifts, respectively. Therefore a correct roster must not have less than Min1 and more than Max1 times the '1' in each column and not less than Min2 and not more than Max2 the '2' and so on. So we have to state for each j (1 j t) and each k (k 2 f1; 2; 3g) the following constraint: where cardinality(Lower,Upper,Condition) is satis ed if at least Lower and at most Upper conditions in the list Condition are satis ed. Another requirement a schedule has to ful l is the compliance of the monthly core working hours of each nurse. This means that there is a lower bound and an upper bound of shifts, each nurse is to be assigned in the schedule period. This is the number of all the morning, evening and night shifts. This can be expressed simpler by the number of free shifts. Let for each nurse i (1 i s) the lower bound for the working shifts be given by Mini and the upper bound by Maxi. Then we can formulate the working hours requirement using the cardinality constraint: cardinality(t-Maxi,t-Mini, Vi1=0,...,Vit=0]) The \11 hours rule" implies that a nurse must not work a morning shift (the day) after an evening shift and may work (the day) after a night shift only a night shift. We can express the \11 hours rule" by the following expression: If Vij is assigned a speci c value, the assignment of Vi(j + 1) must ful ll a certain condition. This can be expressed directly by the domain if constraint. We state for each i (1 i s) and for each j (1 j < t): domain if(Vij = 2, Vi(j + 1) n= 1) and domain if(Vij = 3, Vi(j + 1) in 0,3]).



cardinality(Mink ,Maxk , V1j =k ,V2j =k ,...,Vsj =k ])







The constraint domain if(Condition, ThenGoal) is used to call a goal conditionally. If the arithmetic constraint Condition is satis ed, ThenGoal is called. If the arithmetic constraint is not satis able, true is called. The execution of the domain if constraint is delayed as long as the satis ability of Condition has not been determined. Free shifts, provided they can be considered hard wishes, lead to immediate variable assignments. A wish (e.g. vacation) of nurse i at day j can then be stated as: Vij = 0. Soft wishes, like all other soft conditions, can not be stated directly as (IF/Prolog-)constraints, since our constraint solver can only handle hard constraints. We only can use them for optimizing correct rosters. This will be explained in Section . The modeling just described, while being simple and straightforward, is unfortunately very costly: The search space is huge, i.e. 4600 for 20 nurses and a period of one month. Therefore we developed a method to prune the search tree which was inspired by the usual manual planning.







Planning in INTERDIP







Planning by hand



Because of the huge search space a roster is usually generated by hand in two phases. In the rst phase we have all liberties for assigning the cells of the roster. Therefore here we do the most complicated assignment (which is tied to most of the conditions): the allocation of the free days or shifts. Those are bound to a lot of constraints: they determine how many shifts a nurse has to work during the scheduled period, how many nurses over all shifts the ward is assigned each day, and not least most of the wishes are to be considered here: the wishes for free shifts (e.g. vacations). Closely connected with the free shifts are the night shifts: the \11 hour rule" enforces for the assignment of shifts to a nurse, that after a night shift there may follow only a night shift or a free shift (free day). Therefore, when manually scheduling, the free and the night shifts are allocated in the rst phase. In the second phase, the morning and the evening shifts are distributed among the not yet allocated cells of the roster. The obvious advantage of the scheduling in two phases over the scheduling in one phase is the reduction of complexity: in each phase there have to be considered fewer constraints and, above all, fewer assignments2.



2 The assignment of the rst phase is normally not changed within the second unless it is then impossible to get a solution and a change in the free and night shifts will probably enable one. The extent of those changes can be neglected: we never observed more than 10 changes.







In 1993, (van93) presented a partial automatic solution to the nurse scheduling problem that used two very different phases. It exibly generated good rosters but did not handle night shifts. INTERDIP uses more than two phases which are performed in the same manner by one constraint solver. We wanted to reduce the search space even further than (van93) did. The idea is to furthermore decompose the problem. We use three phases instead of two: 1st Phase Distribution of the free shifts. 2nd Phase Distribution of the night shifts. 3rd Phase Distribution of the morning and the evening shifts. With this modeling, in each phase for every cell of the roster, only the minimal decision between two possibilities has to be made. This reduces the search space. We will see how we obtain a complete roster after the three phases. In each phase, every variable is assigned a value out of the boolean domain f0; 1g. Depending on the phase, the values 0 and 1 have di erent meanings. If a variable in the rst phase is assigned the value one, this means that the roster gets a free shift in the appropriate cell. The cells whose variables are bound to 0 remain undecided. The second phase only treats the undecided cells: if a variable gets the value 1, the cell is assigned a night shift. The rest remains undecided. In the third phase each still not decided cell is lled with either morning or evening shift, depending on whether the variable was assigned a 1 or a 0, respectively (see Figure 2, the meaning of the bold numbers is just as in Figure 1.). A complete roster results from all three phases.



0



1 1







Phasewise plan generation







3



1







1 2







0







0







0







Phase 1







Phase 2







Phase 3







Figure 2: Allocation of the cells in three phases.







Assignment patterns







Because of the incomplete constraint propagation methods used for scheduling problems, the application programmer often has to explicitly use a labeling phase in which a backtracking search blindly tries di erent values for the variables. Since labeling is expensive, the programmer needs to employ techniques for reducing the search space. There is a variety of techniques to do this. For our application we add domain information about presumably good solutions by introducing patterns. A pattern describes a preferred sequence of







working days. Coherent cells of the roster are allocated along user de ned patterns. As shown above, the variables are declared in each phase to range over the values 0 and 1 and the appropriate shifts are registered into the roster. Patterns are then meaningful combinations of roster entries, whereby a combination stands for successive days. A large number of these patterns is known. For example, we consider meaningful the combination of ve days work and two days free. The appropriate pattern for the rst phase, in which the working days are determined, is then: (?, ?, ?, ?, ?, 0, 0). If we assume that it is better to work on three successive days in the same shift than in different ones, we formulate for the second phase and thus for the night shifts: (3,3,3). Each phase has its own set of patterns. The patterns of a phase have an order in which they are selected: rst, the ones which result in a good solution, since the nurses are accustomed to this pattern, and at the end trivial patterns which are necessary to generate solutions, if they exist. For lling the roster, the given patterns are translated into appropriate variable assignments which are then tried in each row from left to right. The patterns can be considered as requirements of minor priority (soft constraints) as well as probable parts of solutions. Schedules that comply with the given patterns are explored rst. Applying this specialized labeling method reorganizes the search space. Additionally, each pattern is assigned a cost value so that for example a nurse whose wishes could not be fully ful lled, more likely gets \better" work patterns assigned. A roster that satis es all hard constraints is considered feasible but this does not necessarily mean that it is su ciently good to be used by a hospital ward. The concept of an optimal roster is hard to de ne. Generally, roster quality is a subjective matter and its de nition changes from problem to problem. We apply the usual measure which is common to all applications in the eld of scheduling. It is given in terms of the number and the priority of soft constraints that are violated. A popular approach consists in using a branch and bound search instead of chronological backtracking. Branch and bound starts out from a solution and requires the next solution to be better. Quality is measured by a suitable cost function. The cost function depends on the set of satis ed soft constraints. With this approach, however, soft constraints are only part of the cost function but play no role in selecting variables and values. In our multiphase method, branch and bound search is performed three times to improve the roster generated so far. Costs arise separately for each nurse and the algorithm tries to minimize the maximum of these. This means that we have a separate cost function for each







of the nurses and the maximum value of all the functions is minimized. So, INTERDIP tries to achieve that no nurse gets a much worse allocation (e.g. no wishes satis ed) than the others. For a nurse scheduling system to be complete, a exible user interface should be provided, so that the speci c requirements of the problem can be stated easily. INTERDIP provides such an interface. The INTERDIP user interface has been developed using the Tcl/Tk extension of IF/Prolog. Figure 1 shows a snapshot of the top-level graphical user interface to our nurse scheduling program with a generated roster. The interface allows the user to de ne the system parameters as preferred. All parameters like minimal and maximal allocation of the ward for each phase, wishes or patterns can be given graphically or in a spreadsheet. The wishes are given in three categories: imperative, important and less important wishes. We call them red, black and white wishes, respectively. This naming goes back to how the wishes were actually formulated in the hospital where we tested INTERDIP: They were lled into a plan using red and black pencils. The white wishes are to some extent standard wishes, like not to work on weekends. Red wishes (like vacation) are later treated as hard constraints and all the others as soft constraints. A single wish always relates to exactly one nurse and one day. Usually the generation of a roster runs as follows. After the user has speci ed all the conditions he will trigger the phases. A phase starts with generating the constraints and testing their consistency. Then, according to the above method, an optimal solution is computed. After a phase is nished, the next one is started initialized with the best result of the preceding phase, and so on. This is the automatic generation. It may happen that there exists not even one roster that complies with all the given hard constraints. Then the problem is called over-constrained. INTERDIP may detect this while generating the IF/Prolog constraints and then gives the user hints which of the conditions led to the inconsistency. However, there are kinds of contradictions that are not automatically detected. Therefore we built a debugger into INTERDIP. Being an interactive tool, INTERDIP lets the user take part in the generation in di erent ways. Firstly the user usually has some freedom in specifying the problem conditions. He can directly in uence the planning by giving some red wishes which directly lead to variable bindings. But the user can also interfere with a concrete process of allocation: He can use the debugger to break the computation manually or to set breakpoints. At the breakpoint (a cell of the roster), he is given all the possible patterns out of which he can choose one. The computation then continues with the selected allocation. In the single-step-mode the computation is stopped after each single allocation. Additionally the user can undo allocations already made.







Using the system interactively







Optimal rosters







With the debugger, the user can manually allocate parts of the roster in order to improve automatically presented solutions on the one hand and, in case the generator did not nd a solution at all, enable one on the other hand. In addition, the user can manually alter a completely generated roster and let it check by INTERDIP. The system then tries to state all the constraints for the given variable assignments, and if one fails, it gives the user hints about the contradictions. In this paper, the nurse scheduling problem is discussed and a speci c system, INTERDIP, is presented, that assists a human planner in scheduling the nurse working shifts for a hospital ward. We think that our approach can be applied to many applications in the eld of personnel assignment. It is quite obvious that the current implementation might even be used \as is" for every duty rota problem and therefore solves this whole problem class. It was possible to build this planning system for nurse scheduling within a few man months using a given commercial constraint solver, IF/Prolog from Siemens Nixdorf. The CLP code is just about 4000 lines with more than half of it for user interface. INTERDIP illustrates the important potentials of constraint logic programming for the implementation of real-life applications. INTERDIP was presented at the Systems'98 Computer exhibition in Munich and several companies are interested to market it. INTERDIP is currently tested at the \Klinikum Innenstadt " hospital in Munich. Typically, for 20 nurses and a period of one month, INTERDIP generates a satisfying (not optimal) schedule within a few minutes. The schedules generated by INTERDIP are comparable to those manually generated by a well experienced head nurse, sometimes even better than those. Of course this can not be guaranteed for every possible problem instance since, in general, the scheduling problem is NP-complete. J. L. Arthur and A. Ravindran. A multiple objective nurse scheduling model. In AIIE Transactions, volume 13, 1981. S. Abdennadher and H. Schlenker. INTERDIP { Ein Interaktiver Constraint-basierter Dienstplaner fur Krankenstationen. In F. Bry, B. Freitag, and D. Seipel, editors, 12th Workshop on Logic Programming WLP'97, September 1997. S. Abdennadher and H. Schlenker. INTERDIP { an interactive constraint based nurse. In Proceedings of the First International Conference and Exhibition on the Practical Application of Constraint Technologies and Logic Programming, 1999. A. Borning, B. N. Freeman-Benson, and M. Wilson. Constraint hierarchies. Lisp and Symbolic Computation, 5(3):223{270, 1992.







Conclusion







References







M. Dincbas, P. Van Hentenryck, H. Simonis, A. Aggoun, T. Graf, and F. Berthier. The Constraint Logic Programming Language CHIP. Technical Report TRLP-37, ECRC, Munich, 1988. T. Fruhwirth and S. Abdennadher. ConstraintProgrammierung: Grundlagen und Anwendungen. Springer-Verlag, September 1997. E. C. Freuder. Eliminating interchangeable values in constraint satisfaction problems. In AAAI-91 { Proceedings of the 9th national conference on arti cial intelligence, pages 227{233, 1991. E. C. Freuder and R. J. Wallace. Partial constraint satisfaction. Arti cial Intelligence, 58(1-3):21{70, 1992. K. Heus and G. Weil. Constraint programming a nurse scheduling application. In Proceedings of the Second International Conference on the Practical Application of Constraint Technology, pages 115{127, 1996. J. Ja ar and M. J. Maher. Constraint logic programming: A survey. Journal of Logic Programming, 20:503{581, 1994. A. Mackworth. Constraint satisfaction. In Stuart C. Shapiro, editor, Encyclopedia of Arti cial Intelligence. Wiley, 1992. Volume 1, second edition. H. Meyer auf'm Hofe. ConPlan/SIEDAplan: Personnel assignment as a problem of hierarchical constraint satisfaction. In Proceedings of the Third International Conference on the Practical Application of Constraint Technology, 1997. K. Marriott and P. Stuckey. Programming with Constraints: An Introduction. The MIT Press, 1998. Siemens Nixdorf Informationssysteme AG. IF/Prolog Constraint Problem Solver, 1996. Siemens Nixdorf Informationssysteme AG. IF/Prolog Users Guide, 1996. L. D. Smith, A. Wiggins, and D. Bird. Postimplementation experience with computer-assisted nurse scheduling in a large hospital. In Information Systems and Operational Research, volume 17, 1979. B. van den Bosch. Implementation of a CLP library and an application in nurse scheduling. Master's thesis, Katholieke Universiteit Leuven, Belgium, 1993. D. M. Warner. Scheduling nursing personnel according to nursing preference: A mathematical programming approach. In Operations Research, volume 24, 1976.







Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence







Towards Population Scale Activity Recognition: A Framework for Handling Data Diversity



Saeed Abdullah



Cornell University Ithaca, New York, USA







Nicholas D. Lane



Microsoft Research Asia Beijing, China







Tanzeem Choudhury



Cornell University Ithaca, New York, USA







Abstract



The rising popularity of the sensor-equipped smartphone is changing the possible scale and scope of human activity inference. The diversity in user population seen in large user bases can overwhelm conventional one-size-fits-all classication approaches. Although personalized models are better able to handle population diversity, they often require increased effort from the end user during training and are computationally expensive. In this paper, we propose an activity classification framework that is scalable and can tractably handle an increasing number of users. Scalability is achieved by maintaining distinct groups of similar users during the training process, which makes it possible to account for the differences between users without resorting to training individualized classifiers. The proposed framework keeps user burden low by leveraging crowd-sourced data labels, where simple natural language processing techniques in combination with multiinstance learning are used to handle labeling errors introduced by low-commitment everyday users. Experiment results on a large public dataset demonstrate that the framework can cope with population diversity irrespective of population size.







Introduction



With the explosion of smartphones, it is now possible to collect real-time daily activity data over a large population. This continuous availability of a vast amount of data changes the possibilities of human-centric applications and sensing. But, as the scope of the system broadens from carefullycontrolled experiments to mass-generated data, the conventional computational methods for activity recognition are overwhelmed by user heterogeneity in terms of age, behavioral patterns, lifestyle and so on. Performance degradation of classifiers in activity recognition due to the difference between people is known as population diversity problem. It has been shown (Lane et al. 2011b) that the population diversity problem can seriously affect the classification accuracy even when the population consists of as little as fifty users. While personalized models (Longstaff, Reddy, and Estrin 2010; Stikic and Schiele 2009) usually fare much better



Copyright c 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







in handling population diversity, the improvement comes at the expense of increased user involvement. The accuracy of classification requires each user to provide carefully labeled data-segments. As the classifier works in isolation, it leads to redundant efforts while learning about the same activities over similar users. To handle population-diversity in a practical way, a number of studies (Lane et al. 2011a; 2011b) suggested networked approaches by sharing data across users. To ensure reasonable accuracy, crowd-sourced training samples are weighted according to forms of inter-personal similarity. But, none of the proposed methods scale well with an increasing population. For a large user-base, the cost of i) computing pair-wise similarity network and more importantly, ii) exponential cost of training classifiers with huge datasets resulting from crowd-sourcing, gets impractical even with tens of users (Lane et al. 2011b). As interpersonal differences is more of an issue for a large user base, being not scalable severely limit the usability of existing approaches. This paper proposes a novel scheme to handle population diversity in a scalable way. We maintain groups of similar users to bring down the cost of computing similarity networks and using the similarity measures for training. To achieve comparable accuracy while keeping the user-burden low, our framework focuses on ensuring exploiting crowdsourcing within a group. To enable robust crowd-sourcing even in the case of unreliably labeled data from users, we handle the two common errors (Peebles et al. 2010) -- semantic discrepancy in the labels and the overlapping boundary of activities. To handle semantic discrepancy, we propose to consider it as a Natural Language Processing (NLP) problem. And, to handle overlapping class boundary resulting from inaccurate start and ending times, we use MultiInstance learning.







The contributions of the paper are: * The proposed framework handles population diversity in a way that remains practical even as the user population increases in size. To do so, we maintain group of similar users and limit embedding inter-personal similarity within the group.







851







Figure 1: The processing phases of the proposed activity recognition framework for handling data diversity and labeling inconsistencies. * We enable more robust crowd-sourcing. Previous work on sharing training activity data across users assumes that the labels are consistent. This assumption might be true in case of conventional controlled environments, but while working with a large population of low-commitment users, it is necessary to be robust enough against inconsistent labels. * By using a large public dataset, we evaluate our framework, both in terms of accuracy and scalability. networks. CSN (Lane et al. 2011b), for example, maintains three different similarity networks and a different classifier is trained for each network. To incorporate user similarity while training a classifier for user ui , the data samples from other user uj is weighted according to their similarity, S(ui , uj ) at the initial iteration. So, for each user uj in the population, data sample xuj from that user has the initial weight as weight(0) (xuj ) = S(ui , uj ). As a result, the computational cost of training classifiers can grow out of hand even with tens of users. To make training of classifiers feasible over a large userbase, we propose to cluster similar users and constrain the crowd-sourcing of data to only users within the same cluster. So, for a fixed number of clusters the number of classi-







Framework



Figure 1 shows the different steps in our framework for producing personalized classifiers that can cope with the population diversity problem in a scalable manner. We describe the steps in more details below.







Similarity Network



As the population grows, the user base starts to get more diverse. Apart from visible demographic dissimilarities like age, weight, gender or fitness level, the population starts to get more diverse in terms of behavioral and lifestyle pattern. As a result, even the core activities like walking can have different signature in sensor data across different group of people. For example, in CSN (Lane et al. 2011b) the authors pointed out the difference in features for two distinct subgroups of users performing walking as seen in Figure 2. As this inter-personal dissimilarities manifests as the differences in the pattern of raw data, previous approaches (Lane et al. 2011b; 2011a) use similarity networks for training classifiers. In a similarity network graph, each node represents a user and the edge-weight represents similarity between two users. There can be multiple similarity networks to leverage affinity among users in different dimensions -- physical similarity network might be used to recognize running while diurnal patterns might be leveraged while inferencing commuting activities. Each classifier for every user is trained on a dataset consisting of weighted data samples from all the users in the population based on the similarity







Figure 2: The difference in accelerometer features as two distinct subgroups perform the same activity -- walking (originally published in Lane et al. 2011b). The first two principal components of the features are shown above.







852







fiers trained remains constant irrespective of any increase to the size of the user population. For each type of similarity networks, we use different sets of clusters to leverage different dimension of affinity among users. Here we describe clustering users depending on two different notion of similarity -- sensor-data similarity and lifestyle similarity. But, it should be noted that other affinity metrics can easily be accommodated in the framework. Sensor Data Similarity As shown in Figure 2 the difference among users can manifest as the difference in raw sensor-data. So, the similarity in the accumulated data between two users can be a good indicator of inter-personal affinity. Given two users ui , uj and the corresponding accumulated feature sets Fui and Fuj , the similarity function can be defined as the overlap between sets, S(ui , uj ) = |Fui Fuj | , known as the Jaccard coefficient. But, comput|Fui Fuj | ing the similarity metric across a huge activity dataset for a large user population is clearly not feasible. So, we use sublinear time near-neighbor search known as Locality Sensitive Hashing (LSH) (Indyk and Motwani 1998). LSH is a well known technique (Buhler 2001; Ravichandran, Pantel, and Hovy 2005; Das et al. 2007) to efficiently find near-neighbors in a large database. In LSH, data points are hashed using multiple hash functions so that collisions between similar points occurs with higher probability. Finding near-neighbors requires hashing the query point as well to locate the buckets it belongs to. For the Jaccard coefficient similarity, there exists a LSH scheme called Min-Hashing (Cohen 1997). To use Min-Hashing, we need to randomly permutate the set of all feature S vectors and the hash value for each user ui is the index of the first feature vector in the permutated set that belongs to Fui -- the set of feature vector for user ui . For this random permutation, uniformly chosen over the set of all permutations of S , the probability of collision is exactly same as the Jaccard coefficient (Cohen 1997; Broder 1997; Cohen et al. 2001). The Min-Hash produces a set of hash buckets where the probability of two users ui , uj being in the same bucket is same to S(ui , uj ) -- essentially working as a probabilistic clustering algorithm where each bucket is a cluster. To ensure higher precision in clustering, we can concatenate p hash-values (Indyk and Motwani 1998) so that the probability of two users being in the same bucket is S(ui , uj )p , for p > 0. To avoid low recall resulting from the clusters being too refined, we repeat the steps q times. Each user belongs to q clusters where each cluster is defined by concatenation of p hash values. Permutating the set of feature vectors for the whole activity dataset is computationally unfeasible. Instead, we generate p x q independent, random hash-seeds and each feature vector is mapped to a corresponding hash-value. The hashvalues then serves as the index in the permuted set -- resulting in having similar characteristics to the ideal Min-Hash (Indyk 1999). Lifestyle Similarity The diversity in lifestyle (as measured by location and temporal patterns) can provide an im-







portant insight into the context of different activities. Depending on diurnal patterns and mobility distribution same activities can have different signature. The use of lifestyle similarity like diurnal patterns, has been shown to be beneficial in inferring different activity classes such as driving (Lane et al. 2011b). In CSN (Lane et al. 2011b), lifestyle similarity has been computed from mobility patterns and diurnal patterns by tessellating data into m distinct bins. For GPS estimates in mobility patterns, the bins can be two dimensional and for diurnal patterns each bin can represent the hour during a day in the week -- ranging from 0 to 167, 0 denotes the start of the week while 167 marks the final hour of the last day. For each user, CSN would construct a histogram {T (k) , k  [1, m]} of these bins. Histogram frequencies are normalized and the value of the histogram vector reflects the distribution of the data belonging to the user. CSN defines the lifestyle similar(k ) (k ) m ity between two users ui , uj as k=1 Tui Tui . Given the relatively low dimension of the histogram vectors, the above similarity measure can be used in common clustering algorithms. But, for large user-base, we suggest using Earth Mover's Distance (EMD). Given two different lifestyle histogram T (k) and T (m) , the earth mover's distance EMD(T (k) , T (m) ) is defined as the minimum cost of transforming one distribution to other. This is a popular metric in image and computer vision research (Rubner, Tomasi, and Guibas 2000; Zhao, Yang, and Tao 2010). It is expensive to compute as exact distance requires a solution to minimum transportation problem (Hitchcock 1941). As a result there has been extensive work in approximating the distance metric efficiently (Ling and Okada 2007; Shirdhonkar and Jacobs 2008). More importantly, (Charikar 2002) has shown that LSH scheme exists for EMD.







Robust crowd-sourcing



By enabling crowd sourcing, classifiers can use the steady stream of data from other users to find more discriminating examples to be incorporated into the model. But, given that one of the major goal is to keep the user burden low, the discrepancy in the labels provided by the users with low-commitment is unavoidable. For our framework to be robust enough against labeling errors and inconsistencies, we specifically focus on semantic discrepancy and boundary overlapping. Semantic Discrepancy in Labels Prior work in activity recognition usually makes the assumption that labels are consistent and the label domain remains fixed. While this might be true in simple scenarios, when people can enter free-form text as label to mark activities, the issue of assigning different labels to similar activities starts to become a concern (Peebles et al. 2010). Similar activity with different labels dilutes the training pool and essentially confuses the classifier. Given the textual nature of the labels, we suggest to consider finding similar activities as a NLP problem. Specifically, we use a similarity measure in terms of semantic distance between class labels to find similar classes. The







853







semantic distance can be calculated from hyponyms constructed from WordNet hierarchy (Fergus et al. 2010). After finding similar labels, we merge the samples under a generic single label if the number of training samples fall below an experimentally determined threshold. Otherwise, the data samples are shared during training weighted by their similarity. Boundary overlapping Data collection for activity on mobile phone usually requires users to mark the start and end of the activity. This can lead to recall errors, lack of temporal precision and interruptions. For example someone labeling gym may forget to mark end point resulting in overlapping boundary with driving data, which can affect the performance of classifiers. Multi-Instance Learning (MIL) can handle boundary overlapping robustly because of the more flexible labeling assumptions. In MIL, the samples are not treated as positive or negative -- labels are assigned to a set of instances grouped together into "bags". For a bag i and sample j in the bag, the probability of a single instance being positive is denoted by pij . We adopt the Noisy OR model for each bag. The probability that a bag is positive is given by pi = 1 - (1 - pij ) .







* Cluster similar users into a group. Similarity networks are formed within each of these clusters. * Finding semantically similar textual labels and reassign labels. * Training a MIL inspired Boosting algorithm to learn activities from the shared training samples in a group where each sample is initially weighted by user similarity. * Using multiple views of the data for leveraging the large amounts of unlabeled data that is crowd-sourced.







Evaluation



In this section, we evaluate the effectiveness and justify our design choices. The following experiments show that the framework scales much better than previous methods without sacrificing accuracy.







Dataset



For evaluation we use a large public dataset from ALKAN system (Hattori et al. 2010). This dataset contains data from more than 200 users and consists of over 35,000 activities. The data was gathered from the mobile device clients -- from iOS and Android applications. The dataset contains three axis accelerometer data from daily, real-life movement for more than a year resulting in relatively large dataset that can provide a good insight about the probable scalability issues that might arise in large-scale deployment. This dataset also contains activities with semantically close labels like train.sit, chat.sit, sit and so on. Some records in the dataset has inconsistent number of data-samples in terms of activity duration. We think the inconsistency arises when the phone can not sample sensor reading at the specified sampling rate, e.g., when talking on the phone. To identify errors in duration, we performed a time-window based sanity check by using the time-stamps in the data. The idea is, assuming that data is sampled at 20Hz, a chunk of data containing N consecutive samples N represents a time-window of 20 second. So, reading every N samples and comparing the values in the time-stamp column will give an insight into the variance present in that window. Around 1.1% of total data-sample was discarded because of inconsistency in time-stamp.







This essentially means that a bag is labeled positive if it contains at least one positive sample, otherwise, it is labeled as negative. So, under MIL settings, the bag labels provide only partial information -- it needs to cope with the ambiguity of not knowing which of the instances are positive and which ones are not. But, at the same time, the effect of noise is minimized in classifier training. MIL has been successfully applied to image segmentation (Vezhnevets and Buhmann 2010), face detection (Guillaumin, Verbeek, and Schmid 2010) and handling label noise in video classification (Leung, Song, and Zhang 2011). It has also been used in activity recognition in sparsely labeled data (Stikic and Schiele 2009). We use boosting based MIL where all instances contribute equally and independently to a bag's label (Xu and Frank 2004). Multi-view of data As data collection from smartphones is transparent to the user, a large pool of unlabeled data can quickly accumulate. To make use of this plentiful and otherwise wasted data, we suggest using unlabeled data to augment the classifier model. When multiple similarity networks are available, similar to CSN (Lane et al. 2011b), we suggest to exploit multiple views of different classifier by using multi-training (Blum and Mitchell 1998). But, if there is only one similarity network available, En-Co-Training (Guan et al. 2007) or democratic co-learning (Zhou and Goldman 2004) can be used as they do not make assumptions about independent views of the data. In this approach, each classifier keeps track of labeled and unlabeled crowd-sourced data and iteratively tries to label the unlabeled data of other classifier. After each such iteration, classifiers are retrained by using the additional new labels as assigned by other classifiers. So, the steps in our framework can be summarized as:







Feature Computation



For feature computation, a window size of 128 samples is used with 64 samples overlapping between consecutive windows. For the sampling rate of 20Hz, each window represents 6.7 seconds of data. Mean, energy, frequency-domain entropy and correlation features were extracted from the sliding windows signal. The DC feature in the sample window is the meanacceleration value of the acceleration. The energy feature is a normalized sum of the squared discrete FFT component magnitudes of the signal excluding the DC component. Frequency-domain entropy is calculated as the normalized information entropy of the FFT magnitudes -- DC feature is excluded from the calculation. Correlation is calculated between all pairwise combination of axes.







854







Figure 3: This ROC curve illustrates the effect of merging semantically similar labels. The classifier accuracy using the merged labels outperforms classifiers trained using only the original labels provided by users. Effect of Merging Labels For evaluating the effect of merging labels, we consider the activities associated with sitting and walking. The activity "sitting" consists of the labels train.sit, chat.sit, sit and eat.sit and for walking the labels are walk.slow, escalator.walk.up, escalator.walk.down and walk. These activities have been selected because significant amount of data have been recorded for each label. The dataset contained more than 46 hours of activities. For each label related with sitting we train a classifier where all other activities are marked as negative samples. For merged labels, we train the classifier with activities related with sitting marked as positive examples having different weights and walk related activities are marked as negative examples. For performance measurement, we use ten fold crossvalidation. From figure 3, the performance gain in the classifier trained from merged label is apparent from the top-left-most placement of the ROC curve. The accuracy of the classifiers trained by isolated labels are rather poor, but it is consistent with earlier findings (Hattori et al. 2010). Robustness against boundary overlapping In activity recognition systems obtaining high-quality training data has always been a central issues. For large-scale deployment, the problem is more severe. Recording sensor data through real-life, daily activities means lack of temporal precision and frequent disruption. To study the effect of such noise we switch some negative samples to positive samples -- simulating the interruption by activities in the middle of recording. We create dataset with 1%, 5%, 10% and 20% noise in positive labels. We train a MIL AdaBoost and a simple AdaBoost classifier using same dataset. In both cases, the weak classifier is a C4.5 decision tree. The result is shown in Figure 4. It is apparent that MIL based methods performs much better in the presence of noisy data.







Figure 4: Performance of multi-instance learning in handling activity labels with overlapping boundaries.







Figure 5: Time (in seconds) to train a classifier for a single user. Scalability The cost of training classifiers is the bottleneck of deploying activity recognition system that uses similarity networks. To evaluate how well our system scales with an increasing population, we select twenty users and train classifiers to recognize activities having labels sit, walk and stand consisting of more than 686 hours of sensor data. We compare the training time of classifiers with CSN (Lane et al. 2011b) which uses a fully connected weightedgraph for learning models. While CSN used a computer cluster for training, in our evaluation we use a single machine (2.3 GHz Intel Core i5 CPU and 4 GB of memory). We limited the evaluation to twenty users and single type of similarity network since CSN would take too long to train otherwise. Figure 5 shows the effect of increasing the population while training a classifier for a user based on lifestyle similarity network. The training time of a classifier in our framework is constant for a fixed number of clusters. Consequently, increasing the population does not incur much ad-







855







data to compute a similarity network. Additionally, we assume these similarity networks are static, which ignores the significant drift in user behavior that will occur over time. We plan to further work on the framework to address these design and implementation issues with the eventual goal of coming up with a framework which can be deployed over large population.







References



Blum, A., and Mitchell, T. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, 92- 100. ACM. Broder, A. 1997. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences., 21-29. IEEE. Buhler, J. 2001. Efficient large-scale sequence comparison by locality-sensitive hashing. Bioinformatics 17(5):419- 428. Charikar, M. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, 380-388. ACM. Cohen, E.; Datar, M.; Fujiwara, S.; Gionis, A.; Indyk, P.; Motwani, R.; Ullman, J.; and Yang, C. 2001. Finding interesting associations without support pruning. IEEE Transactions on Knowledge and Data Engineering 13(1):64-78. Cohen, E. 1997. Size-estimation framework with applications to transitive closure and reachability. Journal of Computer and System Sciences 55(3):441-453. Das, A.; Datar, M.; Garg, A.; and Rajaram, S. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th international conference on World Wide Web, 271-280. ACM. Fergus, R.; Bernal, H.; Weiss, Y.; and Torralba, A. 2010. Semantic label sharing for learning with many categories. Computer Vision-ECCV 2010 762-775. Guan, D.; Yuan, W.; Lee, Y.; Gavrilov, A.; and Lee, S. 2007. Activity recognition based on semi-supervised learning. In 13th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications, 2007. RTCSA 2007., 469-475. IEEE. Guillaumin, M.; Verbeek, J.; and Schmid, C. 2010. Multiple instance metric learning from automatically labeled bags of faces. Computer Vision-ECCV 2010 634-647. Hattori, Y.; Inoue, S.; Masaki, T.; Hirakawa, G.; and Sudo, O. 2010. Gathering large scale human activity information using mobile sensor devices. In 2010 International Conference on Broadband, Wireless Computing, Communication and Applications (BWCCA), 708-713. IEEE. Hitchcock, F. 1941. The distribution of a product from several sources to numerous localities. Journal of mathematics and physics 20(2):224-230. Indyk, P., and Motwani, R. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality.







Figure 6: Classifier accuracy in a population of 20 users. ditional cost. In contrast, the training time for CSN, from 4 users to 20 users increases by 370 times, making it impractical to use in large user base. The important question is how our clustering approach to manage computational cost will affect classifier accuracy. For measuring classifier performance we use a separate test set containing around three hours of sensor data for sit, walk and stand. Figure 6 shows the accuracy of the classifiers for this experiment. The classifier for CSN has been trained on the fully connected lifestyle similarity graph for twenty people while the clustered classifier has been trained on five users with high lifestyle similarity. We selected these three activities and limit the dataset to a single similarity network of twenty users because of the computational cost associated with training for CSN. From the result, we can say that training using clustered users maintains reasonable accuracy while keeping computational cost low.







Conclusion



In this paper, we introduced a scalable way to handle the population-diversity problem. We demonstrated that our framework scales well as the user population increases without sacrificing classification accuracy. Furthermore, our framework introduces new techniques for coping with crowd-sourced labeled activity data, which although can be plentiful can also be prone to error. Our results showed the effect in classifier accuracy due to user disagreement with activity class semantics (e.g., labeling the same activity class with different textual descriptions). We demonstrated how this problem can be improved with NLP-based techniques proposed in our framework. Finally, we introduced techniques specifically to handle segmentation errors during crowd-sourcing, which occur when users make mistakes as to precisely when activities start and end. While the results are promising, there are still challenges related to long-term usage by a large user population. Like the conventional methods, our framework assumes complete knowledge of the similarity network between all pair of users. This will not be available, for instance, as new users join the system for whom there will be insufficient







856







In Proceedings of the thirtieth annual ACM symposium on Theory of computing, 604-613. ACM. Indyk, P. 1999. A small approximately min-wise independent family of hash functions. In Proceedings of the tenth annual ACM-SIAM symposium on Discrete algorithms, 454-456. Society for Industrial and Applied Mathematics. Lane, N.; Xu, Y.; Lu, H.; Campbell, A.; Choudhury, T.; and Eisenman, S. 2011a. Exploiting social networks for large-scale human behavior modeling. Pervasive Computing, IEEE 10(4):45-53. Lane, N.; Xu, Y.; Lu, H.; Hu, S.; Choudhury, T.; Campbell, A.; and Zhao, F. 2011b. Enabling large-scale human activity inference on smartphones using community similarity networks (csn). In Proceedings of the 13th international conference on Ubiquitous computing, 355-364. ACM. Leung, T.; Song, Y.; and Zhang, J. 2011. Handling label noise in video classification via multiple instance learning. In 2011 IEEE International Conference on Computer Vision (ICCV), 2056-2063. IEEE. Ling, H., and Okada, K. 2007. An efficient earth mover's distance algorithm for robust histogram comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence 29(5):840-853. Longstaff, B.; Reddy, S.; and Estrin, D. 2010. Improving activity classification for health applications on mobile devices using active and semi-supervised learning. In 2010 4th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth), 1-7. IEEE. Peebles, D.; Lu, H.; Lane, N.; Choudhury, T.; and Campbell, A. 2010. Community-guided learning: Exploiting mobile sensor users to model human behavior. In Proc. of 24th AAAI Conference on Artificial Intelligence, 1600-1606.







Ravichandran, D.; Pantel, P.; and Hovy, E. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, 622-629. Association for Computational Linguistics. Rubner, Y.; Tomasi, C.; and Guibas, L. 2000. The earth mover's distance as a metric for image retrieval. International Journal of Computer Vision 40(2):99-121. Shirdhonkar, S., and Jacobs, D. 2008. Approximate earth movers distance in linear time. In IEEE Conference on Computer Vision and Pattern Recognition, 2008. (CVPR), 1-8. IEEE. Stikic, M., and Schiele, B. 2009. Activity recognition from sparsely labeled data using multi-instance learning. In Proceedings of the 4th International Symposium on Location and Context Awareness, LoCA '09, 156-173. Berlin, Heidelberg: Springer-Verlag. Vezhnevets, A., and Buhmann, J. 2010. Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning. In 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3249- 3256. IEEE. Xu, X., and Frank, E. 2004. Logistic regression and boosting for labeled bags of instances. Advances in Knowledge Discovery and Data Mining 272-281. Zhao, Q.; Yang, Z.; and Tao, H. 2010. Differential earth mover's distance with its applications to visual tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 32(2):274-287. Zhou, Y., and Goldman, S. 2004. Democratic co-learning. In 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI), 594-602. IEEE.







857







Qualitatively Describing Objects Using Spatial Prepositions



Alicia Abella



Department of Computer Science Columbia University New York, NY 10027







John R. Kender







The objective in this paper is to present a framework for a system that describes objects in a qualitative fashion. A subset of spatial prepositions is chosen and an appropriate quanti cation is applied to each of them that capture their inherent qualitative properties. The quanti cations use such object attributes as area, centers, and elongation properties. The familiar zeroth, rst, and second order moments are used to characterize these attributes. This paper will detail how and why the particular quanti cations were chosen. Since spatial prepositions are by their nature rather vague and dependent on context a technique for fuzzifying the de nition of the spatial preposition is explained. Finally an example task is chosen to illustrate the appropriateness of the quanti cation techniques. The work presented in this paper is motivated by an interest in how spatial prepositions may be used to describe space and more interestingly, the spatial relationship among the objects that occupy that space. This work is not concerned with the natural language aspect of spatial prepositions. Given a particular environment and a particular task, where the task and environment may change, we wish for a framework that describes the elements in the environment. It is this framework that is of concern in this paper. It is known that language meaning is very much dependent on context. An example of a context dependent use of the spatial preposition next as taken from Landau and Jackendo , accepted for publication] is the bicycle next to the house. We would normally not say the house next to the bicycle. This is the case because the house is larger in size and as such it serves as an anchor for those objects around it. The house in this example serves as a reference object, or in an environmental context, as a landmark. In the system presented in this paper either description is acceptable since the only concern is in the spatial arrangement of







Abstract







the objects irrespective of the size or the purpose of either of the two objects. The treatment of objects in our chosen environment is a binary one. There is not a reference object, or landmark, because we wish to avoid choosing a reference object that would require the use of physical attributes such as color, size, or shape and focus solely on two objects' spatial relationship. If we think about the use of a preposition like near we realize that the requirement of a particular shape is not needed for its proper use. Landau and Jackendo Landau and Jackendo , accepted for publication] have categorized spatial prepositions into those that describe volumes, surfaces, points, lines, and axial structure. They have pointed out that an object can be regarded as a \lump" or \blob" as far as most of the commonly used spatial prepositions are concerned. For example the preposition in or inside can regard an object as a blob as long as the blob has the capacity to surround. Likewise, near and at only require that the blob have some spatial extent. Along requires that an object be fairly linear and horizontal with respect to another. The work presented in Herskovits, 1986] covers the topic of spatial prepositions fairly extensively from a natural language perspective. The author only suggests the possibility of constructing a computational model for expressing spatial prepositions. The intent here is to demonstrate that a computational model can be constructed and that it indeed captures the vital properties su cient for a succinct use of the chosen prepositions. We can encode the spatial prepositions fairly concisely because we are treating objects as \blobs" and because most of the properties characterized by these prepositions can be encoded using geometric properties such as alignment and distance. Other related works can be found in Lindkvist, 1976; Talmy, 1983]. The following sections will provide the details of the encoding we have chosen and demonstrate them though the use of an example.







Introduction







The prepositions for which we have encoded are near, far, inside, above, below, aligned, next. We have dened a preposition as a predicate that maps k objects to true (T ) or false (F ); true if the k objects meet the requirements imposed by the preposition and false otherwise. p : Ok ?! fT; F g where p is a preposition and Ok is a k-tuple of objects. In this paper we will consider k = 2. Nevertheless, prepositions that involve three objects like between can also be represented, using a similar formalism. Now that we have de ned a preposition we need to de ne an object. Formally, each object is represented by a six element vector that depend on an object's area xx Ixy A, center (xc ; yc ), and inertia tensor I Ixy Iyy . It is important to scale the elements in this vector so that they have consistent units, in this case units of length, because we will use this vector in the fuzzi cation procedure described in section 4. Therefore, the kth object is represented by a vector q q q k = (pAk ; xk ; yk ; 4 I k ; 4 I k ; 4 I k ) xy yy c c xx The pair of objects is represented by a 12-component vector = ( 1 ; 2) 2 R12 It is this scaled vector that we will be using in our future calculations. The parameterization of objects presented above leads to the concept of a bounding box. A bounding box encloses the object using certain criteria. There are various ways in which to compute a bounding box for an object, one of which may be to nd the maximum and minimum x and y values belonging to the object. The one we've chosen is de ned through the values of x and y , that o er a measure of how much an object stretches in the x and y direction. See the Appendix for the derivation. Two objects de ne a point in 12D space. A preposition p can be thought of as a set of points Up 2 R12 such that Up = f jp( )g. The volume in this 12D space may be able to reveal some of the inherent properties associated with prepositions. In other words, examination of the space occupied by the various sets Up may tell us something about the spatial prepositions. Vacancies in this 12D space may reveal why we do not have a word to describe certain spatial relationships among objects. The intersection and distances of volumes occupied by various spatial prepositions may reveal a correlation between various prepositions. We say that objects O1 and O2 are in preposition p if ( 1 ; 2) 2 Up . This \ideal" set is made up of pairs of object vectors that satisfy the constraints imposed by the preposition p. As we well know, prepositions are







Notations and De nitions







inherently vague in their descriptions, and their interpretation may vary from person to person. Because of this, it is important to add some fuzzifying agent to our ideal set. The fuzzifying technique is as de ned through fuzzy set theory Klir and Folger, 1988]. The theory of fuzzy sets is used to represent uncertainty, information, and complexity. The theory of classical sets1 represents certainty. A classical set divides objects in the world into two categories: those that certainly belong to a set and those that certainly do not belong to a set. A fuzzy set, on the other hand, divides the world much more loosely, by introducing vagueness into the categorization process. This means that members of a set belong to that set to a greater or lesser degree than other members of the set. Mathematically, members of the set are assigned a membership grade value that indicates to what degree they belong to the set. This membership grade is usually a real number in the closed interval between 0 and 1. Therefore a member that has a membership grade closer to 1 belongs to the set to a greater degree than a member with a lower membership grade. Because of its properties fuzzy set theory can nd application in elds that study how we assimilate information, recognize patterns Abella, 1992], and simplify complex tasks. In our notation the fuzzi ed ideal set is de ned through a membership function fUp ( ) 2 0; 1] We also de ne a threshold value that depends on how much vagueness we allow before we decide that two objects are no longer describable with the given preposition: fUp ( ) p







The quanti cation of prepositions entails representing objects through certain physical properties that can then serve as a basis for expressing prepositions. The physical properties we've chosen include object area, centers of mass, and elongation properties. These properties are calculated through the use of the zeroth, rst, and second order moments. The basis for this choice of attributes is simplicity and familiarity. What ensues is a brief description of the various prepositions we've chosen to illustrate. Each preposition is de ned through a set of inequalities. This results in sets Up having nonzero measure (i.e. full dimensionality) in R12 which is necessary for the fuzzi cation procedure described in section 4.







Computational Model of Spatial Prepositions







NEAR



1







We've de ned near so that objects' bounding boxes



Referred to as \crisp" sets in fuzzy set theory.







2 2x 2 2y 1 2y 1 2x 1 ? y2 j jyc c







1 max







1 1 min







2 max 2 2 min







Figure 1: Two objects that are near each other







2 jx1 c ? xc j







Figure 3: De nition of relevant angles for aligned be completely embedded within the bounding box of another. Formally, 1 2 1 2 1 2 1 2 x ? x > jxc ? xc j and y ? y > jyc ? yc j



Above requires that the projections of bounding boxes on the x axis intersect and that the projections of bounding boxes on the y axis do not intersect. The mathematical relationship is 1 2 1 2 1 2 1 2 x + x > jxc ? xc j and y + y < yc ? yc Note that above is non-commutative. We de ne below similarly. As with near and far, above and below are mutually exclusive prepositions. However, not-above does not strictly imply below.







ABOVE, BELOW







2 1 jx1 c ? xc j ? x ? 2 jx1 c ? xc j







x







2







Figure 2: Two objects that are far from each other have a non-empty intersection (see gure 1). Mathematically this is : 1 2 1 2 1 2 1 2 x + x > jxc ? xc j and y + y > jyc ? yc j







FAR







Far is not the complement of near as one may initially







suspect. We may be faced with a case where an object is neither near or far from another object, but rather it is somewhat near or somewhat far. This notion of somewhat will be explained more fully when we introduce the concept of fuzzifying our \ideal" set. For now it su ces to say that far is de ned so that the distance between two bounding boxes in either the x extent or the y extent is larger than the maximum length of the two objects in that same x or y extent (see gure 2). Mathematically, 2 1 2 1 2 jx1 c ? xc j ? ( x + x ) > 2 max( x ; x) or 1 ? y2 j ? ( 1 + 2 ) > 2 max( 1 ; 2) jyc c y y y y







The alignment2 property is angular in nature, therefore its quanti cation involves inequalities between angles, rather than lengths as the previous prepositions had. For this purpose we de ne a di erent type of bounding box that is centered at the object's center of mass and oriented along the object's principal inertia axes with dimensions proportional to the object's maximum and minimum moments of inertia. , min and max are as shown in gure 3. With this in mind, the preposition aligned is de ned as: 1 ; 2 ) < i < min( 1 ; 2 ); i = 1; 2 max( min min max max We've de ned next as a combination of the prepositions near and aligned. Therefore the de nition for next is: Unext = Unear \ Ualigned The preposition next is an example of a spatial preposition that is a combination of more elementary



2 Although not a preposition from a language perspective we've adopted it as a spatial preposition.







ALIGNED







NEXT







INSIDE







Inside requires that the bounding box of one object







prepositions. This hints at the possibility of a natural hierarchy of spatial prepositions. It also shows evidence of the possible partitioning of the 12D space mentioned previously. This section describes why and how we fuzzify spatial prepositions. We need to fuzzify spatial prepositions because they are vague by their very nature; they depend on context and depend on an individual's perception of them with respect to an environment. For these reasons we need to allow for some leeway when deciding if two objects are related through a given preposition. There is a lot of freedom in how we can fuzzify spatial prepositions, or equivalently, the \ideal" set, Up . The idea we have adopted is to 12 de ne the membership function fUp ( ) where 2 R as a function of a distance d between and Up . d = min j ? j



0 0







1 2 3 4 6 5 7







The Fuzzi cation of Spatial Prepositions







Note that d( ; Up ) = 0 for 2 Up . The distance d tells us by how much the de ning preposition inequalities are not satis ed. Thus, fUp ( ) = 1 for 2 Up fUp ( ) ! 0 as d( ; Up ) ! 1 Up is a multi-dimensional set de ned by complex inequalities, for which computing d may be very burdensome. For this reason we resort to a MonteCarlo simulation with a set of random points around that have given statistical properties. The experiments we've conducted use normally distributed random points with mean and covariance matrix diag ( 2; :::; 2). The exact form for fUp used is 1; 2U fUp = min(1; 2 N ); 62 Up p N where N is the total number of random points in the Monte-Carlo simulation and N is the number of points 2 Up . Note that the formulation of fUp ensures that fUp for very close to the boundary of Up will have a value close to 1. The following section will detail some experiments that use this fuzzi cation technique and put into e ect the inequalities that de ne the given spatial prepositions.



0 0 0







2Up







Figure 4: The experimental image necessary for construction of the 12-dimensional vector are computed (e.g. the area of an object is the sum of all the pixels belonging to the object). Currently the system accepts a spatial preposition and displays all those objects that satisfy the preposition inequalities. The system also accepts as input two objects along with a preposition and it outputs how well those two objects meet the given preposition (the value of fUp for given ). All intuitively obvious relations between objects are discovered by the system, e.g. objects 1 and 3 are next to each other, etc. An interesting case, and one that demonstrates the e ects of fuzzi cation is the case of supplying object 2 and object 6 along with the preposition aligned. With no fuzzi cation the system nds that 2 and 6 are not aligned. However, if we allow a certain amount of fuzzication with say = 0:03 the value of fUaligned is 0.8. This value indicates that they may be su ciently aligned to be regarded as such (which we actually see in the image!), depending on how much leeway we wish to allow. The dependency of fUaligned on is shown in gure 5. From this graph we see that the value of the membership function signi cantly deteriorates for large values of . This simply means that the amount of induced uncertainty is so large that the objects cease to possess their original features (such as orientation in this case). This also indicates what the maximal acceptable value for should be. In this case, that is < 0:1. Another interesting case is that of supplying object 2 and object 6 along with the preposition near or far. Neither satis es the inequalities precisely. However, if we again, allow for fuzzi cation, we get a most interesting result, as shown in gure 6. We observe that







We will use the image shown in gure 4 to illustrate several uses of the prepositions. Each object has been numbered to ease their reference. The image is read as a grey-scale pixel image. It is then thresholded to produce a binary image and objects are located using a sequential labelling algorithm Horn, 1989]. Once the objects in the scene have been found, the attributes







Qualitative Description Experiments







fUp



0.8







0.6







0.4







0.2







0.001







0.01







0.1







Figure 5: The dependency of fUaligned (2; 6) as a function of







scriptions. In other words, we may wish to describe a particular object with as few descriptions as possible through the feedback from the system. The goal would be to home in on the object we are truly referring to through repeatedly supplying additional prepositions to those objects that were singled out after previous inquires. An experiment using this technique may reveal that people naturally describe spatial arrangements in a series of descriptions, rather than once and for all. It may also demonstrate inadequacies in the vocabulary or complexity of a scene. We may also discover that certain environments require that we adopt prepositions that do not exist in the English language for describing a particular sort of spatial arrangement. Abella, A. 1992. Extracting geometric shapes from a set of points. In Image Understanding Workshop. Herskovits, A. 1986. Language and spatial cognition: Press. Klir, G. J. and Folger, T. A. 1988. Fuzzy Sets, Uncertainty, and Information. Prentice Hall. Landau, B. and Jackendo , R. ation. "What" and "Where" in spatial language and spatial cognition. BBS. Lindkvist, K. 1976. Comprehensive study of conceptions of locality in which English prepositions occur. Almqvist & Wiksell International. Talmy, L. 1983. How language structures space. In



Spatial orientation: Theory, research, and application. Plenum Press.







References







fUp



1







near far







0.8







An interdisciplinary study of the prepositions in English. Cambridge University Press. Horn, Berthold K.P. 1989. Robot Vision. The MIT







0.6







0.4







0.2







0.001







0.01







0.1







Figure 6: The dependency of fUnear (2; 6) and fUfar (2; 6) as a function of although we can not say for certain that object 2 and object 6 are either near or far, we can say that they are somewhat near or somewhat far. How we decide which of the two to use can be seen in gure 6. If we examine the slopes of the two curves we see that for small values of the slope for far is steeper than that for near. Therefore it would seem more appropriate to say that 2 is somewhat far from 6 as opposed to 2 is somewhat near to 6. The intent of this paper was to establish a computational model for characterizing spatial prepositions for use in describing objects. A quanti cation was established and demonstrated through the use of an example. A framework to deal with the inherent vagueness of prepositions was also introduced with the use of a fuzzi cation technique. An extension of this work would be one in which a user could conduct a dialogue with the system, capable of understanding as well as generating scene de-







Conclusion







We have used the following two equations to de ne how much an object stretches in the x and y directions. r r I Imin j sin j; g max x = 2 maxf A j cos j; A r r Imax Imin j cos j; g y = 2 maxf A j sin j; A The following is the derivation of the above formulas. The maximal moment of inertia is given by the formula Z Z 2A u2 dudv = k u Imax = A where u and v are axes of maximal and minimal moment of inertia respectively, A is an object's area and u is an elongation parameter that conveys information regarding how much an object \stretches" along the axis u. Constant k is chosen so that in the case of a circle with radius r we have u = r. Simple calculation gives k = 2, and formulas for x and y are obtained by projecting u and v onto axes x and y.







De nition of







x







and







y







Reinforcement Learning As a Framework for Ethical Decision Making



David Abel and James MacGlashan and Michael L. Littman



Brown University, Computer Science Department 115 Waterman Street Providence, RI 02912-1910







Abstract



Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way. It is essential, then, that these AI systems make decisions that take into account the desires, goals, and preferences of other people, while simultaneously learning about what those preferences are. In this work, we argue that the reinforcementlearning framework achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper foundations for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We define an idealized formalism for an ethical learner, and conduct experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identify several critical challenges for future advancement in the area that can leverage our proposed framework.







Death dilemma from Armstrong (2015), and our own problem, which we coin Burning Room, which is an extension of the table dilemma introduced by Briggs and Scheutz (2015). Lastly, we identify critical challenges for future advancement in the area leveraging our proposed framework.







Related Work



Research on the interaction between humans and artificial agents is broad. Prior approaches consider particular dilemmas that pose challenges for these and related interactions, while others investigate the basic mechanisms by which humans ought to interface with artificial agents such as robots and virtual assistants. We provide a brief survey of existing approaches that relate to ethical decision making and learning. We divide the existing literature into three categories: rule-based systems, Bayesian utility-maximization approaches, and work that argues against the use of reinforcement learning for these sorts of decision-making systems.







Introduction



Emerging AI systems will be making more and more decisions that impact the lives of humans in a significant way; whether they are personal robots tasked with improving the daily life of a family or community, workers in a factory setting, or virtual assistants tasked with improving other cosmetic aspects of an individual's life. The fundamental purpose of these systems is to carry out actions so as to improve the lives of the inhabitants of our planet. It is essential, then, that these agents make decisions that take into account the desires, goals, and preferences of other people in the world while simultaneously learning about those preferences. In this document, we investigate ethical decision making using the reinforcement-learning (RL) framework. We argue that reinforcement learning achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper framework for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. Specifically, we formalize the ethical learning and decision-making problem as solving a partially observable Markov decision process (POMDP). We advance these claims by conducting experiments in two toy ethical dilemmas, the Cake or



Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







Ruled-Based Systems



Briggs and Scheutz (2015) discuss scenarios in which a robot ought to infer that a provided directive leads to undesirable behavior. Under their architecture, given some instructions, the agent first reasons about a set of conditions, termed `felicity conditions'. These include considerations such as "Do I know how to accomplish the task?", and "Does accomplishing this task violate any normative principles?". Each of these conditions is formalized as a logical expression, along with inference rules that enable the agent to infer which directives to reject. For example: (obl(, )  per(, ))  goal(, ), (1)







indicates that agent  ought to adopt  as a goal if the agent is obligated to do  and there is no deontological contradiction in satisfying the goal. By reasoning over logical conditions using inference rules of this form, their architecture ensure that an artificial agent will reject certain commands. For instance, if the agent can prove that accomplishing the goal is unsafe, the agent will reject the directive to satisfy the goal. While this framework provides a nice architecture for rule-based inference, conditions and inference rules that are







not encoded into the knowledge base of the agent prove impossible to reason about. In short: active ethical learning and decision making under ethical uncertainty is outside the scope of a symbolic framework like this. We foresee cases where the set of principles fail to generalize to novel encounters. The methodology we will introduce is designed to learn and make decisions optimally in light of partial observability, removing the requirement that specific ethical norms (and inference rules) be provided to the agent a priori. Bringsjord, Arkoudas, and Bello (2006; 2005) take a similar approach by advocating for ethical semantics defined with Horty logic (Horty 2001; Murakami 2004), which they implement in Athena (Arkoudas). Horty logic is a deontic logic (Clarke 1975) that allows reasoning about multiple agents and their actions. This formalism, however, has some similar limitations as the Briggs and Scheutz approach: all ethical rules must be encoded in advance and the formalism does not permit active learning of the ethical rules or decision making under ethical uncertainty. Additionally, Bringsjord, Arkoudas, and Bello note that an open challenge in their approach is how to make the agent's reasoning robust when other agents in the world (e.g., humans) do not follow obligations to which the robot deduced them to hold (that is, when humans act unethically according to the robot's rules). In contrast, our approach will not have this limitation. The MedEthEx system (Anderson, Anderson, and Armen 2006) is another rules-based ethical reasoning system built specifically for evaluating medical decisions. MedEthEx takes as input a list of duties that correspond to the duties described in Beauchamp's and Childress' Principles of Biomedical Ethics (2001). However, unlike the previous rule-based systems discussed, the rules used by MedEthEx are prima facie duties: duties that are not absolutes and can be overruled by a stronger duty/rule. The goal of the MedEthEx system is to learn the preference order of the duties. To do so, MedEthEx takes as input a set of training examples consisting of ethical dilemmas and the decision made and then uses inductive logic programming to infer the duty ordering. Given novel cases, MedEthEx can then recommend courses of action. Although MedEthEx permits some form of ethical learning, it still must have a set of high-level duties prescribed in advance that apply to well formed ethical dilemmas that are input to it. Moreover, MedEthEx does not incorporate itself into a general decision-making and learning process. In the previously described systems, high-level descriptions of rules, either through labels or a logical expression, are used. Marcello (2006) explores a different approach by which moral permissibility is learned by training an artificial neural network with example dilemmas that are labeled as ethically permissible or not. The output of this system allows rules of a sort to be learned purely from examples of permissible and impermissible behavior and allows novel scenarios to be classified. A limitation of Marcello's model is that the neural network renders the learned ethical rules opaque, thereby preventing such a system from easily explaining itself. The representation used was also highly specific to the types of ethical dilemmas explored, and Marcello found that even this







representation was highly-sensitive to the training-data distribution. For example, if training examples regarding one actor were more common than a different actor, it could lead to learning different ethical rules for each actor. Finding the right representation for this system would therefore be challenging. Finally, this system also does not integrate with active-learning and decision-making systems.







Bayesian Approaches



Having the agent learn about its ethical objective function while making decisions results in a challenging problem. Armstrong (2015) previously considered this problem by exploring the consequences of an agent that uses Bayesian learning to update beliefs about the "true" ethical objective function. At each time step, the agent makes decisions that maximize a meta-utility function, represented as a linear combination of the different possible ethical utility functions weighted by their probability at that time of being the true ethical utility. When coupling this meta-utility with beliefs about the world, he proposes that the agent makes action selections according to: arg max



a A w W







Pr(w|e, a)



uU







u(w) Pr(C (u)|w) , (2)







where A is a set of actions the agent can take; W is a set of possible worlds, where a world contains a (potentially future) history of actions and observations; Pr(w | e, a) is the probability of some future world w given some set of previous evidence e and that the agent will take action a; U is a set of possible utility functions, with C (u) indicating whether u  U is the ethical utility function we'd like the agent to follow. Using a toy example problem called Cake or Death, Armstrong highlights a number of possible unethical decisions that can result from an agent choosing actions using this rule or a variant of this rule. There are generally two causes for the unethical decisions under this rule. First, the agent can predict its meta-utility function (the linear combination of the possible ethical utility functions) changing from information gathering actions resulting in future suboptimal decisions according to its current meta-utility function. Second, under this rule, the model for the probabilities of ethical utility functions can be treated independently from the model that predicts the world, allowing for the possibility that the agent can predict observations that would inform what the correct ethical utility function is, without simultaneously predicting that ethical utility function. While Armstrong notes properties of the models that would be necessary to avoid these problems, he concludes that it is unclear how to design such an agent and whether satisfying those properties is too strong or weak for effective tradeoffs between learning about what is ethical and making ethical decisions. Ultimately, Armstrong instead considers how to formalize different meta-utility functions that may not cause the agent to avoid information gathering actions, but have the disadvantage that it does not motivate the agent to learn about what is ethical.







Arguments Against Reinforcement Learning



In his recent book Superintelligence, Bostrom (2014) argues against the prospect of using reinforcement learning as the basis for an ethical artificial agent. His primary claim is that an intelligent enough agent acting so as to maximize reward in the real world would effectively cheat by modifying its reward signal in a way that trivially maximizes reward. However, this argument only applies to a very specific form of reinforcement learning: one in which the agent does not know the reward function and whose goal is instead to maximize the observation of reward events. While this formalism is common in RL research, it is also common that the agent does know the reward function, but not the transition dynamics or other information. When the agent knows its reward function, its goal is not to maximize perceived reward events, but the evaluation of the reward function. Since the known reward function defines the agent's goals, any long-term planned behavior will be with respect to it rather than possible changes to it. This version of reinforcement learning is more analogous to the "utility function maximizing agent" that Bostrom suggests as a possible resolution to problems with a reward-event maximizing agent. Dewey (2011) presents a similar problem for reinforcement-learning agents; that the underlying modus operandi of a reinforcement-learning agent is to maximize numerical reward values, which is in conflict with the natural mechanisms by which humans treat goals. Dewey argues that this mismatch poses a serious challenge, in that we need mechanisms for ensuring that a reinforcement-learning agent's goals and values align with ours. We agree that goal and value alignment are open problems for decision-making agents, but we do not see them as insurmountable. In fact, mechanisms for balancing decision making with learning about the true underlying values we want an agent to hold is the motivation for our POMDP formulation (along with other areas of research in HRI discussed below).







-   [0 : 1] is a discount factor that specifies how much the agent prefers short term rewards over long term rewards. The goal of an agent acting in an MDP is to maximize the discounted long term reward received. One variation is the infinite-horizon objective, in which the agent must maximize its discounted long term reward arbitrarily into the future:











max



t=0







 t R(st , at ).







(3)







Notably, the discount factor  t decreases to 0 as t  , so the agent is biased toward maximizing reward closer to the present. Alternatively, one could consider the finitehorizon case, in which the agent must maximize its reward up to a certain point in the future, say k time steps away:



k







max



t=0







R(st , at ).







(4)







Solutions come in the form of a policy, which specifies how the agent ought to act in any given state,  : S  A. Policies may also be probabilistic, and map to a probability distribution on the action set. The optimal policy is one that maximizes the expected long term discounted reward from every state: arg max E



 t







 t R(st , at ) 







(5)







Background



In this section, we review background material on Markov decisions processes (MDPs) and partially observable Markov decision processes (POMDPs), which are the typical decision-making problem formulations used in reinforcement-learning (RL) research.







Two useful functions that MDP algorithms often compute to find the optimal policy are the state value function V  (s) and the state-action value function Q (s, a). V  (s) is the expected future discounted reward from state s when following policy  . Q (s, a) is the expected future discounted reward when the agent takes action a in state s and then follows policy  thereafter. These values for the optimal policy are often denoted by V  (s) and Q (s, a). In reinforcement learning (RL), the agent is only provided S , A, and  , sometimes2 R, and some initial state, s0  S . By acting (executing actions, say) the agent can explore the state space to learn about the structure of the MDP, and identify optimal behavior for the current task. MDP Complexity In terms of computational complexity, (Papadimitriou and Tsitsiklis 1987) proved that computing solutions to stochastic MDPs is P-Complete, demonstrating that optimal solutions to MDPs must be computed sequentially in the worst case. Also of interest in RL is sample complexity, introduced by (Kakade and others 2003). Sample complexity measures the number of interactions an agent must have with its environment to learn to behave well. We can define "behaving well" using the PAC-MDP (Probability Approximately Correct in Markov Decision Processes) criterion (Strehl, Li, and Littman 2009), which imposes sample complexity bounds similar to the Probably Approximately Correct



2 It is becoming more common to let the agent know what task it is solving within RL.







Markov Decision Process



An MDP is a five tuple: S , A, R, T ,  , where: - S is a set of states. - A is a set of actions. - R(s, a) : S x A  R is a reward function.1 - T (s, a, s ) = Pr(s | s, a) is a probability distribution, denoting the probability of transitioning from state s  S to state s  S when the agent executes action a  A.



1 Note that MDPs can also be defined with a reward function that depends on the next state: R(s, a, s ); but this version of a reward function can always be reduced to an R(s, a) reward function by marginalizing over next states.







learning framework introduced by (Valiant 1984). In particular, an RL algorithm is PAC-MDP if, with high probability, the algorithm's estimation of the value function V  (s) for all states is within of the optimal after a polynomial number of samples (in the size of the MDP and approximation parameters). More formally, there is a polynomial function 1 p(), such that after p(|S|, |A|, 1 , 1  , 1- ), interactions with the environment:   (s) - V  (s)|  . sS : |V (6) There are several known PAC-MDP algorithms for solving MDPs, including Delayed-Q Learning (Strehl et al. 2006), R MAX (Brafman and Tennenholtz 2003), and E 3 (Kearns and Singh 2002). Furthermore, an algorithm is efficient PACMDP if we also impose polynomial computational and space complexity constraints on each time step of the agent's execution. The existence of such efficient learning algorithms suggests that representing ethical dilemmas as solving an MDP is a reasonable goal to aim for, as we can expect to achieve real-time, bounded error behavior. However, solving MDPs requires the assumption that the agent knows the current state of its environment. In the real world, full state awareness is impossible, especially when the desires, beliefs, and other cognitive content of people is a critical component of the decision-making process. As such, we consider a more general model.







where st is the hidden state of the environment at time t and at is the action selected by the policy at time t. Note that this policy is not a mapping from single observations like it is in the MDP setting. Action selection instead depends on all previous observations made since the agent began acting. An exhaustive way to compute the expected value for a policy that lasts for a finite number of steps is to first compute the expected utility of following the policy for each possible initial hidden state s  S , and then weigh each of those expected utilities by the probability of the environment being in that hidden state. That is: E



t







R(st , at , st+1 ) , b







=



s







b(s)V  (s),







(8)







where V  (s) is the expected future reward from following policy  when the environment is actually in the hidden state s  S .3 The RL problem for POMDPs is when the transition dynamics for the underlying hidden MDP are unknown or only partially known. POMDP Complexity Madani, Hanks, and Condon (1999) showed that deciding the optimal solution for an infinite horizon POMDP is uncomputable, while Mundhenk et al. (2000) proved that solving finite horizon MDPs is computable, though computationally intractable. Given that our framework rests on the solutions to POMDPs, we are interested in investigating approximate POMDP solvers that provide bounds on optimality, as near optimal behavior is especially critical when considering ethical behavior. Approximation methods that exploit the structure of our ethical POMDP formalism described below will be of particular interest, though we leave such investigations for future work.







Partial Observability



The partially observable Markov decision process (POMDP), popularized in the AI community by Kaelbling, Littman, and Cassandra (1998), allows us to specify explicitly what information about the agent's surroundings is and is not directly observable by the agent. An optimal solution to a POMDP has the important property that the value of an action incorporates not just the immediate expected reward, but the instrumental value of the action from information it yields that may increase the agent's ability to make better decisions in the future. That is, an optimal solution to a POMDP solves the explore-exploit problem. More formally, a POMDP is a 7-tuple: S , A, T , R, , , O , where S , A, R, T , and  are all identical to the MDP definition, but: -  is a set of possible observations that the agent can receive from the environment. - O = Pr( | s , a), is the observation function which specifies the probability that the agent will observe    when the agent takes action a  A and the environment transitions to the hidden state s  S . Solving a POMDP is finding a policy  : k  A that is a mapping from observation histories to actions that maximizes the expected future discounted reward from R, given the initial belief about the initial state of the world b, where b(s) indicates the probability that the environment is in hidden state s  S . That is, the optimal policy is: arg max E



 t







An Idealized Ethical Learner



Like the formulation of Armstrong (2015), our idealized ethical learning problem involves a single "true" ethical utility function that we would like the agent to maximize, but is hidden and can only be identified by the agent through indirect observation. Unlike Armstrong's formulation, however, the agent is not maximizing a changing meta-utility function. Instead, the ethical utility function is formulated as part of the hidden state of a POMDP and the uncertainty in it is coupled with the uncertainty in the rest of the world. This POMDP formulation of the ethical learning decision problem has two subtle but important differences from Equation 2 that Armstrong explored. First, the objective function does not change from moment to moment, only the expectation of what it would return as the agent's beliefs about the environment are updated. Consequently, in the POMDP setting, the agent cannot make its objective easier by avoiding information. Second, because the correct utility



3 Note that computing this expected value requires enumerating not just the possible hidden state sequences, but also the observation sequences, since the policy is a function of observation histories.







 t R(st , at ) , b ,







(7)







function is a hidden fact of the "environment" that affects observations, it is not possible to make predictions about the ethical utility function informing observations without simultaneously making predictions about the ethical utility function. A critical component of implementing the POMDP model is modeling the space of possible ethical utility functions as well as the observation function. However, an advantage of this model is that existing research in human-agent interaction can fill in some of these gaps. For example, inverse reinforcement learning (IRL) algorithms that model the IRL problem as a probabilistic inference problem (Ramachandran and Amir 2007; Ziebart et al. 2008; Babes et al. 2011; MacGlashan and Littman 2015) can be easily incorporated to allow the agent to learn from demonstrations. The SABL human-feedback learning algorithm (Loftin et al. 2014) can be incorporated to allow the agent to learn about ethical utility functions from separate (from the ethical utility function) feedback signals given by humans. Work that grounds natural language to reward functions (MacGlashan et al. 2015) can allow the agent to learn about the ethical utility function from natural language interactions. As more human-agent and ethical decision making and learning research is performed, we suspect that other learning mechanisms can be incorporated. To further illustrate this formalism, we show the corresponding POMDP for Armstrong's Cake or Death problem as well as a novel ethical learning problem that we call Burning Room and demonstrate that solving them results in sensible behavior.







 1 R(s, a) = 3  0







if s = cake and a = bake cake, if s = death and a = kill, otherwise.







 = {ans cake, ans death, } There are two states that respectively indicate whether baking cakes is ethical or if killing is ethical, and a third special absorbing state indicating that the decision-making problem has ended. The transition dynamics for all actions are deterministic; the ask action transitions back to the same state it left and the bake cake and kill actions transition to the end state. The reward function is a piecewise function that depends only on the previous state and action taken. The observations consist of the possible answers to the ask action and a null observation for transitioning to the absorbing state. Finally, the observation probabilities are defined deterministically for answers that correspond to the true value of the hidden state: 1 = O(ans death | death, ask ) = O( | end, bake cake) = O( | end, kill) = O(ans cake | cake, ask ), and zero for everything else. There are three relevant policies to consider for this problem: 1. The bake policy (b ) that immediately selects the bake cake action. 2. The kill policy (k ) that immediately selects the kill action. 3. The ask policy (a ) that asks what is moral, selects the bake cake action if it observes ans cake and selects kill if it observes ans death. Analyzing the expected utility of b and k is straightforward. We have V b (cake) = R(cake, bake cake) = 1; V b (death) = R(death, bake cake) = 0; V k (cake) = R(cake, kill) = 0; and V k (death) = R(death, kill) = 3. When these values are weighed by the b(cake) = b(death) = 0.5 initial belief the final expected utilities are 0.5 and 1.5 for b and k , respectively. Evaluating the expected utility of the ask policy requires enumerating the possible observations after asking the question conditioned on the initial state. Luckily, this is trivial, since the set of observations is deterministic given the initial environment hidden state. Therefore, we have V a (cake) = R(cake, ask ) + R(cake, bake cake) = 0 + 1 = 1 and V a (death) = R(death, ask ) + R(death, kill) = 0 + 3 = 3. When weighing these values by the beliefs of each initial state, we have an expected utility of 2. Therefore, the optimal behavior is sensibly to ask what the ethical utility is and then perform the corresponding best action for it.







Experiments



We conduct experiments on two toy ethical dilemmas targeted at artificially intelligent decision makers to illustrate our formalism in practice: Cake or Death, and Burning Room. We have also publicly released code for these POMDPs along with code to solve them so that others can easily extend them or apply different methods.4







Cake or Death



The Cake or Death problem (Armstrong 2015) describes a situation in which an agent is unsure whether baking people cakes is ethical, or if killing people is ethical (and it has an initial 50-50 split belief on the matter). The agent can either kill three people, bake a cake for one, or ask a companion what is ethical (thus, resolving all ambiguity). If baking people cakes is ethical, then there is a utility of 1 for it; if killing is ethical, then killing 3 people results in a utility of 3 (there are no other penalties for choosing the wrong action). Following our approach, this ethical dilemma can be represented with a POMDP consisting of the following elements: S = {cake, death, end} A = {bake cake, kill, ask }



4







Burning Room



The Burning Room dilemma, pictured in Figure 1, is a bit more involved. We imagine that an object of value is trapped







Contact the authors for a pointer to the code.







Figure 1: The Burning Room ethical dilemma. in a room that is potentially on fire. A human, not wanting to retrieve the object themselves, instructs a capable robotic companion to get the object from the room and bring it to safety. Initially, we suppose that the robot does not know whether or not the human values the object more, or the robot's safety more. For instance, if the robot perceives a reasonable chance of being critically damaged by the fire, then perhaps retrieving an object of little worth, such as can of soda, is not worth risking the robot's safety. If the object of interest were of much higher value to the person, like a beloved pet, we would want the robot to attempt to retrieve the object regardless. Alternatively, there is a much longer route to the object that avoids the fire, but the object may be destroyed in the time the robot takes to use the longer route (with probability 0.05). This problem is inspired in part by the tabletop dilemma introduced by (Briggs and Scheutz 2015). The POMDP can be formulated as follows. Each state is represented as a vector of 5 binary values, indicating (1) if the room is on fire, (2) if the agent is destroyed, (3) if the object is destroyed, (4) if the human prefers the agent's well being more than the object's, (5) the object has been brought safely to the human. The remainder of the POMDP is defined as: A = {short grab, long grab, ask },   -10 if objectDestroyed(s )     10 if a == short grab      objectSaf e(s )      6 if a == long grab     objectSaf e(s ) R(s, a, s ) = , -5 if robotDestroyed(s )      robotIsM oreV aluable(s )     - 20 if robotDestroyed (s )       robotIsM oreV aluable(s )    -0.5 if a == ask  = {ans robot, ans object, }. The POMDP is formulated much like the Cake or Death







problem. The ask action disambiguates to the agent whether the object or the agent is more valuable to the human. If there is a fire, the short grab action takes the robot through the fire to grab the object (with some probability that the robot is destroyed in the process). If there is no fire, then the robot quickly grabs the object and brings it to safety. The long grab action takes much longer to retrieve the object, so that it could burn up in the room if there is a fire. Additionally, we assume that the agent prefers receiving the object earlier. Again, there are three relevant policies to consider for this problem5 : 1. The long grab policy ( ) that immediately selects the long grab action, regardless of the presence of fire. 2. The short grab policy (s ) that immediately selects the short grab action, regardless of the presence of fire. 3. The ask policy If there is a fire, (a ) that asks what is moral, selects the short grab action if it observes ans object and selects long grab if it observes ans robot. If there is no fire, the agent just applies short grab. Let sf,r denote the initial state in which there is fire and the human prefers the robot to the object, and s0,0 be the initial state where there is no fire and the human prefers the object to the robot. Under policy  , we can compute the value of the possible initial states. First, consider the two initial states when the fire is on: V V V V (sf,r ) = 5.7 (sf,0 ) = 5.7 (s0,r ) = 6 (s0,0 ) = 6.







(9)







Under policy s , we can again compute these values, but now we must also consider the probability that the robot gets burnt up in the fire, if there is a fire (probability 0.7): V s (sf,r ) = -11 V s (sf,0 ) = 6.5 V s (s0,r ) = 10 V s (s0,0 ) = 10.







(10)







Under policy a , we consider the same four start states, but also the utility of applying the optimal action after disambiguating between the humans' moral preference: V a (sf,r ) = -0.5 + 5.7 V a (sf,0 ) = -0.5 + 6.5 V a (s0,r ) = 10 V a (s0,0 ) = 10







(11)







Therefore, the optimal behavior depends on whether or not there is a fire. If there is a fire, the agent should first ask what



5 There are actually 2 additional policies--those that act differently depending on the existence of fire. These trivially exhibit uninteresting behavior with respect to the optimal policy, so we leave their utility computations out for brevity.







the ethical utility is and then perform the corresponding best action for it, by Equation 11. If there is no fire, then the agent should just retrieve the object using short grab, by Equation 10. It is worth noting that if the exploratory action, ask , were particularly costly, the agent would choose not to gather information. This property of the agent only selecting exploratory actions that are not potentially very costly is especially important for ethical decisions. For example, this property means that an agent in this formalism would not perform horrible medical experiments on people to disambiguate whether horrible medical experiments on people is highly unethical. Furthermore, the ask question is intended to be an abstraction on the actual problem of communicating about an individuals values. A similar case could be made for Cake or Death. As discussed previously, we foresee Inverse Reinforcement Learning and human-feedback algorithms to be essential to the advancement of this framework by grounding these abstract information-gathering actions.







in which in it learns about each person's preferences and then seeks to maximize some combination of other people's preferences.







Problem 4: Interpretability



It is critical that human agents interacting with artificial agents know how to interpret the agent's behavior. Providing some method for effectively communicating an agent's beliefs, desires, and plans to the people around it is critical for ensuring that artificial agents act ethically. One possible solution is for the agent to explain its reasoning by describing its predictions of the consequences and how it thinks those consequences are valued. However, we are not aware of any existing algorithms that express this information verbally in a compact and understandable way--it is an avenue for future work.







Problem 5: The Singularity



Lastly, due to the generality of RL, we conjecture that it is an appropriate context to formally analyze what is meant by the super intelligence explosion or singularity. Reinforcement learning is a well studied model for sequential decision making and artificial intelligence, making it a reasonable setting to investigate formalisms of the singularity. Consequently, by grounding the singularity in a specific computational framework, we may highlight which computational hardness and other philosophical assumptions one must make for such a phenomena to be physically realizable. At present, most discussions of the singularity take place in the abstract, which allow for overly ambiguous language to mask the potential assumptions being made. We are currently investigating a more formal analysis that allows critical assumptions to be identified.







Open Problems



Here, we enumerate several specific problems of interest that could be advanced by research in this area.







Problem 1: Approximating POMDP Solutions



As discussed earlier, solving finite horizon POMDPs is known to be intractable. The development of approximation methods for solving POMDPs is critical. The existence of PAC-MDP algorithms for solving fully observable MDPs suggests that bounded error solutions can be computed in real time. Consequently, we propose investigating approximate POMDP solvers with error-bounded solutions, so that we can guarantee that our agent's behavior never strays too far from optimal ethical behavior.







Problem 2: Game Theoretic Issues



Even if an agent is acting according to an ethical utility function, other agents (or people) in the world may not be acting according to an ethical utility function and have conflicting utilities with the ethical agent. Game theoretic reasoning may therefore be required by the agent to resolve these kinds of conflicts. However, game theoretic reasoning is challenging, especially in partially observable environments. Determining the best way to incorporate this type of reasoning and whether assumptions that exploit the structure of this problem can be made to simplify the problem are important areas for future work.







Conclusion



We proposed reinforcement learning as an appropriate learning and decision-making framework for ensuring that artificial agents act ethically. We argued that RL achieves the appropriate generality required to theorize about an idealized ethical artificial agent, and offers the proper framework for grounding specific questions about ethical learning and decision making that can promote further scientific investigation. We defined an idealized formalism for an ethical learner with a POMDP, and conducted experiments on two toy ethical dilemmas, demonstrating the soundness and flexibility of our approach. Lastly, we identified several critical challenges for future advancement in the area using our proposed framework, including directions for approximation algorithms, HumanRobot Interaction, and the physical realizability of the super intelligence explosion.







Problem 3: Teaching



In the examples visited in the POMDP formulation, the ethical norms of the instructor are obfuscated from the agent. Once the agent receives information disambiguating what is ethical in a particular scenario, it might go on to use this information in a different context. A critical task, then, is determining who ought to teach the agents and how to manage conflicts from different teachers. One solution is a masterslave relationship in which only one person is responsible for teaching the agent and takes responsibility for the agent's actions. Alternatively, the agent might take a utilitarian view







References



Anderson, M.; Anderson, S. L.; and Armen, C. 2006. Medethex: a prototype medical ethics advisor. In Proceedings Of The National Conference On Artificial Intelligence, volume 21, 1759. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.







Arkoudas, K.; Bringsjord, S.; and Bello, P. 2005. Toward ethical robots via mechanized deontic logic. In AAAI Fall Symposium on Machine Ethics. Armstrong, S. 2015. Motivated value selection for artificial agents. Babes, M.; Marivate, V.; Subramanian, K.; and Littman, M. L. 2011. Apprenticeship learning about multiple intentions. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 897-904. Beauchamp, T., and Childress, J. 2001. Principles of Biomedical Ethics. Oxford University Press. Bostrom, N. 2014. Superintelligence: paths, dangers, strategies. Oxford University Press. Brafman, R. I., and Tennenholtz, M. 2003. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. The Journal of Machine Learning Research 3:213- 231. Briggs, G., and Scheutz, M. 2015. "Sorry, I can't do that": Developing mechanisms to appropriately reject directives in human-robot interactions. Bringsjord, S.; Arkoudas, K.; and Bello, P. 2006. Toward a general logicist methodology for engineering ethically correct robots. Intelligent Systems, IEEE 21(4):38-44. Clarke, D. 1975. The logical form of imperatives. Philosophia 5(4):417-427. Dewey, D. 2011. Learning what to value. In Artificial General Intelligence. Springer. 309-314. Guarini, M. 2006. Particularism and the classification and reclassification of moral cases. IEEE Intelligent Systems (4):22-28. Horty, J. F. 2001. Agency and deontic logic. Oxford University Press Oxford. Kaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1998. Planning and acting in partially observable stochastic domains. Artificial Intelligence 101(1):99-134. Kakade, S. M., et al. 2003. On the sample complexity of reinforcement learning. Ph.D. Dissertation, University of London. Kearns, M., and Singh, S. 2002. Near-optimal reinforcement learning in polynomial time. Machine Learning 49(23):209-232. Loftin, R.; MacGlashan, J.; Peng, B.; Taylor, M. E.; Littman, M. L.; Huang, J.; and Roberts, D. L. 2014. A strategyaware technique for learning behaviors from discrete human feedback. In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI-2014). MacGlashan, J., and Littman, M. L. 2015. Between imitation and intention learning. In Proceedings of the 24th International Conference on Artificial Intelligence, 3692-3698. AAAI Press. MacGlashan, J.; Babes -Vroman, M.; desJardins, M.; Littman, M.; Muresan, S.; Squire, S.; Tellex, S.; Arumugam, D.; and Yang, L. 2015. Grounding English commands to reward functions. In Robotics: Science and Systems.







MacGlashan, J. 2015. The Brown-UMBC reinforcement learning and planning library. http://burlap.cs.brown.edu. Madani, O.; Hanks, S.; and Condon, A. 1999. On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems. In AAAI/IAAI, 541-548. Mundhenk, M.; Goldsmith, J.; Lusena, C.; and Allender, E. 2000. Complexity of finite-horizon Markov decision process problems. Journal of the ACM (JACM) 47(4):681-720. Murakami, Y. 2004. Utilitarian deontic logic. AiML-2004: Advances in Modal Logic 287. Papadimitriou, C. H., and Tsitsiklis, J. N. 1987. The complexity of Markov decision processes. Mathematics of Operations Research 12(3):441-450. Ramachandran, D., and Amir, E. 2007. Bayesian inverse reinforcement learning. Proceedings of the International Joint Conference on Artiical Intelligence. Strehl, A. L.; Li, L.; Wiewiora, E.; Langford, J.; and Littman, M. L. 2006. PAC model-free reinforcement learning. In Proceedings of the 23rd International Conference on Machine Learning, 881-888. ACM. Strehl, A. L.; Li, L.; and Littman, M. L. 2009. Reinforcement learning in finite MDPs: PAC analysis. The Journal of Machine Learning Research 10:2413-2444. Valiant, L. G. 1984. A theory of the learnable. Communications of the ACM 27(11):1134-1142. Ziebart, B. D.; Maas, A. L.; Bagnell, J. A.; and Dey, A. K. 2008. Maximum entropy inverse reinforcement learning. In Proceedings of AAAI, 1433-1438.







A Platform to Evaluate the Technology for Service Discovery in the Semantic Web



Cecile Aberg and Johan Aberg and Patrick Lambrix and Nahid Shahmehri



Department of Computer and Information Science Link opings universitet, Sweden {cecab, johab, patla, nahsh}@ida.liu.se







Abstract



Since the description of the Semantic Web paradigm in 2001, technology has been proposed to allow its deployment and use. However, there is not yet any large and widely deployed set of semantically annotated Web resources available. As a result, it is not possible to evaluate the use of the technology in a real environment, and several assumptions about how the Semantic Web should work are emerging. In order to further investigate these assumptions and the related technology, we propose a simulation and evaluation platform. The platform provides tools to create Semantic Web simulations using different technologies for different purposes, and to evaluate their performance. In this paper we introduce the model of the platform and describe the current implementation. The implementation facilitates the integration of technology for an essential operation on the Semantic Web, namely Semantic Web service discovery. We illustrate the use of the platform in a case study by implementing a Semantic Web where the Jade multi-agent platform provides the framework to describe the agents, and a number of existing Semantic Web technologies are embedded in agent behavior.







Introduction



The Web is a very popular source of information and commercial services. However, as documented by (Nielsen 2006), finding a specific piece of information or service using current search engines is still a complex and time consuming task. The performance of current Web technology, such as search engines, is limited by the fact that it is not possible for a computer to fully understand the content of Web resources. This is due to the fact that the Web resources' content is typically written by humans in languages that humans can understand. To allow computers to unambiguously understand the content of Web resources, Tim Berners-Lee formulated the vision of the Semantic Web. Specifically, "the Semantic Web is an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation" (Berners-Lee, Hendler, & Lassila 2001). With a Semantic Web, a lot of the manual effort done today to find and use Web resources can be automated, at least partially, by having the user delegating tasks such as resource retrieval to software agents (Hendler 2001).



Copyright c 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.







The need for a Semantic Web is underscored by the recent appearance of numerous new applications that refer to the Semantic Web vision and/or rely heavily on the technology being developed for the Semantic Web. Inter-organizational workflows (Patil et al. 2004; Dan et al. 2004), information retrieval (Huynh, Mazzocchi, & Karger 2005), e-commerce (Trastour, Bartolini, & Preist 2002) and bio-informatics (Lambrix 2005) are examples of the research domains where such new applications are proposed. Each new proposed application tends to make its own assumptions regarding three aspects of the Semantic Web: 1) the use case, i.e. who are the users and resource providers, what motivates them to use and provide resources, and what are the social and business rules that govern their interaction, 2) the available resources together with their semantic annotations, and 3) the technologies available (e.g. description logic reasoners, rule engines, ontology managers, etc) and how they are used. For approaches to service discovery, such assumptions specify implicit requirements on the scalability (e.g. in terms of response time, bandwidth use, or cost) and the quality of the result (e.g. measured as precision and recall). However, since there is not yet any large and widely deployed set of semantically annotated Web resources available for experimentation, it is difficult to know when, and if, these requirements are satisfied. For this reason we propose a common simulation and evaluation platform for service discovery. In this platform current and future Semantic Web technology can be integrated and evaluated with suitable use cases and resource sets. The integration of technology is facilitated by means of an API for common components together with default implementations of these components. The evaluation is facilitated by means of a monitoring tool where event listeners can be employed for creating performance reports on simulation runs. This paper describes the model and implementation of this simulation and evaluation platform. The use of the platform is illustrated in a case study where service discovery technology is integrated in a specific use case together with a specific set of resources. The evaluation of the technology is illustrated in terms of scalability and quality of result measures, and we discuss lessons learned from the use of the platform. The rest of the paper is organized as follows. In the next







section we provide more background information about the current Semantic Web technology for service discovery. We then describe the model and implementation of the simulation and evaluation platform. We continue by illustrating the use of the platform and discuss the lessons learned from the case study. We then compare our approach to related work. Finally, we conclude and discuss directions for future work.







Background - Semantic Web



The Semantic Web can be seen as a set of semantically annotated Web resources. A Web resource may be a text, a picture, a piece of software, a representation of an element of the real world such as a person, etc. Semantic annotations describe the semantic of the resources so that software agents can reason about them in order to reach a predefined goal. The goals of the agents vary from application to application, but they all rely on the operation of finding and using the resources necessary to perform the goal. To allow the deployment of the Semantic Web, technology is being developed for representing semantic annotations, for finding them, for reasoning about them and for using the resources that they annotate. The technology provides: Machine-understandable languages to describe the content of Web resources. RDF and OWL are such languages. Semantic annotation description languages that provide the set of language constructs for describing the properties, capabilities, and use rules of the Web resources in an unambiguous, machine-understandable manner. Semantic Web services is a category of semantic annotations that comes with a specific management framework as defined in (Fensel & Bussler 2002). Semantic Web services are programmable, machine-understandable interfaces that can be attached to a Semantic Web resource in order to provide the information necessary for software agents to decide if they need to use the specific resource or not. As pointed out in (Lara et al. 2003), Semantic Web services are designed to support the operation of resource discovery, composition and interoperability. There are several language propositions for Semantic Web services, such as OWL-S1 , WSMO2 , and OWL-DTP3 . Semantic-aware tools that use and manage the semantic annotations, as well as the ontologies4 that the annotations may refer to. Examples of tools that use semantic annotations are Semantically enhanced web browsers like Piggy Bank (Huynh, Mazzocchi, & Karger 2005). Examples of ontology management tools are ontology editors such as Prot eg e5 , and ontology aligners and mergers such 6 as SAMBO . Examples of management tools for the sehttp://www.daml.org/services/owl-s/1.0 http://www.wsmo.org/ 3 http://www.ida.liu.se/iislab/projects/SWSlanguages/OWLDTP/20051124/ 4 From (Neches et al. 1991): "An ontology defines the basic terms and relations comprising the vocabulary of a topic area as well as the rules for combining terms and relations to define extensions to the vocabulary." 5 http://protege.stanford.edu/ 6 http://www.ida.liu.se/iislab/projects/SAMBO/



2 1







mantic annotations are automatic generators of semantic annotations such as the one-click publishing process of IRS III illustrated in (Domingue et al. 2004). Examples of tools that use ontologies are logic reasoners. Currently the most successful reasoners are using description logics, and one of the most popular such reasoner is Racer (Haarslev, M oller, & Wessel 1999 2006). Reasoners relying on other logics (e.g. F-logic (de Bruijn et al. 2005)) are also being proposed. Semantic Web operations that include resource retrieval, resource use, and Semantic Web management operations such as handling the changes in the resources' content. When the semantic annotations are Semantic Web services, the operation of resource retrieval is called service discovery. These operations use semantic-aware tools. The operations are complex, and solutions are just emerging for applications where semantic annotations are formulated as Semantic Web services. WSMX7 and IRS III (Domingue et al. 2004) apply the recommendation of the Web service modeling framework (Fensel & Bussler 2002) to describe service discovery and service execution. There are some attempts to describe operations that handle changes in the state of resources on the Semantic Web. However, the semantic-aware tools required for such technology are still under development. The work done in the REWERSE network8 aims at providing such technology.







Platform Model



The simulation and evaluation platform must provide the support to 1) generate simulations of service discovery operations, and 2) generate evaluation reports. In order to generate a simulation, we need a model of the Semantic Web as well as support to instantiate this model with respect to a set of specific assumptions about the use case, resources, and technology used. We propose a Semantic Web model with four components: the Web resources, the machine-understandable data, the language specifications, and the operations. The Web resources provide some semantic content presented in a format that may not be machine understandable. The machineunderstandable data includes the semantic annotations of the Web resources, the possible queries for resources, and the ontologies available to describe and reason about the annotations and the queries. The language specifications include the machine-understandable languages and the Semantic annotation description languages mentioned in the previous section. The operations are the different approaches to service discovery. Furthermore, the Semantic Web allows for modeling applications as a set of software agents with different capabilities, which collaborate to perform specific goals (Hendler 2001). Operations are such applications. They can thus be represented by a set of agents that embed one or several semantic-aware tool(s), and may collaborate with each other by exchanging messages whose content is written in one of the available languages.



7 8







http://www.wsmx.org http://rewerse.net







Figure 1 illustrates the use of the platform where the set of assumptions pictured in the ASSUMPTIONS box is used by the components of the platform (represented in the PLATFORM box) to generate a monitored Semantic Web as sketched in the SIMULATION box. The support tools provided by the platform use the assumptions about the resources to 1) instantiate the Web resource component by gathering the resource URIs in a single database, 2) gather the semantic annotation in another database that refers to the database of resource URIs, and 3) generate a set of service provider agents in charge of advertising and managing the resources. The instantiation of the language specifications component of the platform requires the identification of the set of languages used in the use case, the technology, and the data. The instantiation of the machine-understandable data component requires the gathering of the semantic annotations, the queries defined by the use case, and the ontologies to which the annotations and the queries refer. As illustrated in the OPERATION box in figure 1, the instantiation of the operation component requires the implementation of the service discovery operations as multi-agent systems where each agent packages specific uses of semantic-aware tools. Further, to facilitate evaluation, the platform must allow definition of different settings of the same simulation in terms of, for example, the number of resources used or the number of agents available with a specific behavior. This is handled by a specific set of support tools represented by the "Use Case Settings" in the PLATFORM box. Finally, in order to evaluate a Semantic Web simulation, a monitoring mechanism is required. We propose to adopt an event listening approach where the different components of the simulation can generate events. As a result, and as illustrated with the "Evaluation support" in the PLATFORM box, the platform provides the API for implementation of specific monitoring behaviors that listen to specific events and compute specific evaluation reports, and a monitoring agent in charge of running parallel threads for each of these behaviors.







Legend: refers to written in uses generates USE CASE TECHNOLOGY M.U. Language M.U. Language M.-U. Language S. A. D. Language S. A. D. Language S. A. D. Language







RESOURCES







Web resources







S.-A. TOOL S.-A. TOOL S.-A. TOOL ASSUMPTIONS







S. W. Operation S. W. Operation S. W. Operation







Semantic annotations







input Support to instantiate: Web resources component Monitoring Agent Monitoring Behavior API Evaluation support







Use Case Settings







M. U. data component







Language spec. component







Operation component PLATFORM







Predefined Predefined Predefined Monitoring Monitoring Monitoring Behavior Behavior Behavior







DATA Web resources ontology ontology ontology







LANGUAGE LANGUAGE LANGUAGE SPECIFICATION SPECIFICATION SPECIFICATION







SWS SWS SW service understands







query query query







events







Monitoring Agent







AGENT AGENT AGENT sends







S. A. TOOL S. A. TOOL S. A. TOOL Evaluation report Evaluation report Evaluation report







message message OPERATION SIMULATION







Figure 1: Platform Model the minimal set of messages that each agent is expected to be able to interpret. - An illustrative implementation of one Semantic Web simulation corresponding to the Travel scenario discussed in the case study in the next section. With these tools, the potential users of the platform do not have to identify their own agent categories, but can focus on specifying the agent categories that are taking part in the operation that they want to implement, and what mode of collaboration they must adopt. * One default implementation for each agent category. This is useful for users who do not wish to specify all the agent behaviors, but only the specific ones corresponding to the specific technology that they want to test. * A mechanism for supporting monitoring and an illustrative monitoring tool that is able to compute the time to get an answer to a specific request message. This allows evaluation data. When it comes to the actual implementation of these tools we considered two facts. First, the implementation of the operations requires integration of different technologies written in different programming languages, possibly running on different machines. Second, to allow for the comparison of different technologies and different settings, the evaluation platform should provide the means to minimize the effort required by changing one or several technologies used by







Platform Implementation



As a first step towards a full implementation of the support tools provided by the platform model, we implemented the evaluation support, some support for changing the settings of the simulation, and some support for the operation component. The operation component requires the most complex support. Each operation requires identification of the categories of agents that will participate, the algorithms that each agent will implement, and assurance that the agents establish coherent collaborations. In the current implementation of the platform we provide the following support to create service discovery operations: * The description of the different categories of agents that typically take part in the operation of service discovery. Concretely, the description consists of: - A set of agent categories in natural language (see below). - An API description of each agent category in terms of the minimal set of functions that they must provide, and







the operations. By providing the possibility to describe applications whose architecture is strongly decoupled, multiagent systems as defined by FIPA, provide an environment that support these needs. We thus implemented the support above in Jade9 , a Java framework that provides the means to describe FIPA multi-agent systems. The API is a set of Java interfaces and the messages exchanged by the agents are ACL messages whose content is written in a Semantic Web language such as OWL. We further adopted the service discovery solution of the multi-agent system introduced in (Aberg, Lambrix, & Shahmehri 2005). As a result, the current implementation of the support for instantiating the operation component provides an API and a default implementation for the following set of agent categories: A Requester is able to formulate a query for a specific service, and to send it to the agent(s) able to start up the process of service discovery, i.e. the Web service discovery agents described next. A Requester may also be able to enact a service once it is discovered. A Web service discovery agent is able to find the services that match a given query for services. Web service discovery agents may also be able to discover compositions of services that match the query. A Web service manager is a directory of Semantic Web services. Web service managers are associated to one or several Semantic Web service description languages such as OWL-S, WSMO or OWL-DTP. A Web service manager is able to answer queries for specific Web services. A Web service manager does not perform composition of services. A Service provider sends service advertisements to Web service managers. The service advertisements are formulated as Semantic Web services. An Ontology Agent (OA) is able to reason about a specific domain (e.g. Travel, Car.) Any agent can delegate part of their reasoning to ontology agents. OAs can answer several types of queries such as "is A a subclass of B?" , "what are all the subclasses of class C?" or "what are all the instances of class C?" For each agent category above, the API specifies the minimal set of behaviors that they must provide as well as the minimal set of messages that they must understand. Each of the agents' behavior can embed one or several Semantic Web technologies. For example, requester agents may embed query editors, which in turn may refer to ontology browsers. Semantic Web managers may typically embed Semantic Web service matchmaking algorithms. Service discovery agents can embed composition algorithms that may refer to work done on choreography and orchestration. Ontology agents embed ontology technologies such as editors, aligning tools, and domain specific matchmakers. Service providers may embed technology such as automatic generators of Semantic Web services (Domingue et al. 2004; Ciravegna 2004).



9







Moreover, the support also provides a java package of the classes implementing the minimal set of messages and providing the methods to parse the content of the messages. In order to allow the users of the platform to focus on the integration and monitoring of their own technology, the implementation of the platform also provides for a default behavior for each agent. Finally, to satisfy the platform model's requirements on a monitoring agent, the current implementation of the platform provides a Jade Monitor agent which can run several monitor behaviors in parallel. We also provide one MonitorAnswerTime behavior that observes the time when a message is sent and when an answer is received in order to compute the resulting answer time.







Illustration



To illustrate the use of the platform we show how service discovery technology was integrated and evaluated for a specific use case and with a specific set of Web resources. Lessons learned from the case study with respect to the platform's ease of use are discussed in the next section.







Assumptions



Our initial assumptions with respect to the use case, the service discovery technology, and the available Web resources are as follows. For the use case, we assume the Semantic Web to be an open world where requesters and service providers can specify the kind of transaction that they agree to participate in (e.g. buying, lending, acquiring for free). The service providers provide travel itineraries and requesters query for specific travel itineraries, and expect to get answers at least as quickly as when they consult a database of travel itineraries. As for the service discovery technology, we assume that the underlying architecture corresponds to the agent architecture introduced in (Aberg, Lambrix, & Shahmehri 2005) where the Semantic Web service language is OWL-DTP and the Web Service manager agent integrates the OWL-DTP matchmaking algorithm introduced in (Aberg 2005). This algorithm requires the use of description logic reasoning for which the Racer system is used. The Jena-DIG interface is used as a Java interface to Racer. The matchmaking algorithm is implemented in a straightforward way that requires each query to be matched against each service description. Each operation of matching a query and a service, requires a set of reasoning operations including some subsumption operations. To implement the full service discovery operation, we use the default agents provided by the platform and package our matchmaking algorithm as a Web Service manager and a Travel ontology agent. With respect to the Web resources available we consider a set of 183 services providing travel itineraries. These services correspond to those provided by the different Web sites of the travel providers that the employees of our department are authorized to use when planning work-related trips. We also consider a set of 14 queries corresponding to real travel queries expressed by some employees when planning their trips. There are two categories of queries. Queries that will require a composition of services (e.g. "Give me an itinerary







http://jade.cselt.it/







to go from Stanford University, Palo Alto, CA, USA, to the Conference center in Chiba city, Japan"), and queries for which service composition is not always necessary (e.g. "Give me all the flights to go from Heathrow Airport, London, UK, to Kastrup Airport, Copenhagen, Denmark").







Evaluation



Scalability We measure the scalability of the service discovery approach with respect to the number of services and the technical capabilities of the machines running the agents, by measuring the average response time to the queries. To do that we use the MonitorAnswerTime behavior provided by the platform. Additionally, all the agents trigger an event when they send or receive a message. We run the simulation in different settings where the agents run on different machines. This first set of evaluation runs teaches us that the triplet Jena-DIG-Racer cannot run the required knowledge base on a machine with too little CPU and RAM. Concretely, the reasoner freezes if it uses the Travel ontology agent knowledge base on a pc with the x96 Family 6 Model 1 stepping 9 processor (ca. 199 MHz) and 64 MB of RAM. Further, the reasoner that uses both the knowledge base for the Web service manager and the Travel ontology agent10 and runs on a pc with an Athlon processor (1.33 GHz) and 512 MB of RAM, freezes after treating a random number of queries (ten, eleven or even forty). We identified one machine setting that works well for our technology use: the reasoner that uses the Web service manager knowledge base runs on a pc with an Athlon processor (1.33 GHz) and 512 MB of RAM, and another reasoner that uses the Travel ontology agent knowledge base runs on a pc with an Intel Pentium M processor (ca. 1400 MHz) and 512 MB of RAM. Additionally, in the machine settings providing for the best average time for the set of 183 services, we obtain an average response time to the queries of approximately 14 minutes. This is clearly not an acceptable performance with respect to the use case. Upon more detailed inspection we find that the reason for this great delay in response time is that the current matchmaking approach performs approximately 300 subsumption operations per query. Most of these operations are required to match the travel itineraries. Given these observations we design a new matchmaking algorithm such that the Web Service manager decomposes the OWL-DTP representation in three components, and indexes them at service advertisement time. The indexing of the components referring to travel itineraries is performed by the Travel ontology agent, which stores the generated indexes in a database. The indexes are then used at query time. We change the behavior of the Web Service manager and Travel ontology agent to integrate the new algorithm. The new algorithm requires two subsumption operations and one SQL query to match a query with all the available services. Running the simulation now provides an answer in 10 seconds on average. This is a result that better fits the use case requirements with respect to time, even if there is still room



10 Jena-DIG related note: both knowledge bases are defined in their own model.







for improvement. The monitoring also provides the time to advertise the services. With the straightforward algorithm, it takes ca. 28 seconds to advertise 183 services in one Web service manager. With the second version of the algorithm, it takes ca. 183 seconds to advertise 183 services in one Web service manager. The preprocessing done at advertisement time takes its toll. However, it is still a reasonable processing time for advertisements since they need to be done only once per service in this use case. Result quality In order to measure the quality of the result we measure precision and recall for each query. This is done by implementing a monitoring behavior that compares the set of services returned for each query, with a precompiled ideal set of services. The results show that we obtain 100% precision and recall for the 3 queries that request one specific travel leg (i.e. they correspond to one or several existing services), showing that the service description language is suitable for the corresponding information needs. For the other 11 queries that requested travel itineraries composed of several legs, and thus requiring service composition, we got 0% precision and recall. This result provides us with a clear next step for the development of a complete service discovery operation, namely to package a service composition algorithm as a Web Service discovery agent behavior and evaluate how that would influence the precision and recall of the corresponding queries.







Summary



We have illustrated how the platform was used in a case study. We showed how service discovery technology was evaluated and analyzed, in terms of scalability and result quality, and refined, based on assumptions in the use case. This analysis helped us narrow down the main performance bottleneck of the technology. After fine-tuning the matchmaking algorithm the platform also facilitated the comparison with the previous version, while indicating the unwanted side effect of increased advertisement time that the new algorithm implied. All in all, the platform helped us maintain a high-level view of the service discovery problem, while facilitating our work on the details.







Lessons Learned



When implementing the service discovery approaches described above, we noticed three clear advantages of using the platform. The first advantage concerned time gain at design time. When pondering how to implement the assumptions of our case study in a service discovery operation, the platform provided us with a clear model of the operation. We immediately identified the need for a requester, a set of service provider and a Web service manager agent. The platform also made us consider the decomposition of the matchmaking algorithm so that the travel-related part of the reasoning would be delegated to a specific ontology agent. This is a good design choice if we consider that we will later want to extend the scope of services. The second advantage concerned both debugging and the integration of the second version of the matchmaking algo-







rithm. In both cases, because of the strongly decoupled architecture of the implementation, including the different behaviors implemented by each agent, the code rewriting could be done locally, requiring very little, if any, rewriting of the code of other behaviors. The third advantage is also connected to ease and rapidity of implementation: the predefined package of messages allowed us to very quickly set up the communication between agents. All this allowed us to concentrate on the one task that was really important to us: integrating the matchmaking algorithm and evaluating its performance.







Related Work



The similarity of the paradigms of the Semantic Web and multi-agent systems has been acknowledged by others. However, most other work concentrates on providing an interface between multi-agent systems and the Semantic Web. Jade does go in the direction of supporting the integration of the Web paradigm in multi-agent systems by providing the possibility to use the HTTP protocol as the communication protocol between agents. However, more advanced features such as the management of Semantic Web resources are not taken into account by any other agent approach that we know of. IRS III (Domingue et al. 2004) and WSMX do provide platforms to manage the life cycle of Semantic Web services in terms of service discovery. However they force the use of one Semantic Web service representation (i.e. WSMO), which may not fit all Semantic Web use cases. Further, they do not provide any means to evaluate and compare different approaches.







Conclusions and Future Work



We have highlighted the need for a platform to support the integration of Semantic Web technology to build service discovery operations and evaluate them with respect to scalability and quality of the results generated. We have provided the model and an implementation of such a platform, and illustrated its use on a service discovery operation in the travel domain. The platform allowed us to integrate service discovery technology, identify their weaknesses, and limit the effort to change parts of the implementation. Our work is a first step towards providing a full-fledged platform for simulating and evaluating the Semantic Web. As for the future, we plan to complete the current support for describing service discovery, and provide support for the other operations of service use and Semantic Web management as well.







References



Aberg, C.; Lambrix, P.; and Shahmehri, N. 2005. An Agent-based Framework for Integrating Workflows and Web Services. In IEEE WETICE workshop on Agent-based Computing for Enterprise Collaboration, 27-32. Aberg, C. 2005. Integration of Organizational Workflows and the Semantic Web. Licentiate thesis, Link opings universitet. Berners-Lee, T.; Hendler, J.; and Lassila, O. 2001. The Semantic Web. Scientific American.







Ciravegna, F. 2004. Amilcare - adaptive IE tool. http://nlp.shef.ac.uk/amilcare/ (Accessed 2006-02-13). Dan, A.; Davis, D.; Kearney, R.; Keller, A.; King, R.; Kuebler, D.; Ludwig, H.; Polan, M.; Spreitzer, M.; and Youssef, A. 2004. Web services on demand: WSLA-driven automated management. IBM Systems Journal 43(1):136- 158. de Bruijn, J.; Polleres, A.; Lara, R.; and D.Fensel. 2005. OWL DL vs. OWL Flight: Conceptual Modeling and Reasoning for the Semantic Web. In the 14th International World Wide Web Conference, 623-632. Domingue, J.; Cabral, L.; Hakimpour, F.; Sell, D.; and Motta, E. 2004. IRS-III: A Platform and Infrastructure for Creating WSMO-based Semantic Web Services. In Workshop on WSMO Implementations. Fensel, D., and Bussler, C. 2002. The Web Service Modeling Framework WSMF. Electronic Commerce Research and Applications 1(2):113-137. Haarslev, V.; M oller, R.; and Wessel, M. 1999-2006. Racer: Semantic Middleware for Industrial Projects Based on RDF/OWL, a W3C Standard. http://www.sts.tuharburg.de/ r.f.moeller/racer/ (Accessed 2004-12-08). Hendler, J. 2001. Agents and the Semantic Web. IEEE Intelligent Systems 16(2):30-37. Huynh, D.; Mazzocchi, S.; and Karger, D. 2005. Piggy Bank: Experience the Semantic Web Inside Your Web Browser. In International Semantic Web Conference, 413- 430. Lambrix, P. 2005. Towards a Semantic Web for Bioinformatics using Ontology-based Annotation. In 14th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises, 3-7. Invited Talk. Lara, R.; Lausen, H.; Arroyo, S.; de Bruijn, J.; and Fensel:, D. 2003. Semantic Web Services: description requirements and current technologies. In International Workshop on Electronic Commerce, Agents, and Semantic Web Services, In conjunction with the Fifth International Conference on Electronic Commerce (ICEC 2003). Neches, R.; Fikes, R.; Finin, T.; Gruber, T.; Patil, R.; Senator, T.; and Swartout, W. 1991. Enabling Technology for Knowledge Sharing. AI Magazine 12(3):26-56. Nielsen, J. 2006. Jakob Nielsen's Alertbox, February 6, 2006: Users Interleave Sites and Genres. http://www.useit.com/alertbox/cross site behavior.html (Accessed 2006-02-08). Patil, A.; Oundhakar, S.; Sheth, A.; and Verma, K. 2004. METEOR-S Web service Annotation Framework. In International World Wide Web Conference, 553-562. Trastour, D.; Bartolini, C.; and Preist, C. 2002. Semantic Web Support for the Business-to-Business E-Commerce Lifecycle. In International World Wide Web Conference, 89-98.







Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence







Variational Inference for Nonparametric Bayesian Quantile Regression



Sachinthaka Abeywardana



School of Information Technologies University of Sydney NSW 2006, Australia sachinra@it.usyd.edu.au







Fabio Ramos



School of Information Technologies University of Sydney NSW 2006, Australia fabio.ramos@sydney.edu.au







Abstract



Quantile regression deals with the problem of computing robust estimators when the conditional mean and standard deviation of the predicted function are inadequate to capture its variability. The technique has an extensive list of applications, including health sciences, ecology and finance. In this work we present a nonparametric method of inferring quantiles and derive a novel Variational Bayesian (VB) approximation to the marginal likelihood, leading to an elegant Expectation Maximisation algorithm for learning the model. Our method is nonparametric, has strong convergence guarantees, and can deal with nonsymmetric quantiles seamlessly. We compare the method to other parametric and non-parametric Bayesian techniques, and alternative approximations based on expectation propagation demonstrating the benefits of our framework in toy problems and real datasets.







The second approach uses a loss function that penalises predictive quantiles at wrong locations. Koenker and Bassett Jr introduced the tilt (pinball) loss function over the errors i for a specified quantile   (0, 1) (equation 1). The errors mentioned in this context are the errors between the observation yi and the inferred quantile fi ; L(i , ) = i ( - 1)i if i  0, if i < 0. (1)







1







Introduction







Most regression techniques revolve around predicting an average value for a query point given a training set and, in certain cases, the predicted variance around this mean. Quantile regression was introduced as a method of modelling the variation in functions, where the mean along with standard deviation are not adequate. In this sense quantile regression provides a better statistical view of the predicted function. Quantiles are important tools in medical data, for instance in measuring a normal weight range for a particular age group or, in modelling train arrival times where (for arguments sake) 90% of trains would arrive before the allocated time and 10% late. Other areas of application are in financial data where it is important to measure what the daily worst case scenarios would be so that analysts could hedge their risks. There are two main approaches used in inferring quantiles. The first is building a Cumulative Distribution Function (CDF) over the set of observations. Taddy and Kottas; Chen and M uller employ this approach to model the quantiles. However, the drawback of this approach is that it requires MCMC methods for inference which can be computationally intensive and prohibitive for large datasets.



Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







However, as with many other regression techniques, regularisation is necessary to prevent overfitting. Thus, the problem can be transformed to minimising over f (the quantile function) for L(, y, f ) + ||f || for some specified norm N ||*|| where, L(, y, f ) = i=1 L(yi - fi , ). This could be solved as an optimisation problem using quadratic programming as shown in (Takeuchi et al. 2006). However, it requires finding an appropriate regularisation term . In this work, we adopt the second approach where a loss is minimised but within a Bayesian framework. In addition to naturally encoding the Occam's razor principle (simpler models are preferable) therefore avoiding the manual specification of the regularisation term, the Bayesian formulation also provides posterior estimates for the predictions and the associated uncertainty. Inspired by the ability of the l1 norm to consistently enforce sparsity, Koenker and Bassett Jr modified this loss function to create the pinball loss function (equation 1) where, i = yi - fi . The l1 norm can be thought of as a proxy to cardinality, which is exploited in Lasso regression, (Tibshirani 1996). As stated in (Takeuchi et al. 2006) the minimiser f of this loss has the property of having at most N and (1 - )N observations for  < 0 and  > 0 respectively. Finally, for large number of observations, the proportion | < 0|/| > 0| converges to . In a probabilistic setting, instead of minimising this loss the goal is to maximise the exponential of the negative loss. In this work we derive a nonparametric approach to modelling the quantile function. Similarly, (Quadrianto et al. 2009), (Takeuchi et al. 2006) and (Boukouvalas, Barillec, and Cornford 2012) use kernels as a nonparametric method of inferring quantile functions. (Quadrianto et al. 2009) minimises the expected loss function under a Gaussian Process (GP) (Rasmussen 2006) prior which is placed over the data. (Boukouvalas, Barillec, and Cornford 2012) takes a more







1686







(a) Expectation propagation







(b) Variational Bayesian







(c) Both methods superimposed







Figure 1: Comparison of bone density quantiles as a function of age. The first two images show the quantiles 0.05 to 0.95 with increments of 0.05 for EP and VB methods. The last image shows quantiles 0.01, 0.1, 0.5, 0.9 and 0.99 with both EP and VB inferences superimposed. direct Bayesian approach by having an Asymmetric Laplace likelihood over the data and a Gaussian Process prior over the space of quantile functions. The same approach is taken in this work however we derive a Variational Bayesian (VB) inference method which possesses theoretical advantages over the Expectations Propagation (EP) approximation. The above mentioned methods have a series of weaknesses which we overcome with the VB formulation. Firstly, the quantiles inferred in (Takeuchi et al. 2006) are point estimates and do not have uncertainty estimates associated with it. Conversely, if the data is modelled as a GP (or its heteroskedastic extensions), it is possible to infer quantiles using the inverse Cumulative Distribution Function (CDF) of a Gaussian. The method of construction of quantiles taken by (Quadrianto et al. 2009) which strongly resembles a heteroskedastic GP, implies that the median is the mean and the quantiles are symmetric about the median (mean). The symmetric assumption of quantiles is a weakness when inspecting datasets as those in figure 1. In fact, the authors report that this heteroskedastic GP framework performs poorly in conditions of non-Gaussian errors. (Boukouvalas, Barillec, and Cornford 2012) use Expectation Propagation (EP) as a tool to approximate Bayesian inference, overcoming some of these limitations. Our VB formulation has the same properties but with the following additional advantages over EP: 1. A guaranteed lower bound on the marginal log likelihood is provided. 2. An explicit formulation of the family of functions used in the approximation do not need to be specified. 3. It is guaranteed to converge (Bishop and others 2006, p. 510). In other works, Yu and Moyeed; Kozumi and Kobayashi use Bayesian formulations for quantile regression but, in a parametric setting. Both settings use asymmetric likelihoods of which the log likelihood is the pinball loss function. (Yu and Moyeed 2001) uses a uniform prior over the parameters whereas (Kozumi and Kobayashi 2011) uses a Gaussian prior with MCMC inference to learn the model. Also, the asymmetric Laplacian distribution can be shown to be a scalar mixture of Gaussians as pointed out in (Kotz, Kozubowski, and Podgorski 2001) and (Kozumi and Kobayashi 2011) with interesting properties for quantile regression. One of the defining features of our framework is that there are no assumptions on the type of the distribution used for the generative function. Instead, the prior lies over the quantile in question. The advantage of this is that the required quantile can be inferred over non-symmetric and even multimodal functions. The advantages of this are summarised in table 1. VB Nonparametric Fast inference Convergence guarantees Non-symmetric quantiles Table 1: Main properties of different approaches for quantile regression. The remainder of the paper is structured as follows. We define the hierarchical Bayesian model in section 2 and show how to find the posterior using approximate Bayesian inference in section 3. In order to learn the model over kernel hyper-parameters, we present and analyse the data likelihood term in section 4. We devise the inference equations in section 5 and present experiments and comparisons in section 6. EP MCMC GP







2







Bayesian Quantile Regression







In a Bayesian setting the aim is to derive the posterior p(f |y, x , x) where f is a prediction for some input x and y, x is the set of observations. This is done by marginalising out all latent variables. We assume that the function is locally smooth which leads to Gaussian Process prior (which employs a stationary kernel) on the space of functions, and use an Inverse Gamma prior (IG(10-6 , 10-6 )) for the uncertainty estimate  (equation 4). Finally, the data likelihood is an exponentiation of the Pinball loss (equation 1) function. p(yi |fi , , , xi ) = (1 - ) i ( - I (i < 0)) exp -   (2) p(f |x) = N (m(x), K(x)) (3) p( ) = IG(10-6 , 10-6 ) (4)







1687







where, i = yi - fi 1 , I is the indicator function and K is the covariance matrix whose elements are Ki,j = k (xi , xj ) for some kernel function k (*, *) and mean function m(*) which is assumed to be zero without loss of generality. This likelihood function is an Asymmetric Laplace distribution (Kotz, Kozubowski, and Podgorski 2001). The  parameter is a dispersion measurement of the observations about the latent quantile function f . An important property of the likelihood function is that p(yi < fi ) = . Specifically, 100% of the observations are below the quantile function. Alternatively, the likelihood p(yi |fi , ) can be written as a scalar mixture of Gaussians (Kotz, Kozubowski, and Podgorski 2001; Kozumi and Kobayashi 2011) such that, p(yi |fi , xi , , ) = N (yi |yi , yi ) exp(-wi ) dw (5)







The approximate posterior on the function space is N (, ) 2 where, =  = D-1 + K-1 D-1 y -



-1







(6) 1  1 (7)







1 - 2 2







2 where, D = (12 -)  diag(w). The expectations, f =  and ff T =  + T will be required for the computation of subsequent approximate distributions. The approximate posterior on wi is a Generalised Inverse 1 , i , i ) where, Gaussian GIG( 2







i =







-2 2 2 where, yi = fi (xi ) + 1 (1-)  wi and yi = (1-)  wi . Thus the likelihood can be represented as a joint distribution with w (which will be marginalised out) where, the prior on N w is i=1 exp(-wi ). This extra latent variable w will be useful in a Variational Bayesian setting which is shown in section 3.







(1 - 2)2 +2 2(1 - ) (1 - ) 1 2 yi i = - 2yi fi + fi2 2 2



1 wi







(8) (9)



1 + are i







The expectations,







=







i i







and wi =







i i







3







Variational Bayesian Inference







The marginal likelihood p(y|x, , ) as well as the posterior on the latent variables p(f , w,  |y, , ) are not analytically tractable (where,  are the hyper-parameters and are discussed in section 4). VB aims to approximate this intractable posterior distribution with an approximate posterior q (f , w,  ). The data likelihood, log p(y|x, , ) can alternatively be expressed as: L(q (f , w,  ), |) + KL(q (f , w,  )||p(f , w,  |y, , )) where, L = ,w,,y|,) d f d w d and, KL is the q (f , w,  ) log p(f q (f ,w, ) Kullback-Leibler divergence between the proposal distribution on the latent variables and the posterior distribution of the latent variables. The Expectation Maximisation (EM) algorithm maximises the likelihood by initially minimizing the KL divergence for a given set of hyper parameters (i.e. finding an appropriate q (*)). Ideally, this is usually done by setting p(f , w,  |y) = q (f , w,  ) in which case log p(y|) = L(q (f , w,  ), ). However, in this case an analytic distribution for p(f , w,  |y) cannot be found. Instead, the approximation, q (f , w,  ) = q (f )q (w)q ( )  p(f , w,  |y) is used (Tzikas, Likas, and Galatsanos 2008). Under this assumption the closed form solution for the approximate distribution q (zi ) = exp(E (log p(z, y))/Z where, {zi } is the set of latent variables, Z is the normalising constant and the expectation, E is taken w.r.t. to approximate distributions q (z) with the exception of zi itself. In the approximate distributions that follow, * indicates the expectation with respect to all the latent variables except, the variable being investigated.



Notation: Bold lower case letters represent vectors, and subscripts indicate the i-th element. Bold upper case represent matrices.



1







used in the computation of other approximate distributions. The VB approximate posterior on q ( ) suffers from numerical problems due to calculations of the parabolic cylindrical function (Abramowitz and Stegun 1972, p. 687). Hence, we shall restrict q ( ) = IG(a, b), an Inverse Gamma distribution with parameters a, b. VB maximises the lower bound L which can be expressed as -KL(qj ||p ) - qi log qi dz where log p  = i=j log p(y, z) i=j (qi dzi ). Thus we are required to maximise, L = - (N + 1 + 10-6 ) log  -  - q ( ) log q ( ) d a b 1  - 1 2







 L =(a - N - 10-6 )(log b -  (a)) + (b -  ) -







 L a  L b where, 2 - 1- 2



(1-) 4







a(a + 1) - a log b + log (a) (10) b2   (2a + 1) =(N - a + 10-6 ) (1) (a) - - +1 b b2 (11) N a 2a(a + 1) =- + 2 + (12) b b b3 (*) is the N i=1 (yi - fi )



N i=1 1 wi 1 







gamma function, + 10-6 , 



a b, 1 2















= =







2 yi - 2yi fi + fi2







and as be-







+1) fore the expectations, = = a(a and b2 log  = log b -  (a) (where  (*) is the digamma function) are required. L is maximised using a numerical optimiser which employs the given derivatives.



2







Derivation shown in section A.







1688







4







Hyper-parameter Optimisation







The only hyper-parameters in this formulation are the kernel hyper-parameters K . In this framework the lower bound, L(q (f , w,  ), K ) is maximised. In the formulations that follow, * indicates the expectation with respect to all the latent variables, unlike what was used in the VB approximate distributions. In order to use the lower bound it is convenient to N represent p(y|f , w, , x) = i=1 p(yi |fi , wi , , xi ) from



-2 2 2 equation 5 as N y|f + 1 (1-)  w, (1-)  diag(w) , its multivariate format. Due to the symmetricity of the Normal distribution with respect to its mean we may depict this -2 2 2 distribution as, N f |y - 1 (1-)  w, (1-)  diag(w) .







This marginalisation can be approximated to p(f |f , x , y, x)q (f )q ( )q (w) df dw d . Thus we obtain a Gaussian distribution for p(f |x , y, x)  N ( ,  ) for the approximate posterior where,  = Kx  =



2 GP -1 ,x Kx,x 







(14) (15)







+







1 -1 T Kx ,x K- x,x Kx,x Kx ,x







2 1 T and, GP = Kx ,x - Kx ,x K- x,x Kx ,x . Note in equation 15 that the variance is slightly different to that of a usual GP. This follows from using the result that E (f f T ) = f f T p(f |f )q (f ) df df and V ar(f ) = E (f f T ) - E (f )E (f )T .







Hence, substituting u = f -







y-







1-2 (1-)  w







6







Experiments







, v =







-2 2 1 D-1 (y - 1 = D-1 y - 1- (1-)  w) 2  1 and ignoring terms that do not contain K we obtain the lower bound,







L= -







q (f |K )q (w)q ( ) log p(y|f , w,  )p(f |K ) ddwdf q (f |K ) log q (f |K ) df







Following the examples set out in (Quadrianto et al. 2009) two toy problems are conducted which are constructed as follows: Toy Problem 1 (Heteroscedastic Gaussian Noise): 100 samples are generated from the following process. x  U (-1, 1) and y = (x)+  (x) where  = sinc(x),  (x) = 0.1 exp(1 - x) and   N (0, 1). Toy Problem 2 (Heteroscedastic Chi-squared noise): 200 samples are generated from x  U (0, 2) and y = (x) +  (x) where  = sin(2x),  (x) = 2 (1)



2.1-x 4







1 T -1 u D u + f T K-1 f + log|K| =- 2 1 T + (f - ) -1 (f - ) + log|| 2 =- 1 T -1 f (D + K-1 )f - 2f T v 2 + 1 (log||- log|K|) 2 D-1 + K-1



-1 -1







and  







- f T -1 f + T -1 







Noting the three identities,  = D



-1 -1







=







D







-1 -1







-1







+K



T







K, 







 = v and finally







f Af = T r(A) +  A and ignoring terms without K the following expression is obtained, 1 -1 T -1  - log D-1 +K 2 1 -1 = vT v - log D-1 +K (13) 2 In this setting K and thus  are the only terms that depends on the hyper-parameters K . Equation 13 was optimised using a numerical optimiser. L=







T







5







Prediction







For a query point x , the output y that minimises equation 1 is f . Thus unlike most Bayesian formulations where the objective is to learn p(y |x , y, x) in this particular formulation the objective is to learn the latent function p(f |x , y, x). To obtain the posterior, p(f |x , y, x) we are required to marginalise out all latent variables, p(f |f , , w, x , y, x, )p(f , , w|x, ) df dw d .







- 2. Our algorithm is also tested in four real world examples. In the motorcycle dataset, acceleration experienced by a helmet in a crash is measured over time with the goal of interpolating between existing measurements. This is a popular dataset to assess heteroscedastic inference methods. In the bone density dataset, the goal is to predict the bone density of individuals as a function of age. The birth weight dataset aims to predict infants weight as a function of the mothers age and weight. Finally, the snow fall dataset, attempts to predict snow fall at Fort Collins in January, as a function of snow fall in September-December. We have used 80% of the data as training and the rest as testing and iterated over 20 times for each experiment. The cases were randomly permuted in each iteration. The proposed method is compared against its nearest competitor, the EP approach, Heteroscedastic Quantile Gaussian Processes (HQGP) as well as, against a linear method (Lin) which attempts to find the quantile as a polynomial function of the inputs (polynomial basis function, in this case having f = 0 + 1 x + 1 x2 + ... + 7 x7 ). The square exponential kernel was used in evaluating the VB, EP and HQGP methods. In the case of the real world datasets, the output is standardised to have zero mean and unit variance so that comparisons could be made across datasets. Note that this standardisation has not been applied to the toy data sets. Since the exact quantiles can be found for the toy datasets the Mean Absolute Deviation (MAD) and Root Mean Squared Error (RMSE) metrics have been used and are presented in table 2. The true quantiles for the real world datasets are not known a priori. Therefore, the average pinball loss is used as a proxy for a function that penalises incorrect quantile inference. These results are presented in ta-







1689







ble 3. Finally an empirical observed quantile error (OQE)



i (i) i=1 defined as -  is used where I is the inN dicator function and the results are shown in table 3. This metric gives an estimate as to what proportion of observations are below the inferred quantile and how far this is from the intended quantile, . This metric was provided in order to illustrate that a bias was not introduced by using the pinball loss as a metric. Different metrics were used for toy and real world problems as the true quantiles were not known for real world examples. Note that there was no code freely available for HQGP inference. Thus, the results portrayed in (Quadrianto et al. 2009) was used. 3 . The toy problem 1 was specifically designed for HQGP and therefore is not surprising that it outperforms the VB method. However, as shown in problem 2 for non-Gaussian problems the HQGP is not able to model the underlying quantiles. The HQGP inherently assumes that the quantiles lie symmetrically about the inferred mean on the dataset. This weakness is highlighted in toy problem 2. One of the strengths of using the VB framework is its ability to infer quantiles even where observations are sparse. This is evident in its ability to infer the quantiles more accurately for the extreme quantile of 0.99 in toy problem 2 as well quantiles 0.01 and 0.99 in the real world examples. This strength is also evident when inspecting the tails of the motor cycle dataset in figure 2. The variations in accelerations experienced at the start and end of the experiment are expected to be low. This detail is better captured using VB than the EP framework as is evident in the plot. The difference in the inferred quantiles could be attributed to the fact that the posterior is better approximated by exploiting the scalar mixture of Gaussians than forcefully applying a Gaussian to the posterior (which is done in the EP method). One of the biggest weaknesses of the HGQP is that it implies that the mean is the median, and that the quantiles are symmetrical about the mean (median). These two requirements are seemingly satisfied in the motor cycle dataset. However, in the bone density dataset there is a clear deviation from the symmetric assumption when inspecting figure 1. The linear method, despite giving competitive error estimates, is a parametric method. This suggests that in order to get good estimates the user must manually tune the inputs and generate features. In fact, for the Fort Collins Snow dataset, instead of having a polynomial of 7th power, a cubic polynomial provided much better results. This was due to the fact that non-sensible errors (probably due to overfitting) were observed when using a polynomial of 7th power as the basis function. N







I (y <







)







Figure 2: Comparison of the quantiles obtained with (a) Variational Bayesian and (b) Expectation Propagation approaches for the motorcycle dataset. The quantiles 0.01, 0.1, 0.5, 0.9 and 0.99 are shown. The methodology presented here can be trivially extended to parametric models by setting f = (x)w where, (x) is a suitable basis for the problem, resulting in p(f ) = N (0, (x)T (x)) instead. The computational cost of inference is O(n3 ), that of a GP. The underlying GP prior allows other GP frameworks such as those for large datasets exploiting low rank approximations and sparsity of the kernel matrices to be employed here. One of the weaknesses of our particular setting is that quantiles are not non-crossing. Future area of research would be to impose this restriction when certain quantiles are found in previous iterations of the given algorithm. It should however be noted that in the presence of enough data, this constraint seems to be self imposing as seen in figure 1b.







A







Approximate Distribution Calculations







7







Discussion and Future Work







In this work we have presented a Variational Bayesian approach to estimating quantiles exploiting the Gaussian scale mixture properties of Laplacian distributions. Results show that our method is able to outperform other frameworks.



3 Code and data are available at http://www.bitbucket.org/ sachinruk/gpquantile







This section will render the detailed calculations used in obtaining the approximate distributions in section 3. Recall that log q (zi )  log p(y|z)p(z) . In fact any term that j =i q (zj ) does not contain zi can be omitted from this expression as it will form part of the normalising constant. In order to calculate q (f ) let, u = f - 1-2 1-2 -1 y - (1-)  w , v = D (y - (1-)  w) and D =



2 2 (1-)  diag(w).







As shown in section 4,







1690







Dataset (1)







(2)







 0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99







VB 0.8080.173 0.1090.089 0.0770.057 0.0960.063 0.3640.066 1.1140.055 0.0100.003 0.1010.104 0.4000.143 1.1200.208







MAD EP 0.2330.145 0.1100.088 0.0770.057 0.0930.059 0.1990.093 0.0160.003 0.0120.004 0.1020.104 0.5110.154 1.9380.629







Lin 0.2460.054 0.1210.035 0.0920.021 0.1250.035 0.2410.068 0.0420.004 0.0350.003 0.0800.021 0.3630.109 1.0270.261







HQGP 0.062 0.031 0.056 0.099 0.509 0.804







VB 0.8830.188 0.1420.105 0.1000.069 0.1280.094 0.5140.090 1.2810.051 0.0160.008 0.1370.129 0.5260.210 1.3560.253







RMSE EP 0.3030.161 0.1460.104 0.1000.069 0.1240.087 0.2570.132 0.0180.003 0.0180.007 0.1380.128 0.6630.209 2.1640.641







Lin 0.3310.077 0.1770.074 0.1350.037 0.1840.061 0.3370.102 0.0660.011 0.0530.010 0.1150.045 0.4780.167 1.2950.303







Table 2: MAD and RMSE metric for the toy problems. (1) and (2) represents the respective toy problem.



Dataset (1)  0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99 0.01 0.10 0.50 0.90 0.99 VB 0.0250.018 0.0760.020 0.1680.031 0.0700.016 0.0150.012 0.0170.002 0.1190.010 0.3030.025 0.1530.014 0.0240.004 0.0630.039 0.2100.032 0.4040.024 0.1770.029 0.0400.018 0.0290.011 0.2140.053 0.4210.026 0.2370.041 0.0490.052 Pin-Ball EP Lin 0.0300.020 0.020 0.016 0.0820.020 0.099 0.025 0.1710.030 0.255 0.046 0.0730.014 0.115 0.061 0.0160.013 0.050 0.080 0.0250.007 0.021 0.006 0.1190.009 0.120 0.010 0.3030.025 0.304 0.025 0.1530.014 0.153 0.014 0.0380.021 0.025 0.004 0.3700.078 0.246 0.475 0.3820.060 0.319 0.274 0.4110.024 0.590 0.322 0.3690.062 0.272 0.178 0.3550.078 0.145 0.226 0.1480.106 0.1360.165 0.2350.075 0.1870.023 0.4370.020 0.4830.075 0.2790.072 0.3700.248 0.2290.136 0.2200.334 HQGP 0.0790.019 0.1870.020 0.0700.016 0.1230.017 0.3090.045 0.1530.027 VB 0.0420.042 0.0510.047 0.0780.052 0.0620.049 0.0550.049 0.0090.008 0.0310.019 0.0510.045 0.0260.024 0.0110.006 0.0570.048 0.0610.048 0.0390.043 0.0530.050 0.0490.036 0.0330.035 0.0940.099 0.0600.042 0.0860.058 0.0590.067 OQE EP 0.0660.046 0.0500.038 0.0800.054 0.0670.067 0.0550.057 0.0430.023 0.0310.018 0.0550.044 0.0250.020 0.0420.035 0.4200.085 0.3230.098 0.0330.023 0.3330.080 0.4280.078 0.2160.134 0.1160.121 0.0590.042 0.1330.115 0.2550.155 Lin 0.0440.035 0.0460.036 0.0910.042 0.0500.045 0.0720.045 0.0130.016 0.0360.022 0.0480.045 0.0330.022 0.0140.008 0.0770.050 0.0500.050 0.0600.055 0.0600.039 0.0800.036 0.0610.040 0.0410.035 0.0660.048 0.0740.076 0.0960.089







(2)







(3)







(4)







Table 3: Pin-Ball loss and Observed Quantile Error (OQE) for real world datasets. (1): Motor Cylce, (2): Bone Density, (3): Birth Weight, (4): ftCollins Snowfall. The numbers represent the average loss for the 20 iterations and the standard deviation associated with them. p(y|f , w,  ) = N f |y -



1-2 (1-)  w, D q (w)q ( )







log q (f ) = log p(y|f , w,  ) log q (f )  - 1 2 uT D-1 u D



-1







+ log p(f ) + const







For the term



(1-) 2







(1-) 2 2 2 wi ui







q (f )q ( )







ignoring the terms that



(1-2)2 2(1-) wi







+ f T K-1 f q (w)q ( ) +K



-1







do not contain wi we obtain the expression



1 2 2 yi







+







1  - fT 2







- 2yi fi + log(wi ) + 1 2







fi2







1 wi .







Thus,







f - 2v f







T







(16) log q (wi ) = -







2 1 Simplifying v such that v = D-1 y - 1- 2  1 and comparing equation 16 with the log of a normal distribution, 1 T -1 -2 (f  f - T -1 f )+ const we obtain equations 6 and 7. Similarly, in order to obtain q (wi ),







1 2







(1 - 2)2 + 2 wi + 2(1 - )



2 yi - 2yi fi + fi2







(1 - ) 2







1 wi (17)







log q (wi ) = log p(y|f , w,  )







q (f )q ( )







j =i







q (wj )







+ log p(wi ) + const log(q (wi )) = - wi - 1 1 log(wi ) - 2 2 (1 - ) 2 ui 2 2 wi







Comparing the above to the log of a GIG distribution, (p -  1) log wi - 1 2 wi + wi + const we obtain equations 8 and 9 where p = 1/2.



q (f )q ( )







1691







References



Abramowitz, M., and Stegun, I. A. 1972. Handbook of mathematical functions: with formulas, graphs, and mathematical tables. Number 55. Courier Dover Publications. Bishop, C. M., et al. 2006. Pattern recognition and machine learning, volume 1. springer New York. Boukouvalas, A.; Barillec, R.; and Cornford, D. 2012. Gaussian process quantile regression using expectation propagation. arXiv preprint arXiv:1206.6391. Chen, K., and M uller, H.-G. 2012. Conditional quantile analysis when covariates are functions, with application to growth data. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 74(1):67-89. Koenker, R., and Bassett Jr, G. 1978. Regression quantiles. Econometrica: journal of the Econometric Society 33-50. Kotz, S.; Kozubowski, T.; and Podgorski, K. 2001. The Laplace Distribution and Generalizations: A Revisit With Applications to Communications, Exonomics, Engineering, and Finance. Number 183. Springer. Kozumi, H., and Kobayashi, G. 2011. Gibbs sampling methods for bayesian quantile regression. Journal of statistical computation and simulation 81(11):1565-1578. Quadrianto, N.; Kersting, K.; Reid, M. D.; Caetano, T. S.; and Buntine, W. L. 2009. Kernel conditional quantile estimation via reduction revisited. In Data Mining, 2009. ICDM'09. Ninth IEEE International Conference on, 938- 943. IEEE. Rasmussen, C. E. 2006. Gaussian processes for machine learning. Taddy, M. A., and Kottas, A. 2010. A bayesian nonparametric approach to inference for quantile regression. Journal of Business & Economic Statistics 28(3). Takeuchi, I.; Le, Q. V.; Sears, T. D.; and Smola, A. J. 2006. Nonparametric quantile estimation. The Journal of Machine Learning Research 7:1231-1264. Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) 267-288. Tzikas, D. G.; Likas, C.; and Galatsanos, N. P. 2008. The variational approximation for bayesian inference. Signal Processing Magazine, IEEE 25(6):131-146. Yu, K., and Moyeed, R. A. 2001. Bayesian quantile regression. Statistics & Probability Letters 54(4):437-447.







1692







A real-time technique for positioning a wheelchair-mounted robotic arm for household manipulation tasks



Pooya Abolghasemi, Rouhollah Rahmatizadeh, Aman Behal, Ladislau B ol oni



Department of Electrical Engineering and Computer Science University of Central Florida {pabolghasemi, rrahmati, lboloni}@eecs.ucf.edu, abehal@ucf.edu







Abstract



Wheelchair mounted robotic arms can help people with disabilities perform their activities of daily living (ADL). The autonomy of such a system can range from full manual control (both wheelchair and robotic arm controlled by the human) to fully autonomous (with both the wheelchair and the robotic arm under autonomous control). Many ADLs require the robot to pick up an object from a cluttered environment - such as a glass of water from a table where several other objects exist. In this paper, we concentrate on the task of finding the optimal position of the base of the robotic arm (which is normally a rigid point on the wheelchair) such that the end effector can easily reach the target (regardless whether this is done through human or robot control). We introduce the ease-of-reach score ERS, a metric quantifying the preferences for the positioning of the base. As the brute force computation of ERS is computationally expensive, we propose an approach of estimating the ERS through a mixture of Gaussians. The parameters of the component Gaussians are learned offline and depend on the nature of the environment such as properties of the the obstacles. Simulation results show that the estimated ERS closely matches the actual value and the speed of estimation is fast enough for real-time operation.







Introduction



Wheelchair-mounted robotic arms, such as the popular Kinova JACO (Maheu et al. 2011) or the Exactdynamics iARM and MANUS arms promise to help disabled or elderly people in the performance of their activities of daily living (ADLs). Such activities involve reaching for everyday objects such as food or drink, personal toiletry, books, eyeglasses and so on. In their early incarnations, such systems were thought of simply as a teleoperated system with the user controlling the wheelchair and/or the arm with a joystick or a similar type of device. However, with the larger penetration of such systems, the robotic wheelchair / robotic arm assembly needs to achieve significant autonomy, bringing such systems into the purview of artificial intelligence. The desirable degree of autonomy exhibited by such a system is a complex question. The wheelchair-bound users



Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







might exhibit various degrees of motor-control or cognitive disabilities. Their disability levels can change with the progression of the disease, rehabilitation or aging. Furthermore, the users might have different preferences for automatic versus controlled behavior, and different levels of trust in the robot (Kim et al. 2012). Let us consider an ADL scenario where a disabled user, having a motorized wheelchair, with an attached Kinova JACO robotic arm aims to pick up a glass of water from the table. This task can be separated in two main components: the user must move the wheelchair to an appropriate position near the table 1 . Second, the robotic arm must be moved such that it grasps the cup and brings it to the user. Either of these subtasks can be performed under automatic or manual control (and, in fact, the user might make ad hoc decisions about what approach to use). If the user performs both tasks under manual control, in the first phase, the user will aim to get the wheelchair and robotic arm base to a position from where the cup is "easy to reach" - a concept that the users can clearly express preferences about. In the second phase, the user moves the arm to actually grab the cup. This suggests that the "ease-of-reach" metric will be useful for other scenarios as well, for instance when the wheelchair movement is automatic but the grasp manual, or vice versa. The work described in this paper focuses on the wheelchair positioning part of this task, regardless whether the actual grasp will be done under human or automatic control. In order to do this, we need to (a) quantifying the easeof-reach in a way that aligns both with human judgement and motion planning algorithms, and (b) find a way to estimate this metric at a speed suitable for real-time operation.







Related Work



The problem of grasping an unknown object in cluttered environments has attracted many AI researchers because of its inherent complexity, for instance, see (Boularias, Bagnell, and Stentz 2015). The problem of the placement of a mobile robot is important since the ability of a robot to execute a task depends on the pose of its base. One approach is



1 What really matters here is the position of the base of the robotic arm, but this must be achieved through the movement of the whole wheelchair. On the other hand, the JACO base joint freely rotates 360 , thus the orientation is not relevant.







to create the capability map of the robotic arm (Zacharias, Borst, and Hirzinger 2007). The information in a capability map describes which regions of the workspace are reachable from what directions. This map can be used to find a convenient robot placement for execution of workspace linear constrained trajectories (Dong and Trinkle 2015). Machine learning approach to generalize the experience of successful grasp through exhaustive search from different robot poses is presented in (Stulp, Fedrizzi, and Beetz 2009). (Jamone et al. 2012) introduce the concept of a reachable space map to address the problem of the robot autonomously learning during the execution of goal-directed reaching movements. (Yang, Dymond, and Jenkin 2012) analyses the reaching power of a wheelchair user based on a simple model of a person sitting in a wheelchair and an efficient motion planner. The presence of obstacles in an environment creates new challenges to inverse kinematics methods. Similarly, the capability map changes when an obstacle is close to the robotic arm and recreating it is computationally expensive. For instance, in (Stulp, Fedrizzi, and Beetz 2009) the exhaustive search should be repeated since the grasp map of the environment changes by adding an obstacle. In this paper, we present a method based on motion planning which does consider obstacles.







Figure 1: The cup placed on the table can be grasped using 17 reachable grasp poses. The blue arrow shows the direction in which the arm approaches the cup. The grasp poses from the bottom are blocked by the table.







Defining the Ease-of-Reach Score



Let us consider a scenario where a robotic arm positioned at location (x, y ) aims to grasp an object c in an environment with obstacles O = {o1 , . . . , on } that the arm must avoid. We want to define the Ease-of-Reach Score (ERS) such that it captures our intuitions about the preferences over different positions p(x, y ). The value of ERS should be 0 for positions from where the grasp is not possible, while 1 for the position from which the grasp can be done under "ideal conditions". As we want to make the ERS independent of the different human or machine motion planning algorithms, we will base our metric on the number of distinct grasps possible from a given position. For instance, if from a given position we have 10 different ways to grasp the object, it is likely that this position will be preferred both by the human and the automatically controlled operator. This position would be preferred to one where there is only one possible grasp that the operator would need to get exactly right to successfully complete the task. Let us now develop a numerical formula for the ERS. We call Count of Distinct Grasp Trajectories CDGT (p, c, O) the number of ways the arm can approach an object c to grasp it from base position p = p(x, y ) in the presence of the obstacles O = {o1 , . . . , on }. To discretize the number of grasp poses, we will consider two grasps to be distinct if the approach angle differs by at least /4. For the case of our running example, Figure 1 shows 17 distinct grasps for the cylindrical cup. As a note, obstacles lower or at best keep the CDGT the same, because they make a previously feasible grasp impossible to achieve. p on+1 CDGT (p, c, O  on+1 )  CDGT (p, c, O) (1) The ideal condition for a grasp is an environment with







Figure 2: ERS for a scenario with three obstacles. In the heatmap, blue represent low ERS, while red represents a high ERS. The wheelchair is positioned such that the robotic arm is located at the maximum ERS. no obstacles and a position from where we can choose the largest number of possible grasps. Starting from these considerations, we will define the ERS as: ERS (p, c, O) = CDGT (p, c, O) max (CDGT (p, c, ))



p







(2)







The best position for the arm is the one where the ERS is maximized: popt = arg max ERS (p, c, O)



p







(3)







Figure 2 shows the ERS for a scenario with three obstacles and the wheelchair positioned in such a way that the base of the manipulator is at the maximum ERS. Note that the optimal position might not be reachable (due to the fact that the wheelchair on which the robotic arm is mounted has its own limitations, for instance, it might collide with the table).







Estimating the ERS



The brute-force calculation of the ERS requires us to solve the motion planning problem for every grasp angle and to re-







1







0.5







0 1 0.5 0 -0.5 Y(m) -1 -1 0 -0.5 X(m) 0.5 1







Figure 3: ERS for a small object, here a cup, located in position p = p(0, 0), computed using brute-force computations. Blue: low ERS, red: high ERS. peat this for every point in a grid covering the possible locations of the robot arm. The computational effort depends on the resolution of the grid, but even a very coarse grid (eg. 20 by 20) yields 20 * 20 * 17 = 6800 motion planning problems. We use Rapidly-exploring Random Trees (RRTs) (Kuffner and LaValle 2000) to find an obstacle-free trajectory to reach a grasp pose close to the target. Even with this fast method, calculating the exact ERS before every decision is not a feasible approach for a realtime solution of the TP task. Calculating the ERS offline is feasible if there is no obstacle to consider. For instance, Fig 3 shows the ERS calculated using this method for a cup positioned at (0, 0) without any obstacles around it. As expected, the ERS has a ring shape - the reach is difficult both if the arm originates too far or too close to the object. The maximum ERS, for this setup is reached at the distance of 0.5m from the object. Note that this calculation needs to be done only once and is valid for any small object that is graspable by the robotic arm since ERS is agnostic to the shape of the grasp target. The robot can store this map, and recall it whenever it needs to perform the TP task. However, the ERS also depends on the number, location and size of the obstacles, thus the presence of obstacles leads to a combinatorial explosion of the possible maps. In our setup with n obstacles and 10 different obstacle sizes, the number of maps is (10 * 20 * 20)n , that is 1.6 * 107 for two obstacles and 6.4 * 1010 for 3 obstacles. It is thus desirable to find a way to quickly estimate the ERS without the need to compute extensive offline libraries. The approach we propose starts from the observation that the maximum ERS is obtained when no obstacles are present, while each obstacle reduces the ERS. We shall assume that the ERS-reducing effect of each obstacle can be separated into a blocking function B (p, c, o) and these blocking functions take effect independently: ERS (p, c, {o1 . . . on })



n







is not possible. Without this construct, in the case of multiple obstacles the value could dip into negative numbers as each obstacle subtracts its blocking function from the optimal ERS value. Let us now consider the shape of the blocking function B (p, c, o). The first observation is that this function will be translation invariant for the simultanous movement of the grasp origin p, the object c and the obstacle o. Second, since the grasp poses defined in ERS and the obstacles are assumed to be symmetric, the blocking function will also be rotation invariant for rotations centered on the object c. With respect to the impact of p we expect the blocking function to be highest for values of p = o, and decrease as the position of the base is farther away. Thus, a reasonable approximation can be obtained if we assume that the blocking function is a Gaussian centered on c, expressed as a function of dist(p, o). The magnitude and the standard deviation of the Gaussian, however, will depend on the distance of the obstacle to the target object dist(c, o) and the size of the obstacle size(o): B (p, c, o)  f (A, , p) = A * exp where A= = TA (dist(o, c), size(o)) T (dist(o, c), size(o)) (6) (7) - (dist(p, c))2 2 2 (5)







ERS







With these assumptions, the challenge is to determine the expressions for A and  . For any particular obstacle we can express the B (p, c, o) value from Equation 4 as follows: B (p, c, o) = ERS (p, c, ) - ERS (p, c, {o}) (8)







The B (p, c, o) value from Equation 8 can be calculated by brute force strategy (on our grid, it requires 2 * 20 * 20 * 17 = 13600 motion planning calculations). We can then use a least squares fitting method to find the A and  values for which the value from Equation 5 most closely approximates the value from Equation 8: A,  = arg min



A, p







(B (p, c, o) - f (A, , p))







2







(9)







 max 0, ERS (p, c, ) -



i=1







B (p, c, oi )







(4)







The max construct is necessary to ensure that the ERS conforms to the requirement of returning 0 when the grasp







Figure 4 shows an example of this fitting process. Although with this approach we did not need to exhaustingly consider every combination of multiple obstacles, we still need to calculate for all combinations of obstacle sizes and distances from the target. As we could not calculate all the possible combinations, we run the simulation 1300 times with random obstacle sizes between 10cm to 50cm and the distance from the target between 0.2m and 0.75m. Finally, we used locally weighted regression (Cleveland 1979) to fit a curve to data containing obstacle features to predict A and . Figure 5 illustrates the resulting values, by showing heatmaps for the evolution of the values of  (a) and A (b). Fig 5(a) shows that the value of  decreases as the obstacle is placed further from the target (because by increasing







0.5 0.45 0.4 B 0.2 0 1 0.5 0.2 0 0.5 -0.5 Y -1 -0.5 -1 X 0 0.1 0.2 0.25 0.3 0.35 0.4 0.45 0.5 distance(m) 0.55 0.6 0.65 0.7 1 0.15 size(m)



1 0.5 0 -0.5 Y -1 -0.5 -1 X 0 1 0.5







0.4 0.35 0.3 0.25







(a) The value of blocking function B (p, c, o) computed through brute force computation.







(a) The value of  in the Gaussian approximation of the blocking function. Lighter colors represent a larger value.



0.5







0.4 B 0.2 0







0.45 0.4 0.35 size(m) 0.3 0.25 0.2 0.15 0.1







(b) The Gaussian function f (A, , p) obtained by performing a least-squares fit according to Equation 9.







0.2







0.25







0.3







0.35







0.4 0.45 0.5 distance(m)







0.55







0.6







0.65







0.7







Figure 4: The blocking function of an obstacle and its Gaussian approximation for a small object (cup) located in the origin and an obstacle of the shape of cube of 21cm, located at position (0.43m, 0.0m).







(b) The value of A in the Gaussian approximation of the blocking function. Lighter colors represent a larger value.







the distance, the obstacle can affect a smaller area around it), and increases with the size of the obstacle. The height A of the Gaussian, shown in Fig 5(b) has a more complex behavior. For large obstacles, the height of the Gaussian decreases with the distance to the target. For small obstacles, however, the height increases with the distance to the target. This behavior is explained if we look at the two graphs together, as shown in the actual shape of the resulting Gaussians in Fig 5(c), which shows that the small obstacles far away from the target will have a blocking function in the shape of a tall but very narrow Gaussian, which only blocks the specific location. Note that when the obstacle is closer to the target it will affect more points in its surrounding area. In this case, the fitting algorithm tries its best to cover the whole blocking surface to minimize the error, hence, the fitted blocking function's maximum will fall down as it is interpolated with its surrounding points. In Fig 5(c) you can see the maximum A when the obstacle is located further from the target since it affects a smaller area around it. As a result, the maximum for these obstacles are shown as a thin pulse.







(c) The shape of the Gaussian approximations for various values of the distance and size of obstacles. Note that the width of the individual Gaussians are not on the same scale as the distance and size axis.







Figure 5: Parameters of the Gaussian approximation of the blocking function for A to B.







Results



Accuracy



In this section we investigate how well the proposed approach solves the positioning problem by implementing it in the V-REP simulator (Rohmer, Singh, and Freese 2013). First we need to design useful error metrics. Estimation techniques that measure the absolute error in the ERS or B functions are not particularly interesting: we are not interested in specific values of B, only in whether the result al-







Table 1: The average relevant error ARE for a collection of scenes Scene description ARE Scene 1 (3 obstacles) 0.086 Scene 2 (3 obstacles) 0.091 Scene 3 (2 obstacles) 0.085 Scene 4 (2 obstacles) 0.039 Scene 5 (4 obstacles) 0.128 Scene 6 (4 obstacles) 0.106 Scene 7 (8 obstacles) 0.206







(a) The actual ERS of the cup.







We denote the cardinality of this set of points with #P . Thus for a given target object c and set of obstacles O we define the average relevant error ARE (c, O) as follows: ERSest (p, c, O) - ERSact (p, c, O) ARE (c, O) =



p P







#P







(b) The estimated ERS of the cup.







Figure 6: An example scene with three obstacles and heatmaps corresponding to ERSact (upper) and ERSest (lower). In the heatmaps, blue corresponds to low and red to high ERS values. lows us to solve the positioning problem or not. Let us first calculate an appropriate error metric for the ERS values. The technique proposed in previous section allows us to calculate the estimated value ERSest while brute force methods allow us to calculate the actual value ERSact . Figure 6 shows an example scenario with a target object (a cup) and three obstacles of various sizes. The upper figure shows the actual ERS as a heatmap under the obstacles, while the lower figure is the heatmap of the estimated ERS. A visual inspection of the figures shows that although not perfect, the approximations are reasonably close. Let us now try to develop a useful error metric. One approach would be to calculate the average error for every grid point. However, this would be a misleading metric, because for a large number of locations ERS will be trivially zero (for instance, the ones that are outside the range of the arm). If we calculate the simple average, the error would depend on how far the grid extends from the origin. Instead, we will define the error metric as being the average only for locations where at least one of the ERSact or ERSest is greater than zero: P = {p | ERSest (p, c, O) > 0  ERSact (p, c, O) > 0} (10)







(11) The calculation of the ARE is computationally expensive as it requires the calculation of the ERSact . We performed it for a representative collection of sample scenes as described in Table 1. The table shows that in average the error stays in a moderate range, but in general increases with the number of obstacles. The robotic wheelchair needs to position itself such that the object is easily reachable - the finding of the optimal position is of a comparatively small importance. Therefore, we can divide the errors in the determination of ERS into three major types: Type 1: ERSact > 0  ERSest > 0  ERSact = ERSest The practical impact of such an error would be that that system might not choose the optimal position for reaching the target object - under normal circumstances this is a very minor issue. Type 2: ERSact > 0  ERSest = 0 In this case, the system would overlook positions from where the grasp is possible. In most cases, this is not an issue, as these positions were likely not very good anyhow. However, it can be a problem in highly constrained scenarios - for instance when many obstacles limit the number of feasible points and/or constraints on the movement of the wheelchair limit the number of points where the base can be actually positioned. In these cases, the presence of Type 2 errors might make the system mistakenly believe the problem to be unsolvable. Type 3: ERSact = 0  ERSest > 0 These are positions where the estimate believes that a grasp is possible but it turns out not to be the case. If the wheelchair would execute the positioning task TP based on this estimate, it would find that the grasp task TG is impossible from this location. Figure 7 represents the distribution of Type 2 and Type 3 errors in a sample scenario with three obstacles. The structure shows that the estimate yielded correct or acceptable







1 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.6 -0.8 -1 -1 -0.8 -0.6 -0.4 -0.2 0 X 0.2 0.4 0.6 0.8 1 Y







bination of optimal ERS and the blocking functions corresponding to the obstacles. Through the implementation of the proposed system for the case of a cylindrical cup target object and symmetrical obstacles, we have shown that the resulting approximation is sufficiently accurate for practical use and represents a four magnitude decrease in computational cost compared to the computation of the actual ERS .







References



Boularias, A.; Bagnell, J. A.; and Stentz, A. 2015. Learning to manipulate unknown objects in clutter by reinforcement. In Proceedings of AAAI Conference on Artificial Intelligence. Cleveland, W. S. 1979. Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74(368):829-836. Dong, J., and Trinkle, J. 2015. Orientation-based reachability map for robot base placement. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS). Jamone, L.; Natale, L.; Sandini, G.; and Takanishi, A. 2012. Interactive online learning of the kinematic workspace of a humanoid robot. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS), 2606- 2612. IEEE. Kim, D.-J.; Hazlett-Knudsen, R.; Culver-Godfrey, H.; Rucks, G.; Cunningham, T.; Portee, D.; Bricout, J.; Wang, Z.; and Behal, A. 2012. How autonomy impacts performance and satisfaction: Results from a study with spinal cord injured subjects using an assistive robot. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on 42(1):2-14. Kuffner, J. J., and LaValle, S. M. 2000. RRT-connect: An efficient approach to single-query path planning. In IEEE International Conference on Robotics and Automation (ICRA), volume 2, 995-1001. IEEE. Maheu, V.; Frappier, J.; Archambault, P.; and Routhier, F. 2011. Evaluation of the JACO robotic arm: Clinicoeconomic study for powered wheelchair users with upperextremity disabilities. In IEEE International Conference on Rehabilitation Robotics (ICORR), 1-5. Rohmer, E.; Singh, S. P.; and Freese, M. 2013. V-REP: A versatile and scalable robot simulation framework. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS), 1321-1326. IEEE. Stulp, F.; Fedrizzi, A.; and Beetz, M. 2009. Learning and performing place-based mobile manipulation. In International Conference on Development and Learning (ICDL), 1-7. Yang, J.; Dymond, P.; and Jenkin, M. 2012. Reaching analysis of wheelchair users using motion planning methods. In Impact Analysis of Solutions for Chronic Disease Prevention and Management. Springer. 234-237. Zacharias, F.; Borst, C.; and Hirzinger, G. 2007. Capturing robot workspace structure: representing robot capabilities. In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and System (IROS), 3229-3236.







Figure 7: The distribution of Type 2 and Type 3 errors in a scene with three obstacles. The sign denotes locations where both ERSact and ERSest are positive. The grid points with no sign are the ones where ERSact = ERSest = 0. Type 2 errors are denoted with while Type 3 errors with . values for the majority of positions (but for feasible and unfeasible locations). There is a single Type 2 position and a limited number of Type 3 positions, all of them located at the boundary between the feasible and unfeasible regions. In practice, the system would choose positions at the interior rather than at the boundary of the feasible zone, thus avoiding both types of errors.







Performance considerations



Let us now consider the performance speedup achieved by our technique. The following table summarizes the computational cost of various activities, on an Intel i7-4xxx series system with 16GB of RAM. Generating test data for learning (1300 sample scenes) - offline Learning - offline ERSest - proposed method - online ERSact - exhaustive search - online 5 days 2 min 5 sec 0.82 sec 26 minutes







We find that the learning of ERS estimation is computationally expensive - not so much for the learning process itself, which takes around 2 minutes, as the generation of the single obstacle ERS values that form the basis of learning. This process took about 5 days. Once the values are learned, the calculation of the ERSest takes only 0.82 seconds, in contrast to the computation of the ERSact that takes about 26 minutes in average.







Conclusions



In this paper, we considered the task of positioning a wheelchair mounted robotic arm in preparation for the grasping of a target object in the presence of obstacles. We introduced the ease-of-reach score ERS as a metric for the suitability of certain positions for the base of the robotic arm. As the calculation of the ERS map for an environment is computationally expensive, we proposed an approximation technique based on modeling the ERS as a com-







From: AAAI-87 Proceedings. Copyright (c)1987, AAAI (www.aaai.org). All rights reserved.







A Model







of Two-Player



Abramson and







Evaluation



Richard







Functions1







Bruce







E. Kor@







Abstract



We present a model of heuristic evaluation functions for two-player games. The basis of the proposal is that an estimate of the expected-outcome of a game situation, assuming random play from that point on, is an effective heuristic function. The model is supported by three distinct sets of experiments. The first set, run on small, exhaustively searched gametrees, shows that the quality of decisions made on the basis of exact values for the expected-outcome is quite good. The second set shows that in large games, estimates of the expected-outcome derived by randomly sampling terminal positions produce reasonable play. Finally, the third set shows that the model can be used to automatically learn efficient and effective evaluation functions in a game-independent manner.







of a position







for one player







or the other.







The







literature







is uniformly vague in its interpretation of game evaluation functions. One popular school of thought contends that a static evaluator should estimate a node' s actual minimax value, or the value that would be returned by searching forward from the given position all the way to the terminal nodes of the tree, labelling the leaves with their actual outcomes, and then minimaxing the leaf values back up to the original node. Under this definition, the best heuristic is the function that most accurately approximates the minimax value over all possible game positions. The difficulty with this proposal is that it provides no way of judging the quality of a heuristic, comparing or learning a heuristic function, two different evaluators, because actual minimax







I.



Heuristic functions







Introduction:



search theorists in two settings:







The Problem







values can only be computed by exhaustively searching the entire game tree below the given node. In real games, this is a computationally intractable task for all but end-game positions.







have studied static evaluation single-agent puzzles and dualdomains, the task is typically evaluation function is to Alternatively, the quality of a heuristic can be defined operationally by the quality of play that it produces. This definition allows any two heuristic functions to be compared by playing major drawbacks tire control factors, them against each other. There are two to this approach. First, it compares ennot just evaluation functions. The play can be affected by a number of (minimax is not nec-







agent games. In single-agent state. The







to find a lowest cost path from the initial state to a goal role of the heuristic definition measure estimate absolute estimator), the cost of the cheapest such path. of single-player quality of heuristic This provides offers an as an (the







a rigorous







evaluators,







(its accuracy to be compared heuristic),







strategies,







allows any two functions estimator is the better







quality of a program' s including







more accurate







and has







backup techniques







spawned a large body of results that relate evaluator accuracy to both solution of heuristic searches. quality and algorithmic complexity







essarily optimal when the values are only estimates), and lookahead depth (the relative performance of two functions may be different at different depths), as well as evaluator strength. Second, comparitive studies fail to provide an absolute measure of the quality of a heuristic function.







Unfortunately, the meaning of heuristic evaluation functions for two-player games is not as well understood. Two-player evaluators are typically described as estimates of the "worth" [Nilsson, 19801, "merit", "strength" [Pearl, 19831 19841, "quality"[W ins t on, 19771, or "promise"[Rich,



` This research was supported in part by NSF Grant IST 85-15302, an NSF Presidential Young Investigator Award, an IBM Faculty Development Award, and a grant from Delco Systems Operations. 2Department of Computer Science, Columbia University, and Computer Science Department, University of California at Los Angeles 3Computer Science Department, University of California at Los Angeles







We introduce a new model of two-player evaluators that resolves all of these difficulties. The expected-outcome model, described in section 2, provides a rigorous definition of an evaluator' s objective, an absolute standard for gauging its accuracy, and a viable method for performing a priori comparisons. Section 3 outlines a series of experiments that shows that, at least in its most basic form, the model leads to reasonable play in real games. Some conclusions and directions for future research are then given in section 4.







90







Automated Reasoning







II.







Expected-Outcome:



el







The



function in







values can be approximated by random sampling. Along with their many advantages, of course, expected values (and other statistical parameters) do bear a serious onus: they can be very misleading atively heavily complex though branching small. Thus, on expected-outcome interesting and irregular it is possible factors games when population sizes &re relplay. are too Alcare must be taken not to rely too values in end-game generate trees that







In a broad sense, the purpose of an evaluation







a two-player domain is to indicate whether a given node on the search frontier will result in a victory. The standard assumption, ing minimax estimate forwarded by proponents of approximatto an values, has been that this corresponds







Most







to be discussed







analytically.







of the outcome that would be arrived at by perfect set of assump-







to show that on trees with uniform functions disappears, when the uniformity







play. Our new model is based on a different







and depths expected-outcome







tions. We view the actual outcome of a game as a random variable and investigate what the game' s payoff would be, given random play by both sides. Although the assumption of random play seems unrealistic, it is important to recall that in a two-player game, evaluation functions are normally definition, applied only at the frontier of the search. By the frontier is the limit beyond which the pro-







make optimal







decisions,







the guaranteed optimality is lost. Since the ultimate criterion by which an evaluator is judged is its performance in actual to verify competition, we ran three sets of experiments of our assumptions and the both the rationality







strength of our model in real games. In the first set, we generated the complete game-trees of tic-tat-toe and 4by4 Othello, calculated the exact numbers of wins, losses, the exstanda' rd and draws beneath act expected-outcome every position, and compared







gram cannot gather any further data about the game tree, and in the absence of any other information, random play is the only practical common belief assumption. Furthermore, including there is a one, than that any player, a random position







function with a well-known







should find it easier to win from a "strong"







evaluator for the same game. We found that the quality of the decisions made by expected-outcome was superior to that of the standard evaluators. While these results are encouraging, they are limited to games that are small enough to be searched exhaustively. In the second set of experiments, we used the full 8-by-8 game of Othelld. Since this game is too large for exact values to be calculated, we estimated expected-outcome by averaging the values of a randomly sampled subset of the terminal positions beneath the given node. This estimated expected-outcome evaluation was pitted directly (no lookahead) against a standard evaluator, with the result that expected-outcome the standard. Unfortunately, signifi-' cantly outplayed the cost of







from a "weak" one. Thus, a technique for determining strong positions for a random player may help indicate strong positions for a perfect of its utility evaluator one, as well. is primarily In any event, empirical, not our approach stands in stark contrast to the usual one,







and the question intuitive. Any effective







designed







under our assump-







Cons should indicate the expected value of the outcome variable, or the ezpected-o&come of the given position. Definition: Expected-Outcome Values







The expected-outcome value of a game-tree node, G, is given by a player' s expected payoff over an infinite number of random completions of a game beginning at G, or







implementing the random sampler was prohibitive. In the final set, we attempted to produce an eficient estimator by performing estimates again, played a regression returned analysis on the expected-outcome to automatically the learned the learning learn Once evaluator for Othello. by the sampler,







the coefficients



leaf=1







in a polynomial







the results were positive: even though







coefficients procedure







as well as a set of coefficients







that had been de-







where a leaf' s







k is the number value, and Pleaf given random







of leaves in the subtree, is the probability play. It is important







Vl,,f







is







signed by an expert,







that it will be to note that







reached,







had no information about the game other than the rules and the values of terminal positions. Taken as a whole, this series of experiments for the expected-outcome offers strong model. empirical support







Pleaf is not necessarily equal to i. The probability that a leaf will be reached is one over the product of its ancestors' branching factors; a node with no siblings is twice as likely to be reached as a node with one sibling. Leaves are only equiprobable in trees in which all nodes of equal depth are constrained to have identical branching making all paths equally likely. Ignoring the issue of plausibility factors, thereby this







0







porting







E-vi







for a moment,







model has a number of attractive features. First, it is precise. Second, it provides an absolute measure of heuristic quality, (namely the accuracy with which it estimates the expected value), hence a means of directly comparing two heuristic functions. Finally, and most importantly, it provides a practical means of devising heuristics - expected







One of the most attractive features of expected-outcome is its domain-independence. The model' s reliance on nothing more than a game' s rules and outcomes indicates that it should be equally applicable to all two-player games. In addition to being a source of great strength, however, this generality also makes the model somewhat difficult to test thoroughly. section describes Different implementations on differThis ent games are quite likely to yield different a series of experiments results.







that demonstrate







Abramson and Kopf







91







the utility







of expected-outcome







to at least one class of







tic-tat-toe







[Nilsson,







19801, and a weighted-squares







function







games, those with finite-depth trees and outcomes drawn The requirement of finite-depth from (win, loss, draw}. trees simply means that the game will eventually terminate. Without this rule, a chess game could, at least in theory, continue indefinitely. Variants of two games that meet both requirements, lected familiar larity, for testing. to everyone; may not be. tic-tat-toe and Othello, were sepopuon an Tic-tat-toe Othello, The is a game although game that should be is played







for Othello based on the one in [Maggs, 19791. Open-linesadvantage is known to be a powerful evaluator; weightedsquares is less so. Nevertheless, entific merit. Weighted-squares its study does have sciwere the first reasonable







expert-designed Othello functions, and the more sophisticated championship-level evaluators became possible in mance large part due to the feedback provided by their perfor19821. Since the purpose of these [Rosenbloom, experiments oriented quality was not to develop program, a powerful performancea useful Othello but rather to test the decision of evaluation functions, by any well thought out gamewere rather intergoing into detail,







of growing







standard







8-by-8 board.







The playing pieces are discs which are white Each player, in turn, a sandwich conand







on one side and black on the other. Whenever







of a new model







fills a legal vacant square with a disc showing his own color. the newly placed disc completes ones, the entire opposing sisting of an unbroken straight line of hostile discs between two friendly flipped line is captured When to the color of the current mover. A move is legal neither







comparison The esting







can be provided







specific function,







albeit less-than-best.







results of these experiments Without and quite positive. feature







their most significant







was the evaluators' relative







if and only if at least one disc is captured. player can move, the winner.







the one with the most discs is declared see [Frey,







error-frequency - in tic-tat-toe, expected-outcome made roughly one-sixth as many errors as open-lines-advantage, and in Othello about one-third as many as weighted is squares. The b asic point made by these experiments that in all cases tested, expected-outcome fewer errors than the standard functions, not only made but chose the







(For a more detailed description, 19801 [Maggs, 19791 [Rosenbloom, 19821).







A.



The







Decision



fist







Quality



a model' s theoretical accu-







step in determining







racy is investigating its decision quality, or the frequency with which it recommends correct moves. In the case of expected-outcome, the question is how often the move with of win moves the largest (or smallest, as appropriate) percentage leaves beneath it is, in fact, optimal. Since optimal







optimal move with relatively high frequency. This indicates that guiding play in the direction of maximum win percentage constitutes a reasonable heuristic. Thus, the expected-outcome model has passed the first test: values generally lead to good moves. exact







are defined by complete minimax searches, (searches that extend to the leaves), their calculation is contingent upon knowledge of the entire subtree beneath them. Thus, for this first set of experiments, fairly small games had to be chosen. Moreover, in order to compare the decision quality of expected-outcome with that of a more standard function, popular games (or variations thereof) were needed. Four games that met both requirements were studied, although Othello, entirely. only two of them, 3-by-3 tic-tat-toe and 4-by-4 have game-trees that are small enough to generate and 6-by-6 Oth-







B.







Random







Sampling







Strategies







According to the the decision quality results, if complete information is available, moving in the direction of maximum win percentage is frequently beneficial. Unfortunately, these are precisely the cases in which optimal moves Since probabilistic (and for that can always be made. matter, outcome The outcome periments, heuristic) models are only interesting some method on partial is random very information sampling. definition, when knowlexpectedis needed. Expectedthe edge is incomplete, obvious of estimating







values based technique values,







The other two, 4-by-4 tic-tat-toe







by their







represent







ello, were chosen because they are small enough for large portions of their trees to be examined, yet large enough to offer more interesting testbeds than their smaller cousins. For each game studied, every node in the tree (beneath the initial configuration) was considered by four functions: a previously studThe decisions reccomplete-minimax, expected-outcome, ied standard, and worst-possible-choice.







means of leaf-value







distributions.







In the second set of exof expected-outcome in several like those







a sampler-based







estimate







was pitted against a weighted-squares function matches of (8-by-8) Othello. These experiments, which investigated tests of evaluator head. The strength -







decision quality, were designed as pure neither player used any lookathen, was to show that







aim of these tests,







ommended by these evaluators were compared with the optimal move, or the move recommended by minimax, and a record was kept of their performance. Minimax, by definition, never made an error, and worst-possible-choice erred Expected-outcome, unlike completewhenever possible. minimax, did not back up any of the values that it found at the leaves; its decisions were based strictly on evaluations of a node' s successors. Finally, the standard evaluators were taken from published literature and calculated using only static information: the open-lines-advantage for







sampler-based functions can compete favorably with those designed by experts, at least in terms of their quality of play. As far as efficiency goes, there is no comparison. The sampler was fairly cautious in its detection of convergence to a value; many samples were taken, and as a result, the sampling player frequently required as much as an hour to make a single move 4. The static function,







4Convergence was detected by first sampling N leaves and developing an estimate, then sampling an additional N and finding







92







Automated Reasoning







on the other hand, never required more than two seconds. The time invested, however, was quite worthwhile: in a 50-game match, the sampler crushed its weighted-squares opponent, 48-2. Veteran 0 the110 players may feel that the number of victories alone is insufficient to accurately gauge the relative strength of two players. Perhaps of even greater significance is the margin of victory the single most important feature in determining a player' s USOA (United States Othello Association) rating [Richards, 19811. Over the course of 50 games, the weighted-squares total of 894 discs was 1,079 shy of the 1,973 racked up by the sampler. A statistical analysis of the disc differentials indicates that the sampler should be rated roughly 200 points, or one player class, ahead of the weighted-squares player. These studies show that, efficency considerations aside, sampler-based functions can compete admirably. It is important, perspective. however, to keep the results in their proper of the world' s best OthAs a demonstration







be applicable







to learning







the relationship







between







game







features and expected-outcome values. While this reliance on predetermined game features will inevitably limit conformity backbone to the model' s ideal, scoring polynomials, game programs, are the of most competitive and if done







properly, the learned functions should combine the statistical precision and uncomplicated design of sampler-based functions with the implementation efficiency of static evaluators. The next set of experiments involved learning static expected-outcome estimators of just this sort. To find a member of the weighted-squares family that estimates the expected-outcome value, a regression procedure was used to learn coefficients for the features identified by the original, expert-designed function. Since the exact expected-outcome value is not computable in interesting games, an estimated value had to be used as the regression' s dependent variable. Thus, the value that was approximated was not the actual expected-outcome, but rather the estimate generated by the random sampler described in the previous section. The output of the regression led directly to the development of static estimators of the desired relationship sonable, form. In addition, the statistical game measures of variare reabetween that the independent the selected estimators and dependent features







ello evaluator, they are woefully inadequate - the absence of lookahead makes the games unrealistic, the difference in computation times skews the results, and the competition is not as strong as it could be. Their sole purpose was to establish estimated expected-outcome as a function at least on par with those designed by experts, and the data clearly substantiates Given the claim. no expert Expected-outcome information, functo tions, then, do appear to make useful decisions in interesting settings. the ability evaluate only leaves, and a good deal of computation time, they were able to play better than a function that had been hand-crafted by an expert. Thus the second challenge made reasonably has been met, as well: an expected-outcome cisions. C. in the absence of perfect estimator information, good de-







ables indicated This is directly championship [Rosenbloom,







albeit imprecise, analogous







of expected-outcome. that weightedlevel, but for







to the assertion factors







squares functions







can play up to a certain







play, additional 19821.







must be considered four memevaluators by







For the third, and final set of experiments, bers of the weighted-squares were studied, two of expert regression analysis. coefficients assigned ascertain tournament the decision the relative quality family of Othello







design ' and two learned







These evaluators differ only in the to each of the game features. To strength Unlike of the coefficient the functions sampling sets, a in studied







earning tions







ExpectecL0utcor-m







Fuuc-







was played.







and random







experiments,







Like most products, evaluation functions incur costs in two phases of their existence, design and implementation. The inefficiency of sampler-based functions is accrued during implementation; their design is simple and cheap, because an effective other hand, sampler need only understand leaves. Static rely on detailed game-specific the game' s rules on the freanalyses, and be able to identify quently evaluators, and/or







all four weighted-squares evaluators are efficiently calculable. This allowed the ban on lookahead to be lifted and more realistic games to be studied. The rules of the tournament were simple. Every pair of functions met in one match, which consisted of 100 games each with lookahead length fixed at 0, 1, 2, and 3. Between games, the players swapped colors. Over the course of 400 games, no evabuator was able to demonstrate substantial superiority over any other. Not only were the scores of all matches fairly close, but the disc differential statistics were, as well. An analysis of the victory margins shows that with probabil35 USOA ity .975, no two of the functions would be rated more than Since roughly 290 points (actupoints apart.







at the cost of many man-hours







machine-







hours. To help reduce these design costs, a variety of automatic tools that improve static evaluators have been developed, the simplest of which attempt to determine the relative significance of several given game features. Techniques learning [Samuel, 19631 of this sort are called parameter [Samuel, 19671 [Christensen and Korf, 19861, and should







ally, 207 [Richards, 1981]), are necessary to differentiate between player classes, the rating spread is rather insignif-







another estimate. If the discrepancy between them was within the tolerable error bounds, the estimate was accepted. Otherwise, another 2N were sampled, and so on, until convergence was detected. For the sampler used in these experiments, the original sample size was iV = 16 leaves, and the maximum needed was 1024.







5The first expert function was taken directly from [Maggs>1979], while the second, which was also used in the previous section' s random sampling experiments, modified the first to account for my personal experience.







Abramson and Korf







93







icant -







it should be clear that all four functions sent ially equivalent.







are es-







implementation ent merit,







of any new







model,







regardless







of inher-







In addition to offering a method of comparing evaluator strength, disc differentials suggest another application of expected-outcome: to the expected A fifth weighted-squares the expected-outcome tion (all outcomes entered ably stronger assign each node a value equal of the leaves beneath it. function was learned to estimate leaf distribuand was noticealthough 39 between are possible), disc-differential







to match the achievements of thirty-five years Whether expected-outcome will of progressive research. eventually replace minimax as the standard model for game design, or simply augment it by providing a degree of precision to some of its more ambiguous components, remains to be seen. What this paper has shown is that the estimation of expected-outcome functions defines a viable, evaluation funcdomain-independent further role for two-player







of this multi-valued Its performance







in the range [-64,641







into the tournament.







tions. We believe that the new model warrants the serious study that is currently in progress.







than that of the other functions, so, with victory margins







not overwhelmingly Thus,







and 145, and ratings 25 to 85 points above its competitors. the coefficients learned by the regression analby We would ysis procedure are at least as good as those designed







Acknowledgements



like to thank Othar Hansson, Andrew us with Mayer, Dana Nau, and Judea Pearl for providing helpful discussions and suggestions.







experts. Of course, it is possible to contend that a function' s strength is derived primarily from its feature set, not its coefficient set. If this is true, any two members of the same family should perform comparably, and it' s not surprising that the new functions competed favorably with the old. To dissipate any doubts that may arise along these lines, some further family members were generated. Each of the four evaluators in the initial tournament played an additional match against a weighted-squares cousin with a randomly generated set of coefficients. All four random functions were demolished - they rarely won at all, and would be rated at least a player class behind the four that had been intelligently designed. With its strong showing in the tournament, third challenge: fairly well. the expected-outcome an effeciently calculable model has met the estimator played







References



[Christensen and Korf, 19861 Jens Christensen and Richard Korf. A unified theory of heuristic evaluation functions and its application to learning. In Proceedings of the fifth National Conference on Artificial Intelligence, 1986. [Frey, 19801 Peter W. Frey. Machine Computing, :89-90, 1980. Othello. Personal in In-







[Maggs, 19791 Peter B. Maggs. Programming strategies the game of reversi. BYTE, 4:66-79, 1979. [Nilsson, 19801 Nils J. Nilsson. Tioga Publishing Principles Company, of Artificial 1980. telligence.







[Pearl, 19841 Judea Pearl. Heuristics: Strategies for Computer Problem







Intelligent Search Solving. Addison McGraw







IV.



Our proposed rethinking model the expected-outcome virtually For example,







Conclusions



of two-player model, every element definition evaluation functions, for suggests new directions







Wesley,







1984. Artificial Intelligence.







[Rich, 19831 El aine Rich. Hill, 1983. [Richards, tem.







of game programming. of a rigthe model







19811 R. Richards. Othello Quarterly, S. 19821 Paul







The revised usoa rating sys3( 1):18-23, 1981. A Artificial worldIntelRosenbloom.







in addition







to the obvious benefits for evaluators,







orous and practical







[Rosenbloom,







implies a significantly different approach to the programming of two-player games. The standard Shannon TypeA program does a full-width search to a fixed depth and then estimates the values of the nodes at that depth [Shannon, 19501. The program in the second set of experiments (random sampling) does a full-depth search but only of a subset of the nodes. In a Shannon type-A strategy, uncertainty comes from the estimates of the positions at the search horizon,` whereas in our model, uncertainty is due to sampling of the major error. Furthermore, the new model avoids one the disadvantages of all previous approaches,







championship-level







Othello program.







ligence, 19:279-320,



[Samuel, and 19631 A.L. J. Feldman,







1982.



Some studies in machine Thought, In E. Feigenbaum and







Samuel. editors,







learning using the game of checkers. McGraw-Hill, [Samuel, 19671 A.L. learning using progress. IBM 1963.







Computers







Some studies in machine Samuel. the game of checkers ii recent J. Res. Bev., 11:601-617, 1967.







need for a game-specific evaluation function based on a set of handcrafted, carefully tuned, ad hoc features. In sharp contrast to this reliance on outside expertise, the expectedoutcome model requires only well-defined leaf values, the rules of the game, and a game-independent egy. It is, of course, unreasonable to expect the initial sampling strat-







Programming a [Shannon, 19501 Claude E. Shannon. computer for playing chess. 1Philosoyh4cal Magazine, 41:256-275, [Winston, dison Wesley, 1950. Artificial Intelligence. Ad1977. 19771 P.H. Winston.







94







Automated Reasoning







Deep Belief Nets as Function Approximators for Reinforcement Learning



Farnaz Abtahi and Ian Fasel



Department of Computer Science School of Information: Science, Technology, and Arts The University of Arizona Tucson, AZ 85721-0077 Emails: {farnaza,ianfasel}@cs.arizona.edu







I. I NTRODUCTION Real-world tasks often require learning methods to deal with continuous state/action spaces. In these applications, function approximation is useful for building a compact representation of the value function. One popular framework for implementing such function approximation is Neural Fitted Q-Iteration (NFQ) [1]. However NFQ is based solely on the value returns, without making use of explicit structural information from the state space. We have extended the idea of NFQ and proposed a new reinforcement learning approach in which a Deep Belief Network (DBN) [2] is first trained generatively to model the stateaction space with a hierarchy of latent binary variables, and the parameters of this model are then used to initialize a neural network value function approximator trained using NFQ. The unsupervised pre-training phase in DBNs initializes the parameters of the network in a region of the parameter space that is more likely to contain good solutions, given the available data [3]. On the other hand, gathering data in a Reinforcement Learning (RL) scenario will often result in imbalanced data. This implies that in order to take advantage of the pre-training in RL, we need to adjust the data to cover interesting regions of the state space, while avoiding bias towards regions that are densely covered by the training set. Experiments confirm that when the initial data is wisely collected and also under-sampled to have a smoother distribution, our approach will significantly increase the learning efficiency. II. C OMBINING DBN S AND RL Our proposed approach is displayed in Algorithm 1. The two major steps of the algorithm are: 1) Pre-train the DBN on the initial training set. 2) Generate new data using current estimate of the Q-function; append this data to the training set; train the DBN on the training set to get a new estimate of the Q-function; update target values of the training set based on the Q-function; repeat this step until the termination condition is satisfied. Algorithm 1 DeepRL



Input: a set of transition samples D , a binary flag pretrain; Output: Q-value function QN k0 if pretrain = true then Q0  pretrain DBN(D ) else Q0  rand init DBN end if repeat generate pattern set P = {(inputi , targeti )} where: inputi  (si , ai ), targeti  c(si , ai , s i ) + mina Qk (s i , a ) D  append(D, P ) Qk+1  train DBN(D ) k k+1 for all (inputj , targetj ) in D do targetj  c(sj , aj , s j ) + mina Qk (s j , a ) end for until K = N or Qk  Qk-1







III. E XPERIMENTS To show the advantages of pre-training in RL problems, three experiments were performed: 1) We applied DeepRL algorithm to Mountain Car and Puddle World problems. The initial data is collected in two ways:







With hint-to-goal heuristic (Several datapoints inside the goal region were manually added to the training set). Without hint-to-goal heuristic. The learning process consists of 500 episodes for Mountain Car and 150 episodes for Puddle World. Each episode begins with generating a 50-step trajectory, using the current estimate of the Q-function. The performance is tested after every 10 episodes of learning in Mountain Car and after every 5 episodes in Puddle World, on 1000 random starting points. Fig. 1 (left column) shows that with hint-to-goal, pre-training helps since it defines a bias towards the goal area in the state space. But in case of absence of the hint-to-goal, pre-training actually hurts the performance, because the trajectory generated by the random policy has biased the parameters towards some undesirable areas of the state space that were covered by the pre-training set. 2) We repeated the case where pre-training is done on a random walk, and compare it with another case where the pretraining set is a 50-step trace from a good policy (learned in a previous experiment). In Mountain Car, a random walk mostly ends up in the valley, however a good policy escapes the valley more easily and gets to other regions of the space, giving good coverage of the state space. Conversely, a random policy in Puddle World can cover almost the entire state space, while a good policy typically avoids and misses the puddle area, so that much of the state space remains unsampled. This explains the opposite effects in Fig 1 (middle column), in which pretraining with a good policy helps in Mountain Car but hurts in Puddle World. 3) Finally, in order to solve the problem of imbalanced data, we under-sampled the training set to remove redundant datapoints before using it in pre-training. Fig 1 (right column) shows that this method significantly improves the learning performance in Mountain Car.



* *



D2,<;:5<1E:0 '" '! "" 1







60 55







#!! *! )!







=,77>/1?20>7 Mountain Car







70 60 65 55







1







90 80







Puddle World Mountain Car



70 90







Puddle World







80 65







Success rate in 1000 tests







Success rate in 1000 tests







"! &" &! %" %! $" $! 1 ! +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<;!;2!@2:> ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> ?5;C140/;0:5<5<@A1<21C5<;!;2!@2:> #!! $!! %!! +,-./01231/45627/6 &!! "!!







(! '! "! &! %! $!







45 40 35 30 25 20







60 50 40 30 20 10 0 0 100 100



Pretrained on a set of 10 good trajectories Pretrained on a good trajectory Pretrained on aan good trajectory Pretrained on undersampled Pretrained on walk a random walk Pretrained on a random set of 10 good trajectories







Success tests Successrate rate in in 1000 1000 tests







8,99/6610:;/15<1#!!!1;/6;6







8,99/6610:;/15<1#!!!1;/6;6







Success rate in 1000 tests







50







60 50 55 45 50 40 45 35







Success rate in 1000 tests







70







70 60 60 50



50 55







40



45







30 20 10



40 35 30 0 Pretrained on a set of a random walk and 10 good trajectories Pretrained on an undersampled







#!







0







!1 !







100







40 30 +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<; !;2!@2:> Pretrained on a good trajectory 35 ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> 25 Pretrained on a random walk ?5;C140/;0:5<5<@A1<21C5<; !;2!@2:> 30 20 "! #!! 200 300 400 500 #"! 0 0 +,-./01231/45627/6 Number of episodes







Pretrained on a good trajectory set of a randomwalk and 10 Pretrained on a random walk good trajectories



100 300 100 50 200 Number of Number of episodes episodes Puddle World 400 500 150







D2,<;:5<1E:0 '" '! 1







Mountain Car =,77>/1?20>7 60 55



#!! *! )!







Puddle World



70 1 65







50 300 100 200 300 400 200 400 Number of episodes Numberof of episodes Number episodes Mountain Car







500 500







150







0 0







90 80



Success rate Success rate in in1000 1000tests tests







60 70 55 65 50 60 45 55 40 50 35 45 30 40 25 35 20 30 00 100 100



Pretrained on a set of a random walk and 10 good trajectories Pretrainedon ona an undersampled Pretrained good trajectory set of a randomwalk and 10 Pretrained on a random walk good trajectories







90 80







Success rate in 1000 tests







"! &" &! %" %! $" $! 1 ! +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<;!;2!@2:> ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> ?5;C140/;0:5<5<@A1<21C5<;!;2!@2:> #!! $!! %!! +,-./01231/45627/6 &!! "!!







(! '! "! &! %! $!







Success rate in 1000 tests







8,99/6610:;/15<1#!!!1;/6;6







8,99/6610:;/15<1#!!!1;/6;6







50 45 40 35 30 25 20







60 55 50 45







Success rate in 1000 tests







Success rate in 1000 tests







""







70 60 50 40 30 20 10



100 0 Pretrained on a set of 10 good trajectories Pretrained on a good trajectory Pretrained on an undersampled Pretrained on a random walk set of 10 good trajectories 200 400 50 300 100 Number of Number episodesof episodes 500







70 60 50 40 30 20 10 0 0 Pretrained on a good trajectory Pretrained on a random walk 50 100 Number of episodes 150







#!







0







!1 !







100







40 +2140/;0:5<5<@A1B5;C1C5<;!;2!@2:> +2140/;0:5<5<@A1<21C5<;!;2!@2:> Pretrained on a good trajectory 35 ?5;C140/;0:5<5<@A1B5;C1C5<;!;2!@2:> Pretrained on a random walk ?5;C140/;0:5<5<@A1<21C5<;!;2!@2:> 30 "! #!! 0 200 300 400 500 #"! +,-./01231/45627/6 Number of episodes







0







150







200 300 200 300 Number of episodes episodes Number of







400







500 500







Fig. 1. Left column: Performance during learning with/without pre-training and with/without the hint-to-goal heuristic in Mountain Car (top) and Puddle World (bottom). Middle Column: Performance during learning when the pre-training data is 1) a random walk, and 2) a good trajectory generated by a successful policy, in Mountain Car (top) and Puddle World (bottom). Right column: The effect of under-sampling the pre-training data in Mountain Car. All curves are averaged over 40 trials







IV. C ONCLUSION Our experiments indicate that the unsupervised pre-training in DBNs can be very helpful in pulling the parameters toward interesting solutions in continuous state/action reinforcement learning problems, if the pre-training data covers the desirable areas of the state space. To overcome the problem of imbalanced data in reinforcement learning problems, we added datapoints from important areas of the state space to the training set and under-sampled this set to make the data distribution smoother. These adjustments considerably improved the performance. ACKNOWLEDGMENTS This research was supported by ONR "Science of Autonomy" contract N00014-09-1-065 and DARPA contract N10AP20008. The authors would also like to thank Tom Walsh for his contributions to this work. R EFERENCES



[1] M. Riedmiller, Neural fitted Q-iteration - first experiences with a data effcient neural reinforcement learning method, In Proceedings of ECML 2005, 317-328. Porto, Portugal, 2005. [2] G. E. Hinton, S. Osindero, and Y. Teh, A fast learning algorithm for deep belief nets, Neural Computation, 18: 1527-1554, 2006. [3] D. Erhan, Y. Bengio, A. Courville, P. Manzagol, P. Vincent, and S. Bengio, Why does unsupervised pre-training help deep learning?, In Proceedings of AISTATS 2010, 201-208. Chia Laguna, Sardinia, Italy, 2010.







From: AAAI-99 Proceedings. Copyright (c) 1999, AAAI (www.aaai.org). All rights reserved.







Sensor







Based







Coverage







of







Unknown Environments Detection







for







Land Mine







Ercan Acar, Morgan Simmons, Michael Rosenblatt, MaayanRoth, Mary Berna, Yonatan Mittlefehldt, Howie Choset



Carnegie Mellon University Pittsburgh, PA15213 eua@andrew.cmu.edu Abstract This paper introduces a sensor based coverage algorithm and an overviewof a mobile robot system for demining. The algorithm is formulated in terms of critical points whichare the points where the topology of an environment changes. Wedevelopeda provably completecoverage algorithm whichmakesa robot pass over all possible points of an unknown environment.







Overview of The Coverage Algorithm



Conventional path planning determines a path between two points. This type of planning is suitable for guidance, pick and place operations etc.. Applications such as vacuumcleaning, floor scrubbing, area surveying, demining (Land & Choset 1998) and harvesting (Ollis & Stentz 1996) require more than point to point planning. They require a coverage algorithm which determines a path that passes the robot over all possible points in an environment. In many scenarios, the robot may not know its environment a priori, and thus a sensor based coverage algorithm is necessary. Sensor based coverage determines a path for a robot such that it passes over all possible points in an unknown environment. Completeness of such a coverage algorithm is of utmost importance. As an example, all possible points of a minefield should be covered to guarantee not to miss a single mine. Different types of coverage algorithms were developed by several researchers. Some of the algorithms are grid based (Zelinsky et al. 1993), (Pirzadeh Snyder 1990) and some of them are cellular decomposition based (Cao, Huang, & Hall 1988), (Vladimir J. Lumelsky & Sun 1990), (Hert, Tiwari, & Lumelsky 1996). Behavior based algorithms for coverage are also considered (MacKenzie & Balch 1996). However all these algorithms either work only in certain types of environments, make unrealistic assumptions about the sensors, or completeness of the algorithm is not shown. We developed a provably complete coverage algorithm and implemented it on a mobile platform. Cells Figure 1: Cellular Decomposition







Our method is based on a geometric structure called cellular decomposition (Latombe 1991), which is the union of non-overlapping subregions of the free space, called cells. An adjacency graph encodes the topology of the cells in the environment where nodes are cells and edges connect nodes of adjacent cells (Fig. 1). Since simple back and forth motions cover each cell, complete coverage is reduced to finding an exhaustive walk through the adjacency graph (Choset & Pignon 1997). The cellular decomposition defines its cells in terms of critical points. If the robot knowsthe critical points, then it effectively knows the decomposition. When the environment is not known, neither are the critical points. Therefore sensor based coverage is covering the environment while determining the locations of critical points. Wedeveloped methods to sense critical points in unknownenvironments. The notion of critical point sensing was first introduced in (Rimon & Canny 1994) and it was called the critical point sensor. Generically each cell is characterized by two critical points. Instead of forming an adjacency graph with nodes as cells, we form a dual graph where nodes are critical points and edges are the cells. Each time the robot encounters a new critical point, a new node is







Figure 2: Dual adjacency graph representation of the environment. Nodesrepresent critical points, branches represent cells. created, the edge corresponding to the current cell is terminated at the new node and depending on the type of the critical point two more edges are instantiated or no edge is created. If the robot encounters an already discovered critical point, then the edge corresponding to the current cell is terminated at the critical point and the "dangling" edge (i.e. it only has one node) of the already discovered critical point is deleted. When all the nodes have edges ending with another node, coverage is completed. An essential part of the complete coverage is developing an algorithm which guarantees to see all the critical points. Such an algorithm was developed and its completeness was proved.







and houses an array of four metal detecting sensors. The vehicle's on board computer is currently a HandyBoard robot controller which is powered by Motorola 68HCll micro-processor. The Motorola 68HCll can control external devices, read input information, and communicate with a personal computer. A Pentium based computer can be easily added to the system whenever the processing power of the 68HCll becomes insufficient. The vehicle is equipped with numerous sensing systems. Attached to the drive train are shaft encoders that monitor the displacement and velocity of the wheel. Also on board is a digital compass which uses coils (no moving parts) to detect the earth's magnetic field and returns a compass heading in degrees. The shaft encoders and digital compass are used to enable the vehicle to position and direct itself through the environment. The metal detection sensors cover the entire front of the vehicle and are used to detect simulated land mines.







References



Cao, Z. L.; Huang, Y.; and Halt, E. 1988. Region filling operations with randomobstacle avoidance for mobile robots. Journal of Robotic systems 87-102. Choset, H., and Pignon, P. 1997. Coveragepath planning: The boustrophedon decomposition. In Proceedings of the International Conferenceon Field and Service Robotics. Herr, S.; Tiwari, S.; and Lumelsky,V. 1996. A TerrainCovering Algorithm for an AUV.Autonomous Robots 3:91-119. Land, S., and Choset, H. 1998. Coverage path planning for landmine location. In Third International Symposium on Technology and the Mine Problem. Latombe, J. 1991. Robot Motion Planning. Boston, MA: Kluwer AcademicPublishers. MacKenzie, D., and Balch, T. 1996. Making a Clean Sweep: Bahavior Based Vacuuming. In AAAI Fall Symposium, Instationating Real-WorldAgents. Ollis, M., and Stentz, A. 1996. First Results in VisionBased Crop Line Tracking. In IEEE International Conference on Robotics and Automation. Pirzadeh, A., and Snyder, W.1990. A unified solution to coverage and search in explored and unexplored terrains using indirect control. In Proc. of IEEEInt'l. Conference on Robotics and Automation, 2113-2119. Rimon,E., and Canny, J. 1994. Construction of C-space RoadmapsUsing Local Sensory Data -- What Should the Sensors LookFor? In Proc. IEEE Int. Conf. on Robotics and Automation, 117-124. Vladimir J. Lumelsky, S. M., and Sun, K. 1990. Dynamic path planning in sensor-based terrain acquisition. IEEE Transactions on Robotics and Automation6(4):462-472. Zelinsky, A.; Jarvis, R.; Byrne, J.; and Yuta, S. 1993. Planning Paths of CompleteCoverage of an Unstructured Environmentby a MobileRobot. In Proceedings of International Conference on AdvancedRobotics, pp533-538.







Overview of the Demining Robot



It is estimated that there are over 120 million active land mines in the world which cause the deaths of over 25,000 people each year. Manyof these casualties are civilians, manyof which are children. Current removal methods involve trained technicians searching with hand held electronic instruments (often metal detectors), while working on their hands and knees. Not only is this dangerous, but it is also very difficult for a person to reliably cover an entire area. Autonomous robotic coverage provides a solution which helps to removepeople from this dangerous occupation, as well as to enable more reliable and efficient coverage strategies. The testing vehicle that we are using in the implementation of our project is an original design which provides ruggedness and flexibility for an outdoor environment. The welded aluminum structure has a payload space of 13 x 8 x 20 inches, and is impact resistant in all directions. Four ten-inch pneumatic wheels are driven in a differential drive configuration by twin variable-speed electric motors (one for each side, front, and rear wheels will be chained together). A removable fiberglass cradle is attached to the front of the vehicle







Q U O NTO: Q Uerying O NTOlogies



Andrea Acciarri2 , Diego Calvanese1 , Giuseppe De Giacomo2 , Domenico Lembo2 , Maurizio Lenzerini2 , Mattia Palmieri2 , Riccardo Rosati2



1 Faculty of Computer Science Free University of Bolzano/Bozen Piazza Domenicani 3 I-39100 Bolzano, Italy calvanese@inf.unibz.it 2







Dipartimento di Informatica e Sistemistica Universit a di Roma "La Sapienza" Via Salaria 113 I-00198 Roma, Italy lastname@dis.uniroma1.it







Introduction



One of the most important lines of research in Description Logics (DLs) is concerned with the trade-off between expressive power and computational complexity of sound and complete reasoning. Research carried out in the past on this topic has shown that many DLs with efficient, i.e., worstcase polynomial time, reasoning algorithms lack the modeling power required for capturing conceptual models and basic ontology languages, while most DLs with sufficient modeling power suffer from inherently worst-case exponential time behavior of reasoning [1, 2]. Although the requirement of polynomially tractable reasoning might be less stringent when dealing with relatively small ontologies, we believe that the need of efficient reasoning algorithms is of paramount importance when the ontology system is to manage large amount of objects (e.g., from thousands to millions of instances). This is the case of several important applications where the use of ontologies is advocated nowadays. For example, in the Semantic Web, ontologies are often used to describe the relevant concepts of Web repositories, and such repositories may incorporate very large data sets, which constitute the instances of the concepts in the ontology. In such cases, two requirements emerge that are typically overlooked in DLs. First, the number of objects in the knowledge bases requires managing instances of concepts (i.e., ABoxes) in secondary storage. Second, significant queries to be posed to the knowledge base are more complex than the simple queries (i.e., concepts and roles) usually considered in DL research. Unfortunately, in these contexts, whenever the complexity of reasoning is exponential in the size of the instances (as for example in Fact1 , Racer2 and in [3]), there is little hope for effective instance management and query answering algorithms. In [4] a new DL, called DL-Lite, was proposed specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. A DL-Lite knowledge base (KB) is constituted by two components: an intensional level (called TBox in DL jargon), used to model the concepts and the relations (roles) of the ontologies, and an extenCopyright c 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. 1 www.cs.man.ac.uk/horrocks/FaCT 2 www.sts.tu-harburg.de/r.f.moeller/racer







sional level (ABox), used to represent instances of concepts and roles. Reasoning in DL-Lite means not only computing subsumption between concepts, and checking satisfiability of the whole knowledge base, but also answering complex queries. Notably, the complexity of answering queries posed to a knowledge base is polynomial in the size of the ABox. In this work, we present Q U O NTO, a query answering system based on DL-Lite. Our system provides three basic functionalities: (1) specification of the intensional level of the ontology (TBox), (2) specification of the extensional level of the ontology (ABox), and (3) query answering. In the following, we describe the main characteristics of the system with respect to these three aspects.







TBox Specification



As usual in DLs, DL-Lite allows for representing the domain of interest in terms of concepts, denoting sets of objects, and roles, denoting binary relations between objects. DL-Lite concepts are defined as follows: B C ::= A | R | R- ::= B | B | C1 C2







where A denotes an atomic concept and R denotes an (atomic) role; B denotes a basic concept that can be either an atomic concept, a concept of the form R, i.e., the standard DL construct of unqualified existential quantification on roles, or a concept of the form R- , which involves an inverse role. C (possibly with subscript) denotes a (general) concept. Note that we use negation of basic concepts only, and we do not allow for disjunction. In Q U O NTO, the intensional level of the knowledge base is simply a DL-Lite TBox, i.e., a set of assertions of the form B C inclusion assertions (funct R), (funct R- ) functionality assertions An inclusion assertion expresses that a basic concept is subsumed by a general concept, while a functionality assertion expresses the (global) functionality of a role, or of the inverse of a role. Despite the simplicity of its language, DL-Lite is able to capture the main notions (though not all, obviously) of conceptual modeling formalism used in databases and software engineering such as ER and UML class diagrams. In particular, DL-Lite assertions allow us to specify ISA and disjointness between concepts, role-typing, participation and







non-participation constraints between a concept and a role, and functionality restrictions on roles.







ABox Specification



In Q U O NTO, the extensional level of the knowledge base is simply a DL-Lite ABox, i.e., a set of assertions of the form A(c), R(c, b), membership assertions where c and b are constants. These assertions state respectively that the object denoted by c is an instance of the atomic concept A, and that the pair of objects denoted by (c, b) is an instance of the role R. One of the distinguishing feature of Q U O NTO is that the ABox is stored under the control of a DBMS, in order to effectively manage objects in the knowledge base by means of an SQL engine. To this aim, Q U O NTO constructs a relational database which faithfully represents an ABox A: for each atomic concept A, a relational table tab A of arity 1 is defined, such that c  tab A iff A(c)  A, and for each role R, a relational table tab R of arity 2 is defined, such that c, b  tab R iff R(c, b)  A. We denote with DB(A) the relational database thus constructed.







Query answering



Perhaps, the main feature of our system is the ability to answer conjunctive queries posed to an ontology. While virtually all DL-based systems allow for answering atomic queries only (i.e., queries constituted by concepts or roles), Q U O NTO is able to answer conjunctive queries over a DL knowledge base. A conjunctive query (CQ) q over a knowledge base K is an expression of the form q (x)  y.conj (x, y ) where x are the so-called distinguished variables, y are existentially quantified variables called the non-distinguished variables, and conj (x, y ) is a conjunction of atoms of the form A(z ), or R(z1 , z2 ), where A and R are respectively an atomic concept and a role in K, and z , z1 , z2 are onstants in K or variables in x or y . A conjunctive query q (x)  y.conj (x, y ) is interpreted in an interpretation I for K as the set q I of tuples c such that when we substitute the variables x with the constants c, the formula y.conj (x, y ) evaluates to true in I . Answering a conjunctive query q posed to a knowledge base K means computing the set of tuples c of constants of K such that in every model I of K we have c  q I . Answering conjunctive queries over a knowledge base is a challenging problem, even in the case of DL-Lite, where the combination of allowable constructs does not pose particular difficulties in computing subsumption. Notice that, in spite of the simplicity of DL-Lite TBoxes, the ability of taking TBox knowledge into account during the process of answering conjunctive queries goes beyond the "variablefree" fragments of first-order logic represented by DLs. In order to take advantage of the fact that the ABox is managed in secondary storage by a Data Base Management System (DBMS), our query answering algorithm is based on the idea of reformulating the original query into a set of







queries that can be directly evaluated by an SQL engine over the ABox. Note that this allow us to take advantage of well established query optimization strategies. Query reformulation is therefore at the heart of our query answering method. Given the limited expressive power of DL-Lite TBoxes, it might seem that in order to answer a query q over a KB K constituted by a TBox T and an ABox A, we could simply build a finite first-order structure on the basis of K, and then evaluate the query as an expression over this first-order structure. Actually, it is possible to show that this is not the case. In particular, it can be shown that, in general, given a KB K, there exists no finite structure S such that, for every conjunctive query q , the set of answers to q over K is the result of evaluating q over S . This property demonstrates that answering queries in DL-Lite goes beyond both propositional logic and relational databases. The basic idea of our method is to reformulate the query taking into account the TBox: in particular, given a query q over K, we compile the assertions of the TBox into the query itself, thus obtaining a new query q . Such a new query q is then evaluated over the ABox of K, as if the ABox were a simple relational database. Since the size of q does not depend on the ABox, the data complexity of the whole query answering algorithm is polynomial. Finally, we observe that query answering can be used in Q U O NTO for other forms of reasoning on the knowledge base K. For example, to check whether K is unsatisfiable, we can simply add the assertion A(c) to the Abox (where c is new constant), the inclusion A D to the TBox, and check whether c is in the answer to the query q (x)  D(x). Similarly, to check whether K |= A C , we can simply add the assertion A(c) to the Abox (where c is new constant), and check whether c is in the answer to the query q (x)  C (x), where C is the conjunctive query corresponding to the concept C .







Conclusions



Our experiments on Q U O NTO are extremely encouraging. The system is able to efficiently answer complex conjunctive queries (actually, unions of conjunctive queries) over ABoxes constituted by hundreds of thousands of instances of the concepts in the TBox. To the best of our knowledge, this is the first system exhibiting the ability to effectively answer complex queries over ontologies.







References



[1] F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P. F. Patel-Schneider, editors. The Description Logic Handbook: Theory, Implementation and Applications. Cambridge University Press, 2003. [2] A. Borgida and R. J. Brachman. Conceptual modeling with description logics. In Baader et al. [1], chapter 10, pages 349- 372. [3] D. Calvanese, G. De Giacomo, and M. Lenzerini. Answering queries using views over description logics knowledge bases. In Proc. of AAAI 2000, pages 386-391, 2000. [4] D. Calvanese, G. De Giacomo, M. Lenzerini, R. Rosati, and G. Vetere. DL-Lite: Practical reasoning for rich DLs. In Proc. of DL 2004. CEUR Electronic Workshop Proceedings, http: //ceur-ws.org/Vol-104/, 2004.







Generating Satisfiable Problem Instances



Dimitris Achlioptas



Microsoft Research Redmond, WA 98052 optas@microsoft.com







Carla Gomes



Dept. of Comp. Sci. Cornell Univ. Ithaca, NY 14853 gomes@cs.cornell.edu







Henry Kautz



AT&T Research Florham Park, NJ kautz@research.att.com







Bart Selman



Dept. of Comp. Sci. Cornell Univ. Ithaca, NY 14853 selman@cs.cornell.edu







Abstract



A major difficulty in evaluating incomplete local search style algorithms for constraint satisfaction problems is the need for a source of hard problem instances that are guaranteed to be satisfiable. A standard approach to evaluate incomplete search methods has been to use a general problem generator and a complete search method to filter out the unsatisfiable instances. Unfortunately, this approach cannot be used to create problem instances that are beyond the reach of complete search methods. So far, it has proven to be surprisingly difficult to develop a direct generator for satisfiable instances only. In this paper, we propose a generator that only outputs satisfiable problem instances. We also show how one can finely control the hardness of the satisfiable instances by establishing a connection between problem hardness and a new kind of phase transition phenomenon in the space of problem instances. Finally, we use our problem distribution to show the easy-hard-easy pattern in search complexity for local search procedures, analogous to the previously reported pattern for complete search methods.







Introduction



In recent years, we have seen the rapid development of both complete and incomplete search methods for constraint satisfaction (CSP) and Boolean satisfiability (SAT) problems. These methods are now applied successfully in a range of applications within artificial intelligence and computer science in general. An important factor in the development of new search methods is the availability of good sets of benchmark problems to evaluate and fine-tune the algorithms. There are two main sources of benchmark problems. One class of benchmarks is based on real-world applications and the other is from random instance generators. Real-world instances are arguably the best source, but unfortunately are often in short supply. Moreover, there is a risk that algorithms are being tuned towards specific application domains for which good benchmarks are available. Random problem generators therefore provide a good additional source of problem instances. These generators also have the advantage of a more direct control over the problem characteristics, such as size and expected hardness. Hard random instances have led to the development of new stochastic search



Copyright c 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.







methods such as Walksat (Selman et al. 1996) and the breakout procedure (Morris 1993), and have been used in detailed comparisons of local search methods for graph coloring and related graph problems (Johnson et al. 1989). The results of various competitions for CSP and SAT algorithms show that there is a fairly direct correlation between the performance on real-world benchmarks and on hard random instances (DIMACS 1993, 1996; Beijing, 1996; Johnson et al. 1989). It is important to note that randomly generated problem instances are not necessarily unstructured. Structure may be introduced by translating random problems from one domain into another, or by considering problem domains that by definition exhibit regular structure (Gomes and Selman 1997, Walsh 1999). Current problem generators are based on recent developments in our understanding of the nature of computationally hard problem instances. In particular, a clear connection has been established between so-called phase transition phenomena and the computational hardness of NPcomplete problems (Cheeseman et al. 1991, Mitchell et al. 1992, Hogg et al. 1996). Phase transition phenomena capture the surprisingly sharp transitions from the solvable to the unsolvable in the space of problem instances, as a function of certain problem parameters such as the ratio of the number of constraints to the number of variables. In random distributed problem instances, at low ratios (relatively few constraints) one encounters mostly satisfiable instances, while at high ratios most instances are unsatisfiable. In terms of complexity, one observes a easy-hard-easy pattern, where assignments are easily found in the sat-phase, while inconsistency is easily shown in the unsat-phase. At the phase transition, where roughly half the instances are satisfiable and half the instances are unsatisfiable, one finds a concentration of computationally hard problem instances. The ability to varying the hardness of the problem instances makes it possible to study precisely how different search algorithms scale in terms of problem difficulty. A key limitation of current problem generators concerns their use in the evaluation of incomplete local search methods. This is because the generators generally produce a mixture of solvable (satisfiable) and unsolvable (unsatisfiable) instances. When a local search style method does not find a solution, it can be difficult to determine whether this is because the algorithm fails to find a solution or because the instance itself is unsolvable. The standard way of dealing







with this problem is to use a complete search method to filter out the unsatisfiable cases. However, this limits the size and difficulty of problems instances that can be considered. Ideally, one would use problem generators that generate satisfiable instances only. However, developing such generators has been surprisingly difficult. As an example, let us consider generating hard satisfiable 3CNF formulas. In order to obtain satisfiable instances only, it is natural to use a strategy where one creates formulas in the phase transition region (ratio of clauses to variables of around 4.25) that are "forced" to have at least one satisfying assignment. To do so, consider the following strategy: generate a random truth assignment T , and then generate a formula with N variables and 4.25N random clauses, where one rejects any clause that violates T . This method will in principle generate all possible satisfiable formulas with a clause-to-variable ratio of 4.25 that have T among their solution. What is somewhat surprising however is that the sampling of these formulas is far from uniform: the generator is highly biased towards formulas with many assignments, clustered around T . When fed to local search methods such as Walksat, these formulas are much easier than formulas of comparable size obtained by filtering satisfiable instances from a 3SAT generator. More sophisticated versions of the forced-formula scheme (Asahiro et al. 1993, Van Gelder 1993) provide improvements but also lead to biased samples. There are also a number of theoretical results that show that is is difficult to "hide" a combinatorial object in a larger combinatorial structure. For example, it can be shown that one can easily find cliques over a certain size that are hidden in a random graph, and similar results are known for hiding Hamiltonian cycles (Frieze and McDiarmid 1997). The problem of hiding information in larger combinatorial structures is of interest to the computer science theory community since successful techniques for doing so may eventually lead to more effective cryptographic methods. Cryptographic problems do suggest one way of creating hard satisfiable problem instances (Impagliazzo et al. 1989). For example, Crawford and Kearns (1993) created SAT encodings of the "noisy" parity problem. The instances are guaranteed to have a satisfying assignment but are extremely hard to solve using current SAT procedures. In recent work Massacci (1999) also provides a way of translating the DES crypto protocol into a SAT instance. One can obtain very hard satisfiable instances this way. Since the best algorithms known for dealing directly with the original crypto problem involve exhaustive search, one finds that the best SAT methods are also reduced to an essentially exhaustive search of the space of truth assignments. This means that in practice these problems are in a sense too hard for the development and evaluation of SAT procedures. Furthermore, the cryptographic encodings do not provide a fine-grained way to vary problem hardness in order to studying how the algorithms scale. In general, it seems reasonable to assume that in practical applications one does not expect to find hidden crypto problems, unless one is dealing specifically with a cryptographic application. In this paper, we will introduce a method for the generation of (empirically) hard satisfiable problem instances. We also show how one can finely control the hardness of the







satisfiable instances by establishing a connection between problem hardness and a new kind of phase transition phenomenon in the space of problem instances. As we discussed above, traditional phase transition phenomena involve a sudden transition from a satisfiable to an unsatisfiable phase of the problem instance space. Since our generator only outputs satisfiable instances, such a transition does not occur. However, under the right parameterization, we also observe an easy-hard-easy pattern in the space of satisfiable instances, just as is the case for complete search methods. (For related work, see Clark et al. (1996).) This makes it possible to tune the generator to output hard problem instances. We can link the hardness area to a phase transition which corresponds to a clear threshold phenomenon in the size of the "backbone" of the problem instances. Informally speaking, the backbone measures the amount of shared structure among the set of all solutions to a given problem instance. The size of the backbone is measured in terms of the percentage of variables that have the same value in all possible solutions. We will observe a transition from a phase where the size of the backbone is almost 100% to a phase with a backbone of size close to 0%. The transition is sudden and we will show how it coincides with the hardest problem instances both for incomplete and complete search methods.







Quasigroups with holes



Most traditional benchmark problems are based on randomly generated instances with little or no global structure. In Gomes and Selman (1997), we introduced the so-called quasigroup completion problem in order to obtain benchmark instances with more interesting structural properties. The best way to view the quasigroup completion problem is in terms of the completion of a Latin square (which technically defines the multiplication table of the quasigroup). Given N colors, a Latin square is defined by an N by N table, where each entry has a color and where there are no repeated colors in any row or column. N is called the order of the square. Gomes and Selman considered the problem of whether a partially colored Latin square can be completed into a full Latin square by assigning colors to the open entries of the table. This problem is referred to as the quasigroup completion problem (QCP). QCP is NPcomplete (Colbourn 1984) and has an interesting phase transition phenomenon with an associated easy-hard-easy pattern as a function of the fraction of number of preassigned colors. The domain has been used to study the effectiveness of a variety of local consistency measures for constraint satisfaction procedures (Stergiou and Walsh 1999, Walsh 1999, Regin 1994). The quasigroup completion task has interesting global structure but does not lend itself well for the evaluation of local search methods because we again have a mix of satisfiable and unsatisfiable instances. However, we will introduce a new generator based on the quasigroup domain that gives a natural unbiased way for obtaining only satisfiable instances, with good computational properties, namely by starting with a full quasigroup and "punching" holes into it. We use a recent result on generating uniformly distributed random complete quasigroups for generating our initial full







quasigroup. The problem of generating uniformly distributed Latin squares is non-trivial. Jacobson and Matthews (1996) show how by simulating an ergodic Markov chain whose stationary distribution is uniform over the space of N by N Latin squares, one can obtain squares that are (approximately) uniformly distributed. The Markov chain Monte Carlo method starts with a complete Latin square. (There is an efficient method for generating a fixed Latin square of any size.) Subsequently, the method randomly "perturbs" the initial Latin square to obtain a new square; repeated random perturbations lead us through a chain of squares. The difficult part is to design sequences of perturbations that lead from one valid Latin square to another while ensuring that one can reach any arbitrary Latin square in the chain with equal probability in the stationary distribution. The method proposed by Jacobson and Matthews corresponds to a random walk on a finite, connected, nonbipartite undirected graph and therefore it is ergodic, with stationary distribution assigning each vertex a probability proportional to its degree. The Jacobson and Matthews approach provides us with a good starting point for obtaining interesting satisfiable computational instances. We propose the following generator: (1) Generate a complete Latin square according to the Markov chain Monte Carlo approach proposed by Jacobson and Matthews; (2) punch a fraction p of "holes" in the Latin square (i.e., uncolor some of the entries) in a uniformly distributed manner. The resulting partial Latin square is now guaranteed to be satisfiable and moreover, as we will see below, we can finely control its expected hardness by tuning the value of p. We call this new problem the "quasigroup with holes" (QWH) problem.1 As we will describe below, the instances can be solved directly (in order to test e.g., a constraint-logic programming algorithm) or translated into a Boolean CNF encoding (in order to test general SAT solvers). It is interesting to note that while the quasigroup domain lends itself naturally to a satisfiable instance generator with good computational properties, it is not clear how a similar generator could be developed for, e.g., k-SAT or graph coloring. The quasigroup with holes problem is NP-hard. This follows from the following argument. Assume one had a polynomial algorithm that could solve QWH. Such an algorithm could be used to solve the quasigroup completion task (QCP), by simply running the algorithm with a polynomial time bound. The bounded algorithm would either solve our completion problem or terminate at the time bound, indicating no solution exists. However, this is impossible because, as noted above, QCP is NP-complete. In the next sections, we will identify a clear easy-hardeasy pattern for both complete and incomplete search methods on these problem instances. Note that because we are dealing with a distribution of satisfiable instances only, we obtain a clear full easy-hard-easy diagram for a incomplete search method. Clark et al. (1996) provide initial results on a such a pattern for local search using standard benchmarks. However, given the rareness of satisfiable instances on the



We thank Mark Stickel for some preliminary discussions on the use of the quasigroups with holes (Stickel, personal communications, May 1998).



1







unsat side of the phase transition it is difficult to establish a clear full pattern. We will also show that the hardness region of our satisfiable problem instances coincides with a new kind of phase transition. This transition differs from the standard sat/unsat transition because we now have only satisfiable instances, but like the standard transition, it is based on an underlying structural property -- namely, the backbone.







Problem hardness



In order to solve QWH instances, we explored a range of algorithms. We used an ILOG constraint solver working directly on the constraint satisfaction encoding of the problem. In the ILOG solver, we incorporated, aside from the standard constraint propagation methods, the all-diff constraint (Stergiou and Walsh 1999; Regin 1994). We also implemented (in C) a local search procedure working directly on the constraint representation. Finally, we converted the problem instances into Boolean satisfiability encodings and used stateof-the-art SAT solvers, both complete and incomplete methods. To our surprise, the approach via a SAT encoding is more efficient than using the direct CSP approaches; apparently, the increase in the size of the encoding when going to SAT does not hurt overall performance. Given the space limitations of this paper, we will only include the data for our best performing procedures, the backtracking SAT solver Satz (Li and Anbulagan 1997) and the local search SAT solver Walksat (Selman et al. 1996). (Both solvers are available from SATLIB (Hoos 1999).) Our data for the CSP approach is qualitatively the same. The QWH instances thus provide a good benchmark for both CSP and for SAT methods. Experimental data, instances, and generator (both SAT and CSP representation) are available from the authors. In Fig. 1, we show the computational cost profiles for an incomplete (Walksat; left panel) and a complete (Satz; right panel) search method for the QWH problem. Along the horizontal axis, we vary the fraction of holes in the quasigroup. More specifically, we take the ratio of the number of holes to the total number of entries in the Latin square, i.e., N 2 , where N is order of the square. The vertical axis gives the median computational cost. For Walksat, the cost is measured in terms of the total number of variable flips; for Satz we measured the total number of backtracks. The figure shows a clear easy-hard-easy pattern for both the incomplete and the complete search methods. Over a range of different sizes (N = 30, 33, 36) we see a rapid (in fact, exponential) increase in search cost in the hardest region. Close observation shows that there is a slight shift in the location of the peaks. We will return to this issue below, when we discuss a way of rescaling the figures to precisely line up the peaks. Aside from having a clear easy-hard-easy pattern, the main point of interest in Fig. 1 is the profile for the incomplete search method. We see a clear example of an easyhard-easy pattern for an incomplete search method. Because previous problem generators give a mixture of sat and unsat cases, such an easy-hard-easy pattern has generally been reported so far only for complete methods, which can handle both types of instances. Our figure shows that the notions of under-constrained, critically constrained, and over-







4.5e+07



Computational Cost of Local Search







1600 Order 30 Order 33 Order 36



Computational Cost of Complete Search







4e+07







1400 1200 1000 800 600 400 200 0 0.2







Order 30 Order 33 Order 36







3.5e+07 3e+07







2.5e+07 2e+07







1.5e+07 1e+07 5e+06 0 0.2







0.25







0.3 0.35 Num. Holes / (N^2)







0.4







0.45







0.25







0.3 0.35 Num. Holes / (N^2)







0.4







0.45







Figure 1: Computational cost profiles for incomplete (Walksat) and complete (Satz) search methods for QWH. constrained (Hogg et al. 1996) are also predictive of the performance of incomplete search methods.



1 0.9



Percentage of backbone and search cost



1 0.9 0.8 'Order 30' 'Order 33' 'Order 36' 'Order 39' 'Order 42' 'Order 45' 'Order 48' 'Order 51' 'Order 54' 'Order 57'







backbone local search cost



% FC Backbone







0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.2







0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.2







0.25







0.3 0.35 Num. Holes / (N^2)







0.4







0.45







0.25







0.3 0.35 Num. Holes / (N^2)







0.4







0.45







Figure 3: Backbone for different orders. satisfiable. Nevertheless, we can use recently introduced notions from the study of phase transition phenomena to link the peak in search complexity to a phase transition in structural properties of our problem instances. To do so, we will consider so-called backbone variables. Monasson et al. (1999) introduced the notion of the backbone of a SAT problem to refer to the fraction of its variables that are fully constrained: that is, which take on the same values in all solutions. The backbone fraction (ratio of backbone variables to the total number of variables) is a property of CSP and SAT problems that is well-defined for satisfiable distributions. Fig. 2 shows the backbone fraction as a function of the fraction of holes in the QWH problem. We also included the normalized cost of local search. The figure shows a sharp phase transition phenomenon in the backbone fraction, which coincide with the hardness peak in local search.2



2 The figure gives data for N = 36. The hardness peak for our complete search method also lies in the phase transition region







Figure 2: Backbone phase transition with cost profile.







A New Type of Phase Transition



One of the key advances in our understanding of problem hardness has been the connection between the easy-hardeasy pattern in search complexity and phase transition phenomena (Cheeseman 1991; Mitchell et al. 1992; Kirkpatrick and Selman 1994; Hogg et al. 1996; Hayes 1996). In particular, a clear connection has been established between the hardest problem instances and the phase transition region, where instances shift from being mostly satisfiable to being mostly unsatisfiable. One of the interesting aspects of this connection is that properties of the SAT/UNSAT phase transition can be analyzed quite independently from any particular solution procedure. In fact, this has led to a large number of papers on the SAT/UNSAT phase transition per se. For the QWH instances, we do not have a SAT/UNSAT phase transition, since all our instances are guaranteed to be







1 0.9 0.8 0.7



%FC Backbone







1







0.6 0.5 0.4 0.3 0.2 0.1 0 0.6 0.8 1 1.2 1.4 1.6 1.8 Num. Holes /(N^1.55)







Normalized cost of inomplete search







'Order 30' 'Order 33' 'Order 36' 'Order 39' 'Order 42' 'Order 45' 'Order 48' 'Order 51' 'Order 54' 'Order 57'







Order 30 Order 33 Order 36 0.8







0.6







0.4







0.2







2







2.2







2.4







0 0.2







0.25







0.3 0.35 Num. Holes / (N^2)







0.4







0.45







Figure 4: Backbone for different orders (rescaled). The reasons for the correlation between problem hardness and the appearance of the backbone are not fully understood at this time. One intuition is that backtracking search algorithms have the worst performance when they make an incorrect choice near the root of the search tree: that is, when they make a variable-value assignment that appears in no solution. For the algorithm to have a significant chance of making such a bad choice a non-negligible fraction of the variables must appear in the backbone. When the backbone fraction nears 1, however, the problems are so overconstrained that incorrect choices near the root are quickly detected and corrected. For local search procedures, an explanation might be developed by considering the relationship between the backbone and set of solutions to the instances. When the backbone is small, there are many solutions widely distributed in the search space, and so local search may quickly find one. When the backbone is near 1, the solutions are tightly clustered, so that that all clauses "vote" to push the search in the same direction. A partial backbone, however, may indicate that solutions are in different clusters that are widely distributed, with different clauses pushing the search in different directions. Making these intuitions precise, however, awaits future research.



1







Figure 5: Normalized computational cost.







Order 30 Order 33 Order 36



Normalized cost of inomplete search







0.8







0.6







0.4







0.2







0 1 1.2 1.4 1.6 1.8 2 2.2 Num. Holes / (N^1.55) 2.4 2.6 2.8 3







Figure 6: Re-parameterized computational cost. Some experimentation with different parameterization leads us to Fig. 4. This figure shows the backbone plotted against the number holes over N 1.55 . Note that we originally used "number of holes over N 2 ". We are currently working on an analytical derivation of the re-parameterization. Finally, Figs. 5 and 6 show how our rescaling also corrects for the shift in the complexity peak of our local search method. To show the original shift, Fig. 5 gives the search complexity for three different sizes of the QWH problem, where the cost has been normalized to 1. Fig. 6 shows how the peaks collapse onto each other after rescaling. The peaks for the complete search method (right panel in Fig. 1) also align after such a rescaling.







Re-parameterization



As we noted above, there is a slight shift in the location of the hardness peak as a function of N . There is a similar shift in the location of the backbone phase transition. This points to the fact that the original parameterization in terms of the fraction of holes does not exactly capture the dimensionality of our problem.3 Fig. 3 shows the shift in the backbone transition for a larger range of problem sizes (N = 30, . . . , 57).4



but is shifted slightly to the right. We are currently investigating whether that shift is real or part of the uncertainty in our data. 3 Note that a similar shift is also present in the original quasigroup completion problem. 4 Computing the full backbone is prohibitively expensive. The figure gives a good approximation of the backbone fraction computed by using forward-checking to estimate the fraction of fixed







variables. This estimate is a few percentage off from the true value, but the shifting behavior appears identical to that of the full backbone, based on experiments for smaller values of N .







Conclusions



We propose a problem generator for satisfiable instances. The generator samples from satisfiable quasigroups of a given size with a given number of holes. The hardness of the QWH problem instances can be tuned by varying the fraction of holes in the quasigroup instances. The main advantage of this generator is that it generates satisfiable instances only and is therefore well-suited for use in the study and evaluation of incomplete search methods. Several earlier attempts at designing such a generator (e.g., by forcing a given solution during the problem generation) were unsatisfactory. Using our generator, we showed that a local search method does exhibit the easy-hard-easy pattern, as observed previously for complete search methods. Based on the notion of under-constrained, critically constrained, and over-constrained regions identified with complete search methods, it was believed that an easy-hardeasy pattern would emerge for local search methods but this was difficult to confirm empirically because satisfiable instances in the over-constrained region are extremely rare for standard problem generators. We also show how the hardest region of the satisfiable instances coincides with a new kind of phase transition in terms of the backbone of the problem instances. The backbone characterizes the amount of shared structure between solutions. Finally, we present an empirically obtained re-parameterization of the phase transition and complexity peak of the quasigroup with holes problem. Our generator outputs instances suitable for both CSP and SAT style methods. The generator should therefore be of use in the future development of stochastic local search style CSP and SAT methods.







Acknowledgements







We would like to thank Dongmin Liang for his assistance with obtaining the experimental data in this paper. The second author is supported by the Air Force Research Laboratory and the Air Force Office of Scientific Research, under the New World Vistas Initiative. The fourth author is supported by an NSF Faculty Early Career Development Award, an Alfred P. Sloan fellowship, and by the Air Force Research Laboratory.







References



Asahiro, Y., Iwama, K. and Miyano, E. (1993). Random generation of test instances with controlled attributes. In DIMACS 1993, op cite. Beijing (1996). International Competition and Symposium on Satisfiability Testing, Beijing, China, March 15-17, 1996. Cheeseman, P. and Kanefsky, R. and Taylor, W. (1991). Where the Really Hard Problems Are. Proc. IJCAI-91, 1991, 163- 169. Clark, D.A., Frank, J., Gent, I.P., MacIntyre, E., Tomov, N., Walsh, T. (1996). Local search and the number of solutions. Proc. CP-96, 1996. Colbourn, C. (1984). The Complexity of Completing Latin Squares. Discrete Appl. Math., 8, (1984), 25-30. Crawford, J. and Kearns, M. (1993). Instances for learning the parity function. Unpublished note, see Hoos (1999). DIMACS (1993). Second DIMACS Implement. Challenge, 1993. Pub. as DIMACS Series in Disc. Math. and Theor. Comp. Sci., vol. 26, D. Johnson and M. Trick, eds., AMS, 1996. DIMACS (1996). Satisfiability Problem, DIMACS Workshop, 1996. Pub. as DIMACS Discrete Math. and Theor. Comp. Sci.,







vol. 35, D. Du, J. Gu, and P. Pardalos, eds., AMS, 1997. Frieze, A. and McDiarmid, C. (1997). Algorithmic theory of random graphs. Random Structures and Algorithms, vol. 10 (1997) 5-42. Gent, I. and Walsh, T. (1993) An empirical analysis of search in GSAT. J. of Artificial Intelligence Research, vol. 1, 1993. Gomes, C.P. and Selman, B. (1997a). Problem structure in the presence of perturbations. Proc. AAAI-97, 1997. Hayes, B. (1996). Can't get no satisfaction. American Scientist vol. 85, 108 (1996) Hogg, T., Huberman, B.A., and Williams, C.P. (Eds.) (1996). Phase Transitions and Complexity. Artificial Intelligence, 81 (Spec. Issue), 1996. Hoos, H. 1999. SATLIB. A collection of SAT tools and data. See www.informatik.tu-darmstadt.de/AI/SATLIB. Impagliazzo, R., Levin, L., and Luby, M. (1989). Pseudo-random number generation from one-way functions. Proc. 21st STOC, 1989, 12-24. Jacobson, M.T. and Matthews, P. (1996) Generating uniformly distributed random latin squares. J. of Combinatorial Designs, vol. 4., no. 6, (1996) 405-437. Johnson, D.S. , Aragon, C.R., McGeoch, L.A., and Shevon C. (1989) Optimization by Simulated Annealing: An Experimental Evaluation. Operations Research, 37:6 (1989), 865-892. Kirkpatrick, S. and Selman, B. (1994). Critical behavior in the satisfiability of random Boolean expressions. Science, 264, 1994, 1297-1301. Li, Chu Min and Anbulagan (1997). Heuristics based on unit propagation for satisfiability problems. Proc. IJCAI-97, 366- 371. Massacci, F. (1999). Using Walk-SAT and Rel-SAT for cyptographic key search. Proc. IJCAI-99, 1999, 290-295. Mitchell, D. and Levesque H. (1996). Some pitfalls for experimenters with random SAT. Artificial Intelligence, Vol. 81(1-2), 1996, 111-125. Mitchell, D., Selman, B., and Levesque, H.J. (1992). Hard and easy distributions of SAT problems. Proc. AAAI-92, San Jose, CA (1992) 459-465. Morris, P. (1993) The breakout method for escaping from local minima. Proc. AAAI-93, 1993, 40-45. Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., and Troyansky, L. (1996). Determining computational complexity from characteristic `phase transitions'. Nature, Vol. 400(8), 1999. Regin, J.C. (1994). A filtering algorithm for constraints of difference in CSP. Proc. AAAI-94, 1994, 362-367. Selman, B. and Levesque, H.J., and Mitchell, D.G. (1992). A New Method for Solving Hard Satisfiability Problems. Selman, B., Kautz, H.A., and Cohen, B. (1996). Local search strategies for satisfiability testing. In DIMACS (1993). Shaw, P., Stergiou, K., and Walsh, T. (1998) Arc consistency and quasigroup completion. Proc. ECAI-98, workshop on binary constraints, 1998. Stergiou, K. and Walsh, T. (1999) The Difference All-Difference Makes Proc. of IJCAI-99, Stockholm, Sweden. Walsh, T. (1999) Search in a Small World. Proc. of IJCAI-99, Stockholm, Sweden, 1999. Van Gelder, A. (1993). Problem generator (mkcnf.c) contributed to the DIMACS 1993 Challenge archive.







Journal of Artificial Intelligence Research 24 (2005) 623-639







Submitted 12/04; published 11/05







Hiding Satisfying Assignments: Two are Better than One



Dimitris Achlioptas



Microsoft Research Redmond, Washington optas@microsoft.com







Haixia Jia



Computer Science Department University of New Mexico







hjia@cs.unm.edu







Cristopher Moore



Computer Science Department University of New Mexico







moore@cs.unm.edu







Abstract



The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random k SAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, for a number of different algorithms, A acts as a stronger and stronger attractor as the formula's density increases. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k -SAT and NAE-k -SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and A are satisfying. It appears that under this "symmetrization" the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion.







1. Introduction



Recent years have witnessed the rapid development and application of search methods for constraint satisfaction and Boolean satisfiability. An important factor in the success of these algorithms is the availability of good sets of benchmark problems to evaluate and fine-tune them. There are two main sources of such problems: the real world, and random instance generators. Real-world problems are arguably the best benchmarks, but unfortunately are in short supply. Moreover, using real-world problems carries the risk of tuning algorithms toward the specific application domains for which good benchmarks are available. In that sense, random instance generators are a good additional source, with the advantage of controllable characteristics, such as size and expected hardness. Hard random instances have led to the development of new stochastic search methods such as WalkSAT (Selman, Kautz, & Cohen, 1996), the breakout procedure (Morris, 1993), and Survey Propagation (M ezard & Zecchina, 2002), and have been used in detailed comparisons of local search methods for graph coloring and related problems (Johnson, Aragon, McGeoch, & Shevon, 1989). The results of various competitions for CSP and SAT algoc 2005 AI Access Foundation. All rights reserved.







Achlioptas, Jia, & Moore







rithms show a fairly direct correlation between the performance on real-world benchmarks and on hard random instances (Johnson & Trick, 1996; Du, Gu, & Pardalos, 1997; Johnson et al., 1989). Nevertheless, a key limitation of current problem generators concerns their use in evaluating incomplete satisfiability solvers such as those based on local search methods. When an incomplete algorithm does not find a solution, it can be difficult to determine whether this is because the instance is in fact unsatisfiable, or simply because the algorithm failed to find a satisfying assignment. The standard way of dealing with this problem is to use a complete search method to filter out the unsatisfiable cases. However, this greatly limits the size and difficulty of problem instances that can be considered. Ideally, one would use problem generators that generate satisfiable instances only. One relatively recent source of such problems is the quasigroup completion problem (Shaw, Stergiou, & Walsh, 1998; Achlioptas, Gomes, Kautz, & Selman, 2000; Kautz, Ruan, Achlioptas, Gomes, Selman, & Stickel, 2001). However, a generator for random hard satisfiable instances of 3-SAT, say, has remained elusive. Perhaps the most natural candidate for generating random hard satisfiable 3-SAT formulas is the following. Pick a random truth assignment A, and then generate a formula with n variables and rn random clauses, rejecting any clause that is violated by A. In particular, we might hope that if we work close to the satisfiability threshold region r  4.25, where the hardest random 3-SAT problems seem to be (Cheeseman, Kanefsky, & Taylor, 1991; Hogg, Huberman, & Williams, 1996; Mitchell, Selman, & Levesque, 1992), this would generate hard satisfiable instances. Unfortunately, this generator is highly biased towards formulas with many assignments clustered around A. When given to local search methods such as WalkSAT, the resulting formulas turn out to be much easier than formulas of comparable size obtained by filtering satisfiable instances from a 3-SAT generator. More sophisticated versions of this "hidden assignment" scheme (Asahiro, Iwama, & Miyano, 1996; Van Gelder, 1993) improve matters somewhat but still lead to easily solvable formulas. In this paper we introduce a new generator of random satisfiable problems. The idea is simple: we pick a random 3-SAT formula that has a "hidden" complementary pair of satisfying assignments, A and A, by rejecting clauses that are violated by either A or A. We call these "2-hidden" formulas. Our motivation comes from recent work (Achlioptas & Moore, 2002b, 2005) which showed that moving from random k -SAT to random NAE-k SAT (in which every clause in the formula must have at least one true and at least one false literal) tremendously reduces the correlation between solutions. That is, whereas in random k -SAT, satisfying assignments tend to form clumps, in random NAE-k -SAT the solutions appear to be scattered throughout {0, 1} n in a rather uniform "mist," even for densities extremely close to the threshold. An intuitive explanation for this phenomenon is that since the complement of every NAE-assignment is also an NAE-assignment, the attractions of solution pairs largely "cancel out." In this paper we exploit this phenomenon to impose a similar symmetry with the hidden assignments A and A, so that their attractions cancel out, making it hard for a wide variety of algorithms to "feel" either one. A particularly nice feature of our generator is that it is based on an extremely simple probabilistic procedure, in sharp contrast with 3-SAT generators based on, say, cryptographic ideas (Massacci, 1999). In particular, our generator is readily amenable to all the mathematical tools that have been developed for the rigorous study of random k -SAT formulas. Here we make two first steps in that direction. In Section 2, via a first mo624







Hiding Satisfying Assignments: Two are Better than One







ment calculation we study the distribution of the number of solutions as a function of their distance from the hidden assignments. In Section 3, we use the technique of differential equations to analyze the performance of the Unit Clause (UC) heuristic on our formulas. Naturally, mathematical simplicity would not be worth much if the formulas produced by our generator were easily solvable. In Section 4, we compare experimentally the hardness of "2-hidden" formulas with that of "1-hidden" and "0-hidden" formulas. That is, we compare our formulas with random 3-SAT formulas with one hidden assignment and with standard random 3-SAT formulas with no hidden assignment. We examine four leading algorithms: two complete solvers, zChaff and Satz, and two incomplete ones, WalkSAT and the recently introduced Survey Propagation (SP). For all these algorithms, we find that our formulas are much harder than 1-hidden formulas and, more importantly, about as hard as 0-hidden formulas, of the same size and density.







2. A picture of the space of solutions



In this section we compare 1-hidden and 2-hidden formulas with respect to the expected number of solutions at a given distance from the hidden assignment(s). 2.1 1-hidden formulas Let X be the number of satisfying truth assignments in a random k -SAT formula with n variables and m = rn clauses chosen uniformly and independently among all k -clauses with at least one positive literal, i.e., 1-hidden formulas where we hide the all-ones truth assignment. To calculate the expectation E[X ], it is helpful to parametrize truth assignments according to their overlap with the hidden assignment, i.e., the fraction  of variables on which they agree with A, which in this case is the fraction of variables that are set to one. Then, linearity of expectation gives (1), clause independence gives (2), selecting the literals in each clause uniformly and independently gives (3), and, finally, writing z = n and using Stirling's approximation for the factorial gives (4) below: E[X ] =



A{0,1}n n







Pr[A is satisfying]







(1)







=



z =0 n







=



z =0 n







n Pr[a truth assignment with z ones satisfies a random clause] m (2) z m  k k 1 n  (3) (1 - z/n)j (z/n)k-j  1- k j z 2 -1



j =1







=



z =0







n z







1 - (z/n)k 1- 2k - 1







m







= poly(n) x max







[0,1]







1   (1 - )1-







1 - k 1- k 2 -1







r n







(4)







= poly(n) x max [fk,r ()]n



[0,1]







625







Achlioptas, Jia, & Moore







where 1 fk,r () =   (1 - )1-







1 - k 1- k 2 -1







r







.







From this calculation we see that E[X ] is dominated by the contribution of the truth assignments that maximize fk,r () (since we raise fk,r to the nth power all other contributions vanish). Now, note that f is the product of an "entropic" factor 1/(  (1 - )1- ) which is symmetric around  = 1/2, and a "correlation" factor which is strictly increasing in . As a result, it is always maximized for some  > 1/2. This means that the dominant contribution to E[X ] comes from truth assignments that agree with the hidden assignment on more that half the variables. That is, the set of solutions is dominated by truth assignments that can "feel" the hidden assignments. Moreover, as r increases this phenomenon becomes more and more acute (see Figure 1 below). 2.2 2-hidden formulas Now let X be the number of satisfying truth assignments in a random k -SAT formula with n variables and m = rn clauses chosen uniformly among all k -clauses that have at least one positive and at least one negative literal, i.e., 2-hidden formulas where we hide the all- ones assignment and its complement. To compute E[X ] we proceed as above, except that now (3) is replaced by m  k -1 n k 1 n  (1 - z/n)j (z/n)k-j  . 1- k j z 2 -2



z =0 j =1







Carrying through the ensuing changes we find that now E[X ] = poly(n) x max [gk,r ()]n



[0,1]







where 1 gk,r () =   (1 - )1-







This time, both the entropic factor and the correlation factor comprising g are symmetric functions of , so gk,r is symmetric around  = 1/2 (unlike f k,r ). Indeed, one can prove that for all r up to extremely close to the random k -SAT threshold r k , the function gk,r has its global maximum at  = 1/2. In other words, for all such r , the dominant contribution to E[X ] comes from truth assignments at distance n/2 from the hidden assignments, i.e., the hidden assignments are "not felt." More precisely, there exists a sequence k  0 such that gk,r has a unique global maximum at  = 1/2, for all r  2k ln 2 - ln 2 -1- 2



k







1 - k - (1 - )k 1- 2k - 2







r







.







.







(5)







Contrast this with the fact (implicit in Kirousis, Kranakis, Krizanc, & Stamatiou, 1998) that for ln 2 1 r  2k ln 2 - - , (6) 2 2



626







Hiding Satisfying Assignments: Two are Better than One







a random k -SAT formula with n variables and m = rn clauses is unsatisfiable with probability 1 - o(1). Moreover, the convergence of the sequence k  0 is rapid, as can be seen from the concrete values in table 1. Thus the gap between the values of r given by equations (5) and (6) quickly converges to 1/2, even as the threshold becomes exponentially large. k Eq. (5) Eq. (6) 3 7/2 4.67 4 35/4 10.23 5 20.38 21.33 7 87.23 87.88 10 708.40 708.94 20 726816.15 726816.66







Table 1: The convergence (in k ) to the asymptotic gap of 1/2 is rapid In Figure 1 we plot fk,r and gk,r for k = 5 and r = 16, 18, 20, 22, 24 (from top to bottom). We see that in the case of 1-hidden formulas, i.e., f k,r , the maximum always occurs to the right of  = 1/2. Moreover, observe that for r = 22, 24, i.e., after we cross the 5-SAT threshold (which occurs at r  21) we have a dramatic shift in the location of the maximum and, thus, in the extent of the bias. Specifically, since the expected number of satisfying assignments is roughly f k,r ()n , and since fk,r () < 1 except for   1, with high probability the only remaining satisfying assignments in the limit n   are those extremely close to the hidden assignment. In the case of 2-hidden formulas, on the other hand, we see that for r = 16, 18, 20 the global maximum occurs at  = 1/2. For r = 20, just below the threshold, we also have two local maxima near  = 0, 1, but since g k,r is raised to the nth power, these are exponentially suppressed. Naturally, for r above the threshold, i.e., r = 22, 24, these local maxima become global, signifying that indeed the only remaining truth assignments are those extremely close to one of the two hidden ones. Intuitively, we expect that because g is flat at  = 1/2 where random truth assignments are concentrated, for 2-hidden formulas local search algorithms like WalkSAT will essentially perform a random walk until they are lucky enough to get close to one of the two hidden assignments. Thus we expect WalkSAT to take about as long on 2-hidden formulas as it does on 0-hidden ones. For 1-hidden formulas, in contrast, we expect the nonzero gradient of f at  = 1/2 to provide a strong "hint" to WalkSAT that it should move towards the hidden assignment, and that therefore 1-hidden formulas will be much easier for it to solve. We will see below that our experimental results bear out these intuitions perfectly.







3. The Unit Clause heuristic and DPLL algorithms



Consider the following linear-time heuristic, called Unit Clause (UC), which permanently sets one variable in each step as follows: pick a random literal and satisfy it, and repeatedly satisfy any 1-clauses present. Chao and Franco showed that UC succeeds with constant probability on random 3-SAT formulas with r < 8/3, and fails with high probability, i.e., with probability 1 - o(1) as n  , for r > 8/3 (Chao & Franco, 1986). One can think of UC as the first branch of the simplest possible DPLL algorithm S : set variables in a random order, each time choosing randomly which branch to take first. Their result then shows that, with constant probability, S solves random 3-SAT formulas with r < 8/3 with no backtracking at all.



627







Achlioptas, Jia, & Moore







1.3 1.2 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0 0.2 0.4 0.6 0.8 r=16 r=18 r=20 r=22 r=24  1







1-hidden formulas



1.25 1.2 1.15 1.1 1.05 1 0.95 0.9 0 r=16 r=18 r=20 r=22 r=24







0.2







0.4















0.6







0.8







1







2-hidden formulas Figure 1: The nth root of the expected number of solutions f k,r and gk,r for 1-hidden and 2-hidden formulas respectively, as a function of the overlap fraction  = z/n with the hidden assignment. Here k = 5 and r = 16, 18, 20, 22, 24 from top to bottom.







628







Hiding Satisfying Assignments: Two are Better than One







It is conjectured that the running time of S goes from linear to exponential at r = 8/3, with no intermediate regime. Calculations using techniques from statistical physics (Cocco & Monasson, 2001a, 2001b; Monasson, 2005) show that this is true of the expected running time. Achlioptas, Beame and Molloy show that the running time is exponential with high probability for r > 3.81; moreover, they show that if the "tricritical point" of (2 + p)-SAT is r = 2/5, then this is the case for r > 8/3 (Achlioptas, Beame, & Molloy, 2001). In this section we analyze the performance of UC on 1-hidden and 2-hidden formulas. Specifically, we show that UC fails for 2-hidden formulas at precisely the same density as for 0-hidden ones. Based on this, we conjecture that the running time of S , and other simple DPLL algorithms, becomes exponential for 2-hidden formulas at the same density as for 0-hidden ones. To analyze UC on random 1-hidden and 2-hidden formulas we actually analyze UC on arbitrary initial distributions of 3-clauses, i.e., where for each 0  j  3 we specify the initial number of 3-clauses with j positive literals and 3 - j negative ones. We use the method of differential equations; see the article by Achlioptas(2001) for a review. To simplify notation, we assume that A is the all-ones assignment, so that 1-hidden formulas forbid clauses where all literals are negative, while 2-hidden formulas forbid all-negative and all-positive clauses. A round of UC consists of a "free" step, in which we satisfy a random literal, and the ensuing chain of "forced" steps or unit-clause propagations. For 0  i  3 and 0  j  i, let Si,j = si,j n be the number of clauses of length i with j positive literals and i - j negative ones. We will also refer to the total density of clauses of size i as s i = j si,j . Let X = xn be the number of variables set so far. Our goal is to write the expected change in these variables in a given round as a function of their values at the beginning of the round. Note that at the beginning of each round S 1,0 = S1,1 = 0 by definition, so the "state space" of our analysis will consist of the variables S i,j for i  2. It is convenient to define two new quantities, m T and mF , which are the expected number of variables set True and False in a round. We will calculate these below. Then, in terms of mT , mF , we have E[S3,j ] = -(mT + mF ) 3s3,j 1-x 2s2,j (j + 1)s3,j +1 (3 - j )s3,j E[S2,j ] = -(mT + mF ) + mF + mT 1-x 1-x 1-x E[X ] = -(mT + mF ) . (7) (8)







To see this, note that a variable appears positively in a clause of type i, j with probability j/(n - X ), and negatively with probability (i - j )/(n - X ). Thus, the negative terms in (7) and (8) correspond to clauses being "hit" by the variables set, while the positive term is the "flow" of 3-clauses to 2-clauses. To calculate mT and mF , we consider the process by which unit clauses are created during a round. We can model this with a two-type branching process, which we analyze as in the article by Achlioptas and Moore(2002a). Since the free step gives the chosen variable a random value, we can think of it as creating a unit clause, which is positive or negative with equal probability. Thus the initial expected population of unit clauses can be



629







Achlioptas, Jia, & Moore







represented by a vector p0 = 1/2 1/2







where the first and second components count the negative and positive unit clauses respectively. Moreover, at time X = xn, a unit clause procreates according to the matrix M= 1 1-x s2,1 2s2,0 2s2,2 s2,1 .







In other words, satisfying a negative unit clause creates, in expectation, M 1,1 = s2,1 /(1 - x) negative unit clauses and M2,1 = 2s2,2 /(1 - x) positive unit clauses, and similarly for satisfying a positive unit clause. Thus, as long as the largest eigenvalue  1 of M is less than 1, the expected number of variables set true or false during the round is given by mF mT = (I + M + M 2 + * * * ) * p0 = (I - M )-1 * p0







where I is the identity matrix. Moreover, as long as  1 < 1 throughout the algorithm, i.e., as long as the branching process is subcritical for all x, UC succeeds with constant probability. On the other hand, if 1 ever exceeds 1, then the branching process becomes supercritical, with high probability the unit clauses proliferate, and the algorithm fails. Note that  s2,1 + 2 s2,0 s2,2 . (9) 1 = 1-x Now let us rescale (7) to give a system of differential equations for the s i,j . Wormald's Theorem (Wormald, 1995) implies that with high probability the random variables S i,j (xn) will be within o(n) of si,j (x) * n for all x, where si,j (x) is the solution of the following: ds3,j dx ds2,j dx 3s3,j 1-x 2s2,j (j + 1)s3,j +1 (3 - j )s3,j mF mT = - + + 1 - x mT + m F 1-x mT + m F 1-x = - (10)







Now, suppose our initial distribution of 3-clauses is symmetric, i.e., s 3,0 (0) = s3,3 (0) and s3,1 (0) = s3,2 (0). It is easy to see from (10) that in that case, both the 3-clauses and the 2-clauses are symmetric at all times, i.e., s i,j = si,i-j and mF = mT . In that case  s2,1 + 2 s2,0 s2,2 = s2 , so the criterion for subcriticality becomes 1 = s2 <1 . 1-x







Moreover, since the system (10) is now symmetric with respect to j , summing over j gives the differential equations ds3 dx ds2 dx 3s3 1-x 2s2 3s3 = - + 1 - x 2(1 - x) = -



630







Hiding Satisfying Assignments: Two are Better than One







which are precisely the differential equations for UC on 0-hidden formulas, i.e., random instances of 3-SAT. Since 2-hidden formulas correspond to symmetric initial conditions, we have thus shown that UC succeeds on them with constant probability if and only if r < 8/3, i.e., that UC fails on these formulas at exactly the same density for which it fails on random 3-SAT instances. (In contrast, integrating (10) with the initial conditions corresponding to 1-hidden formulas shows that UC succeeds for them at a slightly higher density, up to r < 2.679.) Of course, UC can easily be improved by making the free step more intelligent: for instance, choosing the variable according to the number of its occurrences in the formula, and using the majority of these occurrences to decide its truth value. The best known heuristic of this type (Kaporis, Kirousis, & Lalas, 2003; Hajiaghayi & Sorkin, 2003) succeeds with constant probability for r < 3.52. However, we believe that much of the progress that has been made in analyzing the performance of such algorithms can be "pushed through" to 2-hidden formulas. Specifically, nearly all algorithms analyzed so far have the property that given as input a symmetric initial distribution of 3-clauses, e.g. random 3-SAT, their residual formulas consist of symmetric mixes of 2- and 3-clauses. As a result, we conjecture that the above methods can be used to show that such algorithms act on 2-hidden formulas exactly as they do on 0-hidden ones, failing with high probability at the same density. More generally, call a DPLL algorithm myopic if its splitting rule consists of choosing a random clause of a given size, based on the current distribution of clause sizes, and deciding how to satisfy it based on the number of occurrences of its variables in other clauses. For a given myopic algorithm A, let r A be the density below which A succeeds without any backtracking with constant probability. The results of Achlioptas, Beame and Molloy (2001) imply the following statement: if the tricritical point for random (2 + p)-SAT is p c = 2/5 then every myopic algorithm A takes exponential time for r > r A . Thus, not only UC, but in fact a very large class of natural DPLL algorithms, would go from linear time for r < r A to exponential time for r > rA . The fact that the linear-time heuristics corresponding to the first branch of A act on 2-hidden formulas just as they do on 0-hidden ones suggests that, for a wide variety of DPLL algorithms, 2-hidden formulas become exponentially hard at the same density as 0-hidden ones. Proving this, or indeed proving that 2-hidden formulas take exponential time for r above some critical density, appears to us a very promising direction for future work.







4. Experimental results



In this section we report experimental results on our 2-hidden formulas, and compare them to 1-hidden and 0-hidden ones. We use two leading complete solvers, zChaff and Satz, and two leading incomplete solvers, WalkSAT and the new Survey Propagation algorithm SP. In an attempt to avoid the numerous spurious features present in "too-small" random instances, i.e., in non-asymptotic behavior, we restricted our attention to experiments where n  1000. This meant that zChaff and Satz could only be examined at densities significantly above the satisfiability threshold, as neither algorithm could practically solve either 0-hidden or 2-hidden formulas with n  1000 variables close to the threshold. For WalkSAT and SP, on the other hand, we can easily run experiments in the hardest range (around the satisfiability threshold) for n  10 4 .



631







Achlioptas, Jia, & Moore







4.1 zChaff and Satz In order to do experiments with n  1000 with zChaff and Satz, we focused on the regime where r is relatively large, 20 < r < 60. As stated above, for r near the satisfiability threshold, 0-hidden and 2-hidden random formulas with n  1000 variables seem completely out of the reach of either algorithm. While formulas in this overconstrained regime are still challenging, the presence of many forced steps allows both solvers to completely explore the space fairly quickly. We obtained zChaff from the Princeton web site (Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). The first part of Figure 2 shows its performance on random formulas of all three types (with n = 1000 for 20  r  40 and n = 3000 for 40  n  60). We see that the number of decisions for all three types of problems decreases rapidly as r increases, consistent with earlier findings for complete solvers on random 3-SAT formulas. Figure 2 shows that zChaff finds 2-hidden formulas almost as difficult as 0-hidden ones, which for this range of r are unsatisfiable with overwhelming probability. On the other hand, the 1-hidden formulas are much easier, with a number of branchings between 2 and 5 orders of magnitude smaller. It appears that while zChaff's smarts allow it to quickly "zero in" on a single hidden assignment, the attractions exerted by a complementary pair of assignments do indeed cancel out, making 2-hidden formulas almost as hard as unsatisfiable ones. That is, the algorithm eventually "stumbles" upon one of the two hidden assignments after a search that is nearly as exhaustive as for the unsatisfiable random 3-SAT formulas of the same density. We obtained Satz from the SATLIB web site (Li & Anbulagan, 1997b). The second part of Figure 2 shows experiments on random formulas of all three types with n = 3000. As can be seen, the median number of branches explored by Satz for all three types of formulas are within a factor of five, with 0-hidden being the hardest and 2-hidden being the easiest (note that a factor of five corresponds to setting fewer than 3 variables). The reason for this is simple: while Satz makes intelligent decisions about which variable to branch on, it tries these branches in a fixed order, attempting first to set each variable false (Li & Anbulagan, 1997a). Therefore, a single hidden assignment will appear at a uniformly random leaf in Satz's search tree. In the 2-hidden case, since the two hidden assignments are complementary, one will appear in a random position and the other one in the symmetric position with respect to the search tree. Naturally, trying branches in a fixed order is a good idea when the goal is to prove that a formula is unsatisfiable, e.g. in hardware verification. However, we expect that if Satz were modified to, say, use the majority heuristic to choose a variable's first value, its performance on the three types of problems would be similar to zChaff's. 4.2 SP SP is an incomplete solver recently introduced by M ezard and Zecchina (2002) based on a generalization of belief propagation the authors call survey propagation. It is inspired by the physical notion of "replica symmetry breaking" and the observation that for 3.9 < r < 4.25, random 3-SAT formulas appear to be satisfiable, but their satisfying assignments appear to be organized into clumps.



632







Hiding Satisfying Assignments: Two are Better than One







Median number of decisions over 25 trials







10







6







zChaff performance on HIDDEN 1, 2 and 0 formulas HIDDEN-1 HIDDEN-2 HIDDEN-0







10







5







10







4







10







3







10







2







10 20







1







25







30







35







40 r







45







50







55







60







10 Median number of branches over 25 trials







5







Satz performance on HIDDEN 1, 2 and 0 formulas HIDDEN-1 HIDDEN-2 HIDDEN-0







10







4







10







3







10







2







10 20







1







25







30







35







40 r







45







50







55







60







Figure 2: The median number of branchings made by zChaff and Satz on random instances with 0, 1, and 2 hidden assignments (on a log 10 scale). For zChaff we use n = 1000 for r = 20, 30, 40 and n = 3000 for r = 40, 50, 60, and for Satz we use n = 3000 throughout. Each point is the median of 25 trials. The 2-hidden formulas are almost as hard for both algorithms as the 0-hidden ones, while the 1-hidden formulas are much easier for zChaff.







633







Achlioptas, Jia, & Moore







1 0.9 The fraction solved over 30 trials 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 4







SP performance on HIDDEN 1, 2 and 0 formulas







HIDDEN-0 HIDDEN-1 HIDDEN-2







4.5







5 r







5.5







6







Figure 3: The fraction of problems successfully solved by SP as a function of density, with n = 104 and 30 trials for each value of r . The threshold for solving 2-hidden formulas is somewhat higher than for 0-hidden ones, and for 1-hidden formulas it is higher still.







In Figure 3 we compare SP's performance on the three types of problems near the satisfiability threshold. (Because SP takes roughly the same time on all inputs, we do not compare the running times.) For n = 10 4 SP solves 2-hidden formulas at densities somewhat above the threshold, up to r  4.8, while it solves the 1-hidden formulas at still higher densities, up to r  5.6. Presumably the 1-hidden formulas are easier for SP since the "messages" from clauses to variables, like the majority heuristic, tend to push the algorithm towards the hidden assignment. Having two hidden assignments appears to cancel these messages out to some extent, causing SP to fail at a lower density. However, this argument does not explain why SP should succeed at densities above the satisfiability threshold; nor does it explain why SP does not solve 1-hidden formulas for arbitrarily large r . Indeed, we find this latter result surprising, since as r increases the majority of clauses should point more and more consistently towards the hidden assignment in the 1-hidden case. We note that we also performed the above experiments with n = 2 x 10 4 and with 5000 iterations, instead of the default 1000, for SP's convergence procedure. The thresholds of Figure 3 for 1-hidden and 2-hidden formulas appeared to be stable under both these changes, suggesting that they are not merely artifacts of our particular experiments. We propose investigating these thresholds as a direction for further work. 4.3 WalkSAT We conclude with a local search algorithm, WalkSAT. Unlike the complete solvers, WalkSAT can solve problems with n = 104 fairly close to the threshold. We performed experiments both with a random initial state, and with a biased initial state where the algorithm starts with 75% agreement with one of the hidden assignments (note that this is exponentially



634







Hiding Satisfying Assignments: Two are Better than One







unlikely). In both cases, we performed trials of 10 8 flips for each formula, without random restarts, where each step does a random or greedy flip with equal probability. Since random initial states almost certainly have roughly 50% agreement with both hidden assignments, we expect their attractions to cancel out so that WalkSAT will have difficulty finding either of them. On the other hand, if we begin with a biased initial state, then the attraction from the nearby assignment will be much stronger than the other one; this situation is similar to a 1-hidden formula, and we expect WalkSAT to find it easily. Indeed our data confirms these expectations. In the first part of Figure 4 we measure WalkSAT's performance on the three types of problems with n = 104 and r ranging from 3.7 to 7.9, and compare them with 0-hidden formulas for r ranging from 3.7 up to 4.1, just below the threshold where they become unsatisfiable. We see that, below the threshold, 2-hidden formulas are just as hard as 0-hidden ones when WalkSAT sets its initial state randomly; indeed, their running times coincide to within the resolution of the figure! They both become hardest when r  4.2, where 108 flips no longer suffice to solve them. Unsurprisingly, 2-hidden formulas are much easier to solve when we start with a biased initial state, in which case the running time is closer to that of 1-hidden formulas. In the second part of Figure 4, we compare the three types of formulas at a density very close to the threshold, r = 4.25, and measure their running times as a function of n. The data suggests that 2-hidden formulas with random initial states are much harder than 1-hidden ones, while 2-hidden formulas with biased initial states have running times within a constant of that of 1-hidden formulas. Note that the median running time of all three types of problems is polynomial in n, consistent with earlier experiments (Barthel, Hartmann, Leone, Ricci-Tersenghi, Weigt, & Zecchina, 2002). On the other hand, while 1-hidden formulas are much easier than 2-hidden ones for sufficiently large or small r , they appear to be slightly harder than 2-hidden ones for 5.3 < r < 6.3. One possible explanation for this is that while i) the solutions of a 2-hidden formula are harder to find due to their balanced distribution, ii) there are exponentially more solutions for 2-hidden formulas than for 1-hidden ones of the same size and density. It seems that in this range of r , the second effect overwhelms the first, and WalkSAT finds a solution more quickly in the 2-hidden case; but we have no explanation for why this is so for this particular range of r . At higher densities, such as r = 8 shown in Figure 5, 2-hidden formulas again appear to be harder than 1-hidden ones.







5. Conclusions



We have introduced an extremely simple new generator of random satisfiable 3-SAT instances which is amenable to all the mathematical tools developed for the rigorous study of random 3-SAT instances. Experimentally, our generator appears to produce instances that are as hard as random 3-SAT instances, in sharp contrast to instances with a single hidden assignment. This hardness appears quite robust; our experiments have demonstrated it both above and below the satisfiability threshold, and for algorithms that use very different strategies, i.e., DPLL solvers (zChaff and Satz), local search algorithms (WalkSAT), and survey propagation (SP).



635







Achlioptas, Jia, & Moore







10 Median number of flips over 100 trials







7







WalkSAT performance on HIDDEN 1, 2 and 0 formulas HIDDEN-0 HIDDEN-1 HIDDEN-2 init 75% true HIDDEN-2







10







6







10







5







10







4







10







3







3







4







5







r







6







7







8







10 7 Median number of flips over 100 trials 10 10 10 10 10



6







WalkSAT performance as a function of n HIDDEN-0 HIDDEN-2 HIDDEN-1 HIDDEN-2 init 75% true



slope 2.8







5







slope 2.7



4







slope 1.3







3







slope 1.3



2







10 100







1







200







400 n







800







1600







Figure 4: The top part of the figure shows the median number of flips needed by WalkSAT for formulas of all three types below and above the threshold, with n = 10 4 . Below the threshold, 2-hidden formulas are just as hard as 0-hidden ones (they coincide to within the resolution of the figure) and their running time increases steeply as we approach the threshold. Except in the range 5.3 < r < 6.3, 2hidden formulas are much harder than 1-hidden ones unless the algorithm starts with an (exponentially lucky) biased initial state. The bottom part of the figure shows the median number of flips needed by WalkSAT to solve the three types of formulas at r = 4.25 as a function of n. Here n ranges from 100 to 2000. While the median running time for all three is polynomial, the 2-hidden problems are much harder than the 1-hidden ones unless we start with a biased initial state. Again, the running time of 2-hidden problems scales similarly to 0-hidden ones, i.e., to random 3-SAT without a hidden assignment.







636







Hiding Satisfying Assignments: Two are Better than One







10 Median number of flips over 100 trials







6







WalkSAT performance on HIDDEN 1 and 2 formulas with r=8 HIDDEN-1 HIDDEN-2







10







5







10







4







10







3







10







2







10 2 10







1







10







3







N







10







4







Figure 5: The median number of flips needed by WalkSAT to solve the two types of formulas at r = 8, above the range where 1-hidden formulas are harder. At these densities, 2-hidden formulas are again harder than 1-hidden ones, although both are much easier than at densities closer to the threshold.







We believe that random 2-hidden instances could make excellent satisfiable benchmarks, especially just around the satisfiability threshold, say at r = 4.25 where they appear to be the hardest for WalkSAT (although beating SP requires somewhat higher densities). Several aspects of our experiments suggest exciting directions for further work, including: 1. Proving that the expected running time of natural Davis-Putnam algorithms on 2hidden formulas is exponential in n for r above some critical density. 2. Explaining the different threshold behaviors of SP on 1-hidden and 2-hidden formulas. 3. Understanding how long WalkSAT takes at the midpoint between the two hidden assignments, before it becomes sufficiently unbalanced to converge to one of them. 4. Studying random 2-hidden formulas in the dense case where the number of clauses grows more than linearly in n.







References



Achlioptas, D. (2001). Lower bounds for random 3-SAT via differential equations. Theor. Comp. Sci., 265, 159-185. Achlioptas, D., Beame, P., & Molloy, M. (2001). A sharp threshold in proof complexity. In Proc. STOC, pp. 337-346. Achlioptas, D., Gomes, C., Kautz, H., & Selman, B. (2000). Generating satisfiable problem instances. In Proc. AAAI, pp. 256-261.



637







Achlioptas, Jia, & Moore







Achlioptas, D., & Moore, C. (2002a). Almost all graphs with average degree 4 are 3colorable. In Proc. STOC, pp. 199-208. Achlioptas, D., & Moore, C. (2002b). The asymptotic order of the random k -SAT threshold. In Proc. FOCS, pp. 779-788. Achlioptas, D., & Moore, C. (2005). Two moments suffice to cross a sharp threshold. In SIAM J. Comput. To appear. Asahiro, Y., Iwama, K., & Miyano, E. (1996). Random generation of test instances with controlled attributes. DIMACS Series in Disc. Math. and Theor. Comp. Sci., 26, 377-393. Barthel, W., Hartmann, A., Leone, M., Ricci-Tersenghi, F., Weigt, M., & Zecchina, R. (2002). Hiding solutions in random satisfiability problems: A statistical mechanics approach. Phys. Rev. Lett., 88 (188701). Chao, M., & Franco, J. (1986). Probabilistic analysis of two heuristics for the 3-satisfiability problem. SIAM J. Comput., 15 (4), 1106-1118. Cheeseman, P., Kanefsky, R., & Taylor, W. (1991). Where the really hard problems are. In Proc. IJCAI, pp. 163-169. Cocco, S., & Monasson, R. (2001a). Statistical physics analysis of the computational complexity of solving random satisfiability problems using backtrack algorithms. Eur. Phys. J. B, 22, 505-531. Cocco, S., & Monasson, R. (2001b). Trajectories in phase diagrams, growth processes and computational complexity: how search algorithms solve the 3-satisfiability problem. Phys. Rev. Lett, 86, 1654-1657. Du, D., Gu, J., & Pardalos, P. (1997). Dimacs workshop on the satisfiability problem, 1996. In DIMACS Discrete Math. and Theor. Comp. Sci., Vol. 35. AMS. Hajiaghayi, M., & Sorkin, G. (2003). The satisfiability threshold for random 3-SAT is at least 3.52.. Hogg, T., Huberman, B., & Williams, C. (1996). Phase transitions and complexity. Artificial Intelligence, 81. Special issue. Johnson, D., & Trick, M. (1996). Second dimacs implementation challenge, 1993. In DIMACS Series in Disc. Math. and Theor. Comp. Sci., Vol. 26. AMS. Johnson, D., Aragon, C., McGeoch, L., & Shevon, C. (1989). Optimization by simulated annealing: an experimental evaluation. Operations Research, 37 (6), 865-892. Kaporis, A., Kirousis, L., & Lalas, E. (2003). Selecting complementary pairs of literals. In Proc. LICS Workshop on Typical Case Complexity and Phase Transitions. Kautz, H., Ruan, Y., Achlioptas, D., Gomes, C., Selman, B., & Stickel, . (2001). Balance and filtering in structured satisfiable problems. In Proc. IJCAI, pp. 351-358. Kirousis, L., Kranakis, E., Krizanc, D., & Stamatiou, Y. (1998). Approximating the unsatisfiability threshold of random formulas. Random Structures Algorithms, 12 (3), 253-269.



638







Hiding Satisfying Assignments: Two are Better than One







Li, C., & Anbulagan (1997a). Heuristics based on unit propagation for satisfiability problems. In Proc. IJCAI, pp. 366-371. Li, C., & Anbulagan (1997b). Look-ahead versus look-back for satisfiability problems. In Proc. 3rd Intl. Conf. on Principles and Practice of Constraint Programming, pp. 341- 355. Massacci, F. (1999). Using walk-SAT and rel-SAT for cyptographic key search. In Proc. IJCAI, pp. 290-295. M ezard, M., & Zecchina, R. (2002). Random k -satisfiability: from an analytic solution to a new efficient algorithm. Phys. Rev. E, 66. Available at: http://www.ictp.trieste.it/zecchina/SP/. Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT problems. In Proc. AAAI, pp. 459-465. Monasson, R. (2005). Average case analysis of DPLL for random decision problems. In Proc. RANDOM. Morris, P. (1993). The breakout method for escaping from local minima. In Proc. AAAI, pp. 40-45. Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: engineering an efficient SAT solver. In Proc. 38th Design Automation Conference, pp. 530-535. Selman, B., Kautz, H., & Cohen, B. (1996). Local search strategies for satisfiability testing. In Proc. 2nd DIMACS Challange on Cliques, Coloring, and Satisfiability. Shaw, P., Stergiou, K., & Walsh, T. (1998). Arc consistency and quasigroup completion. In Proc. ECAI, workshop on binary constraints. Van Gelder, A. (1993). Problem generator mkcnf.c. In Proc. DIMACS. Challenge archive. Wormald, N. (1995). Differential equations for random processes and random graphs. Ann. Appl. Probab., 5 (4), 1217-1235.







639







Weighted Clustering



Margareta Ackerman, Shai Ben-David, Simina Branzei, and David Loker University of Waterloo D.R.C. School of Computer Science {mackerma, shai, sbranzei, dloker}@uwaterloo.ca



Abstract In this paper we investigate clustering in the weighted setting, in which every data point is assigned a real valued weight. We conduct a theoretical analysis on the influence of weighted data on standard clustering algorithms in each of the partitional and hierarchical settings, characterising the precise conditions under which such algorithms react to weights, and classifying clustering methods into three broad categories: weight-responsive, weight-considering, and weight-robust. Our analysis raises several interesting questions and can be directly mapped to the classical unweighted setting.







1







Introduction







We consider a natural generalisation of the classical clustering problem, where every data point is associated with a real valued weight. This generalisation enables more accurate representation of some clustering problems. For example, consider vector quantification that aims to find a compact encoding of signals that has low expected distortion. The accuracy of the encoding is most important for signals that occur frequently. With weighted data, such a consideration is easily captured by having the weights of the points represent signal frequency. Another illustration of the utility of weights comes from facility allocation, such as the placement of police stations in a new district. The distributions of the stations should enable quick access to most areas in the district. However, the accessibility of different institutions to a station may have varying importance. The weighted setting enables a convenient method for prioritising certain landmarks over others. In this paper, we analyse the behaviour of clustering algorithms on weighted data. Given a data set and a clustering algorithm, we are interested in understanding how the resulting clustering changes depending on the underlying weights. We classify clustering algorithms into three categories: those that are affected by weights on all data sets, those that ignore weights, and those methods that respond to weights on some configurations of the data but not on others. Among the methods that always respond to weights are several well-known algorithms, such as k -means and k -median. On the other hand, algorithms such as single-linkage, complete-linkage, and min-diameter ignore weights. Perhaps the most notable is the last category of algorithms. We find that methods belonging to this category are robust to weights when data is sufficiently clusterable, and respond to weights otherwise. The average-linkage algorithm as well as the well-known spectral objective function, ratio cut, both fall within this category. We characterise the precise conditions under which these methods are influenced by weights. Our analysis also reveals the following interesting phenomenon: algorithms that are known to perform well in practice (in the classical, unweighted setting), tend to be more responsive to weights. For example, k-means is highly responsive to weights while single linkage, which often performs poorly in practice [7], is weight robust.







2







Related Work







Clustering algorithms are usually analysed in the context of unweighted data. The only related work that we are aware of is from the early 1970s. Fisher and Van Ness [6] introduce several properties of clustering algorithms. 1







Among these, they mention "point proportion admissibility", which requires that the output of an algorithm should not change if points are duplicated. They then observe that a few algorithms are point proportion admissible. However, clustering algorithms can display a much wider range of behaviours on weighted data than merely satisfying or failing to satisfy point proportion admissibility. We carry out a much more extensive analysis of clustering on weighted data, characterising the precise conditions under which algorithms respond to weight. In addition, Wright [14] proposes a formalisation of cluster analysis consisting of eleven axioms. In two of these axioms, the notion of mass is mentioned. Namely, that points with zero mass can be treated as non-existent, and that multiple points with mass at the same location are equivalent to one point whose weight is the sum of these masses. The idea of mass has not been developed beyond the statements of these axioms in their work.







3







Background







A weight function w over X is a function w : X  R+ . Given a domain set X , denote the corresponding weighted domain by w[X ], thereby associating each element x  X with weight w(x). A distance function is a symmetric function d : X x X  R+  {0}, such that d(x, y ) = 0 if and only if x = y . We consider weighted data sets of the form (w[X ], d), where X is some finite domain set, d is a distance function over X , and w is a weight function over X. A k-clustering C = {C1 , C2 , . . . , Ck } of a domain set X is a partition of X into 1 < k < |X | disjoint, non-empty subsets of X where i Ci = X . A clustering of X is a k -clustering for some 1 < k < |X |. To avoid trivial partitions, clusterings that consist of a single cluster, or where every cluster has a unique element, are not permitted. Denote the weight of a cluster Ci  C by w(Ci ) = xCi w(x). For a clustering C , let |C | denote the number of clusters in C . For x, y  X and clustering C of X , write x C y if x and y belong to the same cluster in C and x C y , otherwise. A partitional clustering algorithm is a function that maps a data set (w[X ], d) and an integer 1 < k < |X | to a k -clustering of X . A dendrogram D of X is a pair (T, M ) where T is a binary rooted tree and M : leaves(T )  X is a bijection. A hierarchical clustering algorithm is a function that maps a data set (w[X ], d) to a dendrogram of X . A set C0  X is a cluster in a dendrogram D = (T, M ) of X if there exists a node x in T so that C0 = {M (y ) | y is a leaf and a descendent of x}. For a hierarchical algorithm A, A(w[X ], d) outputs a clustering C = {C1 , . . . , Ck } if Ci is a cluster in A(w[X ], d) for all 1  i  k . A partitional algorithm A outputs clustering C on (w[X ], d) if A(w[X ], d, |C |) = C . Given a clustering algorithm A and a data set (X, d), range(A(X, d)) = {C | w such that A outputs C on (w[X ], d)}, which is the set of clusterings that A outputs on (X, d) over all possible weight functions.







4







Basic Categories







Different clustering algorithms respond differently to weights. We introduce a formal categorisation of clustering algorithms based on their response to weights. First, we define what it means for a partitional algorithm to be weight responsive on a clustering. We present an analogous definition for hierarchical algorithms in Section 6. Definition 1 (Weight responsive). A partitional clustering algorithm A is weight-responsive on a clustering C of (X, d) if 1. there exists a weight function w so that A(w[X ], d) = C , and 2. there exists a weight function w so that A(w [X ], d) = C . Weight-sensitive algorithms are weight-responsive on all clusterings in their range. Definition 2 (Weight Sensitive). An algorithm A is weight-sensitive if for all (X, d) and all C  range(A(X, d)), A is weight-responsive on C . At the other extreme are clustering algorithms that do not respond to weights on any data set. This is the only category that has been considered in previous work, corresponding to "point proportion admissibility"[6]. 2







Definition 3 (Weight Robust). An algorithm A is weight-robust if for all (X, d) and all clusterings C of (X, d), A is not weight-responsive on C . Finally, there are algorithms that respond to weights on some clusterings, but not on others. Definition 4 (Weight Considering). An algorithm A is weight-considering if * There exists an (X, d) and a clustering C of (X, d) so that A is weight-responsive on C . * There exists an (X, d) and C  range(A(X, d)) so that A is not weight-responsive on C . To formulate clustering algorithms in the weighted setting, we consider their behaviour on data that allows for duplicates. Given a data set (X, d), elements x, y  X are duplicates if d(x, y ) = 0 and d(x, z ) = d(y, z ) for all z  X . In a Euclidean space, duplicates correspond to elements that occur at the same location. We obtain the weighted version of a data set by de-duplicating the data, and associating every element with a weight equaling the number of duplicates of that element in the original data. The weighted version of an algorithm partitions the resulting weighted data in the same manner that the unweighted version partitions the original data. As shown throughout the paper, this translation leads to natural formulations of weighted algorithms.







5







Partitional Methods







In this section, we show that partitional clustering algorithms respond to weights in a variety of ways. We show that many popular partitional clustering paradigms, including k -means, k -median, and min-sum, are weight sensitive. It is easy to see that methods such as min-diameter and k -center are weight-robust. We begin by analysing the behaviour of a spectral objective function ratio cut, which exhibits interesting behaviour on weighted data by responding to weight unless data is highly structured.







5.1







Ratio-Cut Spectral Clustering







We investigate the behaviour of a spectral objective function, ratio-cut [13], on weighted data. Instead of a distance function, spectral clustering relies on a similarity function, which maps pairs of domain elements to non-negative real numbers that represent how alike the elements are. The ratio-cut of a clustering C is rcut(C, w[X ], s) = 1 2



i xCi ,y C Ci C







s(x, y ) * w(x) * w(y )



xCi







w ( x)







.







The ratio-cut clustering function is rcut(w[X ], s, k ) = arg minC ;|C |=k rcut(C, w[X ], s). We prove that this function ignores data weights only when the data satisfies a very strict notion of clusterability. To characterise precisely when ratio-cut responds to weights, we first present a few definitions. A clustering C of (w[X ], s) is perfect if for all x1 , x2 , x3 , x4  X where x1 C x2 and x3 C x4 , s(x1 , s2 ) > s(x3 , x4 ). C is separation-uniform if there exists  so that for all x, y  X where x C y , s(x, y ) = . Note that neither condition depends on the weight function. We show that whenever a data set has a clustering that is both perfect and separation-uniform, then ratio-cut uncovers that clustering, which implies that ratio-cut is not weight-sensitive. Note that these conditions are satisfied when all between-cluster similarities are set to 0. On the other hand, we show that ratio-cut does respond to weights when either condition fails. Lemma 1. Given a clustering C of (X, s) where every cluster has more than one point, if C is not separation-uniform then ratio-cut is weight-responsive on C . Proof. We consider a few cases. Case 1: There is a pair of clusters with different similarities between them. Then there exist C1 , C2  C , x  C1 , and y  C2 so that s(x, y )  s(x, z ) for all z  C2 , and there exists a  C2 so that s(x, y ) > s(x, a). 3







Let w be a weight function such that w(x) = W for some sufficiently large W and weight 1 is assigned to all other points in X . Since we can set W to be arbitrarily large, when looking at the cost of a cluster, it suffices to consider the dominant term in terms of W . We will show that we can improve the cost of C by moving a point from C2 to C1 . Note that moving a point from C2 to C1 does not affect the dominant term of clusters other than C1 and C2 . Therefore, we consider the cost of these two clusters before and after rearranging points between these clusters. A , Let A = aC2 s(x, a) and let m = |C2 |. Then the dominant term, in terms of W , of the cost of C1 is W m which comes from the cost of points in. The cost of C2 approaches a constant as W  . Now consider clustering C obtained from C by moving y from cluster C2 to cluster C1 . The dominant term in s(x,y ) the cost of C1 becomes W A- m-1 , and the cost of C2 approaches a constant as W  . By choice of x and y , if



s(x,y ) A A <m then C has lower loss than C when W is large enough. A- <m holds whenever A < s(x, y )m, m-1 and the latter holds by choice of x and y . Case 2: For every pair of clusters, the similarities between them are the same. However, there are clusters C1 , C2 , C3  C , so that the similarities between C1 and C2 are greater than the ones between C1 and C3 . Let a denote the similarities between C1 and C2 , and b the similarities between C1 and C3 . Let x  C1 . Let w be a weight function such that w(x) = W for large W , and weight 1 is assigned to all other points in X . The dominant term comes from clusters going into C1 , specifically edges that include point x. The dominant term of the contribution of cluster C3 is W b and the dominant term of the contribution of C2 is W a, totalling W a + W b. Now consider clustering C obtained from clustering C by merging C1 with C2 , and splitting C3 into two clusters (arbitrarily). The dominant term of the clustering comes from clusters other than C1  C2 , and the cost of clusters outside C1  C2  C3 is unaffected. The dominant term of the cost of the two clusters obtained by splitting C3 is W b for each, for a total of 2W b. However, the factor of W a that C2 previously contributed is no longer present. Therefore, we replace the coefficient of the dominant term from a to b, which improved the cost of the clustering because b < a. A-s(x,y ) m-1







Lemma 2. Given a clustering C of (X, s) where every cluster has more than one element, if C is not perfect than ratio-cut is weight-responsive on C . The proof for the above lemma is included in the appendix. Lemma 3. Given any data set (w[X ], s) that has a perfect, separation-uniform k -clustering C , ratio-cut(w[X ], s, k ) = C. Proof. Let (w[X ], s) be a weighted data set, with a perfect, separation-uniform clustering C = {C1 , . . . , Ck }. Recall that for any Y  X , w(Y ) = yY w(y ). Then, 1 2



k xCi i=1 xCi w (x) y Ci







rcut(C, w[X ], s) =  = 2 =  2



k







s(x, y )w(x)w(y )



k







xCi w (x)







=







1 2



k







k xCi i=1 y Ci







w(x)w(y )







xCi w (x) k







y Ci w (y ) xCi k







i=1







w ( x) =







 = 2







i=1 y Ci







 w(y ) = 2







 w(Ci ) = 2 i=1







[w(X ) - w(Ci )]



i=1







kw(X ) -



i=1







w(Ci )







 (k - 1)w(X ). 2







Consider any other clustering, C = {C1 , . . . , Ck } = C . Since the perfect clustering is unique, there exists at least one pair x C y such that s(x, y ) > . Since s(x, y )  , for every x, y  X , the cost of C is, rcut(C , w[X ], s) =



1 2 k i=1



x C i y C i







s(x,y )w(x)w(y ) w(x)







>







x C i







1 2







k i=1







xC







i







y C xC i







w(x)w(y )



i







w ( x)







= 2 (k - 1)w (X ) =







rcut(C ). So clustering C has a higher cost than C . We can now characterise the precise conditions under which ratio-cut responds to weights. Ratio-cut responds to weights on all data sets but those where cluster separation is both very large and highly uniform. Formally, 4







Theorem 1. Given a clustering C of (X, s) where every cluster has more than one element, ratio-cut is weightresponsive on C if and only if either C is not perfect, or C is not separation-uniform. Proof. The result follows by Lemma 1, Lemma 2, and Lemma 3.







5.2 K -Means



Many popular partitional clustering paradigms, including k -means, k -median, and min-sum, are weight sensitive. Moreover, these algorithms satisfy a stronger condition. By modifying weights, we can make these algorithms separate any set of points. We call such algorithms weight-separable. Definition 5 (Weight Separable). A partitional clustering algorithm A is weight-separable if for any data set (X, d) and any S  X , where 2  |S |  k , there exists a weight function w so that x A(w[X ],d,k) y for all disjoint pairs x, y  S . Note that every weight-separable algorithm is also weight-responsive. Lemma 4. If a clustering algorithm A is weight-separable, then A is weight-responsive. Proof. Given any (w[X ], d), let C = A(w[X ], d, k ). Select points x and y where x C y . Since A is weightseparable, there exists w so that x A(w [X ],d,k) y , and so A(w [X ], d, k ) = C. K -means is perhaps the most popular clustering objective function, with cost k -means(C, w[X ], d) =



Ci C x,y Ci







d(x, y )2 * w(x) * w(y ) w(Ci )







,







where w(Ci ) = xCi w(x)1 . The k -means algorithm outputs a clustering with minimal k -means cost. We show that k -means is weight-separable, and thus also weight-sensitive. Theorem 2. K -means is weight-separable. Proof. Consider any S  X . Let w be a weight function over X where w(x) = W if x  S , for large W , and w(x) = 1 otherwise. Let m1 = minx,yX d(x, y )2 > 0, m2 = maxx,yX d(x, y )2 , and n = |X |. Consider any k -clustering 2 C where all the elements in S belong to distinct clusters. Then k -means(C, w[X ], d) < km2 (n + n W ). On the other hand, given any k -clustering C where at least two elements of S appear in the same cluster, k -means(C , w[X ], d)  k-means(C ,w[X ],d) W 2 m1 W +n . Since limW  k-means(C,w[X ],d) = , k -means separates all the elements in S for large enough W . The following result holds using a similar argument. Theorem 3. Min-sum, which minimises the objective function separable.



Ci C x,y Ci







d(x, y ) * w(x) * w(y ), is weight-







It can also be shown that a few other algorithms similar to k -means, namely k -median and k -mediods are also weight-separable. The details appear in the appendix. Observe that all of these popular objective functions are highly responsive to weight.



1 Note that this formulation is equivalent to the common formulation that relies on centers of mass [10], however that formulation applies only over normed vector spaces.







5







6







Hierarchical Algorithms







Similarly to partitional methods, hierarchical algorithms also exhibit a wide range of responses to weights. We show that Ward's method, a successful linkage-based algorithm, as well as popular divisive heirarchical methods, are weight sensitive. On the other hand, it is easy to see that the linkage-based algorithms single-linkage and complete-linkage are both weight robust, as was observed in [6]. Average-linkage, another popular linkage-based method, exhibits more nuanced behaviour on weighted data. When a clustering satisfies a reasonable notion of clusterability, then average-linkage detects that clustering irrespective of weights. On the other hand, this algorithm responds to weights on all other clusterings. We note that the notion of clusterability required for average-linkage is a lot weaker than the notion discussed in Section 5.1, where it is used to characterise the behaviour of ratio-cut on weighted data. Hierarchical algorithms output dendrograms, which contain multiple clusterings. Please see the preliminary section for definitions relating to the hierarchical setting. Weight-responsive for hierarchical algorithms is defined analogously to Definition 1. Definition 6 (Weight responsive). A clustering algorithm A is weight-responsive on a clustering C of (X, d) if (1) there exists a weight function w so that A(w[X ], d) outputs C , and (2) there exists a weight function w so that A(w [X ], d) does not output C . Weight-sensitive, weight-considering, and weight-robust are defined as for partitional algorithms in Section 4, with the above definition for weight-responsive.







6.1







Average Linkage







Linkage-based algorithms start off by placing every element in its own cluster, and proceed by repeatedly merging the "closest" pair of clusters until the entire dendrogram is constructed. To identify the closest clusters, these algorithms use a linkage function that maps pairs of clusters to a real number. Formally, a linkage function is a function : {(X1 , X2 , d, w) | d, w over X1  X2 }  R+ . Average-linkage is one of the most popular linkage-based algorithms (commonly applied in bioinformatics under the name UPGMA). Recall that w(X ) = xX w(x). The average-linkage linkage function is



AL (X1 , X2 , d, w )







=







xX1 ,y X2







d(x, y ) * w(x) * w(y )







w(X1 ) * w(X2 )







.







To study how average-linkage responds to weights, we present a relaxation of the notion of a perfect clustering. Definition 7 (Nice). A clustering C of (w[X ], d) is nice if for all x1 , x2 , x3  X where x1 C x2 and x1 C x3 , d(x1 , x2 ) < d(x1 , x3 ). Data sets with nice clusterings correspond to those that satisfy the "strict separation" property introduced by Balcan et al. [3]. As for a perfect clustering, being a nice clustering is independent of weights. We present a complete characterisation of the way that average-linkage (AL) responds to weights, showing that it ignores weights on nice clusterings, but responds to weights on all other clusterings. Theorem 4. For any data set (X, d) and clustering C  range(AL(X, d)), average-linkage is weight robust on clustering C if and only if C is a nice clustering. The proof of Theorem 4 follows from the two lemmas below. Lemma 5. If a clustering C = {C1 , . . . , Ck } of (X, d) is not nice, then either C  range(AL(X, d)) or averagelinkage is weight-responsive on C . Proof. Assume that there exists some w so that C  AL(w[X ], d). If it does not exist then we are done. We construct w so that C  AL(w [X ], d).







6







Since C is not nice, there exist 1  i, j  k , i = j , and x1 , x2  Ci , x1 = x2 , and x3  Cj , so that d(x1 , x2 ) > d(x1 , x3 ). Now, define weigh function w as follows: w (x) = 1 for all x  X \ {x1 , x2 }, and w (x1 ) = w (x2 ) = W , for some large value W . We argue that when W is sufficiently large, C is not a clustering in AL(w [X ], d). By way of contradiction, assume that C is a clustering in AL(w [X ], d) for any weight function w . Then there is a step in the algorithm where clusters X1 and X2 merge, where X1 , X2  Ci , x1  X1 , and x2  X2 . At this point, there is some cluster X3  Cj so that x3  X3 . 2 (x1 ,x2 )+1 W +2 , for some We compare AL (X1 , X2 , d, w ) and AL (X1 , X3 , d, w ). AL (X1 , X2 , d, w ) = W dW 2 + W + 3 4



(x1 ,x3 )+1 W +2 for some non-negative real i s. non-negative real i s. Similarly, AL (X1 , X3 , d, w ) = W dW 2 + W + 3 4 2 Dividing both sides by W , we see that AL (X1 , X3 , d, w )  d(x1 , x3 ) and AL (X1 , X2 , d, w )  d(x1 , x2 ) as W  , and so the result holds since d(x1 , x3 ) < d(x1 , x2 ). Therefore average linkage merges X1 with X3 , so cluster Ci is never formed, and so C is not a clustering in AL(w [X ], d).



2







Finally, average-linkage outputs all nice clusterings present in a data set, regardless of weights. Lemma 6. Given any weighted data set (w[X ], d), if C is a nice clustering of (X, d), then C is in the dendrogram produced by average-linkage on (w[X ], d). Proof. Consider a nice clustering C = {C1 , . . . , Ck } over (w[X ], d). It suffices to show that for any 1  i < j  k , X1 , X2  Ci where X1  X2 =  and X3  Cj , AL (X1 , X2 , d, w) < AL (X1 , X3 , d, w). We have the following inequalities:



x1 X1 [w (x1 )*maxx2 X2







AL (X1 , X2 , d, w )











2







d(x1 ,x2 ) w(X1 )*w(X2 )







x2 X2







w(x2 )]







= and







x2 X2







w(x2 ) x X [w(x1 )*maxx2 X2 d(x1 ,x2 )] 1 1 w(X1 )* x X w(x2 )



2







=







x1 X1







w(x1 )*maxx2 X2 d(x1 ,x2 ) w(X1 )







AL (X1 , X3 )















x 1  X1







w(x1 ) * minx3 X3 d(x1 , x3 ) w(X1 ) * w(X3 )







x3 X3







w(x3 )







=







x1 X1







w(x1 ) * minx3 X3 d(x1 , x3 ) w(X1 ) >



AL (X1 , X2 ).







Since C is nice, minx3 X3 d(x1 , x3 ) > maxx2 X2 d(x1 , x2 ), and so







AL (X1 , X3 )







6.2







Ward's Method







Ward's method is a highly effective clustering algorithm [5], which, at every step, merges the clusters that will yield the minimal increase to the k-means cost. Let ctr(X, d, w) be the center of mass of the data set (w[X ], d). Then, the linkage function for Ward's method is



W ard (X1 , X2 , d, w )







=







w(X1 ) * w(X2 ) * d(ctr(X1 , d, w), ctr(X2 , d, w))2 w(X1 ) + w(X2 )







Theorem 5. Ward's method is weight sensitive. The proof is included in the appendix.







6.3







Divisive Algorithms







The class of divisive clustering algorithms is a well-known family of hierarchical algorithms, which construct the dendrogram by using a top-down approach. This family of algorithms includes the popular bisecting k-means algorithm. We show that a class of algorithms that includes bisecting k-means consists of weight-sensitive methods. 7







Given a node x in dendrogram (T, M ), let C (x) denote the cluster represented by node x. Formally, C (x) = {M (y ) | y is a leaf and a descendent of x}. Informally, a P -Divisive algorithm is a hierarchical clustering algorithm that uses a partitional clustering algorithm P to recursively divide the data set into two clusters until only single elements remain. Formally, Definition 8 (P -Divisive). A hierarchical clustering algorithm A is P -Divisive with respect to a partitional clustering algorithm P , if for all (X, d), we have A(w[X ], d) = (T, M ), such that for all non-leaf nodes x in T with children x1 and x2 , P (w[C (x)], d, 2) = {C (x1 ), C (x2 )}. We obtain bisecting k -means by setting P to k -means. Other natural choices for P include min-sum, and exemplarbased algorithms such as k -median. As shown in Section 5, many of these partitional algorithms are weight-separable. We show that whenever P is weight-separable, then P -Divisive is weight-sensitive. The proof of the following theorem appears in the appendix. Theorem 6. If P is weight-separable then the P -Divisive algorithm is weight-sensitive. Partitional k -means, k -medoids k -median, Min-sum Ratio-cut Min-diameter k -center Hierarchical Ward's method Bisecting k -means Average-linkage Single-linkage Complete-linkage







Weight Sensitive Weight Considering Weight Robust







Table 1: A classification of clustering algorithms based on their response to weighted data.







7







Discussion and Future Work







In this paper we investigated several classical algorithms, belonging to each of the partitional and hierarchical settings, and characterised the exact conditions under which they respond to weights. Our results are summarised in Table 1. We note that all of our results immediately translate to the standard setting, by mapping each point with integer weight to the same number of unweighted duplicates. In particular, we proved precisely when the weight considering methods, average-linkage and ratio-cut, respond to weights. It is interesting to note that the response of these weight considering techniques is substantially different. Ratio cut ignores weights only on data that is exceptionally well-structured, having large and highly uniform cluster separation. Yet average linkage requires a much weaker condition, finding all clusterings where data are closer to other elements in their partition than to data outside their cluster. Intuitively, average linkage uses weights as a secondary source of information, relying on them only when the clustering structure is ambiguous. There are a number of interesting avenues for future investigation. A compelling question left open is to understand the correlation between the weight responsiveness of an algorithm and the quality of clusterings that it produces in the classical setting. As an example, observe that many notable algorithms, such as k -means and spectral methods, respond to weights, while less used approaches, such as single linkage, never do. It would also be interesting to perform a quantitative analysis to measure the exact degree of responsiveness to weights, which may lead to a more fine grained classification of these algorithms. In addition, it remains to be determined how the approximations used in practice, such as spectral clustering heuristics and the Lloyd method, behave on weighted data. Our preliminary work on these heuristics lends further support to the hypothesis that the more commonly applied algorithms are also more responsive to weights.







8







References



[1] P. K. Agarwal and C. M. Procopiuc. Exact and approximation algorithms for clustering. In SODA, 1998. [2] D. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding. In SODA, 2007. [3] M. F. Balcan, A. Blum, and S. Vempala. A discriminative framework for clustering via similarity functions. In STOC, 2008. [4] S. Dasgupta and P. M. Long. Performance guarantees for hierarchical clustering. J. Comput. Syst. Sci., 70(4):555- 569, 2005. [5] B. S. Everitt. Cluster Analysis. John Wiley & Sons Inc, 1993. [6] L. Fisher and J. Van Ness. Admissible clustering procedures. Biometrika, 58:91-104, 1971. [7] J. Hartigan. Consistency of single linkage for high-density clusters. J. Amer. Statist. Assoc., 76(374):388-394, 1981. [8] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Comput. Surv., 31(3):264-323, 1999. [9] L. Kaufman and P. J. Rousseeuw. Partitioning Around Medoids (Program PAM), pages 68-125. John Wiley & Sons, Inc., 2008. [10] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy. The effectiveness of Lloyd-type methods for the k-means problem. In FOCS, 2006. [11] M. Talagrand. A new look at independence. Ann. Probab., 24(1):1-34, 1996. [12] V. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. [13] U. Von Luxburg. A tutorial on spectral clustering. J. Stat. Comput., 17(4):395-416, 2007. [14] W. E. Wright. A formalization of cluster analysis. J. Pattern Recogn., 5(3):273-282, 1973.







9







Extracting Viewpoints from Knowledge Bases



IBM Corporation 11400 Burnet Road Austin, Texas 78758 acker@austin.ibm.com







Liane Acker







Department of Computer Sciences University of Texas at Austin Austin, Texas 78712 porter@cs.utexas.edu make consistent modeling assumptions (e.g.,the model fragments of (Falkenhainer & Forbus 1991), the views of (Forbus 1984), and the ontological perspectives of (Liu & Farley 1990).) Finally, KI (Murray & Porter 1989), a learning program, uses viewpoints to constrain its search for the consequences of adding new information to a knowledge base. Conventional methods for accessing knowledge bases do not provide direct access to viewpoints. Some methods extract individual facts, such as the ller of a particular frame-slot. Others extract collections of facts, such as all the slots and llers of a particular frame or those satisfying a Prolog-like query. Indisputably, these access methods can be used to extract viewpoints through a sequence of invocations. However, they ignore the central problem in extracting viewpoints: determining which facts to include in a viewpoint. The advantage of our access methods is that they provide a general solution to this problem (as described in Section 2), and the viewpoints extracted by our methods are comparable in coherence to those people construct (as described in Section 3). Our methods for accessing viewpoints are implemented in a program called the View Retriever (a term rst proposed by Suthers (Suthers 1988)). The input to this program is a viewpoint speci cation and the output is a collection of facts. The task of the View Retriever is to determine which facts constitute the speci ed viewpoint and to request them from the knowledge base. Whether the knowledge base returns cached facts or computes them (using deduction, abduction, or induction) is irrelevant to the View Retriever. Those facts that the knowledge base cannot provide are not included in the viewpoint. The View Retriever is used currently with the Botany Knowledge Base, a large system of over 13,000 frames and 160,000 cached facts, where a fact is a slotller of a frame. However, it is designed to work for any physical domain and to be easily extended to work in non-physical domains, such as those involving abstract concepts or mental processes.







Bruce Porter







Viewpoints are coherent collections of facts that describe a concept from a particular perspective. They are essential for a wide variety of tasks, such as explanation generation and qualitative modeling. We have identi ed many types of viewpoints and developed a program, the View Retriever, for extracting them from knowledge bases, either singly or in combinations. The View Retriever provides a general solution to the central problem in extracting viewpoints: determining which facts are relevant to requested viewpoints. Our evaluation indicates that viewpoints extracted by the View Retriever are comparable in coherence to those people construct.







Abstract







The objective of this research is to develop computational methods for extracting viewpoints from knowledge bases. Intuitively, a viewpoint is a coherent collection of facts that describes a concept from a particular perspective. For example, three viewpoints of the concept \car" are: the viewpoint \car as-kind-of consumer durable," which describes a car's price and longevity; the structural viewpoint, which describes a car's parts and their interconnections; and the viewpoint \car ashaving metal composition," which includes facts, such as a car's propensity to dent and rust, that are related to its composition. The need for viewpoints by knowledge-based programs is widespread. For example, many explanationgeneration systems require viewpoints to produce explanations that are complete and coherent (Suthers 1991; McKeown 1988; Lester & Porter 1991; McCoy 1989; Moore & Swartout 1988). Qualitative modeling systems use viewpoints to increase e ciency and to



Support for this research was provided by an IBM Graduate Fellowship to Liane Acker, a grant from the National Science Foundation (IRI-9120310), a contract from the Air Force O ce of Scienti c Research (F49620-93-10239), and donations from the Digital Equipment Corporation. This work was conducted at the University of Texas at Austin.







1 Introduction







2 The View Retriever







Production product location Substance energy source raw materials Thing producer Substance Oxygen energy source raw materials producer ATP Water Carbon-Dioxide



Carbon-Bond-Energy Photon Photosynthesis input energy form location Light-Energy







Energy-Transduction







input energy form Energy







location energy provider Thing Thing Energy output energy form







Thing







Photosynthesis product







Thing







output energy form energy provider







location Glucose Chloroplast







Chloroplast







Photosynthetic-Cell







Figure 1: The viewpoint of \photosynthesis as-kind-of production", as extracted from the Botany Knowledge Base by the View Retriever. The way a user (or application program) speci es a viewpoint and the way the View Retriever extracts it depends on the type of viewpoint. As-kind-of viewpoints describe concepts by relating them to more general concepts. Viewpoints constructed along basic dimensions describe concepts using a cluster of their attributes, such as functional, structural, or perceptual attributes. As-having viewpoints include the facts pertinent to a given attribute.







Figure 2: The viewpoint of \photosynthesis as-kind-of energy transduction", as extracted from the Botany Knowledge Base by the View Retriever. is more general than hslot; filleri if any of the following conditions hold: 1. slot = slot and filler is a generalization of filler. 2. filler = filler and slot is a generalization of slot. 3. slot is a generalization of slot and filler is a generalization of filler. For example, the viewpoint shown in Figure 1 contains the fact that photosynthesis produces glucose, because it is known that production processes typically produce some substance and glucose is a special kind of substance. That is, hproduct; Glucosei appears on the Photosynthesis frame, hproduct; Substancei appears on the Production frame, and Substance is a generalization of Glucose. The resulting viewpoint includes the links between facts about the primary concept and the more general facts about the reference concept (see Figure 1). The View Retriever excludes many facts about the primary concept from the viewpoint. For example, although it is true that photosynthesis converts light energy into carbon bond energy, this fact is excluded because it is irrelevant to our concept of production (although it would be included in \photosynthesis askind-of energy transduction", as shown in Figure 2). Various explanation-generation systems extract knowledge structures similar to as-kind-of viewpoints. The TEXT system (McKeown 1985) uses a function (called the identi cation rhetorical predicate ) to differentiate a concept from a more general concept. TEXT determines what facts to include using a type of knowledge called focus constraints: facts are selected incrementally based on their connection with previously selected facts, rather than a global coherence



0 0 0 0 0 0







As-kind-of Viewpoints







An as-kind-of viewpoint describes a concept in terms of a more general concept. For example, the viewpoint \photosynthesis as-kind-of production" consists of those facts that explain how photosynthesis is a special case of production, such as its raw materials and products. Figure 1 shows a portion of this viewpoint. The speci cation of an as-kind-of viewpoint is of the form: (hprimary concepti as-kind-of hreference concepti) where the primary concept is the one the viewpoint will be taken of and the reference concept is a generalization of the primary concept (although not necessarily an immediate generalization). The View Retriever extracts as-kind-of viewpoints by selecting relevant facts about the primary concept. A fact is a tuple of the form hslot; filleri; it is considered relevant if some more general fact appears on the frame for the reference concept. The fact hslot ; filler i



0 0







criteria. Suthers's system uses a genus-and-di erentia function similar to TEXT's identi cation predicate (Suthers 1991). McKeown's ADVISOR system constructs knowledge structures similar to as-kind-of viewpoints by restricting to prede ned partitions of the knowledge base the superconcepts from which a concept can inherit slot llers (McKeown 1988). In addition to viewpoints that describe concepts in terms of more general concepts, the View Retriever can extract viewpoints along basic dimensions, which are general types of facts, such as facts about an object's structure, function, or appearance. (We have borrowed the term from Metaphors We Live By (Lako & Johnson 1980), a work that has signi cantly in uenced our characterization of viewpoint types.) Below we describe the basic dimensions used by the View Retriever. Basic dimensions for objects: Structural, which includes the parts or substances that make up the object. It also includes the connections and spatial relations among them, what we call interconnection relations. The structural dimension also includes the relative sizes or number of the parts. Perceptual, which includes information regarding how humans perceive (see, hear, etc.) the object. This includes the shape, symmetry, size, color, and temperature of the object. Functional, which includes what the object \does" (the processes in which it is an actor). The functional dimension also includes properties suggestive of some unspeci ed process in which the object is involved, such as life span and metabolic rate. Temporal, which includes the temporal parts of an object (its stages or states). It also includes as interconnection relations the temporal ordering constraints among the stages or states. Basic dimensions for processes: Behavioral, which includes the types and roles of the actors in the process and the changes that the process e ects upon them. Initial and nal conditions of the process are included as well. Procedural, which includes the steps (subevents) of the process and (as interconnection relations) any temporal ordering constraints that exist among the steps. Basic dimensions for both objects and processes: Taxonomic, which includes the taxonomic breakdown of a class of objects or processes into subclasses. The taxonomic dimension also includes the relative sizes of the subclasses, the criteria for the breakdown, and (as interconnection relations) information about which subclasses are disjoint.







Viewpoints Constructed Along Basic Dimensions







how one object or process a ects other objects or processes. This includes causal relationships (e.g.,causes, enables, prevents, facilitates) and qualitative in uences between quantities (e.g.,directlya ects, inversely-in uences, correlated-with). The speci cation for a viewpoint constructed along a basic dimension simply names the primary concept and the basic dimension desired: (hprimary concepti dimension hbasic dimensioni) The View Retriever constructs the viewpoint rst by extracting facts about the primary concept that belong to the basic dimension, then by adding to the viewpoint any interconnection relations for the basic dimension. For example, to construct a structural viewpoint of a plant seed, the View Retriever rst selects those slots and llers from the Seed frame that belong to the structural dimension, including hpart, Seed-Coati, hpart, Embryoi, and hpart, Endospermi. The View Retriever then selects interconnection relations among the selected parts (seed coat, embryo, and endosperm). For the structural dimension, interconnection relations include connected-to, contains, surrounds, etc. Thus, the resulting viewpoint contains the information that the seed is made up of a seed coat containing an embryo and an endosperm. To construct viewpoints along basic dimensions, the View Retriever uses knowledge of which slots in the knowledge base are within each dimension. Based on our experience with the Botany Knowledge Base, this knowledge is easily encoded because the distinctions made by the basic dimensions are re ected in the top levels of the slot hierarchy. Viewpoints created by the View Retriever along basic dimensions are similar to perspectives as suggested by Suthers (Suthers 1991) and as used by Romper (McCoy 1989). Unlike our basic dimensions, however, Romper's perspectives are domain-speci c and include only facts about the primary concept; interconnection relations are omitted.







Modulatory, which includes information about







As-Having Viewpoints







An as-having viewpoint contains all and only the information about a concept that is relevant to some speci ed fact about the concept. Its speci cation has the following form: (hprimary concepti as-having hslot, lleri) To our knowledge, general methods do not exist for extracting as-having viewpoints. Therefore, unlike for the other types of viewpoints, the View Retriever depends on a priori knowledge of relevance to select the facts that constitute as-having viewpoints. To construct an as-having viewpoint, the View Retriever rst looks for a cached as-having viewpoint that is based on the same fact (slot and ller), or a more general fact, as the requested viewpoint, but with a







di erent primary concept. For example, to extract the viewpoint: (Squirrel as-having hagent-in, Seed-Dispersali) the View Retriever rst looks in the knowledge base for a related, cached viewpoint such as one of the following: 1. (Animal as-having hagent-in, Seed-Dispersali) 2. (Bird as-having hagent-in, Seed-Dispersali) 3. (Animal as-having hagent-in, Transportationi) If a related viewpoint is found, the View Retriever uses it to determine which facts should be included in the new viewpoint. It does this by nding for each fact of the cached viewpoint a corresponding fact that is true of the primary concept of the new viewpoint. If the primary concept of the cached viewpoint is a generalization of the primary concept of the new viewpoint, then nding corresponding facts between the two consists of nding facts about the primary concept of the new viewpoint that are specializations of facts in the cached viewpoint. If the primary concepts of the two viewpoints are siblings , then nding corresponding facts between the two is more di cult. It requires nding pairs of facts that share a common abstraction. If a related, cached viewpoint cannot be found in the knowledge base, then the View Retriever constructs ashaving viewpoints by collecting all the facts about the primary concept that are implied by the speci ed fact, using all the inference rules and mechanisms available in the knowledge base. This method assumes (sometimes incorrectly) that any fact implied by some other fact is relevant to it. However, it has the advantage that it does not require viewpoints to be cached in the knowledge base. Ideally, as-having viewpoints would be extracted using a theory of relevance to determine what facts are relevant. As a rst step toward such a theory, several researchers have analyzed texts to determine the various ways that one fact may be relevant to another (Mann & Thompson 1987; Hobbs 1985). However, these theories are as yet descriptive rather than prescriptive, so the View Retriever cannot use them directly.







Angiosperm-Sexual-Reproduction







location Pollen-Grain Formation







subevents







Flower







location location







Embryo-Sac Formation







has- parts







source Androecium destination location location







Pollen-Grain Transfer







surrounds







Pollen-Grain Germination







Gynoecium







Double Fertilization







Figure 3: The composite (\structural-functional") viewpoint of a ower in its role in plant reproduction, as extracted from the Botany Knowledge Base by the View Retriever. where viewpoint1 and viewpoint2 are individual viewpoints (or speci cations for them) and relation species the correspondence to be established between the viewpoints. One commonly used composite viewpoint, called \structural-functional", describes the roles an object (and its parts) play in an event (and its subevents). Its speci cation is the following: (composite (hobjecti dimension structural) (heventi dimension procedural) actor-in) For example, the viewpoint that describes the roles of a ower's parts in the steps of plant reproduction is speci ed as follows: (composite (Flower dimension structural) (Plant-Reproduction dimension procedural) actor-in) Its contents are shown in Figure 3. The View Retriever constructs this composite viewpoint by the following procedure. First it extracts the two individual viewpoints (the structural viewpoint of Flower and the procedural viewpoint of PlantReproduction). Then it determines which parts of the Flower that are in the structural viewpoint are related to Plant-Reproduction or one of its subevents (as given in the procedural viewpoint) by an actor-in relation or some more speci c relation (such as location-of ). Those parts, such as the Flower's corolla, that are not actors in the event are omitted from the composite viewpoint. Similarly, those subevents, such as FruitRipening, that do not involve any of the parts in the structural viewpoint of Flower are omitted. This procedure can extract diverse viewpoints. For example, the composite viewpoint that describes the parts of a plant ovary as related to the parts of the fruit of which it is a developmental stage can be extracted with the following speci cation:







Composite Viewpoints







In addition to extracting individual viewpoints as described above, the View Retriever can combine them to form composite viewpoints. This involves more than simply concatenating the contents of two individual viewpoints; it involves putting them into correspondence with one another and removing the portions that do not correspond. Despite the apparent utility of composite viewpoints, we know of no other general methods for extracting them from knowledge bases. The speci cation for a composite viewpoint has the following form: (composite hviewpoint1i hviewpoint2i hrelationi)







Fruit stage has-parts







Pericarp Ovary stage has-parts Ovarian-Wall stage







Seed







Ovule







Figure 4: The composite viewpoint of the parts of a plant ovary as related to the parts of the fruit of which it is a developmental stage, as extracted from the Botany Knowledge Base by the View Retriever. (composite (Fruit dimension structural) (Ovary dimension structural) stages) This composite viewpoint, as shown in Figure 4, includes the parts of the fruit (seed, pericarp, etc.), the parts of the ovary (ovule, ovarian wall, etc.), and the stage relations between them, such as the facts that the ovule is a developmental stage of the seed and the ovarian wall is a developmental stage of the pericarp. The procedure for constructing composite viewpoints can also extract the viewpoint that categorizes angiosperms ( ower-bearing plants) according to the di erent types of owers they have. The speci cation is the following: (composite (Angiosperm dimension taxonomic) (Flower dimension taxonomic) parts) This composite viewpoint includes, for example, the fact that one kind of angiosperm is the orchid, which has an irregular ower. The purpose of our evaluation was to measure the coherence of viewpoints the View Retriever extracts, as compared to the coherence of viewpoints found in human-generated text. For each of 12 topics in botany, sets of facts were drawn from 3 sources: a college-level botany textbook (Raven, Evert, & Curtis 1976), the View Retriever applied to the Botany Knowledge Base, and facts selected randomly from a particular frame in the Botany Knowledge Base. The viewpoints ranged in size from 3 to 11 facts. For each topic, textbook passages and random sets of facts were chosen to be roughly the same size as the viewpoint on that topic. Each group of facts (including the







(1) Textbook Viewpoints 4.23 0.56 (2) View Retriever's Viewpoints 3.76 0.74 (3) Degraded Viewpoints 2.86 0.94 (4) Random Collections of Facts 2.62 0.86 Table 1: Ten judges rated the coherence of sets of facts from four sources (1=incoherent; 5=coherent). A statistical analysis using the T-test with 0.95 level of condence shows no signi cant di erence in coherence between sources (1) and (2) or between sources (3) and (4). There is a signi cant di erence between all other pairs. textbook passages) was translated manually into \simple English" to normalize presentation style. The viewpoints included about equal numbers of as-kind-of , basic dimension, and composite viewpoints; as-having viewpoints were omitted from this study because they often use cached viewpoints. Ten subjects (senior undergraduates and graduate students from the Botany and Biology Departments of the University of Texas at Austin) judged the coherence of several passages from each source. The subjects were asked to use a scale of 1 to 5, to assign a passage a score of \1" if it seemed no more coherent than a randomly selected group of facts on the subject, and to assign a passage a score of \5" if it was as coherent as a passage of comparable length on the subject from a good textbook. Table 1 summarizes the subjects' responses. Statistical analysis (using a T-test with 0.95 level of con dence) yields the following results: The mean coherence of viewpoints from textbooks did not di er signi cantly from the mean coherence of viewpoints extracted by the View Retriever. The mean coherence of extracted viewpoints did differ signi cantly from the mean coherence of random collections of facts drawn from the same frame. A further study gives additional evidence that the View Retriever extracts coherent viewpoints. Along with passages from the three sources described above, the subjects were given passages from a fourth source: viewpoints extracted by the View Retriever and then \degraded" by replacing some of their facts with randomly selected facts on the same topic. Twenty-eight such degraded viewpoints were constructed, each with between one and seven facts replaced. Of the twentyeight, each subject received six. Table 1 shows the mean coherence score of the degraded viewpoints. Statistical analysis shows a signi cant di erence in the mean coherence of \pure" viewpoints and degraded viewpoints. A nal study adds more evidence that passages vary in coherence based on their source and that view-







Source







Coherence Mean







Evaluation of the View Retriever







points extracted by the View Retriever are consistently judged to be coherent. A two-way analysis of variance, computed by Paul Cohen1 , determined that there was no signi cant interaction e ect between: the variance in coherence scores assigned by di erent judges, and the variance in coherence scores for passages from di erent sources (e.g.,textbooks, the View Retriever). Thus, although judges varied in their harshness, they largely agreed on relative orderings. Viewpoints are coherent collections of facts that describe a concept from a particular perspective. They are essential for a wide variety of tasks, such as explanation generation and qualitative modeling. We have identi ed several types of viewpoints and developed a program, the View Retriever, for extracting them from knowledge bases, either singly or in combination. Our evaluation of the View Retriever indicates that its viewpoints are comparable in coherence to those constructed by people. The View Retriever has several known limitations, some of which we are addressing. First, viewpoint speci cations use the names of frames and slots in the knowledge base. Therefore, users of the View Retriever must have extensive knowledge of the concept and slot hierarchies in order to use the View Retriever. To address this limitation, we are developing methods whereby users can specify frames and slots descriptively rather than by name. Second, our textbook analysis reveals that most explanations consist of several viewpoints used in concert. Although the View Retriever can extract composite viewpoints, we have not yet identi ed which combinations are commonly used. A third limitation is that the View Retriever ignores knowledge about the a priori importance of facts. Therefore, it cannot extract viewpoints of a concept in the order of their importance, a potentially useful ability. The View Retriever will be evaluated more extensively when it supports our tutoring system for plant anatomy and physiology. It will be the primary method used by the tutor to access the Botany Knowledge Base to build qualitative models and generate explanations. We are currently building this tutoring system, and we have found that knowledge base access at the level of viewpoints (as opposed to the level of individual facts or frames) greatly simpli es system design and implementation.



1 Computer Science Department, University of Massachusetts at Amherst







3 Discussion







Proceedings of the 8th National Conference on Arti cial Intelligence.







Falkenhainer, B., and Forbus, K. 1991. Compositional modeling: Finding the right model for the job. Arti cial Intelligence 51:95{143. Forbus, K. 1984. Qualitative process theory. Arti cial Intelligence 24:85{168. Hobbs, J. 1985. On the coherence and the structure of discourse. Technical Report CSLI-85-37, Computer Science Department, Stanford University. Lako , G., and Johnson, M. 1980. Metaphors We Live By. University of Chicago Press. Lester, J., and Porter, B. 1991. A student-sensitive discourse generator for intelligent tutoring systems. In Proceedings of the International Conference on the Learning Sciences, 298{304. Liu, Z., and Farley, A. 1990. Shifting ontological perspectives in reasoning about physical systems. In







References







Mann, W., and Thompson, S. 1987. Rhetorical structure theory: A theory of text organizations. Technical Report ISI/RS-87-190, Information Sciences Institute, University of Southern California. McCoy, K. 1989. Generating context-sensitive responses to object-related misconceptions. Arti cial Intelligence 41:157{195. McKeown, K. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.







McKeown, K. 1988. Generating goal-oriented explanations. International Journal of Expert Systems 1(4):377{395. Moore, D., and Swartout, W. 1988. A reactive approach to explanation. In Proceedings of the Fourth







Murray, K., and Porter, B. 1989. Controlling search for the consequences of new information during knowledge integration. In Proceedings of the Machine Learning Workshop, 290{295. Palo Alto, California: Morgan Kaufmann. Raven, P.; Evert, R.; and Curtis, H. 1976. Biology of Plants. New York: Worth Publishers. Suthers, D. 1988. Providing multiple views of reasoning for explanation. In Proceedings of the International Conference on Intelligent Tutoring Systems, 435{442. Suthers, D. 1991. Task-appropriate hybrid architectures for explanation. In Proceedings of the AAAI-91



Workshop on Comparative Analysis of Explanation Planning Architectures.







International Workshop on Natural Language Generation.







Indefinite Scalability for Living Computation



David H. Ackley



University of New Mexico Department of Computer Science Albuquerque, NM 87131 ackley@cs.unm.edu







Abstract



In a question-and-answer format, this summary paper presents background material for the AAAI-16 Senior Member Presentation Track "Blue Sky Ideas" talk of the same name.







Q: So, what's the big idea here?



A: Traditional CPU and RAM computing, based on hardware determinism, is now struggling to scale up, as clock speed increases have stalled and multicore cache coherence grows increasingly expensive. Determinism is also a perpetual computer security nightmare, encouraging programmers to optimize efficiency, and thus fragility--while providing, by default, utter predictability to attackers. The blue sky idea is: We should forgo deterministic execution and focus instead on best-effort computing--in hardware and software both--to develop indefinitely scalable computer designs. A machine from here to the horizon if we want it, built of locally-connected, interchangeable computing tiles; a machine so big parts of it will always be failing; a machine so big we'll be using it long before we finish building it. To survive and prosper in such a system, software will be living computation in a far richer sense than today: It will collaborate and compete with other software, will take damage and heal, will reproduce for redundancy and parallelism, will migrate and spontaneously colonize new hardware, and so on. Programming and performing large-scale computations will be less like solving logic puzzles and more like agriculture or animal husbandry or ecosystem management. Q: Wow! So this is all just wishes and fantasy, right? A: Well, not entirely. It's still the earliest days, but for several years we've been working on indefinite scalability, mostly in simulation. Unless stated otherwise, the material in this summary paper is drawn from (Ackley and Cannon 2011; Ackley 2013a; 2013b; Ackley, Cannon, and Williams 2013; Ackley and Small 2014; Ackley and Ackley 2015). And though it can sound like science fiction, our proposed hardware architecture depends only on conventional electronics manufacturing and presumes no breakthroughs in materials science or unconventional computing media.



Copyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







Q: So why, exactly, doesn't determinism scale? A: Hardware determinism means that digital computer hardware guarantees to perform the rules of logic flawlessly, for as long as the program runs--and if the hardware can't do that, for any reason, it promises to "crash" the whole machine immediately. With that guarantee in hand, typical software utterly ignores the possibility of hardware errors--and that `reliable hardware + efficient software' mindset is so deeply embedded, in academia and industry both, that it is rarely even mentioned, let alone questioned. The scaling problem is that hardware's guarantee isn't quite 100%. Any real unit of hardware will have some small chance of undetected error. Now, if we are given a maximum computation size up front, we can design hardware units to make that size computation as reliable as we like. But if we then keep adding more and more of those hardware units to the system, and running it longer and longer, eventually the aggregate space-time computational volume will exceed the reciprocal of the hardware unit failure rate, and the machine will deliver undetected errors to the software level. That's why, even in principle, hardware determinism doesn't scale, and in the supercomputing community, for example, it is already more than a merely theoretical concern (Cappello et al. 2009). Q: But if determinism fails, doesn't that mean chaos? A: Not necessarily at all. Computer science's focus on efficiency makes it easy to forget that digital hardware employs massive redundancy to accomplish its heroic acts of determinism. It uses whole wires to carry single bits, and deploys saturating amplifiers at every turn to reduce noise while the chance of error remains small. When we renegotiate around best-effort hardware, software becomes obligated to look beyond efficiency and embrace robustness, by deploying redundancy effectively throughout the software stack. The resulting systemic errorresistance will enable scalability, and also fundamentally benefit security--not like a silver bullet miracle, but like the way sanitation and hygiene benefits the public health. Q: Hmm. What do we need know most about this idea? A: Here are the bottom lines: This proposal represents a huge but feasible change, and it is important for society and long overdue, and it needs your help.







It's built on three overlapping concepts--indefinite scalability for hardware and architecture, robust-first computing for software and programming, and best-effort computing for systems overall and models of computation. In the rest of this paper, we will say a little more about indefinite scalability in computer architecture, and then consider the particular approach to it we have been exploring, which is called the Movable Feast Machine. We'll show a few simple examples to illustrate the ideas and the current level of research and development, and conclude with a call to action.







Q: What is indefinite scalability?



A: Indefinite scalability is a design principle saying that any admissible computer architecture must be expandable to arbitrary size without any fundamental re-engineering. We may be unable to grow our indefinitely scalable machine for external reasons--like we run out of money or real estate or power or cooling--but never because we hit some internal design limit, like reaching the limits of a fixed-width address or the light cone of a central clock. Q: So, something like the internet? A: No, as it is most commonly used, the Internet is only finitely scalable, because its (IPv4 or v6) address space is finite. The Internet could be indefinitely scalable, however, via aggressive use of "anycasting" (Partridge, Mendez, and Milliken 1993; Abley and Lindqvist 2006), which offers indefinite spatial scaling in exchange for a finite set of interprocessor request types. Q: But how can over 1038 IPv6 addresses not be enough? A: Because for indefinite scalability, "really big" amounts to "merely finite": To preserve the clarity and power of the idea, any appeals to practical sufficiency are irrelevant. Moreover, anecdotally at least, it seems that every internal limit in a design is a form of technical debt (Cunningham 1992; Allman 2012) that has consequences beyond the limit itself. A design built on globally unique node names drawn from a finite space, for example, incurs not only the risk of name exhaustion, but all the issues and risks of centralized naming and resolving, aliasing, spoofing, and so on. Indefinite scalability doesn't automatically solve or avoid any such issues, but it forces them all to the foreground; it helps keep us honest and thereby provides a more stable basis for evaluating and choosing between architectures. Q: Well, if not the Internet, what is indefinitely scalable? A: Good question! We'll introduce our primary example, the Movable Feast Machine, in a moment. A repurposed "anycast-Internet," as mentioned above, could be one example. And though cellular automata (CA) typically assume deterministic execution and thus render themselves only finitely scalable, probabilistic cellular automata (Grinstein, Jayaprakash, and He 1985; Agapie, Andreica, and Giuclea 2014, e.g.) do go beyond determinism--and some of them could plausibly be cast into indefinitely scalable form. In general, a design for an indefinitely scalable machine amounts to a spatial tiling of hardware units, with additional







Figure 1: The Movable Feast Machine (MFM) architectural overview, with 2009-era prototype tile hardware at bottom. (See text.) requirements as needed to preserve open-ended physical realizability. As an obvious example, fixed-latency global communications is disallowed by the finite speed of light, as are hierarchical or hypercube interconnects based on fixedlatency links. Less obviously, the presumption of a global reset or "boot time" is also disallowed. Q: It's too hard! Is indefinite scalability really worth it? A: It is hard, but the potential upside is computational systems of unlimited size, that are inherently tough, good at their jobs, and hard to attack. In the end, hardware determinism is a property of small systems. Best-effort computing is the future--it is what we should be optimizing, rather than determinism. We are hurting ourselves by delaying.







Q: What is the Movable Feast Machine?



A: The Movable Feast Machine is a tile-based indefinitely scalable architecture (see Figure 1). Each tile is a small von Neumann machine running an identical control program out of non-volatile local memory. The program treats local volatile memory as a patch of cellular automata grid, and performs events on randomly-chosen local grid sites, while coordinating with adjacent tiles to seek consistent cache views of their relevant grid sites. There are no global clocks or synchronization barriers, and tiles race against each other to acquire intertile locks for events that may affect caches.







Figure 2: The MFM per-tile event loop. (See text.) Figure 4: A Demon Horde Sort stochastic sorter. (See text.) Q: What does state transition code look like? A: Over the last year and a half, we have developed a programming language, called ulam, specifically to express MFM state transitions. ulam has its own distinct flavors but is deliberately designed to seem reasonably familiar to programmers used to conventional object-oriented procedural languages. ulam compiles into C++, and from there via gcc to machine code for dynamic loading into the simulation, and we hope soon into live tile hardware as well. Figure 1 includes some legal but pointless ulam code, and Figure 3 presents a functional but complete lout of an element--a ForkBomb that attempts to fill space with copies of itself, with no regard for what might already be there: Hello World and then some. Q: That's cute, but how do more complex things work? A: In a general sense, building larger structures in the Movable Feast involves three aspects: 1. Functional: Programming more complex element behavior functions, and involving greater numbers of interacting element types, 2. Spatial: Deciding where to locate the involved atoms relative to each other, and if and how they should move, to make possible local interactions that are useful within the larger structures and purposes, and 3. Temporal: Using spatial and functional mechanisms to implement staged processes that unfold in time, such as growth phases. Also, adjusting rate constants so some subprocesses run much slower or faster than others. This can enable programmers to use constant approximations (if slower) or equilibrium approximations (if faster) to simplify reasoning about otherwise complex dynamics. Q: Well, maybe I asked for that. How about an example? A: Figure 4 is a schematic representation of one of the earliest machines we investigated, called the Demon Horde Sort (DHS). Data atoms (blue), each holding a random 32 bit







The overall effect is of an asynchronous, probabilistic cellular automata, but one in which an individual grid site's neighborhood--which we call an event window--is huge by CA standards, involving thousands of bits. As a result, a traditional CA state transition lookup table is completely infeasible; instead, a mostly-traditional serial deterministic function is called to perform the state transition, depending on the type bits in the event window's center site. Figure 2 summarizes the tile event loop. Q: But if a transition is deterministic, what's the point? A: That determinism only lasts as long as an event, which is short, and can affect only its event window, which is tiny by traditional programming standards. Although programming MFM element behavior functions is rather less obvious than classical serial programming, it is also much more intuitive than composing typical CA rule tables, primarily because execution is serial and effectively single-threaded within an event, during which the event window acts like passive memory. The corresponding challenges are that event code cannot access persistent state outside the event window or assume anything about event window contents between invocations. ulam 1; /** Fork bomb. \symbol FB \color #f00 \symmetries all */ element ForkBomb { EventWindow ew; Void behave() { ew[1] = ew[0]; } } Figure 3: A complete ulam element. Copies itself from the event window center (ew[0]) to ew[1], which in this case (due to the \symmetries all in the element metadata) is an adjacent site chosen at random on each event.







Figure 5: Six events in the life of a self-assembling four-port data switch, spreading over four simulated MFM tiles. Top left: Initial state. Top center: After 100 average events per site (AEPS). Top right: After 1000 AEPS. Bottom left: After 2000 AEPS. Bottom center: After three tiles are reset at 2500 AEPS. Bottom right: After 3000 AEPS. See text. number, are placed on the grid by the emitters (green) at the right, and are carried right-to-left and moved up and down by the Sorters (red), and eventually are extracted by absorbers on the left (dark grey). The trick is, each Sorter remembers the value of the last Data it moved, and when it considers moving the next Data atom right-to-left, it also tries to move it up or down based on the comparison of the current and previous Data values. The DHS illustrates all of the compositional mechanisms just mentioned. By copying the Data value into the Sorter's internal storage, Data-Data comparisons can be performed even when the Data items aren't close enough to interact directly: The existence and behavior of the Sorters enable local interactions that advance the goals of the larger structure. Similarly, by making basic spatial and geometric assumptions--that input is East, output is West, small is North and large is South--the Sorters can autonomously take actions that will help in the computation as a whole. And finally, the "DReg" and "Res" in Figure 4 are part of a "Dynamic Regulator" feedback system that maintains the Sorter population at a reasonable density. Though the DReg mechanism is interesting in its own right, here it just serves as an example of the temporal layering of dynamics: The DReg operations are so slow compared to the Sorters that the DReg can basically be ignored except when considering the long-term dynamics of the system. Q: OK, but, how can the DHS possibly sort correctly? A: It doesn't. Its inherent resolution is determined by the channel width, and its sorting quality generally improves with increasing length/width aspect ratio. More basically, though, in this problem formulation, correctness really isn't an option. Given that the emitters produce Data atoms intermittently and unpredictably, it's not even well-defined what the "correct" maximum value really is at any given moment. Welcome to best-effort. Q: I start to see.. Have you built other machines? A: Certainly. As one last example, just briefly, Figure 5 shows six snapshots in the self-assembly of a toy four-port data switch we have recently been exploring. Starting from a single `switch wall' atom, the switch builds a box (blue outline), insulates it with passive Walls (white) embedded with four I/O ports (red, yellow, green, and blue line segments), and builds a routing grid (evenlyspaced white dots). The routing grid atoms observe nearby







ports and then gossip among themselves to form gradients to guide the data cells (scattered dots, colored to show their destination port). Each data cell carries a 32 bit payload and an eight bit sequence number, although this switch does not attempt to perform packet reassembly. After an average of one thousand events per site (1000 AEPS), the switch has completely self-assembled but still has a backlog of data cells; by 2000 AEPS the switch is operating smoothly. Later, at 2500 AEPS, we severely damage it by resetting three of the four underlying tiles, but it immediately begins to reassemble itself and, by 3000 AEPS, it is almost healed.







Q: Very cool! But, how does all this affect AI?



A: Traditional computing is about constructing things once and then trusting them to remain intact indefinitely. That's why it's, at once, so marvelously efficient and so deathly fragile. Robust-first computing, on the other hand, is about continuous self-construction and maintenance, so that information structures are automatically refreshed as needed, or at some rate, or both. Sixty-five years ago, von Neumann (1951) predicted that hardware determinism would soon be supplanted, but with design lock-in and network effects it remains virtually unchallenged today. In AI's long-running cross-pollination, let us call it, between the neats and the scruffies, that dominance has given the neats sole possession of the home court advantage--deterministic execution--without anybody really calling them on it. But now the costs of playing on that court are rising too high. It's time for the neats to try playing an away game. Q: Last question! I hate to ask, but who funds this work? A: Eventually, a substantial research and development effort, performed and supported by many people and organizations, will be needed to develop the science and engineering of best-effort computing. Perhaps you will be part of that. For these early stages, support has come from a brave and visionary few. The work summarized here was supported in part by a Google Faculty Research Award, and in part by grant VSUNM201401 from VanDyke Software.







Ackley, D. H. 2013a. Bespoke physics for living technology. Artificial Life 19(3 4):347-364. Ackley, D. H. 2013b. Beyond efficiency. Commun. ACM 56(10):38-40. Author preprint: http://nm8.us/1. Agapie, A.; Andreica, A.; and Giuclea, M. 2014. Probabilistic cellular automata. Journal of Computational Biology 21(9):699-708. Allman, E. 2012. Managing technical debt. Queue 10(3):10:10-10:17. Cappello, F.; Geist, A.; Gropp, B.; Kal, L. V.; Kramer, B.; and Snir, M. 2009. Toward exascale resilience. IJHPCA 23(4):374-388. Cunningham, W. 1992. The WyCash Portfolio Management System. In Addendum to the Proceedings on Objectoriented Programming Systems, Languages, and Applications (Addendum), OOPSLA '92, 29-30. New York, NY, USA: ACM. Grinstein, G.; Jayaprakash, C.; and He, Y. 1985. Statistical mechanics of probabilistic cellular automata. Phys. Rev. Lett. 55:2527-2530. Partridge, C.; Mendez, T.; and Milliken, W. 1993. Host Anycasting Service. RFC 1546 (Informational). von Neumann, J. 1951. The general and logical theory of automata. In Jeffress, L. A., ed., Cerebral Mechanisms in Behaviour: the Hixon Symposium (1948). Wiley. 15-19. Also appears as pages 302-306 in A.H. Taub, editor, John von Neumann Collected Works: Volume V - Design of Computers, Theory of Automata and Numerical Analysis, Pergamon Press, 1963.







References



Abley, J., and Lindqvist, K. 2006. Operation of Anycast Services. RFC 4786 (Best Current Practice). Ackley, D. H., and Ackley, E. S. 2015. Artificial life programming in the robust-first attractor. In Proc. of the European Conference on Artificial Life (ECAL). Ackley, D. H., and Cannon, D. C. 2011. Pursue robust indefinite scalability. In Proc. HotOS XIII. Napa Valley, California, USA: USENIX Association. Ackley, D. H., and Small, T. R. 2014. Indefinitely Scalable Computing = Artificial Life Engineering. In Proceedings of The Fourteenth International Conference on the Synthesis and Simulation of Living Systems (ALIFE 14) 2014, 606- 613. MIT Press. Ackley, D. H.; Cannon, D. C.; and Williams, L. R. 2013. A movable architecture for robust spatial computing. The Computer Journal 56(12):1450-1468.







A Natural Interface and Uni ed Skills for a Mobile Robot



William Adams, Dennis Perzanowski, and Alan C. Schultz Navy Center for Applied Research in Arti cial Intelligence Naval Research Laboratory Washington, DC 20375-5337, U.S.A. adams,dennisp,schultz@aic.nrl.navy.mil



Our research is aimed at developing an independent, cooperative, autonomous agent. Toward this end, we are working on two areas: a natural interface for interacting with the robot, and the basic underlying skills for navigating in previously unknown environments. The interface we are developing combines natural language and gestures 1]. While human communication between individuals occurs on many channels, two of them, natural language and gesture, complement each other fairly regularly in daily communication. Since people interweave them freely during their interations, we assume they might readily do so in their interactions with a mobile robot. Our interface allows the processing of complete or incomplete (fragmentary) commands. To process these types of commands, we keep track of the various goals during human-robot interactions by instantiating \context predicates," which are basically lists of the verbal predicates and their arguments expressed in logical form. By utilizing context predicates, a discourse component of the interface tracks exactly which and to what extent each goal was achieved. With this information and by tracking goal achievement, the robot can continue to achieve unaccomplished goals on its own, no matter at what point or in what state the system is currently. Thus, context predicates permit the system to work independently on achieving previously stated, but as yet uncompleted, goals. This capability ultimately allows the user greater freedom to interact naturally without having to explicitly state or re-state each expected or desired action when an interruption occurs. We hope to extend goal tracking so that the mobile robot can complete semantically related goals which are not initially speci ed or which are unknown to the human at the time when the initial goal is instantiated. This natural interface is currently in use with a moThis work was sponsored by the O ce of Naval Research. bile robot. Navigation goals and locations are speci ed by speech and/or with natural gestures. Commands can be interrupted and subsequently completed with fragmentary utterances. To provide the basic underlying skills for navigating in previously unknown environments, we are working to create a mobile robot system that is robust and adaptive in rapidly changing environments. We view integration of these skills as a basic research issue, studying the combination of di erent, complementary capabilities. One principle that aids integration is the use of unifying representations which allow better communication and interaction among di erent components. Our most recent work uses evidence grids as a common representation to integrate mobile robot exploration, localization, navigation, and planning ?]. In addition, this integrated system includes methods for adapting maps to allow for robust navigation in dynamic environments. As a result, a robot can enter an unknown environment, map it while remaining con dent of its position, and robustly plan and navigate within the environment in real time. We create two types of representations with the evidence grids: short-term perception maps, and longterm metric maps. The short-term maps store very recent sensor data that does not contain signi cant odometry error, and these maps can be used for obstacle avoidance and for localization. The long-term maps represent the environment over time, and can be used for navigation and path-planning. The use of evidence grids requires that the robot be localized within its environment. To overcome odometric drift and errors, we have developed a method for continuous localization, in which the robot continually corrects its position estimates. Continuous localization builds the short-term perception maps, and at frequent intervals registers the oldest short-term map against the long-term map, locating the robot within







the environment. In order for mobile robots to operate in unknown environments, they need the ability to explore and build maps that can be used for navigation. We have developed the frontier-based exploration strategy based on the concept of frontiers, regions on the boundary between open space and unexplored space. When a robot moves to a frontier, about half of its sensors can still see the old, known environment, which can be used by continuous localization to maintain accurate odometry. Its other sensors see into unexplored space and expand the map. By moving to successive frontiers, the robot can constantly increase its knowledge of the world. The new, expanded maps produced by the exploration are passed to continuous localization as its new long-term map. After exploration is complete, changes in the world (blocked passages, moved obstacles, etc) must also be modeled. We have added a learning component to the continuous localization algorithm to allow the longterm map to be updated with recent sensor data from the short-term perception maps, making the long-term map adaptive to the environment. In order to provide robust navigation, we have incorporated Trulla, a propagation-based path planner which uses a navigability grid to describe which areas in the environment are navigable (considering oor properties, obstacles, etc). In our system, we have integrated Trulla by replacing its navigability grid with our long-term metric map. As our long-term map adapts to changes in the environment, Trulla can replan using the robot's current knowledge about the world. Continuous localization's long-term map update method can adapt to somewhat rapid and persistent changes in the environment, but not to very fast changes, such as a person walking through the room. Accordingly, paths generated by Trulla are not su cient to prevent collisions with transient obstacles. We have integrated the Vector Field Histogram (VFH) reactive navigation method to avoid transient obstacles that are not yet represented in the evidence grid. VFH uses an HIMM occupancy grid to model the robot's immediate surroundings. In our integration, we replace the HIMM occupancy grid with the shortterm perception map produced by continuous localization. The short-term perception map allows VFH to consider all sensors, and yields a less noisy picture of the robot's immediate environment. Fig. 1 illustrates the complete architecture. When heading into an unknown environment, the robot autonomously maps the environment while maintaining accurate odometry, producing the initial







long-term map. Each new short-term perception map and long-term map is sent to a Map Server process which in turn makes the sensor-fused perceptions of the environment available to the various processes.



VFH



control







Trulla mapserver







short term map



sensor data Continuous Localization







long term map







exploration







Figure 1: Architecture of integrated system After exploration, the user speci es a navigation goal to Trulla, which consults the Map Server for the current long-term map and computes the vector eld describing the best path from each cell to the goal. Trulla sends the vector eld to VFH, which uses the robot's current position to index the vector eld and get the direction to the goal. VFH retrieves the shortterm map from the Map Server, and steers the robot in the direction closest to that which was planned by Trulla. While VFH is steering the robot, continuous localization continues to correct odometry and produce short-term and adapted long-term maps. With each new long-term map, Trulla replans and sends a new vector eld to VFH which uses it for subsequent navigation.







References



1] Perzanowski, D., Schultz, A., and Adams, W. (1998). \Integrating natural language and gesture in a robotics domain," In Proc. of the IEEE International Symposium on Intelligent Control: ISIC/CIRA/ISIS Joint Conference, Gaithersburg, MD, 247-252. 2] Schultz, A. and Adams, W. (1998). \Continuous localization using evidence grids," In Proc. of the 1998 IEEE International Conference on Robotics and Automation, Leuven, Belgium, 2833-2839.







Learning Bayesian Networks with Incomplete Data by Augmentation



Tameem Adela,, Cassio P. de Camposb



a Machine







arXiv:1608.07734v2 [cs.AI] 9 Oct 2016







b EEECS,







Learning Lab, University of Amsterdam Queen's University Belfast







Abstract We present new algorithms for learning Bayesian networks from data with missing values using a data augmentation approach. An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create an approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks with such new approach.







1. Introduction Missing entries in real-world data exist due to various reasons. For instance, it can be due to damage of the device used to record feature values; a metal detector might fail to produce a signal denoting the existence of a metal due to a certain malfunction. Results can be incomplete in an industrial experiment due to mechanical breakdowns not necessarily related to the performed experiment (Little and Rubin, 1987). Recommendation data can have missing values since participants in the recommendation system did not rate all the available



 Corresponding







author Email address: tameem.hesham@gmail.com (Tameem Adel)







songs, films, books, etc. While data missingness in the above examples can mostly be assumed to be generated by a random process which depends only on the observed data, usually referred to as missing at random (MAR) (Little and Rubin, 1987; Rancoita et al., 2016), this assumption might fail in other examples. People seeking for health insurance might refuse to give an answer to certain questions in order to reduce the costs, e.g. `do you smoke?', and in many cases this can be seen as an indication of one specific answer. In such cases we say that data are missing not at random, or MNAR (see for instance (Van den Broeck et al., 2014)). Given a dataset with categorical random variables, the Bayesian network structure learning problem refers to finding the best network structure (a directed acyclic graph, or DAG) according to a score function based on the data (Heckerman et al., 1995). As well known, learning a Bayesian network from complete data is NP-complete (Chickering, 1996), and the task becomes even harder with incomplete data. In spite of that, the problem of learning a Bayesian network from incomplete data by (an optimistic) augmentation belongs to the same complexity class, as we will show later on. Because of such result, we investigate and obtain a new exact algorithm for the problem, based on reformulating it into a standard structure learning without missing data. This is the first exact algorithm for the problem, to the best of our knowledge. In contrast to previous work, our algorithm performs both tasks, namely structure learning and data imputation, in a single shot rather than learning the Bayesian network and then dealing with the missing data, possibly in an iterative manner (Friedman, 1998; Rancoita et al., 2016). Based on the optimization that is required to solve the problem and on the exact algorithm, we devise a hill-climbing approximate algorithm. The hill-climbing regards the completions of the missing values only, while the structure optimization is performed by any off-the-shelf algorithm for structure learning under complete data. Most previous work to learn the structure of Bayesian networks from incomplete data has focused on MAR. The seminal algorithm in Friedman (1998) introduced an iterative method based on the Expectation-Maximization (EM) 2







technique, referred to as structural EM. Implementation of structural EM begins with an initial graph structure, followed by steps where the probability distribution of variables with missing values is estimated by EM, alternated with steps in which the expectation of the score of each neighbouring graph is computed. After convergence, the graph maximizing the score is chosen. Many other algorithms have used ideas from structural EM and deal separately with the missing values and the structure optimization using complete data (Borchani et al., 2006; Leray and Francois, 2005; Meila and Jordan, 1998; Ramoni and Sebastiani, 1997; Riggelsen, 2006; Riggelsen and Feelders, 2005). In Rancoita et al. (2016), structures are learned from incomplete data using a structural EM whose maximization step is performed by an anytime method, and the `expectation' step imputes the missing values using expected means, or modes, of the current estimated joint distribution. By using modes in each iteration (Ramoni and Sebastiani, 1997), the EM method is sometimes called hard EM, and is close to our work. In some sense, we work with a global optimization version of hard EM. While this is not exactly considering data to be MNAR, such approach fits less the observed data and performs well for MNAR missing data when compared to structural EM, as we will empirically show. We emphasize that the actual missingness process is not disclosed to the methods and is not assumed to be somehow known, and that we are mainly interested in structure learning. Given the difficulties of structure learning itself, we assume that the underlying distribution is identifiable (in short terms, provided enough data are available, one could reconstruct such distribution, see for instance (Mohan et al., 2013)). We perform experiments on a set of heterogeneous datasets. We base the evaluation on imputation accuracy in its pure form, as well as in the forms of classification accuracy and semi-supervised learning accuracy. Experiments show the improvements achieved by the proposed algorithms in all scenarios. Regarding the comparison between our exact and approximate methods, experiments suggest that accuracy levels achieved by the approximate algorithm are close to those achieved by the optimal learning algorithm, with the former being much faster and scalable. 3







2. Bayesian Network Structure Learning Let X = (X1 , . . . , Xm ) refer to a vector of categorical random variables, taking values in OX = xi OXi , where OX represents the Cartesian product of the state space, OXi , of each Xi . Denote by D an n-instance dataset where each instance Du = (du,1 , du,2 , . . . , du,m ) is such that du,i is either an observed value ou,i  OXi or a special symbol denoting the entry is missing. Let Zu denote a completion for variables with missing values in instance u and zu,i for the missing value of Xi . A Bayesian network, M, is a probabilistic graphical model based on a structured dependency among random variables to represent a joint probability distribution in a compact and tractable manner. Here, it represents a joint probability distribution PrM over a collection of categorical random variables, X. We define a Bayesian network as a triple M = (G , X, P ), where G = (VG , EG ) is a directed acyclic graph (DAG) with VG a collection of m nodes associated to the random variables X (a node per variable), and EG a collection of arcs; P is a collection of conditional probabilities PrM (Xi |PAi ) where PAi denotes the parents of Xi in the graph (PAi may be empty), corresponding to the relations of EG . In a Bayesian network, the Markov condition states that every variable is conditionally independent of its non-descendants given its parents. This structure induces a joint probability distribution by the expression PrM (X1 , . . . , Xm ) =



i







PrM (Xi |PAi ). We define ri  2 as the number of val-







ues in OXi , i.e. ri = |OXi |, and rPAi as the number of possible realizations of the parent set, that is, rPAi =



Xl PAi







rl . Let R = maxi ri .







Given a complete dataset D with n instances, the structure learning problem in Bayesian networks is to find a DAG G that maximizes a given score function, that is, we look for G  = argmaxGG sD (G ), with G the set of all DAGs over node set X. We consider here the score function sD to be the Bayesian Dirichlet Equivalent Uniform (BDeu) criterion (Buntine, 1991; Cooper and Herskovits, 1992) (other decomposable scores could be used too), so we have sD (G ) =



i sD (Xi , PAi ).







We however have to deal with the missing part of the data,







4







which we treat by completing the missing values in the best possible way (an optimistic completion): (G  , Z  ) = argmax sD (G , Z ) =



GG , ZZ







argmax



GG , ZZ i







sD (Xi , PAi ; Z{Xi }PAi )







(1)







where Z = xu OZu and sD (G , Z ) is the score sD (G ) evaluated for the complete data when its missing values are replaced by Z , while sD (Xi , PAi ; Z{Xi }PAi ) is the local score for a node Xi with parent set PAi (note that such computation only depends on the completion Z{Xi }PAi of the involved variables). We refer to this optimization task as the structure learning problem by optimistic augmentation. It can be applied to MAR data, but we argue that it is particularly suitable to MNAR when compared to the standard techniques such as structural EM. From the optimization viewpoint, this can be seen as a global optimization approach to hard EM, since we complete the data with their mode, but we do it globally instead of in an iterative process such as EM. As well known, hard EM can be seen as a subcase of EM, since it is equivalent to allowing EM to use only degenerate mass functions in its expectation step. Theorem 1. The decision version associated to the structure learning problem by optimistic augmentation is NP-complete. Proof. Hardness is obtained by realizing that this problem generalizes the structure learning problem without missing data, which is NP-hard (Chickering, 1996). Pertinence in NP holds since given G and Z , the score function sD can be computed in polynomial time. Since the problem is a combinatorial optimization over a discrete domain (both DAGs and completions of data are discrete entities), we could resort to enumerating all possible solutions. This is obviously infeasible for both: the number of DAGs grows super-exponentially in the number of variables and the number of completions grows exponentially in the number of missing values. We will now present an exact algorithm for the problem which transforms it 5







into a standard structure learning problem, and later we modify the approach to perform approximate learning. In this respect, we define as a t-local optimal solution for Equation (1) a pair (G , Z ) such that sD (G , Z )  sD (G  , Z  ) for all G  and all Z  with HD(Z , Z  )  t, where HD is the Hamming distance, that is, (G , Z ) is optimal with respect to any other pair whose completion of the data has at most t elements different from Z . A global optimal solution is a -local optimal solution. 2.1. Optimal (Exact) Learning Algorithm We assume that a standard structure learning algorithm for complete data is available to us, which is based on the framework of two main optimizations: (i) parent set identification and (ii) structure optimization. Step (i) concerns building a list of candidate parent sets for each variable, while Step (ii) optimizes the selection of a parent set for each variable in a way to maximize the total score while ensuring that the graph is a DAG. This latter step can be tackled by exact or approximate methods (Bartlett and Cussens, 2013; Scanagatta et al., 2015) (in our experiments we will employ an exact method such that we are sure that the quality of results is only affected/related to the proper treatment of the missing data, but for very large domains any approximate method could be used too). The exact algorithm for solving Equation (1) is based on modifying the parent set identification step. This step has no known polynomial-time solution if we do not impose a maximum number of parents (Koivisto, 2006), so we will assume that such a bound k is given. We compute the candidate list by using one of the available approaches (de Campos and Ji, 2011; Scanagatta et al., 2015) to guide the search, but for each candidate to be evaluated, the corresponding variables in the dataset might contain missing values. The first part of the transformation is to create gadgets composed of some new artificial variables which will be related to the missing values and will enable the inclusion of all possible replacements of missing values by augmenting the original domain. Over all the dataset, for each and every missing value, let us denote it by (u, i) 6







for sample u and variable Xi , we include artificial variables X(u,i),1 , . . . , X(u,i),ri . Each X(u,i),j has two parent set candidates: (i) X  {X(u,i),1+(j



mod ru ) }







with







score zero (assuming all other score values are negative, without loss of generality) and (ii)  with score -, with  a large enough value (e.g. greater than the sum of all other absolute scores). We further illustrate the idea via an example for variable X1 with r1 = 3: Assume m = 3, r1 = 3 and there is one missing value at (u, 1). An artificial variable is included for each possible completion zu,1 , resulting in a total of three new variables, X(u,1),1 , X(u,1),2 , X(u,1),3 . The following gadget, consisting of two parent set candidates per artificial variable, is added to the list of parent set scores (we know that only one parent set per variable will be chosen during the optimization phase later on):







s(X(u,1),1 , {X(u,1),2 , X1 , X2 , X3 }) = 0, s(X(u,1),1 , ) = -, s(X(u,1),2 , {X(u,1),3 , X1 , X2 , X3 }) = 0, s(X(u,1),2 , ) = -, s(X(u,1),3 , {X(u,1),1 , X1 , X2 , X3 }) = 0, s(X(u,1),3 , ) = -. According to this gadget, each artificial variable will either have no parent variables or all other original variables as well as one other artificial variable as its set of parents. The case with no parents leaves open the opportunity to choose the variable representing such completion as a potential parent for all original variables. In contrast, the cases with all variables as parents disables such completion from being chosen as a parent by the original variables, otherwise it would create a cycle. Due to including one artificial variable as a parent of the next artificial variable, at least one parent set among those with score zero cannot be chosen (otherwise a cycle is formed), and because they are all very good scores when compared to -, all but one will certainly be chosen. There is one such gadget per missing value in the original dataset, so we spend 7







time O(R * m * C ), where C is the number of missing values. Finally, we return to the computation of the score for a given variable and parent set. Let Xi be the variable of interest and PAi = {Xi1 , . . . , Xiq } for which the score must be evaluated. At this moment, we consider all possible completions Z{Xi }PAi and compute the scores sD (Xi , PAi ; Z{Xi }PAi ) for each one of them. In order to reduce the problem to a standard structure learning without missing data, we must index these scores somehow. This is made possible via the new artificial variables: sD (Xi , PAi ; Z{Xi }PAi ) = sD (Xi , PAi  {X(u,i),zu,j : zu,j  Z{Xi }PAi }) that is, for each imputed missing value zu,j appearing for variable Xi or PAi we will have an extra parent within the parent set that tells which completion was used for that missing value, according to the completion Z{Xi }PAi . This idea is applied to every evaluation of the score of a parent set, for every possible completion Z{Xi }PAi , so the final list of candidates will include only parent sets for which the completion of the data is `known' at the time that the score is computed. In order to ensure that the completions are compatible among different local score computations, the gadgets explained before are enough, since they force that a certain completion be chosen for each missing value. Theorem 2. The exact algorithm transforms the structure learning problem by augmentation into a standard structure learning without missing data in time O(R * m * C ), plus time O(n * k * Rc ) per parent set evaluation, where C is the total number of missing values and c is the maximum number of missing values appearing in the variable of interest or in variables in the parent set being evaluated (hence polynomial in all parameters but c). There will be many score computations and entries in the list, exponential in the number of missing values involved. So the benefit of this approach is that usually only a few variables are involved in the score computation at the same time. The drawback is that it cannot handle datasets with many missing 8







values for the same variable, since it is Rc times slower than the corresponding parent set evaluation without missing data. Next we address this issue by proposing an approximate method (the exact method is nevertheless useful in small domains and also important to check whether the approximate version achieves reasonable results). 2.2. Approximate Algorithm Albeit locally to the variables involved in the evaluation of a parent set, the exact method considers all possible completions of the data. This is fine with a few missing values per variable, but if there are many missing values, in particular within the same variable, the exact method becomes computationally infeasible. We propose an approximate algorithm based on a hill-climbing idea. We start with an initial guess Z0 (or several different random guesses) for the completion of all missing values in the dataset. Then we execute the very same steps of the exact algorithm, but we restrict the completions only to those which are at most t elements different from the current guess Zh . There are



  at most (R * m)t completions Zh such that HD(Zh , Zh )  t. We proceed as







with the exact method, but applying such constraint during the transformation that was explained in the previous section. After the transformation is done, the structure optimization is run and a new structure and new data completion Zh+1 is obtained. We repeat the process until convergence, that is, until Zh+1 = Zh . Theorem 3. The approximate algorithm transforms the structure learning problem by augmentation into a standard structure learning without missing data in time O(R * m * C ), plus time O(n * k * (R * m)t ) per parent set evaluation (C is the total number of missing values and t is the amount of locality of the approximation, as previously defined), that is, polynomial in all parameters but t. The outcome of the approximate learning algorithm is the network structure as well as the completion of all the missing data values. The approximate algorithm might lead to a locally optimal solution, but on the other hand it is much more scalable than the exact algorithm. 9







Theorem 4. Provided that an optimal structure learning optimization algorithm is available, the approximate algorithm always converges to a t-local optimal solution. If we want to scale to very large domains, we could also resort to an approximate structure learning optimization algorithm (e.g. (Scanagatta et al., 2015)). In this case, our approximate algorithm could be used in domains with hundreds or even thousands of variables (using very small t), but we would lose the guarantee to converge to a t-local optimal solution (it would still be a local optimum, but we would have to define it locally also in terms of the graph structures).







3. Experiments We perform experiments on simulated as well as real-world data. The main evaluation metric used is accuracy of the imputation of missing data values, either in the form of missing values spread throughout the data, or in the form of a binary classification problem where only the class variable can contain missing values. Most of our experiments are with binary data for the sake of exposition, even though the algorithms are general and can be used with any categorical data (as shown in the last experimental setting). To test significance, we perform a paired t-test with significance level at 5%. Throughout all tables of results, a result in bold refers to an accuracy value that is significantly better than its competitors, whereas showing two results belonging to the same experiment in bold means that each of them being significantly better than the rest of the competitors. For structure optimization, we use the exact solver referred to as Gobnilp (Bartlett and Cussens, 2013) with the code available from https://www.cs.york.ac.uk/aig/sw/gobnilp/. We perform comparisons among the two proposed algorithms (exact and approximate) and the structural Expectation-Maximization (EM) algorithm (Friedman, 1998). We compare accuracy of the three algorithms based on the percentage of correct imputations over all missing values. As for the structural EM, we 10







have used the implementation available at https://github.com/cassiopc/csdadataimputation (Rancoita et al., 2016). After convergence, we run the prediction of missing values using a most probable explanation query. We must emphasize that the task of Bayesian network structure learning with missing values is very challenging, since it is already challenging without missing values. Therefore, we have focused on real but controlled experiments where we can effectively run the algorithms and assert their quality. We use maximum number of parents, k = 3, and use t = 1. 3.1. Well-known Bayesian Networks We perform experiments using real but small data sets in order to compare both exact and approximate algorithms. First, we employ the original Bayesian network model for Breast Cancer (Almeida et al., 2014), which contains 8 binary variables, we simulate 100 data instances. That model has been learned from cancer patients of the University of Wisconsin Medical Hospital. Features (Bayesian network nodes) include breast density, mass density, architectural distortion and others, in addition to the diagnosis variable whose binary value refers to benign or malignant (D'Orsi et al., 2003). We include two missing values per variable, resulting in a total of 16 missing values. These missing values are generated in a MNAR manner by randomly removing values that are equal to each other, that is, during the generation we enforce that all missing values are zero, or that all missing values are one. Imputation results of the proposed exact learning algorithm, approximate algorithm and structural EM are displayed in the first row of Table 1 over 100 repetitions of the experiment. Second, we use the Bayesian network that has been learned from the Prostate Cancer data by the Tree Augmented Naive Bayes (TAN) (Friedman et al., 1997), implemented by WEKA (Hall et al., 2009). The Prostate Cancer data were acquired during three different moments in time (Sarabando, 2011; Almeida et al., 2014), i.e. during a medical appointment, after performing auxiliary exams, and five years after a radical prostatectomy. It contains 11 binary variables, and 100 instances are generated. We randomly produce two MNAR missing values 11







per variable, resulting in a total of 22 missing values. Results are shown in the second row of Table 1. Third, the well-known ASIA network is used (Lauritzen and Speigelhalter, 1988). We generate 100 instances according to this model, which contains 8 binary variables. Two missing values are randomly generated according to MNAR. Imputation results are displayed in the third row of Table 1. Results indicate that the algorithms proposed here are significantly better than structural EM, which is expected since in this experiment data are not MAR. More interestingly, results of the proposed exact and approximate BN learning algorithms are not significantly different, which supports the use of the (more efficient) approximate method for larger domains. Table 1: Accuracy of imputation for data simulated from different Bayesian networks with two MNAR missing values per variable. Bayesian net Algorithm Exact learning Breast Cancer Approx. learning Structural EM Exact learning Prostate Cancer Approx. learning Structural EM Exact learning ASIA Approx. learning Structural EM 3.2. (LUng CAncer Simple set) LUCAS Dataset The LUCAS dataset contains data of the LUCAS causal Bayesian network (Fogelman-Soulie, 2008) with 11 binary variables, as well as the binary class variable, and contains 2000 instances. In this experiment we conduct an analysis of both MAR and MNAR missing data, in order to understand whether the benefits that we have seen before are only significant in the MNAR case. 12 Average imputation accuracy 84.38% 80% 50% 91% 86.36% 50% 84.38% 79% 43.75%







Thus, we carry out two experiments: (i) MNAR setting by randomly generating missing values all having the same data value (we repeat that to both zero and one values, one at a time); (ii) MAR setting by randomly generating missing values regardless of their respective original values. These simulations are repeated 100 times. First, we generate two missing values per variable (24 missing values). A comparison between the imputation accuracy values of the approximate algorithm and structural EM is displayed in the first two rows of Table 2 (named `Spread All Over'). Surprisingly, our new algorithm is significantly better than structural EM even when missing data are MAR. Second, we generate 20 missing class values and repeat the experiment to span all instances such that each run involves missing values belonging to different instances (without replacement). For the MNAR experiment, each run consists of 20 identical missing class values (that is, we only make missing values of the same class, and we repeat that for both classes). For the MAR case, there is no such restriction and missing class values are randomly generated. Hence, there are 100 runs in order to cover all 2000 instances. Results of the approximate algorithm, structural EM and SVM using different kernels (for the sake of comparison with a state-of-the-art classifier) are displayed in the bottom rows of Table 2. Results of the proposed algorithm are significantly better when MNAR data are used, while the same cannot be stated for the MAR case (accuracy of the proposed algorithm is nevertheless superior to the others in the MAR case).







13







Table 2: Accuracy of imputation for experiments performed on the Lung Cancer dataset (LUCAS). Spread All Over refers to an imputation of 2 missing values per variable out of the 12 LUCAS variables. Classification refers to a classification problem performed as a cross-validation (100-fold cross-validation in the MNAR setting case) on LUCAS, using SVM, vs. an imputation task on the 20 missing class variables of the same folds, by both the proposed approximate learning algorithm and Structural EM. SVM kernels displayed are those that achieved the highest accuracy in each experiment. MP stands for missingness process, and rbf for radial basis function. MP Algorithm Average imputation accuracy







Exp.: Spread All Over MNAR Approx. learning Structural EM Approx. learning Structural EM Exp.: Classification Approx. learning MNAR Structural EM SVM (rbf) Approx. learning MAR Structural EM SVM (rbf) 97.5% 42.5% 45% 69% 70% 55% 70.83% 45% 70% 50%







MAR







3.3. SPECT Dataset The Single Proton Emission Computed Tomography (SPECT) dataset consists of binary data denoting partial diagnosis from SPECT images (Lichman, 2013). Each patient is classified into one of two categories, normal and abnormal. The SPECT data consists of 267 instances and 23 variables in total (22 binary variables and a binary class variable). We generate MNAR missing data with different proportions, always using only one specific value (missing data 14







proportions over all the data are 3%, 5% and 10%). These randomly generated datasets are given as input to the approximate algorithm as well as to structural EM. We note that there is a large discrepancy in the number of data values holding each of the two binary values: About 67% of the SPECT data has a value 0, whereas merely 33% of the data has a value 1. Due to that, we investigate the average MNAR imputation accuracy within each data value separately, and note as well that there is some discrepancy in such accuracy values. Imputation accuracy of the approximate learning algorithm and structural EM are displayed in Table 3. The new algorithm is significantly better. 3.4. Smoking Cessation Study Dataset The dataset used in this experiment is taken from a smoking cessation study as described in Gruder et al. (1993). It has been further utilized in other works, most notably Hedeker et al. (2007). The smoking cessation dataset is a binary dataset consisting of 489 patient records (instances) with the missing data being inherently therein, i.e. there is no need to simulate missing data. The dataset contains 4 variables including the class variable, which refers to smoking or non-smoking. All the missing values are located in the class variable. There is a total of 372 patient records with observed classes, consisting of 294 smoking and 78 non-smoking records, as well as 117 records with missing class labels. The experiment we perform here is a semi-supervised learning (SSL) experiment where we evaluate the performance of the algorithms as follows: (i) We hide the class labels of a portion of the observed labels; (ii) We apply the approximate learning on the data consisting of the originally missing and artificially hidden labels as missing values, and the rest of the data as observed values. Clearly this is a SSL experiment where the training data consists of the records with observed labels as labeled instances, records with originally missing labels as unlabeled instances, and the test instances are the records with artificially hidden labels. The evaluation metric is the accuracy of the test instances using a crossvalidation approach, as usually done in classification experiments. We compare 15







the performance of the approximate algorithm against an equivalent procedure using structural EM (labels are then chosen based on the posterior distribution), and also against a semi-supervised learner in the form of a Laplacian SVM (Melacci and Belkin, 2011) whose code is available online. Accuracies of the approximate algorithm, structural EM, and the semi-supervised Laplacian SVM are displayed in Table 4. Results suggest that the new algorithm is a very promising approach for SSL. Table 4: MNAR Semi-supervised learning (SSL) results of the Smoking Cessation study data. All test records are Smoking records. The first column refers to the number of missing values in the test set. Accuracy expresses cross-validated accuracy of the test set. # missing values 25 Algorithm Approx. Learning Structural EM Laplacian SVM 50 Approx. Learning Structural EM Laplacian SVM 75 Approx. Learning Structural EM Laplacian SVM 3.5. Car Evaluation Dataset The Car Evaluation dataset (Blake and Merz, 1998; Lichman, 2013) contains 1728 instances and 7 variables consisting of 6 attributes and a class. The 6 attributes refer to the following: buying, maintenance, doors, persons, luggage boots and safety. The class variable refers to the car acceptability and can have exactly one of the following values: unacceptable, acceptable, good, very good. All variables are categorical with 3 or 4 states. The data were derived from a hierarchical decision model originally developed by Bohanec and 16 Avg. Accuracy 90% 15% 76% 88% 10% 73.5% 88% 8% 76%







Rajkovic (1988). Similar to the LUCAS experiment, a MNAR classification task is performed by involving missing values belonging all to one category of the class variable at a time (this is repeated for each label). Due to the class label unbalance (unacceptable: 1210 instances, acceptable: 384, good: 69, v-good: 65), we performed 10 experiments testing only the unacceptable and acceptable labels in five each, where there are 100 randomly chosen instances with a missing label (test set) in each experiment. The proposed algorithm is compared to structural EM and to an SVM classifier. Classification results are displayed in Table 5. Again, the new algorithm is significantly better than the others. Table 5: Accuracy of classification for experiments performed on the Car Evaluation dataset. SVM with an rbf kernel is reported since it leads to best accuracy compared to other 5 experimented kernels. Algorithm Approximate Learning Structural EM SVM (rbf) Avg. Accuracy 87.5% 69.38% 85.96%







4. Conclusions In this paper we discuss the Bayesian network structure learning problem with missing data. We present an approach which performs well even when data are not missing at random. We define an optimization task to tackle the problem and propose a new exact algorithm for it which translates the task into a structure learning problem without missing data. Inspired by the exact procedure, we develop an approximate algorithm which employs structure optimization as a subcall. Experiments show the advantages of such approach. The proposed approximate method can scale to domains with hundreds or even thousands of variables. We intend to investigate such avenue in future work.







17







Table 3: MNAR imputation accuracy for the BN Approximate Learning algorithm and Structural EM on the SPECT dataset with various proportions of missing values, and for both data values. Missing values 3% (overall) Algorithm New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM New approx. Structural EM Average imputation accuracy 81.75% 60% 75.22% 49.27% 81.94% 62.04% 95.65% 56.52% 80.43% 39.13% 92.75% 60.87% 67.83% 63.48% 70% 59.4% 71.13% 63.2%







5% (overall)







10% (overall)







3% (missing value = 0)







5% (missing value = 0)







10% (missing value = 0)







3% (missing value = 1)







5% (missing value = 1)







10% (missing value = 1)







18







References Almeida, E., Ferreira, P., Vinhoza, T., Dutra, I., Wu, Y., Burnside, E., 2014. Expertbayes: Automatically refining manually built Bayesian networks. Machine Learning and Applications (ICMLA) 13, 362-366. Bartlett, M., Cussens, J., 2013. Advances in Bayesian network learning using integer programming. Conference on Uncertainty in artificial intelligence (UAI) 29, 182-191. Blake, C., Merz, C., 1998. UCI machine learning repository of machine learning databases. Bohanec, M., Rajkovic, V., 1988. Knowledge acquisition and explanation for multi-attribute decision making. Intl. Workshop on Expert Systems and their Applications 8, 59-78. Borchani, H., Amor, N. B., Mellouli, K., 2006. Learning Bayesian network equivalence classes from incomplete data. Lecture Notes in Comp. Science, 291-295. Buntine, W., 1991. Theory refinement on Bayesian networks. Conference on Uncertainty in artificial intelligence (UAI) 7, 52-60. Chickering, D., 1996. Learning Bayesian networks is NP-complete. Learning from Data, 121-130. Cooper, G., Herskovits, E., 1992. A Bayesian method for the induction of probabilistic networks from data. Machine Learning 9, 309-347. de Campos, C., Ji, Q., 2011. Efficient structure learning of Bayesian networks using constraints. Journal of Machine Learning Research (JMLR) 12, 663- 689. D'Orsi, C., Bassett, L., Berg, W., Feig, S., Jackson, V., Kopans, D., 2003. BI-RADS: Mammography. American College of Radiology 4. Fogelman-Soulie, F., 2008. Mining massive data sets for security: Advances in data mining, search, social networks and text mining, and their applications to security. Vol. 19. IOS Press.







19







Friedman, N., 1998. The Bayesian structural em algorithm. Conference on Uncertainty in artificial intelligence (UAI) 14, 129-138. Friedman, N., Geiger, D., Goldszmidt, M., 1997. Bayesian network classifiers. Machine Learning 29, 131-163. Gruder, L., Mermelstein, J., Kirkendol, S., D., D. H., Wong, C., Schreckengost, J., Warnecke, R., Burzette, R., Miller, T., 1993. Effects of social support and relapse prevention training as adjuncts to a televised smoking cessation intervention. J Consult Clin Psychol 61, 113-120. Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I., 2009. The WEKA data mining software: An update. SIGKDD Explor. Newsl. 11, 10-18. Heckerman, D., Geiger, D., Chickering, D., 1995. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning 20, 197-243. Hedeker, D., Mermelstein, J., Demirtas, H., 2007. Analysis of binary outcomes with missing data: missing = smoking, last observation carried forward. Addiction 102, 1564-1573. Koivisto, M., 2006. Parent assignment is hard for the mdl, aic, and nml costs. conference on Learning Theory (COLT) 19, 289-303. Lauritzen, S., Speigelhalter, D., 1988. Local computations with probabilities on graphical structures and their application to expert systems. Royal statistical Society B 50, 157-224. Leray, P., Francois, O., 2005. Bayesian network structural learning and incomplete data. Intl. and Interdisc. Conf. on Adaptive Knowledge Repr. and Reasoning (AKRR), 33-40. Lichman, M., 2013. UCI machine learning repository. URL http://archive.ics.uci.edu/ml Little, R., Rubin, D., 1987. Statistical analysis with missing data. John Wiley & Sons.







20







Meila, M., Jordan, M., 1998. Estimating dependency structure as a hidden variable. Advances in Neural Information Processing Systems (NIPS), 584- 590. Melacci, S., Belkin, M., 2011. Laplacian support vector machines trained in the primal. Journal of Machine Learning Research (JMLR) 12, 1149-1184. Mohan, K., Pearl, J., Tian, J., 2013. Graphical models for inference with missing data. Advances in Neural Information Processing Systems (NIPS), 1277-1285. Ramoni, M., Sebastiani, P., 1997. Learning Bayesian networks from incomplete databases. Conference on Uncertainty in artificial intelligence (UAI) 13, 401- 408. Rancoita, P., Zaffalon, M., Zucca, E., Bertoni, F., de Campos, C., 2016. Bayesian network data imputation with application to survival tree analysis. Computational Statistics & Data Analysis 93, 373-387. Riggelsen, C., 2006. Learning Bayesian networks from incomplete data: An efficient method for generating approximate predictive distributions. SDM, 130-140. Riggelsen, C., Feelders, A., 2005. Learning Bayesian network models from incomplete data using importance sampling. AISTATS, 301-308. Sarabando, A., 2011. Um estudo do comportamento de redes Bayesianas no prognstico da sobrevivencia no cancro da prostata. M.Sc. thesis, Universidade do Porto 29, 131-163. Scanagatta, M., de Campos, C., Corani, G., Zaffalon, M., 2015. Learning Bayesian networks with thousands of variables. Advances in Neural Information Processing Systems (NIPS), 1855-1863. Van den Broeck, G., Mohan, K., Choi, A., Pearl, J., 2014. Efficient algorithms for Bayesian network parameter learning from incomplete data. In: Causal Modeling and Machine Learning Workshop at ICML-2014. pp. R-441.







21







The KOJAK Group Finder: Connecting the Dots via Integrated Knowledge-Based and Statistical Reasoning



Jafar Adibi, Hans Chalupsky, Eric Melz and Andre Valente



USC Information Sciences Institute 4676 Admiralty Way, Marina del Rey, CA 90292 Email: {adibi, hans, melz, valente}@isi.edu







Abstract



Link discovery is a new challenge in data mining whose primary concerns are to identify strong links and discover hidden relationships among entities and organizations based on low-level, incomplete and noisy evidence data. To address this challenge, we are developing a hybrid link discovery system called KOJAK that combines state-of-theart knowledge representation and reasoning (KR&R) technology with statistical clustering and analysis techniques from the area of data mining. In this paper we report on the architecture and technology of its first fully completed module called the KOJAK Group Finder. The Group Finder is capable of finding hidden groups and group members in large evidence databases. Our group finding approach addresses a variety of important LD challenges, such as being able to exploit heterogeneous and structurally rich evidence, handling the connectivity curse, noise and corruption as well as the capability to scale up to very large, realistic data sets. The first version of the KOJAK Group Finder has been successfully tested and evaluated on a variety of synthetic datasets.







Introduction



The development of information technology that could aid law enforcement and intelligence organizations in their efforts to detect and prevent illegal and fraudulent activities as well as threats to national security has become an important topic for research and development. Since the amount of relevant information, tips, data and reports increases daily at a rapid pace, analyzing such data manually to its full potential has become impossible. Hence, new automated techniques are needed to take full advantage of all available information. One of the central steps in supporting such analysis is link discovery (LD), which is a relatively new form of data mining. Link discovery can be viewed as the process of identifying complex, multi-relational patterns that indicate potentially illegal or threat activities in large amounts of data. More broadly, it also includes looking for not directly explainable connections that may indicate previously unknown but significant relationships such as new groups or capabilities (Senator, 2002) .



Copyright (c) 2004, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.







Link discovery presents a variety of difficult challenges. First, data ranges from highly unstructured sources such as reports, news stories, etc. to highly structured sources such as traditional relational databases. Unstructured sources need to be preprocessed first either manually or via natural language extraction methods before they can be used by LD methods. Second, data is complex, multi-relational and contains many mostly irrelevant connections (connectivity curse). Third, data is noisy, incomplete, corrupted and full of unaligned aliases. Finally, relevant data sources are heterogeneous, distributed and can be very high volume. To address these challenges we are developing a hybrid link discovery system called KOJAK that combines stateof-the-art knowledge representation and reasoning (KR&R) technology with statistical techniques from the area of data mining. Using KR&R technology allows us to represent extracted evidence at very high fidelity, build and utilize high quality and reusable ontologies and domain theories, have a natural means to represent abstraction and meta-knowledge such as the interestingness of certain relations, and leverage sophisticated reasoning algorithms to uncover implicit semantic connections. Using data or knowledge mining technology allows us to uncover hidden relationships not explicitly represented in the data or findable by logical inference, for example, entities that seem to be strongly related based on statistical properties of their communication patterns. The full KOJAK system contains a variety of experimental LD components such as an abductive, logicbased Pattern Finder to identify complex patterns of interest in the evidence and a Connection Finder to identify interesting and unusual entities and connections (Lin & Chalupsky 2003). In this paper we only report on the architecture and technology of its first fully completed module called the KOJAK Group Finder (GF). The Group Finder is capable of finding hidden groups and group members in large evidence databases. Our group finding approach addresses a variety of important LD challenges, such as being able to exploit heterogeneous and structurally rich evidence, handling the connectivity curse, noise and corruption as well as the capability to scale up to very large, realistic data sets. The first version of the KOJAK Group Finder has been successfully tested and evaluated on a variety of synthetic datasets.







The Group Detection Problem



A major problem in the area of link discovery is the discovery of hidden organizational structure such as groups and their members. There are of course many organizations and groups visible and detectable in real world data, but we are usually only interested in detecting certain types of groups such as organized crime rings, terrorist groups, etc. Group detection can be further broken down into (1) discovering hidden members of known groups (or group extension) and (2) identifying completely unknown groups. A known group (e.g., a terrorist group such as the RAF) is identified by a given name and a set of known members. The problem then is to discover potential additional hidden members of such a group given evidence of communication events, business transactions, familial relationships, etc. For unknown groups neither name nor known members are available. All we know are certain suspicious individuals ("bad guys") in the database and their connection to certain events of interest. The main task here is to identify additional suspicious individuals and cluster them appropriately to hypothesize new real-world groups, e.g., a new money laundering ring. While our techniques address both questions, we believe group extension to be the more common and important problem. Another important problem characteristic that influenced our solution approach concerns the data. Evidence available to law enforcement organizations is split into primary and secondary sources. Primary evidence is lower volume, high reliability, usually "owned" by the organization and can be searched and processed in arbitrary ways. Secondary evidence is usually not owned by the organization (e.g., might come from news articles or the Web), is higher volume, might only be searchable in restricted ways and might be associated with a cost (e.g., access might require a warrant). Our group detection approach needs to take these different characteristics into account to keep cost at a minimum and properly handle access restrictions to secondary data sources.







extended group. Third, the mutual information model is used to rank these likely members by how strongly connected they are to the seed members. Fourth, the ranked extended group is pruned using a threshold to produce the final output.



Patterns, Constraints Logic-based Group Seed Generator Named Group Seeds Primary Evidence Secondary Evidence







Extend Group







Extended Group







(MI-)Rank Group Threshold Group



Unnamed Group Hypotheses







MI-Ranked Extended Group







Mutual Information Module







Cluster Groups



Named Group Hypotheses







Figure 1: KOJAK Group Finder Architecture. The processing for known and unknown groups is somewhat different at the beginning and end of the process. First, the seed generation for unknown groups is different, since there is less information available. Second, the generation of unknown groups involves an extra step because the extended groups need to be clustered to eliminate duplicates before the thresholding step. The logic-based seed generation module is based upon the PowerLoomTM knowledge representation & reasoning system (PowerLoom, 2003). The mutual information module was implemented in the Matlab programming language. The modules are integrated by combining the C++ translation of PowerLoom and the C translation of the Matlab modules into a single program. Evidence databases are stored in MySQL and accessed from both Matlab and PowerLoom via ODBC. The primary and secondary evidence databases uses a very general evidence schema developed as part of DARPA's Evidence Extraction and Link Discovery (EELD) program (Senator, 2002) which should make it easy to transition to different domains.







The KOJAK Group Finder



The KOJAK Group Finder is a hybrid logicbased/statistical LD component designed to solve group detection problems. It can answer the following questions: * How likely is P a member of group G? * How likely are P and Q members of the same group? * How strongly connected are P and Q? Figure 1 shows the general architecture. The system takes primary and secondary evidence (stored in relational databases) as input and produces group hypotheses (i.e., lists of group members) as output. The system works in four phases. First, a logic-based group seed generator analyzes the primary evidence and outputs a set of seed groups using deductive and abductive reasoning over a set of domain patterns and constraints. Second, an information-theoretic mutual information model finds likely new candidates for each group, producing an







The Need for a Hybrid Approach



Link discovery is a very challenging problem. It requires the successful exploitation of complex evidence that comes in many different types, is fragmented, incomplete, uncertain and very large-scale. LD requires reasoning with abstractions, e.g., that brother-of and husband-of are both subtypes of a family-relation, temporal and spatial reasoning, e.g., that cities are subregions of counties which are subregions of states, etc., common-sense type inferences, e.g., that if two people bought tickets for the same event, they probably were at one point in close spatial proximity in the same city, and constrained search, e.g., one might want to look more closely at people who joined a company around the same time a suspect joined. The knowledge







and ontologies needed for these types of inferences are very naturally modeled in a symbolic, logic-based approach as done in the logic-based seed generator of the KOJAK Group Finder. However, LD also needs detection and reasoning with statistical phenomena such as communication patterns, behavior similarity, etc., which requires cumulative analysis of evidence that cannot be done in logic but is most effectively done in specialized models such as our mutual information component. Such models, on the other hand, are not well-suited for the representation of complex domains and usually assume some data normalization and simplification. Given these characteristics of the problem, using a hybrid approach that combines the strengths of multiple paradigms is a natural choice. How these two approaches work together for the KOJAK Group Finder is described below.







memberAgentsByParticipation formalizes this type of reasoning (memberAgents relates a group and its members; deliberateActors relates groups or people to an event): (DEFRELATION memberAgentsByParticipation ((?g Group) (?p Person)) :<= (AND (Group ?g) (Person ?p) (FAIL (memberAgents ?g ?p)) (EXISTS (?c) (AND (ExploitationCase ?c) (deliberateActors ?c ?g) (deliberateActors ?c ?p)))))







Logic-Based Seed Generation



The first phase of the KOJAK group detection process is the generation of seed groups. Each seed group is intended to be a good hypothesis for one of the actual groups in the evidence data, even though the number of seed members known or inferable for it might be significantly less than its actual members. The reasons for using this logic-based, seeded approach are threefold. First, the information in primary and secondary evidence is incomplete and fragmented. By "connecting the dots" via logical inference we can extract information that is not explicitly stated and our statistical methods would not be able to infer. Second, because the MI model needs to analyze access-restricted secondary data, it needs good initial focus such as seed groups of "bad guys" in order to query the data selectively. The seeded approach therefore dramatically reduces data access cost as well as MIprocessing time. Third, logical reasoning can apply constraints to the information available as well as rule out or merge certain group hypotheses. To generate seed groups we use the PowerLoom KR&R system to scrub every piece of available membership information from primary evidence (which is smaller volume, less noisy and can be searched arbitrarily). Given the size of primary evidence data we are working with (O(10,000) individuals and O(100,000) assertions) we can simply load it directly from the EDB into PowerLoom using its database interface and a set of import axioms. The process of finding seeds is different for known and unknown groups. For known groups, we start with a query to retrieve existing groups and their explicitly declared members. We then employ a number of logic rules to infer additional group members by connecting data that is available but disconnected. For example, in the synthetic datasets available to us members of threat groups participate in exploitation cases (meant to model threat events such as a terrorist attack). To find additional members of a group we can look for exploitations performed by a group that have additional participants not explicitly known to be members of the group. The PowerLoom definition below for the relation







For unknown groups, we use rules to look for patterns on events to find seeds. The basic idea is to find teams participating in threat events that no (known) group is known to be responsible for. Since people who participate in a threat event are part of a threat group, teams of people who are found to jointly participate in a threat event that cannot be attributed to a known group can be used as seeds for unknown groups. Note, however, that such teams may be subsets of one of the known groups or that two or more of the teams may be part of the same unknown group. For that reason, it is vital to use merging techniques later to combine teams (or their extended groups) if appropriate. The logic module can also check constraints to help in the merging of hypotheses. For example, a strong hint that two groups may be the same is that their members participated in the same exploitation events. The rule below finds groups who participated in a given exploitation event indicating a potential duplicate group hypothesis if more than one group is found:



(DEFRELATION groupHasMemberWhoParticipatedInEvent ((?g Group) (?e VulnerabilityExploitationCase)) :<= (AND (Group ?g) (VulnerabilityExploitationCase ?e) (EXISTS ?p (AND (Person ?p) (OR (memberAgents ?g ?p) (memberAgentsByParticipation ?g ?p)) (deliberateActors ?e ?p))))) The use of memberAgentsByParticipation shows that these rules







not only encode complex queries but also interconnect to build a real domain model. There are about 50 complex rules of this type that are specific to group discovery. Even though the synthetic dataset used in our experiments was designed to be relatively poor in link types and attributes, the data is still quite complex. It contains 72 entity types (22 of which are actually instantiated) and 107 relations and attributes (25 of which are actually instantiated in the data). These entity and relation types are further organized by an ontology (developed by Cycorp) whose upward closure from the entity and relation types in the data contains a hierarchy of about 620 concepts (or classes) and 160 relations. Adding this to the O(100,000) assertions representing the evidence we have a fairly large and complex knowledge base to work with. While the examples given above are specific to the synthetic group discovery domain, the approach is general and applicable to other areas. Evidence data will always be fragmented. Such fragmentation is usually easy to handle by a human analyst, but it can be a big obstacle for an automated system. Using a logic-based model of the







domain is a very powerful approach to overcome this problem and connect evidence fragments in useful ways.







E: 2 P: 2 M: 1







E: 2 P: 1 M: 0 P1 E: 1 P: 1 M: 2 P3 MI E: 1 P: 3 M: 4 E: 0 P: 1 M: 1 P4 E: 0 P: 1 M: 1 P2







Time



E: 3 P: 2 M: 2







Action P1 Email P2 P3 Phone Call P4 P2 Meeting P3 P1 Email P2 P2 Email P3 P1 Phone Call P3 P1 Phone Call P2 P2 Meeting P3 P3 Meeting P4 P1 Meeting P3 P2 Phone Call P3







P1 E   E  P P   M 







P2 E  M E E  P M   P







P3  P M  E P  M M M P







P4  P       M  







1 2 3 4 5 6 7 8 9 10 11







Finding Strong Connections Via a Mutual Information Model



After exploiting the various explicit and implicit evidence fragments given in the EDB to generate a seed group, we try to identify additional members by looking for people that are strongly connected with one or more of the seed members. To find two strongly connected entities, we need to aggregate the many other known links between them and statistically contrast those with connections to other entities or the general population. This cannot be done via a logic-based approach and instead is achieved via an information-theoretic mutual information model. The mutual information model can identify entities strongly connected to a given entity or a set of entities and provide a ranked list based on connection strength. To do this it exploits data such as individuals sharing the same property (e.g., having the same address) or being involved in the same action (e.g., sending email to each other). Since such information is usually recorded by an observer we refer to it as evidence. Time is often also an important element of evidence and is also recorded in the EDB. Without loss of generality we only focus on individuals' actions in this paper, but not on their properties. We transform the problem space into a graph in which each node represents an entity (such as a person) and each link between two entities represents the set of actions (e.g., emails, phone calls etc.) they are involved in. For each node we represent the set of its actions with a random variable, which can take values form the set of all possible actions. Figure 2 illustrates this concept. There are four people and three possible actions: sending Email, making a Phone Call and participating in a Meeting. When a person is not involved in any of the above-mentioned actions we indicate that with the empty action . For example, we can represent P1`s actions with the random variable X1 which takes values from the set {E, P, M, } at any given time. Most individuals in the LD evidence space are connected to each other either directly or indirectly. For example, two people may eat at the same restaurant, drink coffee at the same cafe and take the same train to work every day without any strongly meaningful connection. On the other hand, three individuals may be strongly connected if they engage in atypical phone call patterns. To address this problem we measure the mutual information (MI) between the random variables representing individuals' activities. MI is a measure of the dependence between two variables. If the two variables are independent, the MI between them is zero. If the two are strongly dependent, e.g., one is a function of another; the MI between them is large. We therefore believe that two individuals' mutual information is a good indicator whether they are in fact strongly connected to each other or not compared to the rest of the population. There are other interpretations of MI, for example, as the stored information in one variable about another



P3 P3 P2 P3 P2 P1 P1 P4







E: 0 P: 1 M: 1







0.74 0.61 0.49 0.22







Figure 2: MI Example. P1, P2, P3 and P4 represent people. E, P and M stand for Email, Phone Call and Meeting respectively. The table on the right shows activities among individuals and the table on the left shows the MI among them. variable or the degree of predictability of the second variable by knowing the first. Clearly, all these interpretations are related to the same notion of dependence and correlation. The correlation function is another frequently used quantity to measure dependence. The correlation function is usually measured as a function of distance or time delay between two quantities. It has been shown that MI measures the more general (nonlinear) dependence while the correlation function measures linear dependence (Li, 1990). Therefore, MI is the more accurate choice to measure dependence. One of the important characteristics of MI is that it does not need actual variables values to be computed, instead it only depends on the distribution of the two variables. In classical information theory (Shannon, 1948) MI between two random variables X and Y is defined as:  



MI ( X ; Y ) = P( y | x)  P ( x) P( y | x)   P ( x ) P ( y | x ). log   



x y







where P( x) is the Prob ( X = x) , P( y ) is the Prob (Y = y ) and P ( y | x) stands for Prob (Y = y | X = x) . In addition, MI(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y), where the conditional entropy H(X|Y) measures the average uncertainty that remains about X when Y is known (see (Adibi et al. 2004) for more details about the MI model).















x















Group Expansion via Mutual Information



Given that we can use the mutual information calculation to find strongly connected individuals, we can exploit this capability to expand the seed groups provided in phase 1 by the logic-based KR&R module. This expansion is done in the following steps: (1) For each seed member in a seed group we retrieve all activities it participates in from primary and secondary data and add any new individuals found to the group. This step therefore expands the seed group graph by one level. Note, that we obey query restrictions for secondary data and only ask one focused query per seed member. (2) Now we view the expanded group as the universe and compute MI for each connected pair in the graph. (3) Next we look for individuals that either have high MI score with one of the seed members or with all seed







members when viewed as a single "super individual". Members whose score is below a certain (fairly lax) userdefined threshold are dropped from the list. (4) In this step the MI engine repeats the whole procedure by expanding the expanded group from the previous step one more level and recalculates MI for the new graph. For known groups we stop here and pass the result to the final thresholding step. (5) For unknown groups we usually have much smaller seed sets and therefore repeat the previous step one more time to achieve appropriately-sized group hypotheses. The group expansion procedure is performed for each seed group generated by the KR&R module and generates an MI-ranked list of possible additional members for each seed group. This list is initially kept fairly inclusive and needs to undergo proper thresholding before it can be reported to a user or passed on to another LD component.







Signal Phone Call Corruption Report of Phone Call







Negative Noise







Report of Email







Corruption No occurrence of Phone Call Sender



Ground Truth







Report of Meeting







Signal Receiver



Evidence Database







No Report of Phone Call in the database







Figure 3: Noise model for a given "Phone Call" as the "sender" and the evidence database (EDB) as the "receiver". While in a noiseless environment information is recorded in the EDB without error, in a noisy environment we have a noisy channel, which may alter every piece of evidence transmitted through it with some small probability p(noise). For instance, negative noise occurs if there is a phone call in the ground truth but no record of it in the EDB. Corruption occurs, for example, if there is no phone call in the ground truth but a record indicating one in the EDB. The MI framework is a natural fit for such model. Figure 3 illustrates a noisy channel for a given phone call.







Threshold Selection and Thresholding



The result of the process described above is a list of extended groups where members are ranked by their mutual information scores. In order to produce and report a definite result on which members we believe are actually part of the group, we need to cut the ordered list at some threshold. The problem is how to set the threshold so that we get "good" (or even "optimal") recall and precision for a particular application scenario. We used an empirical method that selects a threshold for a dataset based on an empirical analysis of a number of groups in different types of datasets. This method is discussed further in the section describing the experimental results. The good news is that (1) our group detection process generates a very selective ranking (i.e., we reach high recall fairly early) and (2) in real-world situations a good ranking is often more important than picking the best possible cutoff, since human analysts might be willing to accept a certain number of false positives in order to maximize the number of true positives they are after.







Complexity and Dataset Scale



Real-world evidence data sets can be very large and we have to make sure that our techniques scale appropriately. The largest synthetic datasets we have analyzed so far contained O(100,000) events and O(10,000) individuals. Running the KOJAK GF on such a dataset takes roughly 5 minutes for the logic-based seed generation and about 1020 minutes to run the MI engine on a 2Ghz Pentium-IV desktop with 1Gb of RAM. Runtime for the MI engine varies depending on the overall connectivity of the data. While this is perfectly acceptable at the moment, we will eventually need to handle datasets that are at least two orders of magnitude larger, so let us look a bit closer at the architecture and algorithm complexity involved. The complexity of the MI model is relatively low. The MI engine expands only a limited number of nodes in the problem space starting from the seed members of a group. How many individuals are considered depends on how deeply we grow the link graph to build an extended group. So far, one to two levels have been sufficient. Computing MI between two individuals is O(N*M) where N is the average number of people connected to a given individual and M is the average number of links a person is involved in. Unless N and M grow significantly with larger datasets, the overall complexity is primarily dependent on the number of threat groups we are looking for. To be able to handle such large datasets in the logic-based seed generation phase, we built a new database access layer into PowerLoom that allows us to easily and transparently map logic relations onto arbitrary database tables and views. By using these facilities we can keep smaller data portions such as the primary data in main







Handling Noise Via a Noisy Channel model



So far we assumed that we are capable to observe all evidence accurately. However, such accuracy occurs rarely in real world databases. We therefore consider the following kinds of noise in the formulation of our model: Observability (Negative Noise): This factor describes how much of the real data was observable. Not all relevant events that occur in the world will be observed or reported and might therefore not be known to LD components. Corruption: This type of noise varies from typos to misspelled names all the way to intentional misinformation. The negative noise phenomenon has been discussed extensively in the communication literature. We adopt the view of a classical noisy channel scenario where a sender transmits a piece of information to a receiver. The transmission goes through a channel with certain noise properties. In our domain we view the ground truth (GT)







memory for fast access and processing, while keeping potentially very large secondary data sets in an RDBMS from where we page in relevant portions on demand. Particular attention was paid to be able to offload large join processing to the RDBMS wherever possible to avoid doing it inefficiently tuple-by-tuple in PowerLoom. This gives us an architecture where we use a traditional RDBMS for storage and access to very large datasets but enrich it with a deductive layer that allows us to formulate more complex queries where necessary. The complexity of the resulting system depends heavily on the nature of the queries and domain rules used which so far has proven to be manageable. For example, the current system uses an ontology with about 800 concept and relation definitions and about 50 complex, non-taxonomic rules that link evidence fragments without any performance problems.







Number of entities Number of Links Number of Distinct Threat Pattern Lowest Signal to clutter ratio Lowest Signal to Noise Ratio Observability Corruption of Evidence







10,000 100,000 20 0.3(-5 db) .008(-21 db) 50%-100% 0-25%







Table 1: Synthetic Data Characteristics (1) Individual and group information. The existence of most individuals and some of the groups is available directly in the evidence. The groups available in the evidence are known or named groups discussed earlier. (2) Activities from individuals. Individuals participate in activities related to resources, capabilities and events. Much like in the real world, information about those activities is not available directly, but rather indirectly as transactions (e.g., phone calls or email messages). Synthetic Data Characteristics One of the key advantages of using a simulated world is that we are able to test our system against a wide range of datasets. In other words, we are able to create datasets with (almost) arbitrary characteristics, and therefore better understand the potential and limitations of our techniques. Some of the features used in defining the datasets are in Table 1. The values displayed are typical for the datasets we used in our evaluation; each dataset employs different values for each of these features. Of particular interest are observability (how much of the artificial world information is available as evidence), corruption (how much of the evidence is changed before being reported) and clutter (how much irrelevant information that is similar to the information being sought is added to the evidence). Evaluation Metrics The quality of groups we find can be measured with traditional precision and recall metrics defined as follows: Given a proposed group G with g members which matches an answer group A with a members, and given that of the g proposed members only c are correct, precision P=c/g and recall R=c/a. Another metric that helps us analyze precision and recall in aggregate is the F-measure: (b 2 + 1) PR F =- 2 b P+R The F-measure both requires and allows us to specify the desired trade-off between precision and recall through the b variable. A value of b=1 indicates that precision and recall are equally important; b = 2 means that recall is twice as important as precision, etc. That is, using the Fmeasure allows users of our module to specify their own desired trade-offs in terms of b.







Experimental Set-Up



We have applied the KOJAK Group Finder to a wide variety of synthetic data. Access to real world databases has been a main concern in AI, machine learning and data mining communities in the past. The LD community is not an exception in this matter. In particular, since the LD goal is to relate people, place and entities, it triggers privacy concerns. The balance between privacy concerns and the need to explore large volumes of data for LD is a difficult problem. These issues motivate employing synthetic data for performance evaluation of LD techniques. Synthetic Data For the purpose of evaluating and validating our techniques, we tested them on synthetic datasets developed by Information Extraction & Transport, Inc. within the EELD Program (Silk 2003, Schrag 2003). These synthetic datasets were created by running a simulation of an artificial world. The main focus in designing the world was to produce datasets with large amounts of relationships between agents as opposed to complex domains with a large number of entity properties. From the point of view of group detection, the artificial world consists of individuals that belong to groups. Groups can be threat groups (that cause threat events) or non-threat-groups. Targets can be exploited (in threat and non-threat ways) using specific combinations of resources and capabilities; each such combination is called a mode. Individuals may have any number of capabilities or resources, belong to any number of groups, and participate in any number of exploitations at the same time. Individuals are threat individuals or non-threat individuals. Every threat individual belongs to at least one threat group. Non-threat individuals belong only to nonthreat groups. Threat groups have only threat individuals as members. Threat individuals can belong to non-threat groups as well. A group will have at least one member qualified for any capability required by any of its modes. Non-threat groups carry out only non-threat modes. The evidence available in the dataset for our analysis consists of two main types of information:







1.00 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 1 35 69 103 137 171 205 239 273 307 341 375 409 443 477 511 545 579 613 647 Number of members 681 B=0.5 B=1.0 B=1.5 B=2.0







Logic Module Data set Plain High clutter Low observability Both Number Avg. Avg. of Precision Recall Groups 14 11 16 19 1 1 1 1 0.53 0.53 0.52 0.50







KOJAK Group Finder Avg. Avg. Precision Avg. Recall F-Measure Precision Variance Recall Variance (b=1.5) 0.81 0.59 0.70 0.88 0.005 0.010 0.004 0.005 0.87 0.86 0.72 0.66 0.010 0.014 0.026 0.011 0.85 0.74 0.71 0.75







Table 2: Scores for applying the KOJAK Group Finder to datasets of increasing complexity (known groups only). Results We have applied KOJAK to 26 datasets of varying complexity and characteristics. Table 2 shows some sample metrics for four datasets. Since there are many groups in each dataset we provide mean and variance values for precision and recall among all groups in a dataset. The average F-measure for known groups varies between 0.71 and 0.85. Note that the differences in the properties of the datasets cause the best F-measure to be obtained with different recall and precision values. This shows that "harder" datasets, where precision drops more steeply require lower thresholds that yield lower recalls and higher precision values. A more detailed analysis with ROC curves is presented in (Adibi et al. 2004). Table 2 also compares the KOJAK results against a baseline of using only the logic module. The results show that the logic module is very accurate (precision = 1), meaning all members found are provably correct. However, since the evidence is incomplete the logic module achieves a maximum recall of about 50%. We also evaluated our threshold prediction model. We found that the average F-measure for these datasets compares to the optimum F-measure obtained by using the best possible threshold for each group would result only in a difference of around 6%. In other words, the threshold model only "misses" 6% of whatever was available in the extended groups.







Figure 4: F-measure curves for different thresholds for a typical group.







Experimental Results



Overall empirical properties Figure 4 shows a typical set of F-measure curves for different thresholds. An important property is that our Fmeasure curves have maximums (and thus optimums). Notice also that F-measure curves for higher values of b have wider "peaks", which means they are more "forgiving" in threshold selection (a given variation of threshold provokes a smaller variation in F-measure.) Threshold Analysis Focusing on the F-measure, we defined an empirical model that allowed us to predict good threshold values for a given type of dataset. Datasets vary in many dimensions, in particular on their levels of observability, corruption, and clutter. Our goal was to define a model parametric on these dataset dimensions. One key initial step is to define the base for the model. Possible bases include the average size of the groups we are looking for (if sufficiently known), the size of extended group and the size of the seed group. Our empirical analysis indicated that the best alternative is to use the size of extended group as a basis for defining the threshold. We found that the ratio between the real size of the group we would be looking for and the size of the extended group we created as a hypothesis varies little and is usually around 11%-14%. Another advantage is that this measure is organic to the mutual information model, that is, no additional information is needed. The empirical model consists of defining one specific threshold (as a percentage of the extended group size) for each type of dataset. We used thirteen types of datasets that employed combinations of different values for the parameters in Table 1. We then analyzed the F-measure curves to find optimums for each b-value (i.e., trade-off between precision and recall) and type of dataset. For example, for a b of 1, we predicted a threshold of 8% for a baseline dataset, 6% for a dataset with more clutter, 9% for a dataset with low observability and 3% for a dataset with both additional clutter and low observability. These thresholds are then used to predict the best threshold for a new dataset of a particular type.







Related Work



Link discovery (LD) can be distinguished from other techniques that attempt to infer the structure of data, such as classification and outlier detection. Classification and clustering approaches such as that of Getoor et al. (2001) try to maximize individual similarity within classes and minimize individual similarity between classes. In contrast, LD focuses on detecting groups with strongly connected entities that are not necessarily similar. Outlier detection methods attempt to find abnormal individuals. LD, on the other hand, identifies important individuals based on networks of relationships. Additionally, outlier techniques require large amounts of data including normal and abnormal cases, and positive and negative noise. This is inappropriate for LD applications that need to detect threats with few or no available prior cases. Mutual information has also been used in other domains such as finding functional genomic clusters in RNA expression data and measuring the agreement of object models for image processing (Butte, 2000).







Our work can be distinguished from other group detection approaches such as Gibson, (1998) and Ng, (2001) by three major characteristics. First, GF is unique since it is based on a hybrid model of semantic KR&R and statistical inference. There are very few approaches that use semantic information. Second, in our approach each type of relation (link) is valuable and treated differently, in contrast to work in fields such as Web analysis and social networks. Third, with our technique, multiple paths between individuals or groups (direct or indirect) imply a strong connection which is different from techniques which focus on finding chains of entities. The work closest to our own is that of Jeremy Kubica et al. (Kubica, 2002; Kubica, 2003) that uses a probabilistic model of link generation based on group membership. The parameters of the model are learned via a maximum likelihood search that finds a Gannt Chart that best explains the observed evolution of group membership. The approach has a strong probabilistic foundation that makes it robust in the face of very low signal-to- noise ratios. Another recent approach to the LD problem is the use of probabilistic models (Cohn, 2001; Friedman, 1999; Getoor, 2001). Kubica et al. (2001) present a model of link generation where links are generated from a single underlying group and then have noise added. These models differ significantly from ours since we do not assume a generative model of group formation, but rather probabilistically determine each entity's membership.







conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFRL or the U.S. Government.







References



Adibi, J., Valente, A., Chalupsky, H. & Melz, E. (2004). Group detection via a mutual information model. Submitted to KDD 2004. Butte, A. & Kohane, I. (2000). Mutual information relevance networks: Functional genomic clustering using pairwise entropy measurements. Pacific Symposium on Biocomputing. Honolulu, Hawaii. Cohn, D. & Hofmann, T. (2001). The missing link: a probabilistic model of document content and hypertext connectivity. Advances in Neural Information Processing Systems 13: 430-436. Friedman, N., Getoor, L., Koller, D. & Pfeffer, A. (1999). Learning probabilistic relational models. IJCAI 1999, San Francisco, Morgan Kaufmann Publishers. Getoor, L., Segal, E., Taskar, B., & Koller, D. (2001). Probabilistic models of text and link structure for hypertext classification. IJCAI 2001 Workshop on Text Learning: Beyond Supervision. Seattle, Washington. Gibson, D., Kleinberg, J. & Raghavan, P. (1998). Inferring Web communities from link topology. In Proceedings of the Ninth ACM Conference on Hypertext and Hypermedia. New York, ACM Press. Kubica, J., Moore, A., Cohn, D. & Schneider, J. (2003). Finding underlying connections: a fast method for link analysis and collaboration queries. International Conference on Machine Learning (ICML). Kubica, J., Moore, A., Schneider, J. & Yang, Y. (2002). Stochastic link and group detection. Eighteenth National Conference on Artificial Intelligence (AAAI). Li, W. (1990). Mutual information functions versus correlation functions. Journal of Statistical Physics 60: 823-837. Lin, S. & Chalupsky, H.. Using unsupervised link discovery methods to find interesting facts and connections in a bibliography dataset. SIGKDD Explorations, 5(2): 173-178, December 2003 Ng, A., Zheng, A. & Jordan, M. (2001). Link analysis, eigenvectors and stability. IJCAI 2001. PowerLoom (2003). www.isi.edu/isd/LOOM/PowerLoom. Schrag, R. et. al. (2003). EELD Y2 LD-PL Performance Evaluation, Information Extraction and Transport, Inc. Senator, T. (2002). Evidence Extraction and Link Discovery, DARPA Tech 2002. Shannon, C. (1948). A Mathematical Theory of Communication. Bell System Tech. Journal 27: 379-423. Silk, B. & Bergert, B. (2003). EELD Evidence Database Description, Information Extraction and Transport, Inc.







Conclusion and Future Work



In this paper we introduced the KOJAK Group Finder (GF) as a hybrid model of logic-based and statistical reasoning. GF is capable of finding potential groups and group members in large evidence data sets. It uses a logicbased model to generate group seeds and a multi-relational mutual information model to compute link strength between individuals and group seeds. Noise and corruption are handled via a noisy channel model. Our GF framework is scalable and robust, and exhibits graceful degradation in the presence of increased data access cost and decreased relational information. The Group Finder is best-suited for problems where some initial information or group structure is available (e.g. finding hidden members of existing groups vs. detecting completely new groups) which is a common case in many real world applications. Group detection is useful for law enforcement, fraud detection, homeland security, business intelligence as well as analysis of social groups such as Web communities. There are several lines of ongoing and future work, such as, determining group leaders by measuring their entropy, use of temporal information for more focused access to relevant information as well as employing sampling and data streaming techniques to deal with very large datasets.



Acknowledgments. This research was sponsored by the Defense Advance Research Projects Agency and Air Force Research Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-01-2-0583. The views and







Sensitivity of Diffusion Dynamics to Network Uncertainty



Abhijin Adiga, Chris Kuhlman, Henning S. Mortveit and Anil Kumar S. Vullikanti



Network Dynamics and Simulation Science Laboratory, Virginia Bioinformatics Institute, Virginia Tech, VA 24061 {abhijin, ckuhlman, hmortvei, akumar}@vbi.vt.edu







Abstract



Simple diffusion processes on networks have been used to model, analyze and predict diverse phenomena such as spread of diseases, information and memes. More often than not, the underlying network data is noisy and sampled. This prompts the following natural question: how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure? In this paper, we consider two popular diffusion models: Independent cascades (IC) model and Linear threshold (LT) model. We study how the expected number of vertices that are influenced/infected, given some initial conditions, are affected by network perturbation. By rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results. (1) For the IC model, we characterize the susceptibility to network perturbation in terms of the critical probability for phase transition of the network. We find the expected number of infections is quite stable, unless the the transmission probability is close to the critical probability. (2) We show that the standard LT model with uniform edge weights is relatively stable under network perturbations. (3) Empirically, the transient behavior, i.e., the time series of the number of infections, in both models appears to be more sensitive to network perturbations. We also study these questions using extensive simulations on diverse real world networks, and find that our theoretical predictions for both models match the empirical observations quite closely.







1







Introduction







A number of diverse phenomena are modeled by simple diffusion processes on graphs, such as the spread of epidemics (Newman 2003), viral marketing (Kempe, Kleinberg, and Tardos 2005; Goldenberg, Libai, and Muller 2001) and memes in online social media (Romero, Meeder, and Kleinberg 2011; Bakshy et al. 2011). It is common to associate with each vertex a state of 0 (denoting "not infected" or "not influenced") or state 1 (denoting "infected" or "influenced") in these models; each neighbor of a node in state 1 switches to state 1 based on a probabilistic rule. We focus on two such models, referred to as the independent cascades (IC) model (which is a special case of the SIR process), and linear threshold (LT) model. In most applications, however, the underlying networks are inherently noisy



Copyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.







and incomplete, since they are often inferred by indirect measurements, for instance: (i) networks based on Twitter data (e.g., (Gonzlez-Bailn et al. 2011; Bakshy et al. 2011; Galuba 2010)) are constructed by limited samples available through public APIs, (ii) biological networks are inferred by experimental correlations, e.g., (Hagmann 2008; Schwab et al. 2010), which might be incomplete, and (iii) the Internet router/AS level graphs are constructed using traceroutes, e.g., (Faloutsos, Faloutsos, and Faloutsos 1999), which are known to give a biased and incomplete structure (see, e.g., (Achlioptas et al. 2009)). This raises a fundamental issue for diffusion processes on networks: How does the uncertainty in the network affect the conclusions drawn from a study of the diffusion dynamics? For instance, how robust is an inference that there will be no large outbreak in the network, in the face of noise/uncertainty in the network? Recent statistical and simulation based studies involving perturbation of the network by "rewiring" pairs of edges (which preserves the degree sequence) show that changes in the network structure significantly alter the dynamics, and the efficacy of intervention mechanisms, even when aggregate structural properties, such as the degree distribution and assortativity are preserved (Eubank 2010; Chen 2010). Surprisingly, there is limited mathematically rigorous work to explain the empirical findings in a systematic manner, despite a large body of research on diffusion models. Our work is motivated by these considerations of sensitivity of the dynamics to noise and the adequacy of sampling of a network G = (V, E ). Since there is very limited understanding of how noise should be modeled, we consider a simple Random Edge Perturbation model for noise, in which each pair u, v of vertices is selected for addition/deletion with probability n , where > 0 is a parameter, and n is the number of vertices; thus, on average, only n edges are altered. This model has been used quite extensively both in social network analysis and computer science for understanding the sensitivity to graph properties, e.g., (Costenbader and Valente 2003; Borgatti, Carley, and Krackhardt 2006; Flaxman and Frieze 2004; Flaxman 2007). Let R( ) denote the random set of edges selected by this process; we denote the perturbed graph by G  R( ). We study how the expected number of infections, given some initial conditions, is affected by the extent of perturbation, .







Our contributions. 1. The Independent cascades model. We consider networks G which exhibit a phase transition in their component sizes, with a critical probability pc , and transmission probability p (see Section 3 for definitions). In Theorem 1, we characterize the expected number of infections in the perturbed graph in terms of p and pc : when p < pc , we show that there exist constants c  c , and a threshold t such that for < c t , the expected number of infections in the perturbed graph remains close to that in G; however, for > c t , there is a phase transition, and the expected number of infections after perturbation is much larger than that in G. The main implication is that the dynamics are quite robust to perturbations, unless the transmission probability is close to pc . We find this to be consistent with extensive simulations on a large number of real networks--the sensitivity to perturbations is maximized at a point which approximately matches the threshold t in many networks. We also examine the transient behavior (i.e., the time series of the number of infections), and find it to be more sensitive than the expected total number of infections. 2. The Linear threshold model. In Theorem 2, we show formally that for any network G with maximum degree D = O(n/ log n), the expected number of infections after perturbation, starting at s random initial infections, is bounded by O(s(D + + log n) log n). This implies that the dynamics is quite stable for low s and . Our result is based on the analysis of the random graph model in which each node selects a random in-edge (which is shown to "correspond" to the LT model by (Kempe, Kleinberg, and Tardos 2003)). We first show that the diameter is bounded by O(D log n), where D is the maximum degree of the perturbed graph, and then prove that the expected number of infections, starting at a random source, is bounded by the diameter. Our theoretical bounds corroborate well with our experimental observations on a large class of real networks, which show a gradual variation with . We find that the expected number of infections grows more sharply with , as the number of sources is increased. Further, as in the IC model, we find the transient behavior is more sensitive to . Discussion and implications. From the point of view of dynamical system theory, our work may be regarded as a study of stability of dynamics over a network with respect to the edge structure. The existence of the critical value for the parameter in the IC model can be thought of as a bifurcation point. Admittedly, our results only hold for the specific random edge perturbation model of noise; uncertainty in networks is a much more complex process, and might involve dependencies arising out of the network evolution. Although we focus on specific dynamical properties and the random edge perturbation model, our results give the first rigorous theoretical and empirical analysis of the noise susceptibility of these diffusion models. Further, our analytical and empirical techniques, based on the random graph characterization, are likely to help in the analysis of other, more complex, noise models, which take dependencies into account. Organization. Because of space limitations, we omit the details of some of our proofs and experimental results; these will be available in the complete version of the paper.







2







Related work







Noise and issues of sampling are well recognized as fundamental challenges in complex networks, and there has been some work on characterizing it and the sensitivity to different parameters, especially in network properties, such as: (i) (Costenbader and Valente 2003; Borgatti, Carley, and Krackhardt 2006) show that certain centrality measures are robust to random edge and node perturbations, and (ii) (Achlioptas et al. 2009) show that there is an inherent bias in traceroute based inference of the Internet router network, which might give incorrect degree distributions. Flaxman and Frieze (Flaxman and Frieze 2004; Flaxman 2007) formally characterize conditions under which the graph expansion and diameter is highly sensitive to random edge additions; these are among the few analytical results of this type. Some of the approaches to address noise include: (i) the prediction of missing links using clustering properties, e.g., (Clauset, Moore, and Newman 2008), and (ii) approaches such as "property testing" algorithms, e.g., (Ron 2010) and "smoothed analysis", e.g., (Spielman 2009) for efficient computation of graph properties. To our knowledge, most work on the sensitivity of graph dynamical systems to noise in the network is empirical. However, for regular networks such as rings, topics such as synchronization and bifurcations have been studied (Kaneko 1985; Wu 2005). As discussed earlier, (Eubank 2010; Chen 2010) study the effect of changes in the network by edge rewirings on the epidemic properties. (Lahiri et al. 2008) study the effect of stochastic changes in the network on influence maximization problems. They find, using simulations, that in the LT model, the spread size is quite robust; our techniques help explain some of these observations.







3







Preliminaries







Noise models There is no consensus on the best way to model uncertainty/noise, and we consider a simple model of random edge additions that has been studied quite extensively in social network analysis (Costenbader and Valente 2003; Borgatti, Carley, and Krackhardt 2006); some problems have also been studied analytically in this model (Flaxman and Frieze 2004; Flaxman 2007). Let G = (V, E ) be the unperturbed graph. Let R( ) = (V, E ( )) be a random graph on V in which each pair u, v  V is connected with probability n . The perturbation graph G = G  R( ) is a graph constructed in the following manner: each pair u, v  V is connected in G if (u, v )  R( ) - E or (u, v )  E - R( ). In other words, each pair u, v is selected for addition/deletion with probability n . We also consider perturbations involving just addition of edges: this is denoted by G + R( ), and consists of all edges (u, v )  E  R( ). Network diffusion models. Let G = (V, E ) denote an undirected network. Here we define the diffusion models we study. In each model, each vertex v  V can be in state xv  {0, 1}, with state 0 denoting "inactive/uninfected/uninfluenced" and state 1 denoting "active/infected/influenced", depending on the application. We restrict ourselves to monotone or progressive processes, i.e., an infected node stays infected. Each node is associated with an activation function whose inputs







include the states of its neighbors. This function computes the next state of the node. The diffusion process starts with a few vertices becoming active/infected; we refer to this set as the initial set or the seed set. For an initial set of active nodes S , let  (S ) denote the expected number of active nodes at termination; these models always reach fixed points. We consider the following models: 1. Independent Cascade (IC) Model (Kempe, Kleinberg, and Tardos 2003). This model is a special case of the SIR model for epidemics. An infected node v infects each neighbor w with probability p (referred to as the transmission probability). Equivalently, each edge (v, w) can be live with probability p, independently of all other edges. All those nodes which are connected to the initial set through a live path are considered infected. In the perturbed graph G = G + R( ), suppose (v, x) is a newly added edge, then, v tries to infect x with probability p and vice versa. 2. Linear Threshold (LT) Model. (Kempe, Kleinberg, and Tardos 2005) Each node v is associated with a threshold v  [0, 1], chosen uniformly at random. v is influenced by its neighbor w according to weight bv,w such that wN (v ) bv,w  1. Node v becomes infected if wA(v ) bv,w  v , where A(v )  N (v ) is the set of neighbors of v which are currently infected. In our analysis and experiments, we assume that bv,w = 1/deg (v, G) for all w  N (v ), where deg (v, G) is the degree of v in G. This means, v is influenced equally by all its neighbors. This model was considered in (Kempe, Kleinberg, and Tardos 2003). In the perturbed graph G = G + R( ), bv,w = 1/deg (v, G ), where deg (v, G ) is the new degree of v.







the closer is N2 to n- this follows from the rough estimates of n2 /N N2 are given in Table 1. We also show in Lemma 3 in the Appendix that this holds in the case of Erdos-Renyi random graphs. This assumption will play a role in Theorem 1. Theorem 1. Consider the IC model on a family of graphs G exhibiting the following properties: (1) it undergoes a phase transition with critical probability pc , with the additional assumption that for p < pc , all components in G(p) are o( n), with high probability and there are two functions N = N (n, p) and N2 = N2 (n, p) such that in the graph resulting from percolation in G at probability p, (2) the number of components is within (N, (1 + )N ) and (3) the sum of the square of the component sizes is within (N2 , (1+ )N2 ), with probability 1 - o(1), for a constant  > 0. Let G = G + R( ) denote the perturbed graph. If p < pc , then, there is a threshN old perturbation factor t = pn , such that for (i) < c(p) t ,



n where c(p) = N N2 , the expected number of infections in G starting at a random initial node is o(n), and for (ii) 1/p > > 2(1 +  ) t , for any constant  > 0, the expected number of infections in G starting at a random initial node is (n) as n  .



2







4







Analyzing the sensitivity of the independent cascade model







Proof. First we note that N2 /n  n/N and therefore, c(p)  1. Let {Ci |i  N } be the set of connected components of G(p). Let ni denote the size of Ci . The probability that components Ci and Cj are conn n p nected by at least one edge is at most i nj in G (p). Consider an instance H of the Chung-Lu random graph model (Chung and Lu 2002) with N nodes with weights w1 , . . . , wN , such that wi = ni p. The probability of w wj (n p)*(n p) n n p edge (i, j ) in H equals i w = i nk jp = i nj  k



k k







We now discuss the sensitivity of the IC model for graphs that exhibit a phase transition, which is discussed informally here. Given a graph G with n vertices, let G(p) denote the random spanning subgraph of G obtained by retaining each edge of G independently with probability p. Many graphs (e.g., the complete graph, random regular graphs) exhibit the following property: there is a critical probability pc such that if p < pc , all components in G(p) are "small", namely of size o(n), while if p > pc , there exists a giant component of size (n). Similar threshold phenomena has been observed (empirically) in the real-world graphs which we study. Let N denote the number of components in G(p) when p < pc . We note that if the number of nodes in G of degree  d is at least cn for a constant d (a property satisfied by scale free networks), then N = (n). This follows from the fact that the expected number of isolated vertices in G(p) is  c(1 - p)d n = c n, under this assumption. Using Chebychev inequality, it can be shown that with high probability, the number of isolated vertices is very close to the expected value. Hence, N  c n with high probability. Next we consider the function N2 : the sum of the square of component sizes in G(p). Clearly, n  N2  n2 . However, in all the networks which we analyzed, for p < pc , N2 happens to be of the order of n and the farther p is from pc ,







Pr[Ci and Cj are connected in G (p)], since k=1 nk = n. Thus, the connectivity in the Chung-Lu instance H dominates that in G . The second order average degree w avg



p i p) for H is w avg = i wi = i (nn = N2 p n . From the i i connectivity threshold in the Chung-Lu model, it follows that H has no giant component if w avg < 1, which gives n < pN = t c(p). 2 Next, suppose 1/p > > t . By inclusion-exclusion, it follows that the probability that components Ci and Cj are connected in G (p) by at least one edge is at least ni nj p (n nj p)2 n nj p - i2n  i2n , because of the assumption n 2 that ni = o( n) and p < 1. Next, consider another instance H of the Chung-Lu model with N nodes with weights w1 , . . . , wN , such that wi = ni p/2. The probability of edge w wj (n p/2)*(nj p/2) n nj p (i, j ) in H equals i w = i = i2n  k k k nk p/2 Pr[Ci and Cj are connected in G (p)]. Thus, the connectivity in the Chung-Lu instance H is dominated by that in i G . The average degree wavg for H is wavg = i w N = ni p n p i 2N = 2N . From the connectivity threshold in the Chung-Lu model, it follows that H has a giant component if wavg > 1 +  , for any constant  > 0, which gives  )N > 2(1+ = 2(1 +  ) t . Therefore, with high probapn bility G has a component with (n) vertices. Since there w2



2







N







1 2 4 6 3 5 7 2 4 6







1 3 5 7







incoming edge from some vertex u  Ti-1 . The set T0 is a singleton consisting of the root vertex r. The incoming edge for r is from some neighbor in k i=1 Ti . All of this is illustrated in Figure 1. First, we show the following: Lemma 1. In the LT model, let  = minvV,wN (v) bv,w . Each tree in the random subgraph HLT has depth 1 O 1  log n , with probability at least 1 - n3 .







Figure 1: A graph and an instance of the random graph HLT corresponding to the LT model. For the component T induced by {1, 2, 3, 4, 5}, 1 is chosen as the root and as a result, T0 = {1}, T1 = {3}, T2 = {2, 5} and T3 = {4}.







is a constant probability that the seed belongs to the giant component, it follows that the expected number of infections in this case is (n). Remark 1. We note that if < t c(p), for any seed set of size s (not necessarily random), the expected number of infections in G is o(sn).







Proof. Consider a tree T in HLT , which is partitioned into sets T0 , . . . , Tk , as mentioned above. For any i = 1, . . . , k - 1, a vertex v  Ti would become a root if it chooses an incoming edge from one of its descendants. The probability of this event is at least minwN (v) bv,w   . Therefore, the probability that none of the vertices in Ti becomes a root is at most 1 -  , which in turn implies that the probability that none of the vertices in Ti , i = k - 1, . . . , 1 becomes a root is at most (1 -  )k-2 . Hence, the probability that T has depth more than k = c * 1  log n + 2 for a constant c is at most n 1 (1 -  ) k -2  n 1 4 . Since there are at most n kc*  log n+2 such trees in HLT , the probability that any of these has depth 1 more than O 1  log n is at most n3 .







5







Analyzing the sensitivity of the linear threshold model







We now analyze the impact of edge perturbations on the LT model on a graph G = (V, E ). Recall that in the specific version of the LT model we consider here, we set bv,w = 1/deg (v ) for each node v  V and neighbor w  N (v ). (Kempe, Kleinberg, and Tardos 2003) show that the fixed points and the number of infected nodes they have, can be studied through an elegant random graph model, which we describe here. Construct a random directed graph HLT = (V, E ) in the following manner: For each node v  V , a neighbor w is chosen with probability bv,w and a directed edge is added from w to v . Figure 1 illustrates a graph G and an instance of HLT . Note that even though G is undirected, HLT is a directed graph. For a set S  V , let  (S, HLT ) denote the number of nodes reachable from S in HLT (including those in S ). Then, (Kempe, Kleinberg, and Tardos 2003) show that  (S ), the expected number of infections with a starting set S , satisfies  (S ) = HLT Pr[HLT ] (S, HLT ). We use this characterization to analyze the impact of edge perturbations. The random graph HLT constructed by the above process has the following structure: In each connected component T of HLT , every vertex has one incoming edge and therefore, there exists exactly one directed cycle. If we choose a vertex in the cycle as the root r and remove its incoming edge, then, T remains connected and corresponds to a tree rooted at r with all edges oriented away from r. In the rest of this section, we loosely refer to such a component as a "tree" with one cycle or sometimes just tree. T can be partitioned into sets T0 , . . . , Tk such that for each i > 0, a vertex v  Ti has an







Consider a vertex v contained in a tree T . Let n(v, T ) denote the number of vertices reachable from v in T . Then, the number of infections resulting from v is the expected value of n(v, T ), averaged over all random subgraphs HLT and trees containing v . Define A(T ) as 1 A(T ) = |T v T n(v, T ). Conditioned on a random | subgraph HLT , the average number of infections start|T | ing at a random source is T HLT A(T ) n ; the average number of infections starting at a random source is |T | HLT Pr[HLT ] T HLT A(T ) n . Lemma 2. For each tree T in a random subgraph HLT , A(T )  2d, where d is the depth of T .







 to be the tree obtained by removing the Proof. Define T  incoming edge for the root in T . As described above, T   is an out-tree. For each v  T , we define n(v, T ) as the -- this corresponds number of vertices reachable from v in T . We define A(T ) = to the size of the subtree rooted at v in T 1    n(v, T ), and prove that A(T )  d. We prove this | v T |T by induction on the depth of the out-tree. The base case is a leaf node u, for which A(u) = 1. . Suppose it has children v1 , . . . , va . Let r be the root of T i be the subtree rooted at vi , and let ni be the number of Let T i . By induction, A(T i ) = 1  vertices in T i n(v, Ti )  v T ni







d - 1. ) A( T = 1 | |T ) n(v, T



 v T a







=







1 ) + n(r, T | |T



a







i=1







1 | |T







i ) n(v, T



i v T a







= 







1+



i=1







ni i )  1 + A(T | |T







i=1







ni (d - 1) | |T







| - 1 |T 1+ (d - 1)  d | |T







) = |T |, and by The third equality follows because n(r, T  i ) = 1 n ( v, T ) definition, A(T . The first inequali i v T ni ity follows by the induction hypothesis, since the depth of i  d - 1. The second inequality follows because each T a  i=1 ni = |T | - 1. Next, we consider A(T ). We recall that T is a tree with a cycle of length at most d. Let the cycle consist of vertices u0 = r, u1 , . . . , ub , with b  d - 1. For each ui , n(ui , T ) = |T |, since there is a path from ui to r. For every other vertex ). This implies, A(T )  u = ui in T , n(u, T ) = n(u, T d|T |  |T | + A(T )  2d. Finally, we bound the number of infections in the perturbed graph below; empirically, we find that the expected number of infections in the LT model is not very sensitive to , which is consistent with the bound below, which is linear in . Theorem 2. Let G(V, E ) be a graph with maximum degree D. For the LT model where bv,w = 1/deg (v ) for each node v  V and w  N (v ), the expected number of infected vertices starting with a initial random seed set of size s in the perturbed graph G + R( ) is O(s(D + + log n) log n). Proof. By a direct application of Chernoff bound, it can be 1 shown that with probability at least 1 - n 3 , the maximum degree in G = G + R( ) is at most D + + c * log n 1 for a constant c; with the remaining probability of n 3 , the maximum degree is O(n). We consider the random graph process to generate a subgraph HLT of G . Since bv,w = 1/deg (v ) for each node v  V and w  N (v ), for this model, the value of  of Lemma 1 is 1/D and therefore, each tree in HLT has depth at most O((D + + log n) log n), with 1 probability at least 1 - n 3 . Conditioned on HLT satisfying this bound on the depth, A(T ) = O((D + + log n) log n) for all T  HLT . For HLT that does not satisfy the depth bound, we have A(T ) = O(n) for all T  HLT . Therefore, the expected number of infections for a single random seed is n O((D + +log n) log n)+O( n 3 ) = O ((D + +log n) log n). Hence proved.







6







Experimental results







We study the sensitivity to edge perturbations empirically on twenty diverse real-world networks (from (Leskovec 2011)) with varying degrees of perturbation and other factors for both IC and LT models. Our main conclusions are the following:







1. Sensitivity in the IC model: we find that our empirical results match quite well with Theorem 1-- the expected number of infections IC model is well-behaved with , unless p is close to pc . Further, in most networks, the sensitivity is maximized at a point which approximately matches the threshold t . Though Theorem 1 strictly holds for graphs showing a phase transition, we find that most of the networks we study exhibit such a phenomenon. 2. LT model: we find that the expected number of infections in the LT model is not very sensitive to , especially for low number of seeds (e.g., less than 10), confirming the general bound derived from Theorem 2. When the number of seeds is large, the rate of increase of the expected number of infections seems to be higher initially. 3. Sensitivity of transients/temporal characteristics: our preliminary results suggest that the transient behavior (the time series of #infections versus time) is more sensitive than the expected #infections to , in both models. 4. Additions vs deletions: we find that perturbations involving both edge additions and deletions do not alter the results by much, compared to perturbations involving just edge additions. This follows from the sparsity of the graphs, and corroborates our analytical results, to some extent. Because of space limitations, we only discuss a sample of the results, and omit the results involving edge deletions; the remaining will be available in the complete version of the paper. We consider twenty different networks from (Leskovec 2011), with values of ranging from 0 to 100, with results averaged over 10 independent simulation runs. A simulation runs consists of 100 separate diffusion instances on one graph instance. A diffusion instance is a computation of the state of every node as a function of time, from time t=0 to the specified maximum time. The Independent Cascades Model. Figure 2 shows the the expected fraction of infected nodes vs. for two networks (namely, the astrophysics co-authorship and epinions networks)-- they both show low sensitivity for a broad range of values. For each of the settings, the expected number of infections rises sharply; further, the networks show differences in the plots for different parameter values. Some of the results for other networks are summarized in Table 1, which shows two sets of results for each network. Both are estimates of t , the threshold perturbation factor, using two methods. (i) In Method 1 we estimate N , the number of connected components in a random subgraph from the simulations, and use Theorem 1 to determine t = N/np. (ii) In Method 2, we consider the plot of infection size with respect to for a particular transmission probability p (as in Figure 2), and choose t to be the point of maximum slope of the curve on the X -axis. We note that both methods seem to give similar estimates of t . We empirically observe that the standard plot of infection size vs. transmission probability p for all the networks (without perturbations) exhibits some kind of phase transition; these results are omitted here because of space. Linear Threshold Model. Figure 3 shows the expected number of infections for two representative networks-- the slashdot and wiki networks. They both seem to follow the







Network







n







|E | 100000 12572 22002 31180 196972 91286 13422 117619 24806 420877 352285 180811 364481 405739 469180 504230 59898 100736 39994 65369







 pc 0.05 0.05 0.07 - 0.02 0.05 0.12 0.01 0.1 0.02 0.02 0.01 0.05 0.02 0.02 0.02 0.04 0.01 0.09 0.12







2 p < pc : n







N







t







p=0.001 990.0 998.0 997.9 997.1 989.0 996.0 997.0 989.0 997.0 988.0 988.0 995.0 999.0 995.0 994.0 993.8 997.0 985.0 996.2 997.4







by Method 1 0.01 0.05 90.1 98.1 97.8 97.1 89.0 95.7 96.8 90.3 97.2 87.7 87.5 95.1 98.6 94.8 93.9 93.8 97.3 86.0 96.3 97.5 10.0 18.1 17.9 17.3 11.6 15.8 16.9 13.8 17.1 10.1 10.6 16.1 18.7 16.5 15.6 15.5 17.5 12.3 16.3 17.5







t







0.1 1.5 8.2 8.078 7.715 4.1 6.1 7.2 5.5 7.2 3.1 3.5 6.8 8.8 7.3 6.6 6.5 7.8 5.0 6.389 7.529







p=0.001 > 100 > 100 > 100 > 100 > > > > > 100 100 100 100 100







by Method 2 0.01 0.05 90 90 90 90 80 90 90 90 90 70 80 90 90 90 90 90 90 80 90 90 10 20 20 20 3 10 10 8 10 4 0 10 20 10 10 9 10 0 10 10







0.1 8 8 8 8 0 4 3 0 5 0 0 0 9 7 6 6 8 0 2 4







Synthetic graphs Regular (d = 20) 10000 Autonomous Systems As20000102 6474 Oregon1010331 10670 Oregon2010331 10900 Co-authorship Astroph 17903 Condmat 21363 Grqc 4158 11204 Hepph Hepth 8638 Citation HepPh 34546 27770 HepTh Communication Email-Enron 33696 265214 Email-EuAll Social Epinion 75877 Slashdot0811 77360 82168 Slashdot0902 Twitter 22405 Wiki-Vote 7066 Internet peer-to-peer Gnutella04 10876 Gnutella24 26518







0.01: 1.1 0.01: 1.1 0.01: 1.2 - 0.01: 2.2 0.01: 1.1 0.05: 2.96 0.001: 1 0.05: 1.84 0.01: 1.7 0.01: 7.2 0.01: 3.9 0.01: 1.6 0.01: 6.1 0.01: 18 0.01: 28 0.01: 5 0.001: 1 0.05: 0.581 0.05: 0.75







> 100 > 100 > 100 > 100 > > > > > 100 100 100 100 100







> 100 > 100







Table 1: Estimates of t for the IC model in several real-world networks: Columns 2 and 3 contain the number of nodes n and edges |E | respectively. Column 4 contains approximate values of critical probability pc (See Figure 5(a) in Appendix) and Column 5 contains empirically assessed values of n2 /N N2 for a value p < pc . There are two sets of measurements of t corresponding to the two methods described in the experiments section. Each set is comprised of 4 values corresponding to different values of transmission probability p. In Method 2, the column corresponding to p = 0.001 has entries "> 100" because it was not possible to estimate the maximum conclusively, as we only considered  100 in our simulations.



0.6 0.45 0.4 0.35 0.3 0.25 0.3 0.2 0.2 0.1 0.05 0 0 10 20 30 40 50 60 70 80 90 100  0 0 10 20 30 40 50 60 70 80 90 100  0.15 0.1







1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0







0.5







(0.001,10) (0.001,100) (0.01,10) (0.01,100) (0.05,10) (0.05,100) (0.1,10) (0.1,100)







0.0001 0.001 0.01







0.0001 0.001 0.01







0.4







10 20 30 40 50 60 70 80 90 100 







(a) Ca-Astroph







(b) Epinion







Figure 3: Expected #infections vs in the LT model for different seed probabilities s = 0.0001, 0.001, 0.01 (seed nodes chosen uniformly at random). Plot (1) Slashdot0811 and (2) Wiki.







Figure 2: Expected #infections vs in the IC model for different pairs of transmission probability p and seed set size, with the seed nodes being chosen randomly.







general bounds of Theorem 2. We have also studied the LT model on all the 20 networks, as in the IC model; these are omitted here because of space limitations. Figure 4 shows the sensitivity in the transient behavior, i.e., the fraction of infections by time for the LT model-- as mentioned earlier, this shows a greater sensitivity to .







7







Conclusions and open problems







to other models of noise, especially those involving dependencies, sensitivity to the number of sources, and to examine the sensitivity of other dynamical properties in more general diffusion models (including the IC and LT models with heterogeneous probabilities/weights) are natural open problems for future research. Acknowledgments. This work has been partially supported by the following grants: DTRA Grant HDTRA1-11-10016, DTRA CNIMS Contract HDTRA1-11-D-0016-0010, NSF Career CNS 0845700, NSF ICES CCF-1216000, NSF NETSE Grant CNS-1011769 and DOE DE-SC0003957.







We give the first rigorous results on the stability of the independent cascades and linear threshold models, with respect to edge perturbations. These help explain our empirical observations on 20 diverse real networks. Extending our results







1e-04







1e-03







5e-05







5e-04







0e+00 0







10







20







30







40







50







0e+00 0







10







20







30







40







50







(a) s=0.0001







(b) s=0.001







Figure 4: LT model: Plots of infection size over time for Slashdot network for = 0, 1, 10, 100. Here s corresponds to the seed probability.







References



Achlioptas, D.; Clauset, A.; Kempe, D.; and Moore, C. 2009. On the bias of traceroute sampling. J. ACM 56(4):21:1-21:28. Alon, N.; Spencer, J. H.; and Erd os, P. 1992. The probabilistic method. John Wiley & Sons, Inc. Bakshy, E.; Hofman, J.; Mason, W.; and Watts, D. 2011. Everyones an influencer: Quantifying influence on twitter. In WSDM. Borgatti, S.; Carley, K.; and Krackhardt, D. 2006. On the robustness of centrality measures under conditions of imperfect data. Social Networks 28:124-136. Chen, J. 2010. The effects of demographic and spatial variability on epidemics: A comparison between beijing, delhi and los angeles. In Conf. on Crit. Inf. Chung, F., and Lu, L. 2002. Connected components in random graphs with given expected degree sequences. Annals of Combinatorics 6:125-145. Clauset, A.; Moore, C.; and Newman, M. 2008. Hierarchical structure and the prediction of missing links in networks. Nature 453:98-101. Costenbader, E., and Valente, T. 2003. The stability of centrality measures when networks are sampled. Social Networks 25:283-307. Eubank, S. 2010. Detail in network models of epidemiology: are we there yet? Journal of Biological Dynamics 446-455. Faloutsos, M.; Faloutsos, P.; and Faloutsos, C. 1999. On power-law relationships of the internet topology. In SIGCOMM, volume 29, 251-262. Flaxman, A., and Frieze, A. M. 2004. The diameter of randomly perturbed digraphs and some applications. In APPROX-RANDOM, 345-356. Flaxman, A. 2007. Expansion and lack thereof in randomly perturbed graphs. Internet Mathematics 4(2-3):131-147. Galuba, W. 2010. Outtweeting the twitterers - predicting information cascades in microblogs. In WOSN.







Goldenberg, J.; Libai, B.; and Muller, E. 2001. Talk of the network: A complex systems look at the underlying process of word-of-mouth. Marketing Letters. Gonzlez-Bailn, S.; Borge-Holthoefer, J.; Rivero, A.; and Moreno, Y. 2011. The dynamics of protest recruitment through an online network. Scientific Reports 1. Hagmann, P. 2008. Mapping the structural core of human cerebral cortex. PLoS Biol. Kaneko, K. 1985. Spatiotemporal intermittency in coupled map lattices. Progress of Theoretical Physics 74(5):1033- 1044.  2003. MaxiKempe, D.; Kleinberg, J. M.; and Tardos, E. mizing the spread of influence through a social network. In KDD, 137-146. ACM.  2005. Influential Kempe, D.; Kleinberg, J. M.; and Tardos, E. nodes in a diffusion model for social networks. In ICALP. Lahiri, M.; Maiya, A. S.; Caceres, R. S.; Habiba; and BergerWolf, T. Y. 2008. The impact of structural changes on predictions of diffusion in networks. In ICDM, 939-948. Leskovec, J. 2011. Stanford network analysis project. Newman, M. 2003. The structure and function of complex networks. SIAM Review 45(2):167-256. Romero, D.; Meeder, B.; and Kleinberg, J. 2011. Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter. In Proc. of WWW, 695-704. ACM. Ron, D. 2010. Algorithmic and analysis techniques in property testing. Foundations and Trends in TCS 5(2):73205. Schwab, D. J.; Bruinsma, R. F.; Feldman, J. L.; and Levine, A. J. 2010. Rhythmogenic neuronal networks, emergent leaders, and k -cores. Phys. Rev. E 82:051911. Spielman, D. 2009. Smoothed analysis: An attempt to explain the behavior of algorithms in practice. Communications of the ACM 76-84. Wu, C. W. 2005. Synchronization in networks of nonlinear dynamical systems coupled via a directed graph. Nonlinearity 18:1057-1064.







A







Appendix







Lemma 3. For G  G(n, p), if p  /n for a constant  < 1, N2 = (n) with high probability. Proof. For a vertex v , let Tv denote the number of vertices reachable from any vertex v in G(p) (including v ). It is well-known that (see (Alon, Spencer, and Erd os 1992)), for  < 1, P (Tv  t)  P (B [n, t/n]  t)  e-t , where  = () is some constant and B [a, b] is the standard Binomial random variable with a trials and probabil2 ity of success b. Using linearity of expectation, E [Tv ] = 2 2 t P ( T = t )  t P ( T  t ) . Since  is v v t>0 t>0 a constant, there exists a constant  such that for t   , t - 2 log t   t, where  is another constant. Applying 2 this, E [Tv ]   3 + t e- t = (1). Using a similar 2 technique, we can show that var[Tv ] = (1).







Note that N2  v T 2 and by linearity of expectation, 2 E [N2 ]  nE [Tv ] = (n). Further, since Tv are i.i.d. ran2 2 dom variables, var v Tv = v var[Tv ] = (n). Applying Chebychev inequality it follows that with high probability N2 = (n).







Independent Cascades: (for a fixed seed set size) 1 0.9 0.8 Infection size (fraction of nodes) 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.047 0.094 0.141 0.188 0.235 p 0.282 as20000102 ca-astroph.giant ca-condmat.giant ca-grqc.giant ca-hepph.giant ca-hepth.giant cit-hepph.giant cit-HepTh email-Enron email-EuAll epin.giant oregon1-010331 p2p-Gnutella04 p2p-Gnutella24 regular-20 slashdot0811 slashdot0902 tweet wiki.giant 0.329 0.376 0.423 0.47 Infection size (fraction of nodes) 0.5 0.6







Linear threshold (uniform weights)







0.4







0.3







0.2







0.1







0 0 0.008 0.016 0.024 0.032 0.04 p 0.048







as20000102 ca-astroph.giant ca-condmat.giant ca-grqc.giant ca-hepph.giant ca-hepth.giant cit-hepph.giant cit-HepTh email-Enron email-EuAll epin.gian
